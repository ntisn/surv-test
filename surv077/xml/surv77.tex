\documentclass{surv-l}

\usepackage{amssymb}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{hyperref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem1}{}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{\sc{Definition}}[chapter]
\newtheorem*{definition1}{\sc{Definition}}
\newtheorem*{problem}{\sc{Problem}}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem*{proof1}{}
\newtheorem*{proof2}{\it Proof}

\theoremstyle{remark}
\newtheorem*{remarks}{}


\newcommand\circledinfty{\ooalign{$\bigcirc$\cr$\infty$\cr}\space}
\newenvironment{copyrightpage}{\clearpage\null\thispagestyle{empty}}{\thispagestyle{empty}\clearpage}

\numberwithin{equation}{section}
\numberwithin{section}{chapter}
\numberwithin{figure}{chapter}

\makeindex

\begin{document}


\begin{titlepage}
The Semicircle Law, Free Random Variables and Entropy
\end{titlepage}


\title{The Semicircle Law, Free Random Variables and Entropy}

\author{Fumio Hiai}

\author{D\'{e}nes Petz}

\maketitle

\begin{copyrightpage}
\begin{center}
\textbf{Editorial Board}

\begin{tabular}{lllll}
Georgia Benkart & & & & \quad Michael Loss\\
Peter Landweber & & & & \quad Tudor Ratiu, Chair\\
\end{tabular}
\end{center}

\begin{center}
2000 \textit{Mathematics Subject Classification}. Primary 46L54; Secondary 15A52, 60F10, 94A17, 46N50, 60J65, 81S25, 05A17.
\end{center}

\noindent \textsc{abstract}. This is an expository monograph on free
probability theory. The emphasis is put on entropy and random matrix
models. The highlight is the very far-reaching interrelation of free
probability and random matrix theories. Wigner's theorem and its
broad generalizations, such as asymptotic freeness of independent
matrices, are expounded in detail. The parallelism between the
normal and semicircle laws runs through the book. Many examples are
included to illustrate the results. The frequent random matrix
ensembles are characterized by maximization of their Boltzmann-Gibbs
entropy under certain constraints, and the asymptotic eigenvalue
distribution is treated in the almost everywhere sense and in the
form of large deviation. Voiculescu's multivariate free entropy is
presented with full proofs and extended to unitary operators. Some
ideas about applications to operator algebras are also given.

\hrulefill\\

\noindent \textbf{Library of Congress Cataloging-in-Publication
Data}

\noindent Hiai, Fumio, 1948--

\quad The semicircle law, free random variables, and entropy / Fumio Hiai, D\'{e}nes Petz.

\qquad p. cm. --- (Mathematical surveys and monographs, ISSN 0076-5376 ; v. 77)

\quad Includes bibliographical references and index.

\quad ISBN 0-8218-2081-8 (alk. paper) \quad ISBN 0-8218-4135-1 (softcover)

\quad 1. Free probability theory. 2. Random matrices. 3. Entropy. I. Petz, D\'{e}nes, 1953--

II. Mathematical surveys and monographs\, ; no. 77.

QA326 .H52 \quad 2000

512$'$.55--dc21 \hfill 99-088288

\hrulefill\\

\textbf{Copying and reprinting.} Individual readers of this publication, and nonprofit libraries acting for them, are permitted to make fair use of the material, such as to copy a chapter for use in teaching or research. Permission is granted to quote brief passages from this publication in reviews, provided the customary acknowledgment of the source is given.

Republication, systematic copying, or multiple reproduction of any material in this publication is permitted only under license from the American Mathematical Society. Requests for such permission should be addressed to the Acquisitions Department, American Mathematical Society, 201 Charles Street, Providence, Rhode Island 02904-2294, USA. Requests can also be made by e-mail to \texttt{reprint-permission@ams.org.}

\begin{center}
\textcopyright\ 2000 by the American Mathematical Society. All
rights reserved.\\
The American Mathematical Society retains all rights\\
except those granted to the United States Government.\\
Printed in the United States of America.

\circledinfty\ The paper used in this book is acid-free and falls
within the guidelines\\
established to ensure permanence and durability.\\
Visit the AMS home page at \texttt{http://www.ams.org/}

10 9 8 7 6 5 4 3 \qquad 11 10 09 08
\end{center}
\end{copyrightpage}

\frontmatter

\tableofcontents

\chapter*{Preface}

This book is based on the recent brilliant discoveries of Dan
Voiculescu, which started from free products of operator algebras,
but grew rapidly to include all sorts of other interesting topics.
Although we both were fascinated by Voiculescu's beautiful new world
from the very beginning, our attitude changed and our interest
became more intensive when we got an insight into its interrelations
with random matrices, entropy (or large deviations) and the
logarithmic energy of classical potential theory.

There are many ways to present these ideas. In this book the
emphasis is not put on operator algebras (Voiculescu's original
motivation), but on entropy and random matrix models. It is not our
aim to make a complete survey of all aspects of free probability
theory. Several important recent developments are completely missing
from this book. Our emphasis is on the role of random matrices.
However, we do our best to make the presentation accessible for
readers of different backgrounds.

The basis of this monograph was provided by lectures delivered by
the authors at E\"{o}tv\"{o}s Lor\'{a}nd University in Budapest, at
Hokkaido University in Sapporo, and at Ibaraki University in Mito.

The structure of the monograph is as follows.
Chapter~\ref{ch01:chap01} makes the connection between the concepts
of probability theory and linear operators in Hilbert spaces. A sort
of ideological foundation of noncommutative probability theory is
presented here in the form of many examples.
Chapter~\ref{ch02:chap02} treats the fundamental free relation.
Again several examples are included, and the algebraic and
combinatorial aspects of free single and multivariate random
variables are discussed. This chapter is a relatively concise,
elementary and self-contained introduction to free probability. The
analytic aspects come in the next chapter. The infinitely divisible
laws show an analogy with classical probability theory. This chapter
is not much required to follow the rest of the monograph.
Chapter~\ref{ch04:chap04} introduces the basic random matrix models
and the limit of their eigenvalue distribution. Voiculescu's concept
of asymptotic freeness originated from independent Gaussian random
matrices. Since its birth, asymptotic freeness has been a very
important bridge between free probability and random matrix theory.
The strong analogy between the free relation and statistical
independence is manifested in the asymptotic free relation of some
independent matrix models. Entropy appears on the stage in
Chapter~\ref{ch05:chap05}---first the Boltzmann-Gibbs entropy, which
is considered here as the rate function in some large deviation
theorems. The frequent random matrix ensembles are characterized by
maximization of the Boltzmann-Gibbs entropy under certain
constraints. Several large deviation results are given following the
pioneering work of Ben Arous and Guionnet on symmetric Gaussian
random matrices. The main ingredient of the rate functional is the
logarithmic energy, familiar from potential theory. For an $n$-tuple
of noncommutative random variables, the probabilistic--measure
theoretic model is lacking; hence Chapter~\ref{ch06:chap06} is
technically in the field of functional analysis. Properties of
Voiculescu's multivariate free entropy are discussed in the setting
of operator algebras, and we introduce an analogous concept for
$n$-tuples of unitaries.
Chapters~\ref{ch03:chap03}--\ref{ch06:chap06} comprise the main part
of the monograph. The last chapter is mostly on free group factors,
and gives ideas on applications to operator algebras.

Since rather different areas in mathematics are often combined, it was our intention to make the material nearly self-contained for the sake of convenience. This was a heavy task, and we had to cope with the combination of probabilistic, analytic, algebraic and combinatorial arguments. Each chapter concludes with some notes giving information on our sources and hints on further developments. Furthermore, we supply standard references for the reader who is not familiar with the general background of the chapter. The ``Overview'' is an attempt to show the place of the subject and to give orientation. It replaces an introduction, and the reader is invited to consult this part either before or after studying the much more technical chapters.

We thank many colleagues for helping us to finish this enterprise. Imre Csisz\'{a}r, Roland Speicher, and Masaki Izumi can be named specifically. We are also grateful to several institutions for supporting our collaboration: funds of the Hungarian Academy of Sciences (OTKA F023447 and AKP 96/2-6782), of the Canon Foundation, of the Grant-in-Aid for Scientific Research (C)09640152, and of the Erd\H{o}s Center are acknowledged.

\hfill Fumio Hiai and D\'{e}nes Petz

\mainmatter

\chapter*{Overview}

\section{The isomorphism problem of free group factors}
\label{ch00:sec0.1}

\noindent John von Neumann established the theory of so-called \textit{von Neumann algebras}\index{von Neumann algebra} in the 1930's. The comprehensive study of ``rings of operators'' (as von Neumann algebras were called at that time) was motivated by the spectral theorem of selfadjoint Hilbert space operators and by the needs of the mathematical foundation of quantum mechanics. A von Neumann algebra is an algebra of bounded linear operators acting on a Hilbert space which is closed with respect to the topology of pointwise convergence. \textit{Factors} are in a sense the building blocks of general von Neumann algebras; they are von Neumann algebras with trivial center. In a joint paper with F.J. Murray, a classification of the factors was given. Von Neumann was fond of the type $\mathrm{II}_{1}$ factors, which are continuous analogues of the finite-dimensional matrix algebras. The normalized trace functional on the algebra of $n\times n$ matrices is invariant under unitary conjugation, and it takes the values $k/n\ (k=0,1,\ldots,n)$ on projections. A type $\mathrm{II}_{1}$ factor admits an abstract trace functional $\tau$ which is invariant under unitary conjugation, and it can take any value in $[0,1]$ on projections. The $N$-fold tensor product of $2\times 2$ matrix algebras is nothing else than the matrix algebra of $2^{N}\times 2^{N}$ matrices on which the normalized trace of projections is in the set $\{k/2^{N}:k=0,1,2,\ldots,2^{N}\}$. In the limit as $N\rightarrow\infty$, the dyadic rationals fill the interval $[0,1]$ and we arrive at a type $\mathrm{II}_{1}$ factor. What we are constructing in this way is the infinite tensor product of $2\times 2$ matrices, and the construction yields the \textit{hyperfinite} type $\mathrm{II}_{1}$ factors. (``Hyperfinite'' means a good approximation by finite-dimensional subalgebras; in the above case approximation is by the finite tensor products with growing size.) This was the first example of a type $\mathrm{II}_{1}$ factor.\index{factor} Murray and von Neumann showed that any two hyperfinite type $\mathrm{II}_{1}$ factors are isomorphic, and they were looking for a non-hyperfinite factor.\index{factor!hyperfinite}\index{hyperfinite!factor}

Countable discrete groups give rise to von Neumann algebras; in fact one can associate to a discrete group $G$ a von Neumann algebra $\mathcal{L}(G)$ in a canonical way. On the Hilbert space $\ell_{2}(G)$ the group $G$ has a natural unitary representation $g\mapsto L_{g}$, the so-called left regular one, which is given by
\begin{equation*}
(L_{g}\xi)(h):=\xi(g^{-1}h) \qquad (\xi\in \ell^{2}(G),\ g,h\in G).
\end{equation*}
The group ring $R(G)$ is the linear hull of the set $\{L_{g}:g\in G\}$ of unitaries. The \textit{group von Neumann algebra}\index{group!von Neumann algebra}\index{group!amenable} $\mathcal{L}(G)$ associated to $G$ is by definition the closure of $R(G)$ in the topology of pointwise convergence. If the group under consideration is ICC (i.e. all its non-trivial conjugacy classes contain infinitely many elements), then the von Neumann algebra $\mathcal{L}(G)$ is a factor. When the closure of $R(G)$ is taken with respect to the norm topology of $B(\ell^{2}(G))$, we arrive at another important object, that is, the \textit{reduced group $C^{*}$-algebra}\index{reduced group $C^{*}$ algebra} $C_{r}^{*}(G)$. There exists a \textit{canonical trace} $\tau$ on $\mathcal{L}(G)$, which is given by the unit element $e$ of $G$. Let $\delta_{e}\in \ell_{2}(G)$ stand for the characteristic function of $\{e\}$ and define
\begin{equation*}
\tau(\,\cdot\,):=\langle\,\cdot\,\delta_{e},\delta_{e}\rangle.
\end{equation*}
Then it is easy to check that $\tau$ is a trace, i.e. it satisfies
\begin{equation*}
\tau(ab)=\tau(ba) \quad \mathrm{for \ all} \ \, a, b\in \mathcal{L}(G).
\end{equation*}

Von Neumann started from the free group with two generators and proved that the corresponding factor is not hyperfinite. Historically this led to the first example of two non-isomorphic type $\mathrm{II}_{1}$ factors. Much later it was discovered that the group factor of an ICC group\index{ICC group}\index{group!ICC} is hyperfinite if and only if the group itself is amenable, and free groups are the simplest non-amenable groups. (The concept of \textit{amenable groups}\index{amenable group} also goes back to von Neumann.) Actually, von Neumann showed that the free\index{free!group} product of groups\index{group!free} leads to a non-hyperfinite factor. It seems that Richard Kadison was the person who explicitly posed the question of whether free groups with different numbers of generators could produce the same factor. This question is still open, and it was the main motivation for Dan Voiculescu to study the free relation and to develop free probability theory.

Let $\mathbf{F}_{n}$ denote the \textit{free group with $n$ generators}. If $n\neq m$ then $\mathbf{F}_{n}$ and $\mathbf{F}_{m}$ are not isomorphic. This can be seen by considering the group homomorphisms from $\mathbf{F}_{n}$ to $\mathbb{Z}_{2}$, the two element group, which is actually a field. Consider the space $X_{n}$ of all homomorphisms from $\mathbf{F}_{n}$ to $\mathbb{Z}_{2}$. This is a bimodule over $\mathbb{Z}_{2}$ of dimension exactly $2^{n}$. Since a group isomorphism between $\mathbf{F}_{n}$ and $\mathbf{F}_{m}$ induces a module isomorphism between $X_{n}$ and $X_{m}$, the groups $\mathbf{F}_{n}$ and $\mathbf{F}_{m}$ for $n\neq m$ cannot be isomorphic. Although the free group $\mathbf{F}_{n}$ is contained in $\mathcal{L}(\mathbf{F}_{n})$ in the form of the group ring, $\mathcal{L}(\mathbf{F}_{n})$ is much larger than $R(\mathbf{F}_{n})$ due to the closure procedure in the definition of the group factor. Hence simple proofs, for example the proof that $\mathbf{F}_{2}$ is not isomorphic to $\mathbf{F}_{3}$, do not extend to the topological closures, to the group $C^{*}$-algebras, or to the group von Neumann algebras.

If one expects $\mathcal{L}(\mathbf{F}_{n})$ and $\mathcal{L}(\mathbf{F}_{m})$ to be non-isomorphic, a possible strategy to prove this is to read out $n$ intrinsically from $\mathcal{L}(\mathbf{F}_{n})$. Each von Neumann factor of type $\mathrm{II}_{1}$ has a unique canonical (faithful normal) tracial state $\tau$, so this is intrinsically at our disposal. A.N. Kolmogorov in 1958 and Ya.G. Sinai in 1959 introduced the so-called Kolmogorov-Sinai (or dynamical) entropy of a measure-preserving transformation on a probability space, which has been a very successful tool in the isomorphism problem of Bernoulli shifts. Bernoulli shifts are simple, but they are the most important probabilistic dynamical systems. It turned out that the Kolmogorov-Sinai entropy is a complete invariant for Bernoulli shifts; in 1970 Donald Ornstein proved that two Bernoulli shifts are conjugate if and only if they have the same dynamical entropy. It seems to the authors that Dan Voiculescu was deeply influenced by these ideas. Independently of the success of his approach toward the isomorphism problem, his analysis created a new world in which the free relation and the free entropy of noncommutative random variables play the leading roles. Random matrices give the simplest examples of noncommutative random variables; they can model the free relation and are present in the definition of free entropy.

\section{From the relation of free generators to free probability}
\label{ch00:sec0.2}

\noindent The free group factor $\mathcal{L}(\mathbf{F}_{n})$ is an important example in von Neumann algebra theory, since it is the first-found and simplest non-hyperfinite type $\mathrm{II}_{1}$ factor. The free group itself is an important object in harmonic analysis; in fact, there is a strong and intimate relation between harmonic analysis on the free group and the structure of the free group factor.

The fundamental relation of the generators of a free group is the algebraic ``free relation''; namely, there are no identities expressed in operations of the group which are satisfied by the generators. Eventually, this algebraic free relation is present in non-free groups as well. For instance, if $G$ is the free product of $G_{1}$ and $G_{2}$, then the elements of $G_{1}$ are in free relation to the elements of $G_{2}$. The study of harmonic analysis on free product groups is the obvious continuation of its study on free groups.

Next we outline an idea which starts from a random walk on free groups and leads to the key concept of free relation; on top of that, the role of the semicircle law becomes clear as well. Let $g_{1}, g_{2}, \ldots, g_{n}$ be generators of $\mathbf{F}_{n}$ and consider a random walk on this group which starts from the group unit and goes from the group element $g$ to $hg$ with probability $1/2n$ if $h\in\{g_{1}, g_{2}, \ldots, g_{n}, g_{1}^{-1}, g_{2}^{-1}, \ldots, g_{n}^{-1}\}$. Then the probability of return to the unit $e$ in $m$ steps is of the form
\begin{equation*}
P(n,m):=\frac{1}{(2n)^{m}}\langle(L_{g_{1}}+L_{g_{1}^{-1}}+L_{g_{2}}+L_{g_{2}^{-1}}+\ldots+L_{g_{n}}+L_{g_{n}^{-1}})^{m}\delta_{e},\delta_{e}\rangle \, .
\end{equation*}
(This vanishes if $m$ is not even.) In the probabilistic interpretation of quantum mechanics, it is standard to interpret the number $\langle A\xi, \xi\rangle$ as the expectation of the operator $A$ in the state vector $\xi$. Adopting this view, we have the expectation
\begin{equation*}
\left\langle\left(\sum_{i=1}^{n}A_{i}^{(n)}\right)^{m}\delta_{e},\delta_{e}\right\rangle
\end{equation*}
of selfadjoint operators, $A_{i}^{(n)}$ standing for $(L_{g_{i}}+L_{g_{i}^{-1}})/\sqrt{2}$. The operators $A_{i}^{(n)}$ might be called random variables in the sense that they have expectations, and to distinguish this concept from classical probability theory, we speak of \textit{noncommutative random variables}. The asymptotic behavior of the return probability $P(n, 2m)$ is given as follows:
\begin{equation*}
P(n, 2m)\approx\frac{1}{(2n)^{m}}\frac{1}{m+1} \left(\begin{array}{c}
2m\\
m\\
\end{array}\right) \quad \mathrm{as} \quad n\rightarrow\infty.
\end{equation*}
What we have is a sort of \textit{central limit theorem}\index{central limit theorem} for the array
\begin{equation*}
A_{1}^{(n)}, A_{2}^{(n)}, \ldots, A_{n}^{(n)}
\end{equation*}
of noncommutative random variables, because
\begin{equation*}
\frac{A_{1}^{(n)}+A_{2}^{(n)}+\ldots+A_{n}^{(n)}}{\sqrt{n}}
\end{equation*}
converges in moments (or in distribution) to the even classical probability density whose $2m\mathrm{th}$ moment is the so-called \textit{Catalan number}\index{Catalan number}
\begin{equation*}
\frac{1}{m+1}\left(\begin{array}{c}
2m\\
m\\
\end{array} \right) \qquad (m\in \mathbb{N}).
\end{equation*}
This limit density is the \textit{semicircle}\index{law!semicircle}\index{semicircle law}\index{free!relation} or \textit{Wigner law}\index{Wigner law}\index{law!Wigner} $\frac{1}{2\pi}\sqrt{4-x^{2}}$ with support on $[-2, 2]$.

The different generators $g_{i}$ are free in the algebraic sense, and this led Voiculescu to call the relation of the selfadjoint operators $A_{i}^{(n)}$ (for a fixed $n$) free as well. When one aims to formulate the concept in the spirit of probability, the plausible free relation of these noncommutative random variables must be formulated in terms of expectation. The noncommutative random variables $A_{1}, A_{2}, \ldots, A_{n}$ are called \textit{free} with respect to the expectation $\varphi$ if
\begin{equation*}
\varphi(P_{1}(A_{i(1)})P_{2}(A_{i(2)})\ldots P_{k}(A_{i(k)}))=0
\end{equation*}
whenever $P_{1}, P_{2}, \ldots, P_{n}$ are polynomials, $P_{j}(A_{i(j)})=0$ and $i(1)\neq i(2)\neq\ldots\neq i(k)$. In the motivating example we had $\varphi(\,\cdot\,)=\langle\, \cdot\,\delta_{e}, \delta_{e}\rangle$, and of course the above defined operators $A_{1}^{(n)}, A_{2}^{(n)}, \ldots, A_{n}^{(n)}$ are free in the sense of Voiculescu's definition. It is not immediately obvious from the definition that the free relation of the noncommutative random variables $A_{1}, A_{2}, \ldots, A_{n}$ is a particular rule to calculate the joint moments
\begin{equation*}
\varphi(A_{i(1)}^{m(1)}A_{i(2)}^{m(2)}\ldots A_{i(k)}^{m(k)})
\end{equation*}
(for positive integers $m(1), m(2), \ldots, m(k)$) from the moments $\varphi(A_{i}^{m})$ of the given variables.

It is always tempting to compare Voiculescu's free relation with the independence of classical random variables. The comparison cannot be formal, since the free relation and independence do not take place at the same time. What we have in mind is only the analogy of the free relation in noncommutative probability theory with independence in classical probability theory. The part of noncommutative probability in which the free relation plays a decisive role is called \textit{free probability theory}.

Free probability theory has its celebrated distributions. The semicircle law is one of those celebrities; it comes from the free central limit theorem and its moment sequence is the Catalan numbers. The free analogue of the Poisson limit theorem can be established, and it gives the free analogue of the Poisson distribution. Given measures $\mu$ and $\nu$, we can find noncommutative random variables $a$ and $b$ with these distributions. When $a$ and $b$ are in free relation, the distribution measure of $a+b$ (or $ab$) can be called the \textit{additive} (or \textit{multiplicative}) \textit{free convolution}\index{free!convolution, multiplicative}\index{multiplicative free convolution} of $\mu$ and $\nu$. The notations $\mu\boxplus \nu$ and $\mu\boxtimes\nu$ are used to denote the two kinds of free convolution. The class of semicircle laws is closed under additive free convolution.\index{free!convolution, additive}\index{additive free convolution} The distributions $\mu_{0} \boxplus w_{\sqrt{4t}}$ from  a convolution semigroup when $w_{\sqrt{4t}}$ stands for the semicircle law with variance $t$ (which corresponds to radius $\sqrt{4t}$). The analytic machinery to handle additive free convolution is based on the Cauchy transform of measures. If $\mu_{t}$ is a freely additive convolution semigroup and $G(z,t)$ is the Cauchy transform of $\mu_{t}$, then the complex Burger equation
\begin{equation*}
\frac{\partial G(z,t)}{\partial t}+R(G(z,t))=\frac{\partial G(z,t)}{\partial z}
\end{equation*}
is satisfied with initial condition $G(z, 0)=G_{\mu_{0}}(z)$, and $R$ is the so-called $R$-transform of $\mu_{1}$. If $\mu_{t}$ is the semigroup of semicircle laws, then $R(z)=z$ and we have the analogue of the heat equation.

There is an obvious way to associate a measure to a noncommuting random variable via the spectral theorem if it is a normal operator. If the operator is not normal, then a measure can still be constructed by using type $\mathrm{II}_{1}$ von Neumann algebra techniques. (What we have in mind is the Brown measure of an element of a type $\mathrm{II}_{1}$ von Neumann algebra.) However, if we deal with several noncommuting random variables which are really noncommuting operators, then there is no way to reduce the discussion to measures. At this level of generality free probability theory has to work with joint moments, power series on noncommuting indeterminates, and a new kind of combinatorial arguments. The picture becomes very different from classical probability theory. Computation of the joint moments of free noncommutative random variables is a rather combinatorial procedure.

\section{Random matrices}
\label{ch00:sec0.3}

\noindent The joint eigenvalue density for certain symmetric random matrices has been known for a long time. J. Wishart found it for the so-called (real) Wishart matrix back in 1928. In quantum physics the energy is represented by the Hamiltonian operator, and one is interested in the point spectrum of the Hamiltonian, which is the set of eigenvalues. The problem is difficult; we do not know the exact Hamiltonian, and even if we did, it would be too complicated to find the eigenvalues. An approach to this problem is based on the following statistical hypothesis: The statistical behavior of the energy levels is identical with the behavior of the eigenvalues of a large random matrix. Reasoning along this line, in 1955 E.P. Wigner obtained the semicircle law for the limiting eigenvalue density of random matrices with independent Bernoulli entries. Wigner's work initiated a huge and deep interest in random matrices from theoretical and nuclear physicists. From the point of view of physical applications the most interesting question is to treat the correlation functions of the eigenvalues and the so-called ``level spacing''.

Random matrices are noncommutative random variables with respect to the expectation
\begin{equation*}
\tau_{n}(H):=\frac{1}{n}\sum_{i=1}^{n}E(H_{ii})
\end{equation*}
for an $n\times n$ random matrix $H$, where $E$ stands for the expectation of a classical random variable. It is a form of the \textit{Wigner theorem}\index{Wigner theorem} that
\begin{equation*}
\tau_{n}(H(n)^{2m})\rightarrow\frac{1}{m+1} \left(\begin{array}{c}
2m\\
m\\
\end{array}\right) \quad \mathrm{as} \quad n\rightarrow\infty
\end{equation*}
if the $n\times n$ real symmetric random matrix $H(n)$ has independent identical Gaussian entries $N(0,1/n)$ so that $\tau_{n}(H(n)^{2})=1$. The semicirle law is the limiting eigenvalue density of $H(n)$'s as well as the limiting law of the free central limit theorem in the previous section. The reason why this is so was made clear by Voiculescu. Let $X_{1}(n), X_{2}(n), \ldots$ be independent random matrices with the same distribution as $H(n)$. It follows from the properties of Gaussians that the distribution of the random matrix
\begin{equation*}
\frac{X_{1}(n)+X_{2}(n)+\cdots+X_{n}(n)}{\sqrt{n}}
\end{equation*}
is the same as that of $H(n)$. Hence the convergence in moments to the semicircle law would be understandable if $X_{1}(n),X_{2}(n), \ldots, X_{n}(n)$ were in free relation. Their free relation with respect to the expectaion $\tau_{n}$ would include the condition
\begin{equation*}
\tau_{n}\left([X_{1}(n)^{k}-\tau_{n}(X_{1}(n)^{k})]\, [X_{2}(n)^{l}-\tau_{n}(X_{2}(n)^{l})]\right)=0 \, ,
\end{equation*}
which is equivalently written as
\begin{equation}
\tau_{n}(X_{1}(n)^{k}X_{2}(n)^{l})=\tau_{n}(X_{1}(n)^{k})\tau_{n}(X_{2}(n)^{l})\, .
\tag{1}
\label{ch00:eqn1}
\end{equation}
For notational simplicity we write $A$ and $B$ for $X_{1}(n)$ and $X_{2}(n)$, respectively. Then what we have on the left hand side is
\begin{align*}
&\frac{1}{n^{2}}\sum E \negthinspace \left(A_{i(1)i(2)}A_{i(2)i(3)}\cdots A_{i(k)i(k+1)}B_{i(k+1)i(k+2)}\cdots B_{i(k+l)i(1)}\right)\\
& \quad = \frac{1}{n^{2}}\sum E\left(A_{i(1)i(2)}A_{i(2)i(3)}\cdots A_{i(k)i(k+1)}\right)\negthinspace E \negthinspace \left(B_{i(k+1)i(k+2)}\cdots B_{i(k+l)i(1)}\right)
\end{align*}
with summation for all indices. The matrix elements are independent and have zero expectation. Hence a term in which a matrix element appears only once among the factors must vanish. On the right hand side of (\ref{ch00:eqn1}) we have
\begin{align*}
&\frac{1}{n}\sum E \negthinspace \left(A_{i(1)i(2)}A_{i(2)i(3)}\cdots A_{i(k)i(1)}\right)\\
& \qquad \times\frac{1}{n}\sum E \negthinspace \left(B_{i(k+1)i(k+2)}\cdots B_{i(k+l)i(k+1)}\right).
\end{align*}
The two expressions are equal in many cases, in particular when $k$ or $l$ is odd or when both are $0$. However the summation is over slightly different sequences of indices. The difference goes to $0$ as $n\rightarrow\infty$. In this way, instead of the equality in (\ref{ch00:eqn1}) for a finite $n$, we have identical limits as $n\rightarrow\infty$. The free relation appears only in the limit; this is called the \textit{asymptotic freeness}\index{free!asymptotically}\index{asymptotically free} of $X_{1}(n)$ and $X_{2}(n)$. More generally, the asymptotic freeness of the sequence $X_{1}(n), X_{2}(n), \ldots$ is formulated. The free relation is present in the random matrix context asymptotically, and this fact explains why the semicircle law is the limiting eigenvalue distribution of the random matrix $H(n)$.

Independent symmetric Gaussian matrices are asymptotically free, but there are several other interesting examples too. For instance, independent Haar distributed unitary matrices are asymptotically free (as the matrix size tends to infinity). The asymptotic freeness may serve as a bridge connecting random matrix theory with free probability theory.

In a very abstract sense, the distribution of a not neccessarily selfadjoint noncommutative random variable $A$ is the collection of all joint moments
\begin{equation*}
\varphi(A^{m(1)}A^{*m(2)}A^{m(3)}A^{*m(4)}\cdots)
\end{equation*}
of $A$ and its adjoint $A^{*}$. A random matrix model of $A$ is a sequence of random matrices $X(n)$ such that $X(n)$ is $n\times n$ and the joint moments of these matrices reproduce those of $A$ as $n\rightarrow\infty$, that is,
\begin{equation*}
\tau_{n}(X(n)^{m(1)}X(n)^{*m(2)}X(n)^{m(3)}\cdots)\rightarrow\varphi(A^{m(1)}A^{*m(2)}A^{m(3)}\cdots)
\end{equation*}
for all finite sequences $m(1), \ldots, m(k)$ of nonnegative integers. It is really amazing that many important distributions appearing in free probability theory admit a suitable random matrix model. The selfadjoint Gaussian matrices with independent entries were mentioned above in connection with the Wigner theorem. The free analogue of the Poisson distribution is related to the Wishart matrix, and the circular and elliptic laws come from non-selfadjoint Gaussian matrices. These facts provide room for the interaction between random matrices and free probability. On the one hand, approximation by random matrices gives a powerful method to study free probability theory, and on the other hand techniques of free probability can be used to determine the limiting eigenvalue distribution of some random matrices, for example. It seems that bi-unitarily invariant matrix ensembles form an important class; they give the random matrix model of $R$-diagonal distributions. One way to define an $R$-diagonal noncommutative random variable is to consider its polar decomposition $uh$. If $u$ is a Haar unitary and it is free from $h$, then $uh$ is called $R$-diagonal. A bi-unitarily invariant random matrix has a polar decomposition $UH$ in which $U$ is a Haar distributed\index{eigenvalue distribution!empirical} unitary matrix independent of $H$. As the matrix size grows, independence is converted into freeness according to some asymptotic freeness result.

The \textit{empirical eigenvalue distribution}\index{empirical!eigenvalue distribution} of an $n\times n$ random matrix $H$ is the random atomic measure
\begin{equation*}
R_{H}:=\frac{1}{n}(\delta(\lambda_{1})+\delta(\lambda_{2})+\cdots+\delta(\lambda_{n})) \, ,
\end{equation*}
where $\lambda_{1},\lambda_{2}, \ldots, \lambda_{n}$ are the eigenvalues of $H$. It is a stronger form of the Wigner theorem that $R_{H(n)}$ goes to the semicircle law almost everywhere when $H(n)$ is symmetric with independent identically distributed entries. The almost sure limit of the empirical eigenvalue distribution is known to be a non-random measure in many examples, and it is often called the \textit{density of states}.\index{density of states} Sometimes it cannot be given explicitly, but is determined by a functional equation for the Cauchy transform or by a variational formula. Results are available for non-selfadjoint random matrices as well.

The best worked out example of symmetric random matrices is the case of independent identically distributed Gaussian entries. If the entries are Gaussian $N(0, 1/n)$ and if $G$ is an open subset of the space of probability measures on $\mathbb{R}$ such that the semicircle law (the density of states) $w$ is not in the closure, then we have
\begin{equation*}
\mathbf{Prob} (R_{H(n)}\in G)\approx\exp(-n^{2}C(w, G))
\end{equation*}
with a positive constant $C(w, G)$ depending on the distance of $w$ from $G$. The bigger the distance of $w$ from $G$, the larger the constant $C(w,G)$ is. Large deviation theory expresses this constant as the infimum of a so-called \textit{rate function I} defined on the space of measures:
\begin{equation*}
C(w, G)=\inf\{I(\mu):\mu\in G\}.
\end{equation*}
In our example, $I$ is a stricly convex function which is stricly positive for $\mu\neq w$. Ingredients of $I(\mu)$ are the logarithmic energy and the second moment of $\mu$. What we are sketching now is the pioneering large deviation theorem of Ben Arous and Guionnet for symmetric Gaussian matrices. More details can be found in the next section.

\section{Entropy and large deviations}
\label{ch00:sec0.4}

\noindent Originally entropy was a quantity from physics. Entropy as a mathematical concept is deeply related to large deviations, although the two had independent lives for a long time. A typical large deviation result was discovered by I.N. Sanov in 1957; however, the general abstract framework of large deviations was given by S.R.S. Varadhan in 1966.

Let $\xi_{1},\xi_{2},\ldots$ be independent standard Gaussian random variables and let $G$ be an open set in the space $\mathcal{M}(\mathbb{R})$ of probability measures on $\mathbb{R}$ (with the weak topology). The \textit{Sanov theorem}\index{Sanov theorem} says that if the standard Gaussian measure $\nu$ is not in the closure of $G$, then
\begin{equation*}
\mathbf{Prob} \left(\frac{\delta(\xi_{1})+\delta(\xi_{2})+\cdots+\delta(\xi_{n})}{n}\in G\right)\approx\exp(-nC(\nu,G))
\end{equation*}
and
\begin{equation*}
C(\nu,G)=\inf\{I(\mu):\mu\in G\}\, .
\end{equation*}
In the above case, the rate function $I(\mu)$ is the \textit{relative entropy}\index{Boltzmann-Gibbs entropy}\index{relative entropy} (or the \textit{Kullback-Leibler divergence})\index{Kullback-Leibler divergence} $S(\mu, \nu)$ of $\mu$ with respect to $\nu$. So it is also written as
\begin{equation}
I(\mu)=-S(\mu)+\frac{1}{2}\int x^{2}\,d\mu(x)+\frac{1}{2}\log(2\pi) \tag{2}
\label{ch00:eqn2}
\end{equation}
with the \textit{Boltzmann-Gibbs entropy}
\begin{equation}
S(\mu):=-\int p(x)\log p(x)\,dx \, , \tag{3}
\label{ch00:eqn3}
\end{equation}
whenever $\mu$ has the density $p(x)$ and the logarithmic integral is meaningful. This rate function $I$ is a strictly convex function such that $I(\mu)>0$ if $\mu\neq\nu$.

The rate functions in some large deviation results are called entropy functionals. Eventually, this could be the definition of entropy. The logarithmic integral (\ref{ch00:eqn3}) of a probability distribution $p(x)$ has been used for a long time, but it was identified much later as a component of the rate function in the Sanov theorem.

The Boltzmann-Gibbs entropy $S(\mu)$ can be recovered from the asymptotics of probabilities. Let $\nu^{n}$ be the $n$-fold product of the standard Gaussian measure on $\mathbb{R}$. For each $x\in \mathbb{R}^{n}$ we have the discrete measure
\begin{equation*}
\kappa_{x}:=\frac{\delta(x_{1})+\delta(x_{2})+\ldots+\delta(x_{n})}{n},
\end{equation*}
which can be used to approximate the given measure $\mu$. The asymptotic volume of the approximating $\mathbb{R}^{n}$-vectors up to the first $r$ moments is given by
\begin{equation}
\frac{1}{n}\log\nu^{n} (\{x\in \mathbb{R}^{n}: |m_{k}(\kappa_{x})-m_{k}(\mu)|\leq\varepsilon, \, k\leq r\}). \tag{4}
\label{ch00:eqn4}
\end{equation}
A suitable limit of this as $n\rightarrow\infty, r\rightarrow\infty$ and $\varepsilon\rightarrow 0$ is exactly the above described rate function (\ref{ch00:eqn2}). Furthermore, from (\ref{ch00:eqn4}) the entropy $S(\mu)$ can be recovered. This crucial argument deduces the entropy from the asymptotics of probabilities. The extension of this argument works for multivariables, but first we consider the analogous situation in which the atomic measures $\kappa_{x}$ are replaced by symmetric matrices.

In the large deviation result of Ben Arous and Guionnet the explicit form of the rate function $I$ on $\mathcal{M}(\mathbb{R})$ is
\begin{equation}
I(\mu)=-\frac{1}{2}\iint\log|x-y|\,d\mu(x)\,d\mu(y)+\frac{1}{4}\int x^{2}\,d\mu(x)+ \mathrm{const}. \tag{5}
\label{ch00:eqn5}
\end{equation}
The above double integral is the (negative) logarithmic energy of $\mu$, which is very familiar from potential theory.

Here we give an outline of how the rate function in (\ref{ch00:eqn5}) arises in the large deviation theorem of Ben Arous and Guionnet. In an abstract setting, a large deviation is considered for a sequence $(P_{n})$ of probability distributions, usually on a Polish space $\mathcal{X}$ in the scale $(L_{n})$; in our example $\mathcal{X}=\mathcal{M}(\mathbb{R})$ and $P_{n}(G)= \mathbf{Prob} (R_{H(n)}\in G)$. A standard way of proving the large deviation in this setting is to show the following equality:
\begin{equation*}
I(x)=\sup\left[-\limsup_{n\rightarrow\infty} L_{n}\log P_{n}(G)\right]=\sup\left[-\liminf_{n\rightarrow\infty}L_{n}\log P_{n}(G)\right],
\end{equation*}
where the supremum is over neighborhoods $G$ of $x\in \mathcal{X}$. This equality gives the large deviation of $(P_{n})$ with the rate function $I$ if $(P_{n})$ satisfies an additional property of a stronger form of tightness. The scale in the Sanov large deviation is $L_{n}=n^{-1}$, but the scale in large deviations related to random matrices is $L_{n}=n^{-2}$, corresponding to the number of entries of an $n\times n$ matrix. The joint eigenvalue density of the relevant random matrix $H(n)$ is known to be
\begin{equation}
\frac{1}{Z_{n}}\exp\left(-\frac{n+1}{4}\sum_{i=1}^{n}x_{i}^{2}\right)\prod_{i<j}|x_{i}-x_{j}| \tag{6}
\label{ch00:eqn6}
\end{equation}
on $\mathbb{R}^{n}$ with the normalizing constant $Z_{n}$. This means that for a neighborhood $G$ of $\mu\in \mathcal{M}(\mathbb{R})$ one has
\begin{align*}
&\mathbf{Prob} (R_{H(n)}\in G)\\
& \qquad =\frac{1}{Z_{n}}\int\cdots\int_{\tilde{G}}\exp \negthinspace \left(\sum_{i<j}\log|x_{i}-x_{j}|-\frac{n+1}{4}\sum_{i=1}^{n}x_{i}^{2}\right)\negthinspace dx_{1}\cdots dx_{n}\, ,
\end{align*}
where $\tilde{G}\subset \mathbb{R}^{n}$ is defined by
\begin{equation*}
\tilde{G}:=\left\{x\in \mathbb{R}^{n}:\frac{1}{n}\sum_{i=1}^{n}\delta(x_{i})\in G\right\}.
\end{equation*}
Very roughly speaking, when $G$ goes to a point $\mu$, the approximation
\begin{align*}
&\sum_{i<j}\log|x_{i}-x_{j}|-\frac{n+1}{4}\sum_{i=1}^{n}x_{i}^{2}\\
& \qquad \approx n^{2}\left(\frac{1}{2}\iint\log|x-y|\,d\mu(x)\,d\mu(y)-\frac{1}{4}\int x^{2}\,d\mu(x)\right)
\end{align*}
holds for $x\in\tilde{G}$, and one gets
\begin{align*}
&-\frac{1}{n^{2}}\log\,\mathbf{Prob} (R_{H(n)}\in G)\\
& \qquad \approx-\frac{1}{2}\iint\log|x-y|\,d\mu(x)\,d\mu(y)+\frac{1}{4}\int x^{2}\,d\mu(x)+\frac{1}{n^{2}}\log Z_{n} \, .
\end{align*}
This gives rise to the rate function (\ref{ch00:eqn5}), and the constant term there comes from $\lim_{n\rightarrow\infty}n^{-2}\log Z_{n}$.

Besides the symmetric Gaussian matrix, we know the exact form of the joint eigenvalue density for several other random matrices, such as the selfadjoint or non-selfadjoint Gaussian matrix, the Wishart matrix, the Haar distributed unitary matrix, and so on. The joint densities are distributed on $\mathbb{R}^{n}, \mathbb{C}^{n}, (\mathbb{R}^{+})^{n}, \mathbb{T}^{n}$ depending on the type of matrices, but they have a common form which is a product of two kernels as in (\ref{ch00:eqn6}); one is the kernel of Vandermonde determinant type
\begin{equation*}
\prod_{i<j}|x_{i}-x_{j}|^{2\beta}
\end{equation*}
with some constant $\beta>0$, and the other is of the form
\begin{equation*}
\exp\left(-\sum_{i=1}^{n}Q_{n}(x_{i})\right)
\end{equation*}
with some function $Q_{n}$ depending on $n$. Applying the method outlined above to this form of joint density, we can show the large deviations for the empirical eigenvalue distribution of random matrices as above. Corresponding to the form of joint density, the rate function is a weighted logarithmic potential as in (\ref{ch00:eqn5}) and its main term is always the logarithmic energy.

What is the free probabilistic analogue of the Boltzmann-Gibbs entropy (\ref{ch00:eqn3}) of a probability distribution $\mu$ on $\mathbb{R}$? Voiculescu answered this question by looking at the asymptotic behavior of the Bolzmann-Gibbs entropy of random matrices: The free entropy of $\mu$ should be
\begin{equation}
\Sigma(\mu):=\iint\log|x-y|\,d\mu(x)\,d\mu(y), \tag{7}
\label{ch00:eqn7}
\end{equation}
that is, minus the logarithmic energy. Below we shall explain that the above double integral is the real free analogue of the Boltzmann-Gibbs entropy.

Let $\mathcal{M}$ be a von Neumann algebra with a faithful normal tracial state $\tau$; so $\tau$ gives the expectation of elements of $\mathcal{M}$. Moreover, $m_{k}(a) :=\tau(a^{k})$ is viewed as the $k$th moment of a noncommutative random variable $a\in\mathcal{M}$. In order to approximate a selfadjoint $a$ with $\Vert a\Vert\leq R$ in distribution, Voiculescu suggested using symmetric matrices $A\in M_{n}(\mathbb{R})$; approximation means that $|\mathrm{tr}_{n}(A^{k})-\tau(a^{k})|$ is small. Hence the analogue of (\ref{ch00:eqn4}) is
\begin{equation*}
\frac{1}{n^{2}}\log v_{n}(\{A\in M_{n}(\mathbb{R})^{sa}: \Vert A\Vert\leq R,\ |\mathrm{tr}_{n}(A^{k})-\tau(a^{k})|\leq\varepsilon,\ k\leq r\}) \, .
\end{equation*}
The scaling is changed into $n^{2}$ corresponding to the higher degree of freedom, and the measure $\nu_{n}$ must be a measure on the space of real symmetric matrices; again the standard Gaussian measure will do. The limit as $n\rightarrow\infty$ and then $r\rightarrow\infty, \varepsilon \rightarrow 0$ is
\begin{equation}
\frac{1}{2}\iint\log|x-y|\,d\mu(x)\,d\mu(y)-\frac{1}{4}\tau(a^{2})+ \mathrm{const.} \, , \tag{8}
\label{ch00:eqn8}
\end{equation}
where $\mu$ is the probability measure on $\mathbb{R}$ which has the same moments as $a{:} \int x^{k}\,d\mu(x)=\tau(a^{k})$. This limit is minus the rate function (\ref{ch00:eqn5}), and the first term gives the free entropy $\Sigma(\mu)$. Instead of $M_{n}(\mathbb{R})^{sa}$ one may use the space $M_{n}(\mathbb{C})^{sa}$ of selfadjont matrices together with the standard Gaussian measure on it.

Another analogy between the two entropies $S(\mu)$ and $\Sigma(\mu)$ is clarified by their maximization results. The entropy $S(\mu)$ can take any value in $[-\infty, +\infty]$. Instead of the value itself, rather important is the difference of $S(\mu)$ from the maximum under some constraint. For instance, under the constraint of the second moment $m_{2}(\mu)\leq 1$, the Boltzmann-Gibbs entropy has the upper bound $S(\mu)\leq\tfrac{1}{2}\log(2\pi e)$, and equality is attained here if and only if $\mu$ has the normal distribution $N(0,1)$. This fact is readily verified from the positivity of the relative entorpy $S(\mu, \nu)$ with $\nu=N(0, 1)$. On the other hand, under the constraint of the second moment $m_{2}(\mu)\leq 1$, the free entropy has the upper bound $\Sigma(\mu)\leq-1/4$, and equality is attained if and only if $\mu$ is the semicircle law of radius 2. This maximization of $\Sigma(\mu)$ resembles that of $S(\mu)$; their maximizers are the normal law and the semicircle law, and the latter is the free analogue of the former. The Gaussian and semicircle maximizations are linked by random matrices. The symmetric random matrix with maximal Boltzmann-Gibb entropy under the constraint $\tau_{n}(H^{2})\leq 1$ is the standard Gaussian matrix, which is a random matrix model of the semicircle law.

\section{Voiculescu's free entropy for multivariables}
\label{ch00:sec0.5}

\noindent The free entropy as well as the Boltzmann-Gibbs entropy can be extended to multivariables. The multivariate case is slightly more complicated, but conceptually it is exactly the same. First we consider the Boltzmann-Gibbs entropy of multi-random variables (i.e. random vectors). For a random vector $(X_{1}, X_{2}, \ldots, X_{N})$ one has the joint distribution $\mu$ on $\mathbb{R}^{N}$, and the logarithmic integral (\ref{ch00:eqn3}) is meaningful and functions well whenever $\mu$ has the density $p(x)$ on $\mathbb{R}^{N}$. For $x=(x_{1}, x_{2}, \ldots, x_{n})\in(\mathbb{R}^{N})^{n}$ let $\kappa_{x}$ be the atomic measure on $\mathbb{R}^{N}$ defined analogously to the above single variable case. Now $k=(k(1), k(2), \ldots, k(p))$ must be a multi-index of length $|k| :=p$. For a measure $\mu$ on $\mathbb{R}^{N}$ whose support is in $[-R, R]^{N}$, we define
\begin{equation*}
m_{k}(\mu):=\int x_{k(1)}x_{k(2)}\cdots x_{k(p)}\,d\mu(x)
\end{equation*}
and consider
\begin{equation}
\frac{1}{n}\log\nu^{n} (\{x\in([-R, R]^{N})^{n} : |m_{k}(\kappa_{x})-m_{k}(\mu)|\leq\varepsilon,\ |k|\leq r\})\, , \tag{9}
\label{ch00:eqn9}
\end{equation}
where $\nu^{n}$ is the $n$-fold product of Gaussian measures on $\mathbb{R}^{N}$. The usual limit as $n \rightarrow\infty$ and then $r\rightarrow\infty, \varepsilon \rightarrow 0$ is
\begin{equation*}
-S(\mu)+\frac{1}{2}\int(x_{1}^{2}+x_{2}^{2}+\cdots+x_{N}^{2})\,d\mu(x)+ \mathrm{const}.
\end{equation*}

Now let $(a_{1}, a_{2}, \ldots, a_{N})$ be an $N$-tuple of selfadjoint noncommutative random variables. Due to the noncommutativity we cannot have the joint distribution (as a measure); however the mixed joint moments of $(a_{1}, a_{2}, \ldots, a_{N})$ with respect to the tracial state $\tau$ are available and we can consider the analogue of (\ref{ch00:eqn9}). For a multi-index $k$ we set $m_{k}(a_{1}, a_{2}, \ldots, a_{N}):=\tau(a_{k(1)}a_{k(2)}\cdots a_{k(p)})$, and similarly $m_{k}(A_{1}, A_{2}, \ldots, A_{N}) :=\mathrm{tr}_{n}(A_{k(1)}A_{k(2)}\cdots A_{k(p)})$ for an $N$-tuple $(A_{1}, A_{2}, \ldots, A_{N})$ of $n\times n$ matrices. To deal with the quantity
\begin{align}
& \frac{1}{n^{2}}\log\nu_{n}(\{(A_{1}, A_{2}, \ldots, A_{N})\in(M_{n}(\mathbb{C})^{sa})^{N} : \Vert A_{i}\Vert\leq R, \tag{10} \label{ch00:eqn10}\\
& \qquad \qquad \quad |m_{k}(A_{1}, A_{2}, \ldots, A_{N})-m_{k}(a_{1}, a_{2}, \ldots, a_{N})|\leq\varepsilon,\ |k|\leq r\})\, , \notag
\end{align}
we again put a product measure $\nu_{n}$ on the set of selfadjoint matrices. Since it is not known whether the limit as $n\rightarrow\infty$ of (\ref{ch00:eqn10}) exists, the $\lim \sup$ as $n\rightarrow\infty$ may be considered. The limit as $r\rightarrow\infty,\, \varepsilon \rightarrow 0$ of $\lim\sup_{n\rightarrow\infty}$ of the quantity (\ref{ch00:eqn10}) is of the form
\begin{equation*}
\chi(a_{1}, a_{2}, \ldots, a_{N})+\frac{1}{2}\tau(a_{1}^{2}+a_{2}^{2}+\cdots+a_{N}^{2})+ \mathrm{const.},
\end{equation*}
independently of the choice of $R>\Vert a_{i}\Vert$. This defines Voiculescu's free entropy $\chi(a_{1}, a_{2}, \ldots, a_{N})$. The multivariate free entropy generalizes the above free entropy $\Sigma(\mu)$ (up to an additive constant); to be more precise, the equality
\begin{equation*}
\chi(a)=\Sigma(\mu)+\frac{1}{2}\log(2\pi)+\frac{3}{4}
\end{equation*}
is valid with the distribution $\mu$ of $a$. The term ``free'' has nothing to do with thermodynamics; it comes from the additivity property:
\begin{equation}
\chi(a_{1},a_{2},\ldots,a_{N})=\chi(a_{1})+\chi(a_{2})+\cdots+\chi(a_{N}) \tag{11}
\label{ch00:eqn11}
\end{equation}
when $a_{1}, a_{2}, \ldots, a_{N}$ are in free relation with respect to the expectation $\tau$. In this spirit the Boltzmann-Gibbs entropy must be called ``independent'' since it is additive if and only if $X_{1}, X_{2}, \ldots, X_{N}$ are independent; that is, the joint distribution $\mu$ is a product measure. When the noncommutative random variables are far away from freeness (in particular, when an algebraic relation holds for $a_{1}, a_{2}, \ldots, a_{N})$, their free entropy becomes $-\infty$. This is another reason for the terminology ``free entropy''. The additivity (\ref{ch00:eqn11}) is equivalent to the free relation of the $a_{i}$'s when $\chi(a_{i})>-\infty$, but the subadditivity
\begin{equation*}
\chi(a_{1},a_{2},\ldots,a_{N})\leq\chi(a_{1})+\chi(a_{2})+\cdots+\chi(a_{N})
\end{equation*}
always holds. The free entropy $\chi(a_{1}, a_{2}, \ldots, a_{N})$ is upper semicontinuous in the convergence in joint moments. Furthermore, certain kinds of change of variable formulas are available. Under the constraint for $a_{i}=a_{i}^{*}$ that $\sum_{i}\tau(a_{i}^{2})$ is fixed, $\chi(a_{1}, a_{2}, \ldots, a_{N})$ is maximal when (and only when) all $a_{i}$'s are free and semicircular. There are possibilities to extend $\chi(a_{1}, a_{2}, \ldots, a_{N})$ to the case of non-selfadjoint noncommutative random variables. One possibility is to allow non-selfadjoint matrices in the definition (\ref{ch00:eqn10}), and another is to split the non-selfadjoint operators into their real and imaginary parts. The two approaches give the same result, say $\hat{\chi}(a_{1}, a_{2}, \ldots, a_{N})$, where the $N$-tuple is arbitrary and not necessarily selfadjoint. The subadditivity is still true, and $\hat{\chi}(a_{1}, a_{2}, \ldots, a_{N})=-\infty$ when one of the $a_{i}$'s is normal, in paricular when all of them are unitaries.

For an $N$-tuple of unitaries $(u_{1}, u_{2}, \ldots, u_{N})$ the appropriate way leading to a good concept of entropy is to use unitary matrices in a definition similar to (\ref{ch00:eqn10}), and to measure the volume of the approximating unitary matrices by the Haar measure. In this way we arrive at $\chi_{u}(u_{1}, u_{2}, \ldots, u_{N})$. The free entropy of unitary variables has properties similar to the free entropy of selfadjoint ones; namely, the subadditivity and the upper semicontinuity hold, and additivity is equivalent to freeness. The three kinds of free entropies are connected under the polar decompositions $a_{i}=u_{i}h_{i}$ in the following way:
\begin{equation*}
\hat{\chi}(a_{1},a_{2},\ldots,a_{N})\leq\chi_{u}(u_{1},u_{2},\ldots,u_{N})+\chi(h_{1}^{2},h_{2}^{2},\ldots,h_{N}^{2})+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right),
\end{equation*}
and, furthermore, equality is valid under a freeness assumption.

\section{Operator algebras}
\label{ch00:sec0.6}

\noindent The study of (selfadjoint) operator algebras is divided into two major categories, $C^{*}$-algebras and von Neumann algebras (i.e. $W^{*}$-algebras). $C^{*}$-\textit{algebras}\index{$C^{*}$-algebra} are usually introduced in an axiomatic way: A $C^{*}$-algebra is an involutive Banach algebra satisfying the $C^{*}$-norm condition $\Vert a^{*}a\Vert=\Vert a\Vert^{2}$. But any $C^{*}$-algebra is represented as a norm-closed $^{*}$-algebra of bounded operators on a Hilbert space (Gelfand-Naimark representation theorem). Von Neumann algebras\index{hyperfinite!von Neumann algebra}\index{injective von Neumann algebra} are included in the class of $C^{*}$-algebras; however, the ideas and methods in the two categories are very different. A commutative $C^{*}$-algebra with unit is isomorphic to $C(\Omega)$, the $C^{*}$-algebra of continuous complex functions on a compact Hausdorff space $\Omega$ with sup-norm (another Gelfand-Naimark theorem). So a general $C^{*}$-algebra is sometimes viewed as a ``noncommutative topological space''. On the other hand, a von Neumann algebra is a noncommutative analogue of (probability) measure spaces. In fact, a commutative von Neumann algebra with a faithful normal state is isomorphic to the space $L^{\infty}(\Omega, \mu)$ over a standard Borel space $(\Omega, \mu)$.

According to von Neumann's reduction theory, a von Neumann algebra $\mathcal{M}$ on a separable Hilbert space is a sort of direct integral of factors:
\begin{equation*}
\mathcal{M}=\int_{\Gamma}^{\oplus}\mathcal{M}(\gamma)\,d\nu(\gamma).
\end{equation*}
Therefore factors are building blocks of general von Neumann algebras. Factors are classified into the types $\mathrm{I}_{n} \, (n =1,2,3, \ldots, \infty), \, \mathrm{II}_{1}, \, \mathrm{II}_{\infty}$, and $\mathrm{III}$. The $\mathrm{I}_{n}\, (n<\infty)$ factor is the matrix algebra $M_{n}(\mathbb{C})$ and the $\mathrm{I}_{\infty}$ factor is $B(\mathcal{H})$ with $\dim \mathcal{H}= \infty$. A $\mathrm{II}_{1}$ factor is sometimes said to have continuous dimensions because, as already mentioned in the first section, it has a normal tracial state whose values of projections in $\mathcal{M}$ are all reals in $[0,1]$. A type $\mathrm{II}_{\infty}$ factor is written as the tensor product of a $\mathrm{II}_{1}$ factor and the $\mathrm{I}_{\infty}$ factor, and it has a normal semifinite trace. The type I factors are trivial from the operator algebra point of view. Infinite tensor products of matrix algebras with normalized traces and the group von Neumann algebras of ICC discrete groups are typical examples of type $\mathrm{II}_{1}$ factors; the simplest construction is $\mathcal{R} :=\bigotimes_{n=1}^{\infty}(M_{2}(\mathbb{C}), \mathrm{tr}_{2})$, as described in the first section. All factors except type $\mathrm{I}$ or $\mathrm{II}$ are said to be of type $\mathrm{III}$, and they are further classified into the types $\mathrm{III}_{\lambda}\ (0\leq\lambda\leq 1)$; the latter subclasses were introduced by A. Connes. For $0<\lambda<1$ a typical example of a $\mathrm{III}_{\lambda}$ factor is the \textit{Powers factor}\index{factor!Powers}\index{Powers factor}
\begin{equation*}
\mathcal{R}_{\lambda} :=\bigotimes_{n=1}^{\infty}(M_{2}(\mathbb{C}), \omega_{\lambda}), \quad \mathrm{where}\quad \omega_{\lambda}(\,\cdot\,) :=\mathrm{tr}_{2}\left(\left[\begin{array}{cc}
\frac{1}{1+\lambda} & 0\\
0 & \frac{\lambda}{1+\lambda}\\ \end{array} \right] \cdot \right).
\end{equation*}
Furthermore, the tensor product $\mathcal{R}_{\lambda}\otimes \mathcal{R}_{\mu}$ of two Powers factors with $\log\lambda/\log\mu\not\in \mathbb{Q}$ becomes a $\mathrm{III}_{1}$ factor. The Tomita-Takesaki theory is fundamental in the structure analysis of type $\mathrm{III}$ factors.

A von Neumann algebra $\mathcal{M}$ (on a separable Hilbert space) is said to be \textit{approximately finite dimensional} (AFD), or sometimes \textit{hyperfinite}, if it is generated by an increasing sequence of finite-dimensional subalgebras. On the other hand, $\mathcal{M} \subset B(\mathcal{H})$ is said to be \textit{injective}\index{von Neumann algebra!injective}\index{Gelfand-Naimark theorem} if there exists a conditional expectation (i.e. norm one projection) from $B(\mathcal{H})$ onto $\mathcal{M}$. The epoch-making result of Connes in 1976 shows that the injectivity of $\mathcal{M}$ is equivalent to the AFD of $\mathcal{M}$, and there is a unique injective factor for each type $\mathrm{II}_{1}, \, \mathrm{II}_{\infty}, \, \mathrm{III}_{\lambda}\, (0<\lambda<1)$. The fact that the above $\mathcal{R}$ is a unique hyperfinite type $\mathrm{II}_{1}$ factor had been proved long ago by Murray and von Neumann, and the uniqueness of the injective $\mathrm{III}_{1}$ factor was later proved by U. Haagerup in 1987. Furthermore, all AFD factors of type $\mathrm{III}$ are completely classified in terms of the flow of weights introduced by Connes and Takesaki. In this way, $\mathcal{R}\otimes B(\mathcal{H})$, the Powers factor $\mathcal{R}_{\lambda}$ and the above $\mathcal{R}_{\lambda}\otimes \mathcal{R}_{\mu}$ are unique AFD factors of type $\mathrm{II}_{\infty}, \, \mathrm{III}_{\lambda}\ (0<\lambda<1)$ and $\mathrm{III}_{1}$, respectively. There are many AFD $\mathrm{III}_{0}$ factors; all of them are constructed as \textit{Krieger factors} $L^{\infty}(\Omega, \mu)\rtimes_{T}\, \mathbb{Z}$, where $T$ is an ergodic transformation on a standard Borel space $(\Omega, \mu)$. The so-called measure space-group construction is to make the crossed product von Neumann algebra $L^{\infty}(\Omega, \mu)\rtimes_{\alpha} G$, where $\alpha$ is an action of a group $G$ on $(\Omega, \mu)$. According to J. Feldman and C.C. Moore, there is a more general construction of von Neumann algebras from a measurable equivalence relation on $(\Omega, \mu)$ with countable orbits, and $L^{\infty}(\Omega, \mu)$ is a \textit{Cartan subalgebra}\index{Cartan subalgebra} of the constructed von Neumann algebra. Moreover, the existence of a Cartan subalgebra is sufficient for a von Neumann algebra to enjoy such a measure space construction. Any injective factor has a Cartan subalgebra as a consequence of the above classification result. The notion of amenability can be defined for a measurable relation, and a profound result of Connes, Feldman and Weiss in 1982 says that a measurable relation is amenable if and only if it is generated by a single measurable transformation. A consequence of this is that any two Cartan subalgebras of an injective factor $\mathcal{M}$ are conjugate by an automorphism of $\mathcal{M}$.

After the AFD factors\index{factor!Krieger}\index{$K$-theory} were classified as above, a huge class of non-AFD factors remained unclassified. Since any type $\mathrm{III}$ factor can be canonically decomposed into the crossed product $\mathcal{N}\rtimes_{\theta}\, \mathbb{R}$ with a $\mathrm{II}_{\infty}$ von Neumann algebra $\mathcal{N}$ according to the Takesaki duality, and since type $\mathrm{II}_{\infty}$ can be somehow reduced to type $\mathrm{II}_{1}$, we may say that the problem returns to the type $\mathrm{II}_{1}$ theory in some sense. A type $\mathrm{II}_{1}$ von Neumann algebra with a faithful normal tracial state is a noncommutative probability space most appropriate to free probability theory. A typical example of non-AFD factors is the free group factor $\mathcal{L}(\mathbf{F}_{n})$, and it is generated by a free family of noncommutative random variables. This is the reason why some techniques from free probability and free entropy are so useful in analyzing free group factors. There has been much progress in the theory of free group factors; for instance, the nonexistence of a Cartan subalgebra in $\mathcal{L}(\mathbf{F}_{n})$, proved by Voiculescu, is remarkable because it means the impossibility of the measure space construction as above for $\mathcal{L}(\mathbf{F}_{n})$.

The $K$ theory of $C^{*}$-algebras yields algebraic invariants to study isomorphism problems of $C^{*}$-algebras. Two abelian groups $K_{0}(\mathcal{A})$ and $K_{1}(\mathcal{A})$ are associated with each $C^{*}$-algebra $\mathcal{A}$. The construction of $K_{0}(\mathcal{A})$ follows the old idea of Murray and von Neumann for the classification of von Neumann factors. Since a $C^{*}$-algebra may not have enough projections, we pass to the algebra $\bigcup_{n=1}^{\infty}M_{n}(\mathcal{A})$. An equivalence relation is defined on the collection of all projections in $\bigcup_{n=1}^{\infty}M_{n}(\mathcal{A})$, whose equivalence classes form an abelian semigroup $K_{0}(\mathcal{A})^{+}$ with zero element. Then the Grothendieck group of $K_{0}(\mathcal{A})^{+}$ is the $K_{0}$-group $K_{0}(\mathcal{A})$. On the other hand, the $K_{1}$ group $K_{1}(\mathcal{A})$ is defined as the inductive limit of the quotient groups $\mathcal{U}(M_{n}(\mathcal{A}))/\mathcal{U}_{0}(M_{n}(\mathcal{A}))$, where $\mathcal{U}(M_{n}(\mathcal{A}))$ is the group of unitaries in $M_{n}(\mathcal{A})$ and $\mathcal{U}_{0}(M_{n}(\mathcal{A}))$ is the connected component of the identity.

The first major contribution toward classification of $C^{*}$-algebras was made by G.A. Elliott in 1976. He showed that the AF $C^{*}$-algebras are completely classified by the ordered group $(K_{0}(\mathcal{A}), K_{0}(\mathcal{A})^{+})$, the \textit{dimension group}.\index{free entropy!dimension} A $C^{*}$-algebra $\mathcal{A}$ is said to be \textit{nuclear}\index{$C^{*}$-algebra!nuclear} if the minimal $C^{*}$-norm is a unique $C^{*}$-norm on the algebraic tensor product of $\mathcal{A}$ with any $C^{*}$-algebra $\mathcal{B}$. The class of nuclear $C^{*}$-algebras is in some sense a $C^{*}$-counterpart of the class of injective von Neumann algebras. Many characterizations of nuclear $C^{*}$-algebras are known; for example, $\mathcal{A}$ is nuclear if and only if $\pi(\mathcal{A})''$ is injective for any representation of $\mathcal{A}$. The nuclear $C^{*}$-algebras are closed under basic operations such as inductive limits, quotients by closed ideals, tensor products and crossed products by actions of amenable groups. In particular, $\mathrm{AF}$ algebras are nuclear, and the amenability of a discrete group $G$ is equivalent to the nuclearity of the reduced $C^{*}$-algebra $C_{r}^{*}(G)$ (similarly to the hyperfiniteness of the group von Neumann algebra $\mathcal{L}(G)$).

Exact $C^{*}$-algebras form an important class. A $C^{*}$-algebra is said to be \textit{exact} if the sequence of the minimal $C^{*}$-tensor products
\begin{equation*}
0\rightarrow \mathcal{A}\otimes \mathcal{J}\rightarrow \mathcal{A}\otimes \mathcal{B}\rightarrow \mathcal{A}\otimes(\mathcal{B}/\mathcal{J})\rightarrow 0
\end{equation*}
is exact whenever $\mathcal{J}$ is a closed ideal of an arbitrary $C^{*}$-algebra $\mathcal{B}$. This class includes the nuclear $C^{*}$-algebras.\index{nuclear $C^{*}$-algebra} For example, the reduced $C^{*}$-algebra $C_{r}^{*}(\mathbf{F}_{n})$ of the free group $\mathbf{F}_{n}$ is not nuclear but exact. Indeed, it is open whether $C_{r}^{*}(G)$ is exact for every countable discrete group $G$. A $C^{*}$-subalgebra of an exact $C^{*}$-algebra is exact, and the exact $C^{*}$-algebras\index{$C^{*}$-algebra!exact}\index{exact $C^{*}$-algebra} are closed under the operations of inductive limits, minimal tensor poducts and quotients. It seems that the role of exact $C^{*}$-algebras has increased in recent development of $C^{*}$-algebra theory since the appearance of the work of Kirchberg.

The group $C^{*}$-algebra $C_{r}^{*}(\mathbf{F}_{n})$ is out of the scope of well-established algebraic invariants; nevertheless, $K$-theory can detect $n$ from the reduced $C^{*}$-algebra of $\mathbf{F}_{n}$. In 1982 Pimsner and Voiculescu computed the $K$-groups
\begin{equation*}
K_{0}(C_{r}^{*}(\mathbf{F}_{n}))=\mathbb{Z} \quad \ \mathrm{and} \quad K_{1}(C_{r}^{*}(\mathbf{F}_{n}))=\mathbb{Z}^{n} \, ,
\end{equation*}
and this computation proves that $C_{r}^{*}(\mathbf{F}_{n})$ is not isomorphic to $C_{r}^{*}(\mathbf{F}_{m})$ for $n\neq m$. The isomorphism question of whether $\mathcal{L}(\mathbf{F}_{n})\not\cong \mathcal{L}(\mathbf{F}_{m})$ if $n\neq m$ is still open, and a possible approach uses the free entropy dimension,\index{group!dimension} which is a candidate for a reasonable entropic invariant.

Let $a_{1}, \ldots, a_{N}\in\mathcal{M}^{sa}$ and assume that $S_{1}, \ldots, S_{N}\in \mathcal{M}^{sa}$ is a free family of semicircular elements which are in free relation to $\{a_{1}, \ldots, a_{N}\}$. Then the \textit{free entropy dimension} $\delta(a_{1}, \ldots, a_{N})$ is defined in terms of the multivariate free entropy by
\begin{equation*}
\delta(a_{1}, \ldots, a_{N}):=N+\limsup_{\varepsilon\rightarrow+0}\frac{\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N})}{|\log\varepsilon|}.
\end{equation*}
Note that the above $S_{1}, \ldots, S_{N}$ always exist when we enlarge $\mathcal{M}$ by taking a free product with another von Neumann algebra, and that the joint distribution of $a_{1}+\varepsilon S_{1}, \ldots, a_{N}+\varepsilon S_{N}$ is independent of the choice of $S_{1}, \ldots, S_{N}$, so $\delta(a_{1}, \ldots, a_{N})$ is well-defined.

Let $g_{1}, g_{2}, \ldots, g_{N}$ be generators of $\mathbf{F}_{N}$. Then $a_{i}=L_{g_{i}}+L_{g_{i}^{-1}}$ form a free family of semicircular noncommutative random variables, and $\delta(a_{1}, \ldots, a_{k})=k$ for $k\leq N$. Proving the lower semicontinuity of the free entropy dimension would be very exciting, for the following reason. If $a_{1}, a_{2}, \ldots, a_{N}$ are in free relation and $b_{i}\in\{a_{1}, a_{2}, \ldots, a_{N}\}''$, then
\begin{equation}
\delta(a_{1}, a_{2}, \ldots, a_{N})\leq\delta(a_{1}, a_{2}, \ldots, a_{N}, b_{1}, \ldots, b_{M}) \tag{12}
\label{ch00:eqn12}
\end{equation}
with equality when the $b_{i}$'s are polynomials of the $a_{j}$'s. This is a proven fact. If the lower semicontinuity held, we would have equality in (\ref{ch00:eqn12}) without any further hypothesis on the $b_{i}$'s. The isomorphism $\mathcal{L}(\mathbf{F}_{N})\cong \mathcal{L}(\mathbf{F}_{M})$ would imply that in this von Neumann algebra $\mathcal{M}$ there exist two systems $a_{1}, \ldots, a_{N}$ and $b_{1}, \ldots, b_{M}$ of generators, each of which consists of free semicircular variables. Hence, the above equality in (\ref{ch00:eqn12}) gives
\begin{equation*}
N=\delta(a_{1},\ldots,a_{N})=\delta(a_{1},\ldots,a_{N},b_{1},\ldots,b_{M})=\delta(b_{1},\ldots,b_{M})=M.
\end{equation*}
In this way, the lower semicontinuity of $\delta(a_{1}, \ldots, a_{N})$ would imply the solution of the isomorphism problem.

In the parametrization of the von Neumann algebras $\mathcal{L}(\mathbf{F}_{n})$ the integer $n$ is a discrete parameter when free group factors are considered. However, in the work of K. Dykema and F. R\u{a}dulescu a continuous interpolation $\mathcal{L}(\mathbf{F}_{r})$ appears, where $r$ is real and $r>1$. Those are the so-called \textit{interpolated free group factors}.\index{free!group factor, interpolated}\index{interpolated free group factor} It turned out that they are either isomorphic for all parameter values or non-isomorphic for any two different values of $r$. So one of the two extreme cases holds true. However, the stable isomorphism $\mathcal{L}(\mathbf{F}_{r})\otimes B(\mathcal{H})\cong \mathcal{L}(\mathbf{F}_{s})\otimes B(\mathcal{H})$ is known.

The existence of the interpolation of the free group factors may suggest that the $\mathcal{L}(\mathbf{F}_{n})$ are all isomorphic, contrary to the indication from the free entropy dimension.

\chapter{Probability Laws and Noncommutative Random Variables}
\label{ch01:chap01}

\noindent Random variables are functions defined on a measure space, and they are often identified by their distributions in probability theory. In the simplest case when the random variable is real valued, the distribution is a probability measure on the real line. In this chapter it is first demonstrated that probability distributions can be represented by means of linear Hilbert space operators as well. This observation is as old as quantum mechanics; the standard probabilistic interpretation of the quantum mechanical formalism is strongly related. Examples will be given to show how some basic probability distributions, such as the Poisson distribution and the normal, arcsine and semicircle laws, may arise in the context of a Hilbert space. There is a parallelism between the normal and semicircle laws which runs through this book. The similar combinatorial meaning of the moments is the first appearance of this parallelism. Another emergence is in the Fock space. The normal law is the distribution of field operators in the symmetric Fock space. When the symmetry condition is dropped and the full Fock space is considered, the analogous operators are semicircularly distributed. Creation operators on the full Fock space are of central importance in our later considerations.

In an algebraic generalization, elements of a typically noncommutative algebra together with a linear functional on the algebra are regarded as noncommutative random variables. The linear functional evaluated on an element is the expectation value, and application to powers of the selected element yields the moments of the noncommutative random variable. One does not distinguish between two random variables when they have the same moments. A very new feature of this theory appears when several noncommutative random variables are really noncommuting with each other. Then one cannot have a joint distribution in the sense of classical probability theory, but a functional of the algebra of polynomials of noncommuting indeterminates may work as an abstract concept of joint distribution. Random matrices with respect to the expectation of their trace are natural ``noncommuting'' noncommutative random variables.

This chapter is mostly a collection of examples. Many of them are strongly related to our main stream, but some of them are presented on the basis of curiosity; they merely indicate possible other directions, or a sort of interpolation between the normal and semicircle laws.

\section{Distribution measure of normal operators}\index{distribution!measure}\index{spectral theorem}
\label{ch01:sec1.1}

\noindent Let $N$ be a bounded normal operator acting on a Hilbert space $\mathcal{H}, N^{*}N=NN^{*}$. According to the \textit{spectral theorem} ([\citen{bib163}], Sec. 111), there exists a projection-valued measure $E$ on the complex plane $\mathbb{C}$ such that
\begin{equation}
N=\int_{\mathbb{C}}z\,dE(z),
\label{ch01:eqn1.1.1}
\end{equation}
and more generally, if $q$ is a polynomial of two (commuting) variables, then
\begin{equation*}
q(N, \, N^{*})=\int_{\mathbb{C}}q(z,\bar{z})\,dE(z) \, .
\end{equation*}
Here the integration with respect to an operator-valued measure may be understood as
\begin{equation*}
\langle q(N, \, N^{*})\xi, \eta\rangle=\int_{\mathbb{C}}q(z,\bar{z})\,d\langle E(z)\xi, \eta\rangle
\end{equation*}
for every $\xi, \eta\in \mathcal{H}$, and $\langle E(\,\cdot\,)\xi, \eta\rangle$ is a complex-valued measure depending on the vectors $\xi$ and $\eta$. It is a part of the spectral theorem that all these measures are concentrated on the spectrum of the operator $N$.

For a normal operator $N$ and a vector $\xi\in \mathcal{H}$, we define the \textit{distribution measure} of $N$ at $\xi$ as the Borel measure
\begin{equation}
H\mapsto\langle E(H)\xi, \xi\rangle \qquad (H\subset \mathbb{C}),
\label{ch01:eqn1.1.2}
\end{equation}
where $E$ is the projection-valued measure from the spectral decomposition (\ref{ch01:eqn1.1.1}) of $N$. Since $\mu(\mathbb{C})=\Vert\xi\Vert^{2}, \, \mu$ is a probability measure if and only if $\xi$ is a unit vector.

\begin{example}\label{ch01:exa1.1.1}
If $N$ is a normal $n\times n$ matrix with $n$ different eigenvalues and with (pairwise orthogonal) eigenvectors $\eta_{1}, \eta_{2}, \ldots, \eta_{n}$, then the distribution measure of $N$ at $\xi$ is the discrete measure which gives the weight $|\langle \eta_{i}, \xi\rangle|^{2}$ to the eigenvalue corresponding to $\eta_{i}$.
\end{example}

\begin{example}\label{ch01:exa1.1.2}
Let $G$ be a countable group, and consider the unitary operator $U$ on $l^{2}(G)$, defined as
\begin{equation*}
(Uf)(h) :=f(g^{-1}h) \qquad (f\in l^{2}(G), \, h\in G),
\end{equation*}
where $g$ is a fixed element of $G$. ($U$ is the left-translation by $g$.) If $g^{n}$ is never the group unit $e$ for $n\in \mathbb{N}$ and
\begin{equation*}
\delta(h):=\left\{\begin{array}{l}
1\quad \mathrm{if}\ h=e,\\
0\quad \mathrm{otherwise},
\end{array}\right.
\end{equation*}
then the distribution measure of $U$ at the vector $\delta\in l^{2}(G)$ is the normalized Lebesgue measure on the unit circle.

The spectrum of a unitary is in the unit circle $\mathbb{T}$. Any measure $\mu$ on $\mathbb{T}$ is determined by the integrals
\begin{equation*}
\int_{\mathbb{T}}z^{n}\,d\mu(z) \qquad (n\in \mathbb{Z}).
\end{equation*}
This follows from the fact that every continuous function on $\mathbb{T}$ can be approximated by linear combinations of positive and negative powers of $z$; they are the trigonometric polynomials. If $\mu$ is the distribution measure of $U$ at $\delta$, then the equality
\begin{equation*}
\int_{\mathbb{T}}z^{n}\,d\mu(z)=\langle U^{n}\delta, \delta\rangle \qquad (n\in \mathbb{Z})
\end{equation*}
must hold. Since $\langle U^{n}\delta, \delta\rangle=0$ for $n \neq 0$, the normalized Lebesgue measure of $\mathbb{T}$ satisfies this condition.
\end{example}

We shall speak of the distribution measure of an unbounded selfadjoint operator at a vector in the domain of the operator. This is defined by the spectral theorem and by (\ref{ch01:eqn1.1.2}) similarly to the bounded case. The distribution measure of a selfadjoint operator is a measure on the real line. The next example is essential in quantum mechanics.

\begin{example}\label{ch01:exa1.1.3}
Let $\mathcal{H}$ be the Hilbert space $L^{2}(\mathbb{R})$ of square integrable functions on $\mathbb{R}$. The \textit{position operator}\index{position operator}\index{operator!position} is the selfadjoint operator $Q$ with domain $\mathcal{D}(Q) := \{f\in L^{2}(\mathbb{R}):xf(x)\in L^{2}(\mathbb{R})\}$, and it is defined by $(Qf)(x) :=xf(x)$. A unit vector $g\in L^{2}(\mathbb{R})$ is usually called a wave function in quantum mechanics. The distribution measure of $Q$ at the vector $g$ gives the probability that a quantum particle of one degree of freedom is confined to a subset $H\subset \mathbb{R}$. Since the spectral projection of $Q$ corresponding to $H$ is the operator of multiplication by the characteristic function of $H$, we have that $\int_{H}|g(x)|^{2}\,dx$ is the probability that a particle of quantum state $g$ is in $H$.
\end{example}

Let $\mathcal{H}$ be a Hilbert space and denote by $\mathcal{H}^{\otimes n}$ the $n$-fold tensor product $\mathcal{H}\otimes \mathcal{H}\otimes\cdots\otimes \mathcal{H}$. A permutation $\pi$ of the set $\{1, 2, \ldots, n\}$ gives rise to a unitary $U_{\pi}$ on $\mathcal{H}^{\otimes n}$ which is determined by
\begin{equation*}
U_{\pi}:\eta_{1}\otimes\eta_{2}\otimes\cdots\otimes\eta_{n}\mapsto\eta_{\pi(1)}\otimes\eta_{\pi(2)}\otimes\cdots\otimes\eta_{\pi(n)}\, .
\end{equation*}
(In this way one gets an action of the symmetric group $\mathbf{S}_{n}$.) The subspace of all common fixed vectors of all $U_{\pi}$'s is called the $n$th symmetric tensor power of $\mathcal{H}$, and we denote it by $\mathcal{H}^{\odot n}$. The \textit{full Fock space} $\mathcal{F}(\mathcal{H})$ and the \textit{symmetric Fock space} $\mathcal{F}_{s}(\mathcal{H})$ over $\mathcal{H}$ are defined as the orthogonal sum of all tensor powers and, respectively, symmetric tensor powers of $\mathcal{H}$. Namely,
\begin{equation*}
\mathcal{F}(\mathcal{H}):=\bigoplus_{n=0}^{\infty}\mathcal{H}^{\otimes n},\qquad \mathcal{F}_{s}(\mathcal{H}):=\bigoplus_{n=0}^{\infty}\mathcal{H}^{\odot n}.
\end{equation*}
If $\mathcal{H}=\mathbb{C}$ then both kinds of Fock space\index{symmetric Fock space}\index{Fock space!symmetric} reduce to $l^{2}(\mathbb{Z}^{+})$. It is customary to set $\mathcal{H}^{\otimes 0}=\mathcal{H}^{\odot 0}=\mathbb{C}\Phi$ and to call the unit vector $\Phi$ the \textit{vacuum vector}.\index{vacuum vector} Another name motivated by physics is the \textit{particle number operator}.\index{operator!particle number}\index{particle number operator} This is a positive selfadjoint operator on $\mathcal{F}_{s}(\mathcal{H})$ such that $\mathcal{H}^{\odot n}$ is an eigensubspace corresponding to the eigenvalue $n(n\in \mathbb{Z}^{+})$.

For $\eta\in \mathcal{H}$ the vector
\begin{equation}
\psi(\eta):=\bigoplus_{n=0}^{\infty}\frac{1}{\sqrt{n!}}\eta^{(1)}\otimes\eta^{(2)}\otimes\cdots\otimes\eta^{(n)} \qquad (\eta^{(i)}=\eta) \label{ch01:eqn1.1.3}
\end{equation}
belongs to $\mathcal{F}_{s}(\mathcal{H})$ and is called an \textit{exponential vector}\index{exponential vector}\index{Fock space!full} or \textit{coherent vector}.\index{coherent vector}\index{full Fock space} It is easy to check that
\begin{equation*}
\langle\psi(\eta_{1}),\psi(\eta_{2})\rangle=\exp\langle \eta_{1},\eta_{2}\rangle \, ,
\end{equation*}
and it is useful to know that the linear hull of all exponential vectors is dense in $\mathcal{F}_{s}(\mathcal{H})$.

\begin{example}\label{ch01:exa1.1.4}
The distribution measure of the number operator at the normalized exponential vector
\begin{equation*}
\xi:=\exp(-\Vert\eta\Vert^{2}/2)\psi(\eta)
\end{equation*}
is the \textit{Poisson distribution}\index{distribution!Poisson}\index{Poisson!distribution} with parameter $\lambda=\Vert\eta\Vert^{2}$.

One has to compute $\langle P_{n}\xi, \xi\rangle$, where $P_{n}$ is the projection of $\mathcal{F}_{s}(\mathcal{H})$ onto $\mathcal{H}^{\odot n}$:
\begin{equation*}
\langle P_{n}\xi,\xi\rangle=\exp(-\Vert\eta\Vert^{2})\frac{\Vert\eta\Vert^{2n}}{n!},
\end{equation*}
and this is really a Poisson distribution.
\end{example}

For each $h\in \mathcal{H}$ a bounded operator $\ell(h)$ is defined on the full Fock space $\mathcal{F}(\mathcal{H})$ by the formula
\begin{equation}
\ell(h)\eta:=\left\{\begin{array}{ll}
h & \mathrm{if}\ \eta=\Phi,\\
h\otimes\eta & \mathrm{if}\ \langle\eta, \Phi\rangle=0.\\
\end{array}\right.
\label{ch01:eqn1.1.4}
\end{equation}
$\ell(h)$ is called the \textit{left creation operator},\index{left creation operator} or for short just the \textit{creation operator}.\index{creation operator}\index{operator!creation} For its adjoint $\ell(h)^{*}$, called the \textit{annihilation operator},\index{annihilation operator}\index{operator!annihilation} we have $\ell(h)^{*}\Phi=0$ and
\begin{equation*}
\ell(h)^{*}\eta_{1}\otimes\eta_{2}\otimes\cdots\otimes\eta_{k}=\langle\eta_{1},h\rangle \eta_{2}\otimes\cdots\otimes\eta_{k} \, .
\end{equation*}
Evidently $\ell(h_{1})^{*}\ell(h_{2})=\langle h_{2}, h_{1}\rangle \mathbf{1}$ ($\mathbf{1}$ denotes the identity operator). In particular, $\ell(h)$ is an isometry for every unit vector $h\in \mathcal{H}$.

The \textit{semicircle law}\index{law!semicircle}\index{semicircle law} is a probability measure on $\mathbb{R}$ whose density is
\begin{equation}
w_{m,r}(x):=\left\{\begin{array}{ll}
\dfrac{2}{\pi r^{2}}\sqrt{r^{2}-(x-m)^{2}} & \mathrm{if}\ m-r\leq x\leq m+r,\\
\\
0 & \mathrm{otherwise},
\end{array}\right.
\label{ch01:eqn1.1.5}
\end{equation}
where $m$ and $r>0$ are real numbers. Instead of $w_{0,r}$ we write simply $w_{r}$. The graph of the semicircle law is a semiellipse. The expectation value of the semicircle law $w_{m,r}$ is $m$ due to the symmetry, and the variance is $r^{2}/4$.

\begin{theorem}\label{ch01:the1.1.5}
The distribution measure of $\ell(h)^{*}+\ell(h)$ at the vacuum vector $\Phi\in \mathcal{F}(\mathcal{H})$ is the semicircle law $w_{r}$ with $r=2\Vert h\Vert$.
\end{theorem}

\begin{proof2}
We may assume that $h$ is a unit vector, and we write $\ell$ for $\ell(h)$. Since $\Vert \ell \Vert=1$, the distribution measure of $\ell^{*}+\ell$ is concentrated on $[-2, 2]$. A compactly supported measure is determined by its moments, due to the Weierstrass approximation theorem. The distribution measure $\mu$ of $\ell^{*}+\ell$ at $\Phi$ is determined by the conditions
\begin{equation*}
\int x^{n}\,d\mu(x)=\langle(\ell^{*}+\ell)^{n}\Phi,\Phi\rangle \qquad (n\in \mathbb{Z}^{+}).
\end{equation*}
We compute $m_{n}:=\langle(\ell^{*}+\ell)^{n}\Phi, \Phi\rangle$ for every $n$. We have
\begin{equation*}
m_{n}=\sum \langle a_{i(n)}a_{i(n-1)}\cdots a_{i(1)}\Phi,\Phi\rangle,
\end{equation*}
where $a_{i(k)}\in\{\ell, \ell^{*}\}$ and the summation is over all such possibilities.

\begin{figure}
\includegraphics{chap01-vend-scan-01.eps}
\caption{The polygonal line associated to the product
$\ell\ell^{*}\ell^{*}\ell\ell$.} \label{ch01:fig1.1}
\end{figure}

We can associate to an $n$-fold product $a_{i(n)}a_{i(n-1)}\cdots a_{i(1)}$ a polygonal line on the $x$-$y$-plane, where the $x$-axis is horizontal. The associated polygonal line starts from the origin $(0,0)$ and possesses slope 1 on the interval $(k-1, k)$ if $a_{i(k)}=\ell$ and slope $-1$ if $a_{i(k)}=\ell^{*}$. If the polygonal line touches the horizontal line $y=-1$, or its end point is not on the $x$-axis, then $\langle a_{i(n)}a_{i(n-1)}\cdots a_{i(1)}\Phi, \Phi\rangle=0$. So, for an odd $n$ we have $m_{n}=0$, because the corresponding polygonal line cannot end on the $x$-axis. If $n=2k$, then we have to count the number of polygonal lines that contribute to $m_{2k}$ (see [\citen{bib76}], III.1 or [\citen{bib166}], Sec. I.10). So
\begin{equation}
m_{n}=\left\{\begin{array}{ll}
0 & \mathrm{if}\ n=2k+1,\\
\\
\left(\begin{array}{l}
2k\\
k\\ \end{array}
\right) - \left( \begin{array}{c}
2k\\
k-1\\
\end{array}\right) = \dfrac{1}{k+1} \left(\begin{array}{l}
2k\\
k\\
\end{array} \right) & \mathrm{if}\ n=2k.
\end{array}\right.
\label{ch01:eqn1.1.6}
\end{equation}
Now we have to show that the semicircle law has these moments.

We integrate by parts, and get
\begin{align*}
\alpha_{2k} & \ := \ \frac{1}{2\pi}\int_{-2}^{2}x^{2k}\sqrt{4-x^{2}}\,dx=-\frac{1}{2\pi}\int_{-2}^{2}\frac{-x}{\sqrt{4-x^{2}}}x^{2k-1}(4-x^{2})\,dx\\
&\ = \ \frac{1}{2\pi}\int_{-2}^{2}\sqrt{4-x^{2}}(x^{2k-1}(4-x^{2}))'\,dx\\
& \ = \ 4(2k-1)\alpha_{2k-2}-(2k+1)\alpha_{2k}.
\end{align*}
In this way the recursion
\begin{equation*}
\alpha_{2k}=\frac{2(2k-1)}{k+1}\alpha_{2k-2} \qquad (k\in \mathbb{N})
\end{equation*}
is obtained, and the moment sequence $m_{2k}$ in (\ref{ch01:eqn1.1.6}) satisfies this recursion.
\end{proof2}

We shall use the notation
\begin{equation}
c_{k} :=\frac{1}{k+1} \left(\begin{array}{c}
2k\\
k\\
\end{array}\right)
\label{ch01:eqn1.1.7}
\end{equation}
for the so-called \textit{Catalan numbers}.\index{Catalan number}

Before we turn to the distribution of the field operators in the symmetric Fock space, we give a second proof for the previous theorem. The starting point will be the observation that computation of the $n$th moment of $\ell(h)^{*}+\ell(h)$ is essentially an $n$-dimensional problem.

\textit{Second proof:} Let $e_{1}, e_{2}, \ldots, e_{n}$ be an orthonormal basis in an $n$-dimensional space and consider the tridiagonal matrix
\begin{equation*}
\mathbf{A}_{n}:=\left[\begin{array}{cccccc}
0 & 1 &  &  &  & \\
1 & 0 & 1 &  & 0 & \\
& 1 & 0 & 1 & & \\
& & \ddots & \ddots & \ddots & \\
& & & \ddots & \ddots & 1\\
& 0 & &  & 1 & 0\\
\end{array}\right].
\end{equation*}
Then it is easy to see that
\begin{equation*}
\int x^{m}\,d\mu(x)=\langle\mathbf{A}_{n}^{m}e_{1},e_{1}\rangle
\end{equation*}
must hold for the distribution measure $\mu$ of $\ell^{*}+\ell$ when $m<2n$. The matrix $\mathbf{A}_{n}$ is rather well-known; its eigenvalues were computed in 1759 by Lagrange. The eigenvalues are $2 \cos j\pi/(n+1)$, and the eigenvectors are
\begin{equation*}
\sqrt{\frac{2}{n+1}}\sum_{k=1}^{n}\left(\sin\frac{jk\pi}{n+1}\right)e_{k} \qquad (j=1,2,\ldots,n).
\end{equation*}
Therefore,
\begin{equation*}
\int x^{m}\,d\mu(x)=\frac{2^{m+1}}{n+1}\sum_{j=1}^{n}\cos^{m}\frac{j\pi}{n+1}\sin^{2}\frac{j\pi}{n+1}.
\end{equation*}
For finite $m$ this seems to be complicated, but it is easy to take the limit as $n\rightarrow\infty$, since the sum is an approximation of an integral. In this way we get
\begin{equation*}
\int x^{m}\,d\mu(x)=\frac{2^{m+1}}{\pi}\int_{0}^{\pi}\cos^{m}t  \sin^{2}t\,dt=\frac{1}{2\pi}\int_{-2}^{2}x^{m}\sqrt{4-x^{2}}\,dx.
\end{equation*}

Let $\mathcal{H}$ be a Hilbert space. For each $f\in \mathcal{H}$ we introduce three operators on the symmetric Fock space $\mathcal{F}_{s}(\mathcal{H})$. All of them are defined on the linear hull of the exponential vectors:
\begin{align}
W(f)\psi(h) & \ := \ \exp(-\Vert f\Vert^{2}/2-\langle h, f\rangle )\psi(f+h)\, ,\notag \\
a(f)\psi(h)& \ := \ \langle h, f\rangle\psi(h) \, , \label{ch01:eqn1.1.8}\\
a^{*}(f)\psi(h) & \ := \ \frac{d}{dt}\psi(h+tf)\Big|_{t=0}. \notag
\end{align}
It is immediate from the definition that the $W(f)$'s satisfy the so-called \textit{Weyl commutation relation}\index{Weyl commutation relation}
\begin{equation*}
W(f)W(g)=\exp(\mathrm{i}\,\mathrm{Im}\langle f, g\rangle)W(f+g)
\end{equation*}
on exponential vectors. (More about the \textit{canonical commutation relation}\index{canonical!commutation relation} is in [\citen{bib148}].) This relation implies that $W(f)$ can be extended to $\mathcal{F}_{s}(\mathcal{H})$, and a unitary operator arises. It is worthwhile to note that the normalized exponential vectors form the orbit of the vacuum vector under the \textit{Weyl unitaries}.\index{Weyl unitary} Namely,
\begin{equation*}
W(f)\Phi=\exp(-\Vert f\Vert^{2}/2)\psi(f) \, .
\end{equation*}

The correspondence $t\mapsto W(tf)$ gives a strongly continuous group of unitaries acting on $\mathcal{F}_{s}(\mathcal{H})$. The \textit{Stone theorem}\index{Stone theorem} ([\citen{bib163}], Sec. 137) tells us that there exists a selfadjoint operator $B(f)$ such that
\begin{equation*}
W(tf)=\exp(\mathrm{i}\,tB(f)) \qquad (t\in \mathbb{R})
\end{equation*}
and
\begin{equation}
-\mathrm{i}\,\frac{d}{dt}W(tf)\xi\Big|_{t=0}=B(f)\xi
\label{ch01:eqn1.1.9}
\end{equation}
whenever $\xi$ is in the domain of $B(f)$. Performing this derivation at exponential vectors, we obtain
\begin{align*}
-\mathrm{i}\,\frac{d}{dt}W(tf)\psi(g)\Big|_{t=0}& \ =\ \mathrm{i}\,\langle g, f \rangle \psi(g)-\mathrm{i}\,\frac{d}{dt}\psi(g+tf)\Big|_{t=0}\\
& \ = \ \mathrm{i}\,a(f)\psi(g)-\mathrm{i}\,a^{*}(f)\psi(g)\\
& \ = \ (a(-\mathrm{i}\,f)+a^{*}(\mathrm{-i}\,f))\psi (g) \, .
\end{align*}
Therefore the operator $a(-\mathrm{i}\,f)+a^{*}(-\mathrm{i}\,f)$ defined on exponential vectors has a selfadjoint extension $B(f)$. In fact, $B(f)$ is the closure of the densely defined operator $a(-\mathrm{i}\,f)+a^{*}(-\mathrm{i}\,f)$. This can be seen as follows. The linear hull $\mathcal{D}$ of exponential vectors is invariant under $\{W(tf):t\in \mathbb{R}\}$; therefore $\mathcal{D}$ is a core of the generator $B(f)$ (cf. [\citen{bib162}], Theorem VIII.10).

We recall that the \textit{normal distribution}\index{normal distribution}\index{distribution!normal} $N(m, \sigma^{2})$ with mean $m$ and varance $\sigma^{2}$ has the density function
\begin{equation*}
\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-m)^{2}}{2\sigma^{2}}\right).
\end{equation*}

\begin{theorem}\label{ch01:the1.1.6}
For $f\in \mathcal{H}$ the selfadjoint closure of the operator $a(f)+a^{*}(f)$ is normally distributed at the vacuum vector $\Phi\in \mathcal{F}_{s}(\mathcal{H})$.
\end{theorem}

\begin{proof2}
For the distribution measure $\mu$ of $B:=\overline{a(f)+a^{*}(f)}$ the equality
\begin{equation}
\langle \exp(\mathrm{i}\,tB)\Phi, \Phi\rangle=\int e^{\mathrm{i}\,tx}\,d\mu(x) \qquad (t\in \mathbb{R})
\label{ch01:eqn1.1.10}
\end{equation}
holds. Since $\exp(\mathrm{i}\,tB)$ is the Weyl unitary $W(\mathrm{i}\,tf)$, the left-hand side is easily computed, and we have
\begin{equation*}
\langle W(\mathrm{i}\,tf)\psi(0),\psi(0)\rangle=\exp(-t^{2}\Vert f\Vert^{2}/2).
\end{equation*}
This must be the characteristic function (or Fourier transform) of $\mu$ according to (\ref{ch01:eqn1.1.10}), and we conclude that $\mu$ is $N(0, \Vert f\Vert^{2})$ (cf. [\citen{bib77}], Part XV).
\end{proof2}

The notation for creation operators in the symmetric and full Fock spaces is not compatible. The left creation operator $\ell(h)$ on the full Fock space is unstarred while the creation operator $a^{*}(h)$ on the symmetric space is starred. The difference fades away when the selfadjoint field operators are considered. The distribution of $\ell(h)^{*}+\ell(h)$ was identified by means of the moments. In the case of $a(f)+a^{*}(f)$ the Fourier transform offered a faster way. However, moments of the normal law can be easily obtained either from the Fourier transform or by direct integration. The $2k$th moment is
\begin{equation*}
(2k-1)!\,!:=\frac{(2k-1)!}{2^{k-1}(k-1)!},
\end{equation*}
which is the number of all pair partitions of a set of $2k$ elements. In comparison, the $2k$th moment $c_{k}$ of the semicircle law is the number of syntactically correct orders of $k$ copies of ``('' and $k$ copies of ``)''. It will be seen in the next chapter that $c_{k}$ is the number of pair partitions of a certain kind.

\begin{example}\label{ch01:exa1.1.7}
Let $U$ be a Haar distributed unitary from Example~\ref{ch01:exa1.1.2}. We want to compute the distribution measure of the selfadjoint operator $U+U^{*}$ at the vector $\delta$. The $n$th moment is
\begin{equation*}
\langle(U+U^{*})^{n}\delta,\delta\rangle,
\end{equation*}
which is $0$ for $n\neq 2k$; otherwise it is the number of orders of $k$ copies of $U$ and $k$ copies of $U^{*}$. We have
\begin{equation*}
\int_{-2}^{2}\frac{4x^{2k-2}-x^{2k}}{\sqrt{4-x^{2}}}\,dx=\int_{-2}^{2}x^{2k-2}\sqrt{4-x^{2}}\,dx=\frac{1}{2k-1}\int_{-2}^{2}\frac{x^{2k}}{\sqrt{4-x^{2}}}\,dx
\end{equation*}
integrating by parts, and this gives the recursion $\alpha_{k}=2k^{-1}(2k-1)\alpha_{k-1}$ for $\alpha_{k} := \pi^{-1}\int_{-2}^{2}x^{2k}/\sqrt{4-x^{2}}\,dx$. Thus the \textit{arcsine law}\index{arcsine law}\index{law!arcsine}
\begin{equation*}
\frac{1}{\pi\sqrt{4-x^{2}}}\chi_{(-2,2)}(x)\,dx
\end{equation*}
has exactly the desired even moments $\left(\begin{subarray}{c}
2k\\
k\\
\end{subarray}\right)$, so it is the distribution of $U+U^{*}$.
\end{example}

Beyond the classical probability laws, rather unusual probability distributions may come up in the Hilbert space probability theory. Next we want to exhibit such an example. Given a real number $q\in(-1,1)$, we construct a selfadjoint operator on $l^{2}(\mathbb{Z}^{+})$ from a certain weighted shift operator. Let $\delta_{n}$ be the standard orthonormal basis in $l^{2}(\mathbb{Z}^{+})$. Set
\begin{equation*}
[n]_{q}:=1+q+\cdots+q^{n-1}=\frac{1-q^{n}}{1-q} \qquad (n\in \mathbb{N}).
\end{equation*}
The \textit{weighted shift} $S_{q}$, defined as
\begin{equation*}
S_{q}\delta_{n}:=\sqrt{[n+1]_{q}}\delta_{n+1} \qquad (n\in \mathbb{Z}^{+}),
\end{equation*}
is a bounded operator such that
\begin{equation*}
\Vert S_{q}\Vert=\sup\{\sqrt{[n]_{q}}:n\in \mathbb{N}\}=\left\{\begin{array}{ll}
\dfrac{1}{\sqrt{1-q}} & \mathrm{if}\ q\in[0,1),\\
\\
1 & \mathrm{if}\ q\in(-1,0].\\
\end{array}\right.
\end{equation*}
For $q=0$ the ordinary shift is recovered. It is worthwhile to note that $S_{q}^{*}S_{q}= \mathbf{1}+qS_{q}S_{q}^{*}$ is a generalization of the canonical commutation relation, called the $q$-\textit{commutation relation}.\index{$q$-commutation relation}\index{weighted!shift operator} Thus $S_{q}$ seems to be the ``$q$-creation operator'', and the spectrum of the ``$q$-number operator'' $S_{q}S_{q}^{*}$ consists of the ``$q$-natural numbers'' $[n]_{q}$.

\begin{example}\label{ch01:exa1.1.8}
The distribution measure of the selfadjoint operator $S_{q}+S_{q}^{*}$ at the vector $\delta_{0}$ is a measure supported on the interval $[-2/\sqrt{1-q}, 2/\sqrt{1-q}\,]$. Its density is known to be
\begin{equation*}
C_{q}\sqrt{4-(1-q)x^{2}}\prod_{k=1}^{\infty}\left(1-\frac{(1-q)q^{k}x^{2}}{(1+q^{k})^{2}}\right),
\end{equation*}
where
\begin{equation*}
C_{q}:=\frac{\sqrt{1-q}}{2\pi}\prod_{k=1}^{\infty}(1 -q^{2k})\prod_{k=1}^{\infty}(1 +q^{k}) \, .
\end{equation*}

This probability distribution may be called the $q$-deformed Gaussian distribution, or simply the $q$-\textit{Gaussian distribution}.\index{$q$-Gaussian distribution}\index{distribution!$q$-Gaussian} The value $q=0$ gives the semicircle law, and the limit distribution as $q\rightarrow 1$ is the normal law. Indeed,
\begin{equation*}
\lim_{q\rightarrow 1}\sum_{k=1}^{\infty}\log\left(1-\frac{(1-q)q^{k}x^{2}}{(1+q^{k})^{2}}\right)=\lim_{q\rightarrow 1}\sum_{k=1}^{\infty}\left(-\frac{(1-q)q^{k}x^{2}}{(1+q^{k})^{2}}\right)=-\frac{x^{2}}{2}
\end{equation*}
because $\sum_{k=1}^{\infty}(q^{k}-q^{k+1})/(1+q^{k})^{2}$ is an infinite Riemannian sum approximation to $\int_{0}^{1}dt/(1+t)^{2}$.
\end{example}

Above, the distribution measure of a weighted shift operator at the vacuum led to the $q$-normal distribution. However, many more distributions can be obtained from weighted shifts. Now we review this subject and show its relation to the theory of orthogonal polynomials.

Let $(b_{n})_{n=1}^{\infty}$ be a sequence of positive real numbers, and define a \textit{weighted shift operator}\index{weighted!shift operator}
\begin{equation*}
S\delta_{n}:=b_{n+1}\,\delta_{n+1} \qquad (n\in \mathbb{Z}^{+})
\end{equation*}
on the space $l^{2}(\mathbb{Z}^{+})$. When $(b_{n})$ is bounded, so is $S$, and we mainly consider this case. The truncation of the selfadjoint operator $S+S^{*}$ is the tridiagonal matrix
\begin{equation*}
\mathbf{A}_{n}:= \left[\begin{array}{cccccc}
0 & b_{1} &  &  &  & \\
b_{1} & 0 & b_{2} &  &  & 0\\
& b_{2}& 0 & b_{3} & & \\
& & \ddots & \ddots & \ddots & \\
& & & \ddots & \ddots & b_{n-1}\\
& 0 & & & b_{n-1}&0\\
\end{array}\right].
\end{equation*}
The eigenvalues of $\mathbf{A}_{n}$ are the roots of the monic polynomial $P_{n}(x)$ defined by the recursion formula
\begin{equation}
P_{n+1}(x)=xP_{n}(x)-\lambda_{n}P_{n-1}(x) \qquad (n \in \mathbb{Z}^{+})
\label{ch01:eqn1.1.11}
\end{equation}
$(P_{-1}(x)\equiv 0,\,P_{0}(x)\equiv 1)$, where $\lambda_{n} :=b_{n}^{2}$. In fact, this is nothing else but the recursion for the characteristic polynomials of the $\mathbf{A}_{n}$'s. The very basics of the theory of orthogonal polynomials say (see the book [\citen{bib49}], Chap. I, II, for example) that $(P_{n}(x))_{n=0}^{\infty}$ is an \textit{orthogonal polynomial sequence}\index{orthogonal polynomial sequence} with respect to a symmetric positive-definite moment functional represented by a probability measure $\mu$ (of compact support), a solution of the Hamburger moment problem.

The monic polynomial $P_{n}(x)$ has the simple real zeros $x_{n1}, x_{n2}, \ldots, x_{nn}$, which are the eigenvalues of $\mathbf{A}_{n}$. The famous Gauss quadrature formula says that there are positive numbers $c_{n1}, c_{n2}, \ldots, c_{nn}$ (the so-called Christoffel numbers) with $\sum_{k=1}^{n}c_{nk}=1$ such that
\begin{equation}
\int q(x)\,d\mu(x)=\sum_{k=1}^{n}c_{nk}q(x_{nk})
\label{ch01:eqn1.1.12}
\end{equation}
for every polynomial $q(x)$ of degree less than $2n$. From this we know that $\mu$ is the limit of the sequence of atomic measures $\mu_{n} :=\sum_{k=1}^{n}c_{nk}\delta(x_{nk})$ supported on the zeros of $P_{n}(x)$, where $\delta(x_{0})$ denotes the point measure at $x_{0}$. Moreover, it is known ([\citen{bib49}], Chap. III, Theorem 4.3) that the \textit{Cauchy transform} of $\mu_{n}$,
\begin{equation*}
\int\frac{d\mu_{n}(x)}{z-x}=\sum_{k=1}^{n}\frac{c_{nk}}{z-x_{nk}},
\end{equation*}
is equal to the $n$th partial approximant of the following \textit{continued fraction}:\index{continued fraction}
\begin{equation}
\cfrac{1}{z-
\cfrac{\lambda_{1}}{z-
\cfrac{\lambda_{2}}{z-_{\ddots}
}}}.
\label{ch01:eqn1.1.13}
\end{equation}
In this way, we can conclude that the Cauchy transform\index{Cauchy transform} of $\mu$ has the above continued fraction expansion. (See Sec.~\ref{ch03:sec3.1} for more about the Cauchy transform for measures.)

\begin{proposition}\label{ch01:pro1.1.9}
The distribution measure of $S+S^{*}$ at the vector $\delta_{0}$ is $\mu$, given above. More generally,
\begin{equation*}
\langle(S+S^{*})^{m}\delta_{i},\delta_{j}\rangle=\int x^{m}p_{i}(x)p_{j}(x)\,d\mu(x) \qquad (m,i,j\in \mathbb{Z}^{+}),
\end{equation*}
where the $p_{n}(x)$ are the normalized orthogonal polynomials for $\mu$.
\end{proposition}

\begin{proof2}
We need the following facts from the theory of orthogonal polynomials (see [\citen{bib49}]):
\begin{equation*}
\int P_{n}(x)^{2}\,d\mu(x)=\lambda_{1}\lambda_{2}\cdots\lambda_{n} \qquad (n\in \mathbb{N})
\end{equation*}
and
\begin{equation*}
\sum_{i=0}^{n-1}p_{i}(x_{nk})^{2}=\frac{1}{c_{nk}} \qquad (1\leq k\leq n,n\in \mathbb{N}).
\end{equation*}
From the recursion (\ref{ch01:eqn1.1.11}) together with these facts, one can easily check that
\begin{equation*}
\sqrt{c_{nk}}(p_{0}(x_{nk}),p_{1}(x_{nk}),\ldots,p_{n-1}(x_{nk}))^{t}
\end{equation*}
is the normalized eigenvector of $\mathbf{A}_{n}$ corresponding to the eigenvalue $x_{nk}$. From the spectral decomposition of $\mathbf{A}_{n}$ we have
\begin{equation*}
\langle \mathbf{A}_{n}^{m}\delta_{i},\delta_{j}\rangle=\sum_{k=1}^{n}c_{nk}x_{nk}^{m}p_{i}(x_{nk})p_{j}(x_{nk}) \, .
\end{equation*}
Now we can use (\ref{ch01:eqn1.1.12}) with $q(x)=x^{m}p_{i}(x)p_{j}(x)$ when $m+i+j<2n$, to conclude that
\begin{equation*}
\langle (S+S^{*})^{m}\delta_{i},\delta_{j}\rangle=\lim_{n\rightarrow\infty}\langle \mathbf{A}_{n}^{m}\delta_{i},\delta_{j}\rangle=\int x^{m}p_{i}(x)p_{j}(x)\,d\mu(x) \, .
\end{equation*}
\end{proof2}

It is a consequence of the proposition that the distribution measure of $S+S^{*}$ is determined by (\ref{ch01:eqn1.1.13}). (Every symmetric measure can arise in this way.)

In the example of a creation operator in Theorem~\ref{ch01:the1.1.5} we have $b_{n}=1$ for every $n\in \mathbb{N}$. Then the measure $\mu$ is the semicircle law, and the corresponding orthogonal polynomials are the \textit{Jacobi polynomials}\index{Jacobi polynomial} $P_{n}^{(\alpha,\beta)}$ with $\alpha=\beta=1/2$. In this case, the number $\langle(S+S^{*})^{m}\delta_{i}, \delta_{j}\rangle$ is an integer, and is interpreted as the number of certain polygonal lines. In the spirit of the proof of Theorem~\ref{ch01:the1.1.5} we count the polygonal lines from $(0,i)$ to $(m,j)$ which go through lattice points, have slopes $\pm 1$ and do not touch the horizontal line $y=-1$ on the $x\hbox{-}y$-plane. (It is not difficult to write the explicit formula, which can be the difference of two binomial coefficients.)

The weighted shift $S_{q}$ in Example~\ref{ch01:exa1.1.8} arises when $b_{n}=\sqrt{[n]_{q}} \ (n\in \mathbb{N})$. In this case, we have to treat the recursion formula
\begin{equation*}
P_{n+1}(x)=xP_{n}(x)-\frac{1-q^{n}}{1-q}P_{n-1}(x) \qquad (P_{-1}\equiv 0,\ P_{0}\equiv 1);
\end{equation*}
but these orthogonal polynomials are known as ``continuous $q$-Hermite polynomials'' and the associated measure is contained in Example~\ref{ch01:exa1.1.8}.

\section{Noncommutative random variables}
\label{ch01:sec1.2}

\noindent It was shown in the previous section how a Hilbert space operator together with a unit vector of the Hilbert space can induce a probability distribution. Now we continue to generalize the usual setting of classical probability theory in an algebraic way. Random variables over a probability space form an algebra. Indeed, they are measurable functions defined on a set $\Omega$, and so are the product and sum of two of them. The \textit{expectation value}\index{expectation value}\index{random matrix} is a linear functional on this algebra. The algebraic approach to probability stresses that point. If $\mathcal{A}$ is a unital algebra over the complex numbers and $\varphi$ is a linear functional of $\mathcal{A}$ such that $\varphi(\mathbf{1})=1$, then $(\mathcal{A}, \varphi)$ will be called a \textit{noncommutative probability space}\index{noncommutative!probability space} and an element $a$ of $\mathcal{A}$ will be called a \textit{noncommutative random variable}.\index{noncommutative!random variable} The number $\varphi(a^{n})$ is called the $nth$ \textit{moment} of $a$.

\begin{example}\label{ch01:exa1.2.1}
Let $B(\mathcal{H})$ denote the algebra of all bounded operators acting on a Hilbert space $\mathcal{H}$. If the linear functional $\varphi : B(\mathcal{H})\rightarrow \mathbb{C}$ is defined by means of a unit vector $\xi\in \mathcal{H}$ as $\varphi(A)=\langle A\xi, \xi\rangle$, then any element of $B(\mathcal{H})$ is a noncommutative random variable.
\end{example}

If $A\in B(\mathcal{H})$ is selfadjoint, then a probability measure is associated to $A$ and $\varphi$, as discussed in the previous section. The algebra used in the definition of a noncommutative random variable is often replaced by a $^{*}$-algebra\index{${}^{*}$-algebra}. Actually, $B(\mathcal{H})$ is a $^{*}$-algebra if the operation $A^{*}$ stands for the adjoint of $A$.

A \textit{*-algebra} is a unital algebra over the complex numbers which is endowed with an \textit{involution}\index{involution}\index{moment} $^{*}$. The involution recalls the adjoint operation of Hilbert space operators as follows:
\begin{enumerate}
\item[(1)] $a\mapsto a^{*}$ is conjugate linear,

\item[(2)] $(ab)^{*} =b^{*}a^{*}$,

\item[(3)] $a^{**}=a$.
\end{enumerate}
When $(\mathcal{A}, \varphi)$ is a noncommutative probability space over a $^{*}$-algebra $\mathcal{A}, \varphi$ is always assumed to be a \textit{state}\index{state} on $\mathcal{A}$, that is, a linear functional such that
\begin{enumerate}
\item[(4)] $\varphi(\mathbf{1})=1$, where $\mathbf{1}$ denotes the unit of $\mathcal{A}$,

\item[(5)] $\varphi(a^{*})=\overline{\varphi(a)}$ and $\varphi(a^{*}a)\geq 0$ for every $a\in \mathcal{A}$.
\end{enumerate}

A matrix whose elements are (classical) random variables on a (classical) probability space is called a \textit{random matrix}. Random matrices form a $^{*}$-algebra. For instance, let $a_{11},a_{12},a_{21},a_{22}$ be four bounded (classical) random variables on a probability space. Then
\begin{equation*}
T=\left[\begin{array}{ll}
a_{11} & a_{12}\\
a_{21} & a_{22}\\
\end{array}\right]
\end{equation*}
is a bounded $2\times 2$ random matrix. The set $\mathcal{A}$ of all such matrices has a $^{*}$-algebra structure when the usual matrix operations are considered, and $(\mathcal{A}, \varphi)$ is a noncommutative probability space\index{$C^{*}$-probability space} when $\varphi(T)=E(a_{11})$, for example.

A $C^{*}$-\textit{algebra} is a $^{*}$-algebra $\mathcal{A}$ which is endowed with a norm such that
\begin{enumerate}
\item[(6)] $\Vert a^{*}a\Vert=\Vert a\Vert^{2},\,\Vert ab\Vert\leq\Vert a\Vert\Vert b\Vert$ for every $a, b\in \mathcal{A}$, and $\Vert \mathbf{1}\Vert=1$,
\end{enumerate}
and furthermore $\mathcal{A}$ is a Banach space with respect to this norm. Two important theorems (due to Gelfand and Naimark) concern the representation of $C^{*}$-algebras. A commutative unital $C^{*}$-algebra is isometrically isomorphic to the algebra of all continuous complex functions on a certain compact Hausdorff space if the function space is endowed with the supremum norm and the involution of pointwise conjugation. A general $C^{*}$-algebra is isometrically isomorphic to an algebra of operators acting on a Hilbert space if this algebra is endowed with the operator norm and the involution of adjoint operation. (Combination of the two Gelfand-Naimark theorems yields a form of the spectral theorem.)\index{Gelfand-Naimark theorem} For a linear functional $\varphi$ of a $C^{*}$-algebra,\index{$C^{*}$-algebra} $\Vert\varphi\Vert=\varphi(1)$ is equivalent to $\varphi(a^{*}a)\geq 0\ (a\in \mathcal{A})$.

A noncommutative probability space $(\mathcal{A}, \varphi)$ will be called a $C^{*}$-\textit{probability space} when $\mathcal{A}$ is a $C^{*}$-algebra and $\varphi$ is a state on $\mathcal{A}$.

\begin{example}\label{ch01:exa1.2.2}
All real bounded classical random variables may be considered as noncommutative random variables.

Let $\xi$ be a bounded real random variable with distribution $\mu$ and let $K\subset \mathbb{R}$ be the compact support of $\mu$. The continuous complex functions defined on $K$ form a $C^{*}$-algebra $\mathcal{A}$ with the supremum norm. A state $\varphi$ is defined on $\mathcal{A}$ as $\varphi(f):=\int f(x)\,d\mu(x)$. So $(\mathcal{A}, \varphi)$ is a degenerate noncommutative probability space because $\mathcal{A}$ is actually commutative. The identity function $f_{0}$ of $K$ (viewed as an element of $\mathcal{A}$) corresponds to the classical random variable $\xi$. In particular, $\xi$ and $f_{0}$ have the same moments: $\varphi(f_{0}^{n})=E(\xi^{n}) \ (n\in \mathbb{Z}^{+})$. The $C^{*}$-algebra $\mathcal{A}$ has a representation on the Hilbert space $L^{2}(\mu)$ by multiplication. If $M_{0}$ is the multiplication operator by the above $f_{0}$, then the distribution\index{distribution} measure of $M_{0}$ at the unit vector 1 (the identically 1 function) is exactly $\mu$.
\end{example}

The representation of a classical random variable as a noncommutative one is far from unique. Let $\mathbb{C}\langle X\rangle$ denote the algebra of polynomials in an indeterminate $X$ over $\mathbb{C}$, and let $\xi$ be a classical random variable whose moments $E(\xi^{n})$ exist for all $n\in \mathbb{N}$. A linear functional $\varphi : \mathbb{C}\langle X\rangle\rightarrow \mathbb{C}$ is produced as $\varphi(X^{k})=E(\xi^{k})$. In this way the indeterminate $X$ is a noncommutative random variable with the same moment sequence as $\xi$. When $A$ is an operator on a Hilbert space $\mathcal{H}$ and $(A,\Phi)$ is a noncommutative random variable with $\Phi\in \mathcal{H}$, one is tempted to call the linear functional $\varphi : \mathbb{C}\langle X\rangle\rightarrow \mathbb{C}$ given as $\varphi(X^{k})=\langle A^{k}\Phi, \Phi\rangle$ the \textit{distribution} of $(A, \Phi)$. For our purposes two noncommutative random variables are equivalent if they have the same distributions (that is, the same moments).

\begin{example}\label{ch01:exa1.2.3}
Let $\mathcal{A}$ be the $^{*}$-algebra generated by the creation and annihilation operators $(a^{*}(f)$ and $a(f))$ acting on the symmetric Fock space $\mathcal{F}_{s}(\mathcal{H})$. Then $\mathcal{A}$ with the functional $\varphi(b)=\langle b\Phi, \Phi\rangle$ is a noncommutative probability space.

For the sake of simplicity, we assume that $\mathcal{H}$ is one-dimensional. Then $\mathcal{F}_{s}(\mathcal{H})=l^{2}(\mathbb{Z}^{+})$, and up to a constant factor we have only one creation operator and one annihilation operator. In (\ref{ch01:eqn1.1.8}) they were defined on exponential vectors. Any polynomial of the annihilation operator $a$ leaves invariant the linear hull of exponential vectors. However, the creation operator $a^{*}$ does not behave so. In order to have the algebra generated by $a$ and $a^{*}$ on a commom (large) domain, we need something else---the linear hull of exponential vectors will not do. Let $\delta_{n}$ be the canonical basis vectors of $l^{2}(\mathbb{Z}^{+})$. The operators $a$ and $a^{*}$ act on these vectors as
\begin{equation}
a\delta_{n}=\left\{\begin{array}{ll}
\sqrt{n}\delta_{n-1}& \mathrm{if}\ n>0,\\
0 & \mathrm{if}\ n=0,\\
\end{array} \right. \quad a^{*}\delta_{n}=\sqrt{n+1}\,\delta_{n+1}.
\label{ch01:eqn1.2.1}
\end{equation}
Hence any polynomial of $a$ and $a^{*}$ leaves invariant the (finite) linear combinations of basis vectors, and $a^{*}$ is really the adjoint of $a$. The algebra generated by $a$ and $a^{*}$ makes sense, and $(\mathcal{A}, \varphi)$ is a noncommutative probability space.

We note that the functional $\varphi$ is called the expectation value at the vacuum, and $\mathcal{A}$ is not a $C^{*}$-algebra because it consists of unbounded operators. The latter statement is obvious from (\ref{ch01:eqn1.2.1}). According to Theorem~\ref{ch01:the1.1.6}, the noncommutative random variable $a+a^{*}$ has the normal distribution.
\end{example}

From our point of view the distribution of the operator $\ell(h)^{*}+\ell(h)$ from Theorem~\ref{ch01:the1.1.5} is rather important. It is an example of what we call a semicircular noncommutative random veriable. Let $a$ be a random variable in the noncommutative probability space $(\mathcal{A}, \varphi)$. We say that $a$ is (selfadjoint) \textit{standard semicircular}\index{standard!semicircular variable} if its distribution is the same as that of $\ell(h)^{*}+\ell(h)$ with $\Vert h\Vert=1$ at the vacuum vector $\Phi$. In other words, a standard semicircular\index{semicircular, standard} variable is characterized by its moment sequence (\ref{ch01:eqn1.1.6}).

Let $a_{1}$ and $a_{2}$ be two random variables in the same noncommutative probability space $(\mathcal{A}, \varphi)$. Their \textit{joint distribution}\index{distribution!joint}\index{joint distribution} will be defined as a linear functional of a noncommutative algebra of polynomials. Let $\mathbb{C}\langle X_{1},  X_{2}\rangle$ be the algebra of polynomials in two noncommuting indeterminates $X_{1}$ and $X_{2}$. The joint distribution of $a_{1}$ and $a_{2}$ is the functional $\mu : \mathbb{C}\langle X_{1}, X_{2}\rangle\rightarrow \mathbb{C}$ defined by
\begin{equation*}
\mu(X_{s_{1}}X_{s_{2}}\cdots X_{s_{m}}) :=\varphi(a_{s_{1}}a_{s_{2}}\cdots a_{s_{m}}),
\end{equation*}
where $s_{i}\in\{1,2\}$. We note that the algebra $\mathbb{C}\langle X_{1},  X_{2}\rangle$ may be regarded as a $^{*}$-algebra, where $X_{1}$ and $X_{2}$ are considered to be selfadjoint. The concept of joint distribution generalizes to several random variables in an obvious way.

\begin{example}\label{ch01:exa1.2.4}
Let $U$ be the unitary in Example~\ref{ch01:exa1.1.2}. The joint distribution of $U$ and $U^{*}$ at the vector $\delta$ is as follows: $\mu(X_{s_{1}}X_{s_{2}}\cdots X_{s_{m}})=1$ when the number of $1$'s is the same as the number of $2$'s in the 1-2-sequence $(s_{1}, s_{2}, \ldots, s_{m})$; otherwise $\mu(X_{s_{1}}X_{s_{2}}\cdots X_{s_{m}})=0$.
\end{example}

From the last example the following definition emerges. Let $(\mathcal{A}, \varphi)$ be a noncommutative probability space with a unital $^{*}$-algebra $\mathcal{A}$. A noncommutative random variable $u\in \mathcal{A}$ will be called a \textit{Haar unitary}\index{unitary, Haar}\index{Haar unitary} if the joint distribution of $u$ and $u^{*}$ is the same as in the previous example. $\mathrm{A}$ Haar unitary appeared earlier in Example~\ref{ch01:exa1.1.2}.

\begin{example}\label{ch01:exa1.2.5}
For $0\neq h\in \mathcal{H}$ let $\ell(h)$ be the creation operator defined on the full Fock space $\mathcal{F}(\mathcal{H})$ (defined in (\ref{ch01:eqn1.1.4})). Compute the joint distribution of $\ell(h)$ and $\ell(h)^{*}$ at the vacuum vector.

We need to compute
\begin{equation*}
\mu(X_{s(n)}X_{s(n-1)}\cdots X_{s(1)})=\langle a_{s(n)}a_{s(n-1)}\cdots a_{s(1)}\Phi,\Phi\rangle \, ,
\end{equation*}
where $s(1), s(2), \ldots, s(n)\in\{1,2\}$ and
\begin{equation*}
a_{s(i)}=\left\{\begin{array}{ll}
\ell(h) & \mathrm{if}\ s(i)=1\\
\ell(h)^{*} & \mathrm{if}\ s(i)=2
\end{array}\right. \quad (1\leq i\leq n).
\end{equation*}
This computation has been essentially done in the proof of Theorem~\ref{ch01:the1.1.5}, and it is convenient to use the geometric language introduced there. It was found that $\langle a_{s(n)}a_{s(n-1)}\ldots a_{s(1)}\Phi, \Phi\rangle$ is $0$ or $\Vert h\Vert^{n}$ depending on the polygonal line associated with the long product. When the line ends on the $x$-axis and does not touch the horizontal line $y=-1$, we get a nonzero expectation value. In particular, $\mu(X_{s(n)}X_{s(n-1)}\cdots X_{s(1)})=0$ when $n$ is odd.

For example, $\mu(X_{2}X_{1}X_{2}X_{1})=\mu(X_{2}^{2}X_{1}^{2})=\Vert h\Vert^{4}$, and in all other cases we have $\mu(X_{s(4)}X_{s(3)}X_{s(2)}X_{s(1)})=0$.
\end{example}

\begin{example}\label{ch01:exa1.2.6}
For $g_{1}, g_{2}\in \mathcal{H}$ let $B(g_{1})$ and $B(g_{2})$ be the unbounded selfadjoint operators defined by (\ref{ch01:eqn1.1.9}) on the symmetric Fock space $\mathcal{F}_{s}(\mathcal{H})$. Compute the joint distribution of $B(g_{1})$ and $B(g_{2})$ at the vacuum vector.

We have to know the expectation value of long products of field operators $B(f_{i})$. We benefit from the formula
\begin{align*}
& \langle B(f_{n})B(f_{n-1})\cdots B(f_{1})\Phi,\Phi\rangle\\
& \qquad =(-\mathrm{i})^{n}\frac{\partial^{n}}{\partial t_{n}\cdots \partial t_{1}}\langle W(t_{n}f_{n})\cdots W(t_{1}f_{1})\Phi,\Phi\rangle \Big|_{t_{1}=\ldots=t_{n}=0} .
\end{align*}
Since
\begin{align*}
& W(t_{n}f_{n})W(t_{n-1}f_{n-1})\cdots W(t_{1}f_{1})\\
& \qquad =W(t_{n}f_{n}+t_{n-1}f_{n-1}+\cdots+t_{1}f_{1})\cdot\exp\left(\mathrm{i}\sum_{j>k}t_{j}t_{k}\mathrm{Im}\langle f_{j},f_{k}\rangle\right)
\end{align*}
and $\langle W(f)\Phi, \Phi\rangle=\exp(-\Vert f\Vert^{2}/2)$ according to (\ref{ch01:eqn1.1.8}) (in which $h=0$ should be taken), we have
\begin{align}
& \langle W(t_{n}f_{n})\cdots W(t_{1}f_{1})\Phi,\Phi\rangle \notag\\
& \qquad =\exp\left(-\frac{1}{2}\sum_{m=1}^{n}t_{m}^{2}\Vert f_{m}\Vert^{2}\right)\exp\left(-\sum_{j>k}t_{j}t_{k}\langle f_{k},f_{j}\rangle\right). \label{ch01:eqn1.2.2}
\end{align}
What we need is the coefficient of $t_{1}t_{2}\cdots t_{n}$ in the power series expansion. Such a term comes only from the second factor of (\ref{ch01:eqn1.2.2}) and only in the case of an even $n$. One obtains $t_{1}t_{2}\cdots t_{n}$ as a product of factors $t_{j}t_{k}(j>k)$ in many different ways; each of the possibilities is associated with a pair partition of the set $\{1, 2, \ldots, n\}$. Hence
\begin{equation}
\langle B(f_{n})B(f_{n-1})\cdots B(f_{1})\Phi,\Phi\rangle=\sum\prod_{m=1}^{n/2}\langle f_{k_{m}},f_{j_{m}}\rangle \, ,
\label{ch01:eqn1.2.3}
\end{equation}
where the summation is over all pair partitions $\{\mathcal{V}_{1}, \mathcal{V}_{2}, \ldots, \mathcal{V}_{n/2}\}$ of $\{1, 2, \ldots, n\}$ such that $\mathcal{V}_{m}=\{j_{m}, k_{m}\}$ with $j_{m}>k_{m} (m=1,2, \ldots, n/2)$. Observe that $\langle f_{k}, f_{j}\rangle=\langle B(f_{j})B(f_{k})\Phi, \Phi\rangle$.

In this way we conclude that
\begin{equation*}
\mu(X_{s(n)}X_{s(n-1)}\cdots X_{s(1)})=\left\{\begin{array}{ll}
0 & \mathrm{if}\ n\ \mathrm{is\ odd},\\
\\
\sum\prod_{m=1}^{n/2}\langle g_{s(k_{m})}, g_{s(j_{m})}\rangle & \mathrm{if}\ n\ \mathrm{is\ even},\\
\end{array} \right.
\end{equation*}
where the summation is over the pair partitions exactly as in (\ref{ch01:eqn1.2.3}). For example, we have
\begin{align*}
\mu(X_{1}X_{2}X_{1}X_{2}) &  \ = \ \Vert g_{2}\Vert^{2} \Vert g_{1}\Vert^{2}+\langle g_{2},g_{1}\rangle^{2}+ |\langle g_{2},g_{1}|^{2} \, ,\\
\mu(X_{1}^{3}X_{2}) & \ = \ 3\Vert g_{1}\Vert^{2}\langle g_{2},g_{1}\rangle \, ,\\
\mu(X_{1}X_{2}X_{1}^{2}) & \ = \ \Vert g_{1}\Vert^{2} \langle g_{2},g_{1}\rangle + 2\Vert g_{1}\Vert^{2} \langle g_{1},g_{2} \rangle \, .
\end{align*}
\end{example}

This is a good place to make a remark about the previous example. Since for every $g\in \mathcal{H}$ we have a noncommutative random variable $B(g)$ , the correspondence $g\mapsto B(g)$ is a ``noncommutative stochastic process'' indexed by $\mathcal{H}$. According to Theorem~\ref{ch01:the1.1.6}, the $B(g)$'s are normally distributed. On the other hand, it follows from the computation above that for $g_{1}\perp g_{2}$ we have the factorization
\begin{align}
& \mu(B(f_{1})B(f_{2})\cdots B(f_{n})) \notag\\
& \qquad =\mu(\tilde{B}(f_{1})\tilde{B}(f_{2})\cdots\tilde{B}(f_{n}))\mu(\hat{B}(f_{1})\hat{B}(f_{2})\cdots\hat{B}(f_{n})),
\label{ch01:eqn1.2.4}
\end{align}
where each $f_{i}$ is $g_{1}$ or $g_{2}$ and
\begin{equation*}
\tilde{B}(f_{i}):=\left\{\begin{array}{ll}
B(g_{1}) & \mathrm{if}\ f_{i}=g_{1},\\
I & \mathrm{otherwise}\\ \end{array} \right. \quad \hat{B}(f_{i}):=\left\{\begin{array}{ll}
B(g_{2}) & \mathrm{if}\ f_{i}=g_{2},\\
I & \mathrm{otherwise}\\ \end{array} \right.
\end{equation*}
For example, $\mu(B(g_{1})^{2}B(g_{2})B(g_{1})^{2}B(g_{2}))=\mu(B(g_{1})^{4})\mu(B(g_{2})^{2})$. It seems that $B(g_{1})$ and $B(g_{2})$ are independent in a sense, and what we have seems to be a noncommutative analogue of a Gaussian process:

\begin{example}\label{ch01:exa1.2.7}
Let $\mathcal{H} :=L^{2}(\mathbb{R}^{+})$ and $B_{t}:=B(\chi_{[0,t)})$, where $\chi_{[0,t)}$ stands for the characteristic function of the interval $[0,t),\,t\geq 0$. Then $B_{t}$ is a noncommutative analogue of a Gaussian process in the symmetric Fock space $\mathcal{F}_{s}(L^{2}(\mathbb{R}^{+}))$, and it is indexed by $t\geq 0$.

We have
\begin{equation*}
\langle B_{t}B_{s}\Phi,\Phi\rangle=t\wedge s,
\end{equation*}
and the increments $B_{t(2)}-B_{t(1)}$ and $B_{s(2)}-B_{s(1)}$ are normally distributed and independent in the sense of (\ref{ch01:eqn1.2.4}) whenever $0\leq t(1)<t(2)\leq s(1)<s(2)$. (\ref{ch01:eqn1.2.4}) enables us to compute all joint moments of the noncommutative random variables $B_{t(2)}-B_{t(1)}$ and $B_{s(2)}-B_{s(1)}$.
\end{example}

Next we set the previous example in the full Fock space.

\begin{example}\label{ch01:exa1.2.8}
Let $\mathcal{H} :=L^{2}(\mathbb{R}^{+})$ and consider the full Fock space $\mathcal{F}(L^{2}(\mathbb{R}^{+}))$. If $X_{t} :=\ell(\chi_{[0,t)})^{*}+\ell(\chi_{[0,t)})$, then we have
\begin{equation*}
\langle X_{t}X_{s}\Phi,\Phi\rangle =t\wedge s
\end{equation*}
by a simple computation. For $0\leq t(1)<t(2)$ the increment $X_{t(2)}-X_{t(1)}= \ell(\chi_{[t(1),t(2))})^{*}+\ell(\chi_{[t(1),t(2))})$ has the semicircular distribution with mean $0$ and variance $t(2)-t(1)$ at the vacuum vector. However, it is difficult at the moment to say anything about the joint distribution of two increments $X_{t(2)}-X_{t(1)}$ and $X_{s(2)}-X_{s(1)}$ for $0\leq t(1)<t(2)\leq s(1)<s(2)$. What we need is a new concept, the subject of the next chapter. It will turn out that the increments are in free relation, which is a particular rule for the computation of the joint distribution; cf. Example~\ref{ch02:exa2.2.10}.
\end{example}

\subsection*{Notes and Remarks}The concept of the distribution of a selfadjoint operator in a vector goes back to the 1920's, when quantum mechanics was created. Vectors are called wave functions in quantum mechanics, and the abstract definition appeared in the famous book ``Mathematical Foundation of Quantum Mechanics'' published in 1932 in German by John von Neumann. Our Example~\ref{ch01:exa1.1.3} was the first and simplest case in which the statistical feature of quantum mechanics was recognized; von Neumann gave Max Born credit for it. Certainly quantum mechanics was one of the main motivations for developing a noncommutative probability theory based on linear functionals of a noncommutative algebra (see [\citen{bib54}], for example).

See standard texts such as [\citen{bib188}] and [\citen{bib108}] for general theory on $C^{*}$-algebra (in particular, the two Gelfand-Naimark theorems) and also on von Neumann algebras.

The Fock space appeared in a physical context in the work of V.A. Fock in 1932. We suggest the book of K.R. Parthasarathy [\citen{bib143}] for a mathematical treatment of Weyl unitaries, exponential vectors, and several topics related to the symmetric Fock space. Details on quantum statistical mechanics based on creation and annihilation operators on the symmetric Fock space as well as the antisymmetric Fock space are also found in the book of O. Bratteli and D.W. Robinson [\citen{bib44}].

The full Fock space over an $n$-dimensional space is nothing but the $\ell^{2}$ space over the free semigroup generated by $n$ elements. Theorem~\ref{ch01:the1.1.5} has many extensions. One can change the inner product (for which the adjoint of the left creation operator is defined), or the distribution is not taken at the vacuum vector. The $q$-Gaussian distribution can be obtained in this way. Concerning the relevant Fock space, we refer to [\citen{bib39}] and [\citen{bib37}]. The density of Example~\ref{ch01:exa1.1.8} is from [\citen{bib7}] and [\citen{bib41}]. Moments and other details of the $q$-Gaussian distribution are found in [\citen{bib131}] and [\citen{bib106}]. The formula for the moments is given in the Notes and Remarks to Chap.~\ref{ch02:chap02}, below. The $q$-Gaussian distribution is only a small slice in the world of ``$q$'' ($q$-derivation, $q$-Hermite polynomials, etc.). Another extension of the Fock space formalism is the \textit{interacting Fock space},\index{Fock space!interacting}\index{interacting Fock space} see [\citen{bib118}]. When $\mathcal{H}$ is the $L^{2}$-space over a measure space, on the tensor powers $\mathcal{H}^{\otimes n}$ a modified scalar product is introduced with the help of weight functions $\lambda_{n}(x_{1}, \cdots, x_{n})$ enjoying certain growth properties, and this leads to the modified Fock space. Then the creation and annihilation operators are introduced in a standard way. The distribution of the selfadjoint operator $\ell^{*}+\ell$ will be a sort of averaged-out semicircle law in the vacuum state.

\chapter{The Free Relation}
\label{ch02:chap02}

\noindent If two noncommutative random variables $a,b$ belonging to an algebra $\mathcal{A}$ are in a generic relation from the algebraic point of view, then $ab=ba$, or $a=b^{2}$, or any algebraic equation for $a$ and $b$ should \textit{not} hold. The generic relation is similar to the relation of generators of a free group. When $a$ and $b$ are in such a generic relation there is no joint distribution measure for $a$ and $b$, but $a+b$ might have a distribution measure. The free relation of $a$ and $b$ defined and studied in this chapter resembles the generic relation of generators of a free structure; however, the formal definition of freeness will go through expectations. The free relation is a very central concept of the analysis of noncommutative random variables, proposed by Voiculescu.

It is tempting to compare the free relation of noncommutative random variables with independence of classical random variables. If $a$ and $b$ are in free relation or stochastically independent, then $\varphi(ab)=\varphi(a)\varphi(b)$. In the case of stochastic independence, $\varphi(abab)=\varphi(a^{2})\varphi(b^{2})$. However, if $a$ and $b$ are in free relation, then $a^{2}b^{2}$ and $abab$ are different random variables with possibly different expectations. The free relation of $a$ and $b$ provides some rules for computing the expectation of a long product with factors $a$ and $b$; that is, the joint distribution of $a$ and $b$ is expressed by the moments of $a$ and $b$. This procedure is different from the simple rule $\varphi(a^{k}b^{m})=\varphi(a^{k})\varphi(b^{m})$ for independent classical variables.

When $a$ and $b$ are free noncommutative random variables, the distribution of $a+b$ defines the free convolution of the distributions of $a$ and $b$. It turns out that the proper way of book-keeping for free convolution is not the method of moments but the use of free cumulants (or $R$-series). The sequence of free cumulants linearizes the free convolution, and in this sense it replaces the logarithm of the Fourier transform in its classical role. The representation of a noncommutative random variable by a formal infinite series of creation operators gives the free cumulants. The transformation between moments and free cumulants is a sort of M\"{o}bius inversion procedure on the lattice of non-crossing partitions.

The central limit theorem of classical probability theory leads to the Gaussian law as the limit distribution. Parallel to the classical central limit theorem, the distribution measure of the standardized sum of noncommutative random variables in free relation converges, and the limit distribution is the semicircle law.

The notions of moments and free cumulants as well as the central limit theorem extend to $k$-tuples of noncommutative random variables. The analogue of the multivariate Gaussian law is the semicircular multivariable. Some noncommutative multivariables are without a direct analogue in the classical theory. The $R$-diagonal pairs could be of this type.

\section{The free product of noncommutative probability spaces}
\label{ch02:sec2.1}

\noindent Given two groups $G_{1}$ and $G_{2}$, their free product\index{free!product of groups} is a quotient of the free group generated by the set $G_{1}\cup G_{2}$. Namely, we take the free group generated by the disjoint union $G_{1}\cup G_{2}$ (identifying both group units) and we divide by all multiplicative relations holding in $G_{1}$ or in $G_{2}$. Even if we start with finite groups, their free product will be infinite. The free product of two unital algebras\index{free!product algebra} is slightly more complicated, but roughly speaking the free product of algebras is the algebra generated by them, with no relations connecting the two algebras except for identification of the unit elements. To define the free product, a state should be specified on each of the algebras. The free product of noncommutative probability spaces will replace the usual product of measure spaces in Voiculescu's theory. The relation of free product to freeness to be defined in the next section is the same as the relation of product measure space to statistical independence.

\begin{example}\label{ch02:exa2.1.1}
Let $G_{1} :=\{e, f\}$ and $G_{2} :=\{e, g\}$ be two copies of the group $\mathbb{Z}_{2}$. ($e$ stands for the unit in both groups.) The free product $G_{1}\star G_{2}$ consists of all alternating finite sequences of $f$ and $g$. The product is juxtaposition followed by the simplification rule that $ff$ and $gg$ should be canceled out.
\end{example}

The \textit{free product of groups}\index{free!group} is an associative operation, and we can take the free product of several groups. For axample, the $n$-fold free product $\mathbb{Z}\star \mathbb{Z}\star\ldots \star \mathbb{Z}$ is the \textit{free group} $\mathbf{F}_{n}$ with $n$ generators. Next we give the formal definition of the free product of noncommutative probability spaces.

Let $(\mathcal{A}_{1}, \varphi_{1})$ and $(\mathcal{A}_{2}, \varphi_{2})$ be two noncommutative probability spaces and let $\mathbf{1}$ denote the unit elements. (The same notation for both units---a sort of identification.) We decompose $\mathcal{A}_{i}$ as $\mathbb{C}\mathbf{1}\oplus \mathcal{A}_{i}^{0}$, where $\mathcal{A}_{i}^{0}:=\{a\in \mathcal{A}_{i}:\varphi_{i}(a)=0\}$. The \textit{free product algebra} is a huge direct sum of tensor products:
\begin{align*}
\mathcal{B} & \ := \ \mathbb{C}\mathbf{1} \oplus \mathcal{A}_{1}^{0}\oplus \mathcal{A}_{2}^{0}\oplus(\mathcal{A}_{1}^{0}\otimes \mathcal{A}_{2}^{0})\oplus(\mathcal{A}_{2}^{0}\otimes \mathcal{A}_{1}^{0})\\
& \qquad \oplus(\mathcal{A}_{1}^{0}\otimes \mathcal{A}_{2}^{0}\otimes \mathcal{A}_{1}^{0})\oplus(\mathcal{A}_{2}^{0}\otimes \mathcal{A}_{1}^{0}\otimes \mathcal{A}_{2}^{0})\oplus(\mathcal{A}_{1}^{0}\otimes \mathcal{A}_{2}^{0}\otimes \mathcal{A}_{1}^{0}\otimes \mathcal{A}_{2}^{0})\oplus\cdots\\
& \ =\ \mathbb{C}\mathbf{1}\ \bigoplus\left\{\mathcal{A}_{i(1)}^{0}\otimes \mathcal{A}_{i(2)}^{0}\otimes\cdots\otimes \mathcal{A}_{i(n)}^{0}:i(k)\in\{1,2\}, 1\leq k\leq n, \right.\\
& \qquad \qquad \qquad \qquad \qquad  \ \ \left. i(k)\neq i(k+1),\,1\leq k\leq n-1,\,n\in \mathbb{N}\right.\} \, .
\end{align*}

Since $\mathcal{A}_{n}^{0}$ is a vector space, this definition makes clear the linear structure of $\mathcal{B}$. If $a_{i}\in \mathcal{A}_{i}^{0}$, then their product\index{free!product} $a_{1}\cdot a_{2}$ is $a_{1}\otimes a_{2}\in \mathcal{A}_{1}^{0}\otimes \mathcal{A}_{2}^{0}\subset \mathcal{B}$. More generally, if
\begin{enumerate}
\item[(i)] $x=a_{1}\otimes a_{2}\otimes\cdots\otimes a_{n}\in \mathcal{B},\,a_{k}\in \mathcal{A}_{i(k)}^{0},\,i(k)\in\{1,2\}\ (1\leq k\leq n)$,

\item[(ii)] $y=b_{1}\otimes b_{2}\otimes\cdots\otimes b_{m}\in \mathcal{B},\,b_{k}\in \mathcal{A}_{j(k)}^{0},\,j(k)\in\{1,2\}\ (1\leq k\leq m)$,

\item[(iii)] $i(n)\neq j(1)$,
\end{enumerate}
then
\begin{equation}
x\cdot y=a_{1}\otimes a_{2}\otimes\cdots\otimes a_{n}\otimes b_{1}\otimes b_{2}\otimes\cdots\otimes b_{m} \, .
\label{ch02:eqn2.1.1}
\end{equation}
If instead of (iii) we have
\begin{enumerate}
\item[(iii)$'$] $i(n)=j(1)$,
\end{enumerate}
then the product cannot be given explicitly, because it is not true in general that $a_{n}b_{1}\in \mathcal{A}_{i(n)}^{0}$. So we write $a_{n}b_{1}=\lambda \mathbf{1}+a$ with $a\in \mathcal{A}_{i(n)}^{0}$, and we set
\begin{align}
x\cdot y & \ = \ a_{1}\otimes a_{2}\otimes\cdots\otimes a_{n-1}\otimes a\otimes b_{2}\otimes\cdots\otimes b_{m} \label{ch02:eqn2.1.2}\\
& \qquad \quad +\lambda a_{1}\otimes a_{2}\otimes\cdots\otimes a_{n-1}\otimes b_{2}\otimes b_{3}\otimes\cdots\otimes b_{m} \, .\notag
\end{align}
This procedure is applied again, since $i(n-1)=j(2)$ (because we have just two indices 1, 2 here). By means of (\ref{ch02:eqn2.1.1}) and (\ref{ch02:eqn2.1.2}) the multiplication is determined on $\mathcal{B}$, and $\mathcal{B}$ becomes an algebra. The involution is also defined on $\mathcal{B}$ whenever both of the $\mathcal{A}_{i}$ are $^{*}$-algebras. The definition is straightforward because $\mathcal{A}_{i}^{0}$ is selfadjoint in $A_{i}$; that is,
\begin{equation*}
x^{*}=a_{n}^{*}\otimes a_{n-1}^{*}\otimes\cdots\otimes a_{1}^{*} \, .
\end{equation*}

Beyond the algebra (or $^{*}$-algebra) structure, we have a natural linear functional $\omega$ on $\mathcal{B}$ which takes the coefficient of $\mathbf{1}$ in the expansion of the element. Namely,
\begin{equation*}
\omega:\lambda \mathbf{1}+\sum a_{1}\otimes a_{2}\otimes\cdots\otimes a_{n}\mapsto\lambda.
\end{equation*}
We call the noncommutative probability space $(\mathcal{B}, \omega)$ the \textit{free product} of the noncommutative probability spaces $(\mathcal{A}_{1}, \varphi_{1})$ and $(\mathcal{A}_{2}, \varphi_{2})$, and use the notation
\begin{equation*}
(\mathcal{B},\omega)=(\mathcal{A}_{1},\varphi_{1})\star(\mathcal{A}_{2},\varphi_{2}) \, .
\end{equation*}
Furthermore, the canonical embedding of $\mathcal{A}_{i}$ into $\mathcal{B}$ is $\iota_{i} : \mathcal{A}_{i}\rightarrow \mathcal{B}$, defined as $a_{i}\mapsto\varphi_{i}(a_{i})\mathbf{1}\oplus(a_{i}-\varphi_{i}(a_{i})\mathbf{1})$, which pulls back $\omega$ to $\varphi_{i}$. The free product of arbitrarily many noncommutative probability spaces may be defined similarly.

\begin{example}\label{ch02:exa2.1.2}
Let $G_{1}$ and $G_{2}$ be two (discrete) groups. The \textit{group algebras} $R(G_{i})$ consist of complex functions on $G_{i}$ with finite support, i.e. finite linear combinations $\sum_{h}\lambda_{i}(h)h\,(\lambda_{i}(h)\in \mathbb{C}, \, h\in G_{i}), \, i=1,2$. Writing $e$ for both group units, we set
\begin{equation*}
\varphi_{i}\left(\sum_{h}\lambda_{i}(h)h\right):=\lambda_{i}(e) \, ,
\end{equation*}
and we have two noncommutative probability spaces $(R(G_{i}), \varphi_{i}),\,i=1,2$. Then $R(G_{i})^{0}$ is linearly spanned by the group elements different from $e$. We can see that $R(G_{i})^{0}$ is not closed under multiplication. The free product $(R(G_{1}), \varphi_{1})\star (R(G_{2}), \varphi_{2})$ is $R(G_{1}\star G_{2})$ endowed with a similarly defined state $\varphi$.
\end{example}

\begin{example}\label{ch02:exa2.1.3}
Let $\mathcal{A}_{i}:=\mathbb{C}\langle X_{i}\rangle$ be \textit{polynomial algebras}.\index{polynomial algebras}\index{group!algebra}\index{free!relation} Define the state $\varphi_{i}$ to be the constant term of a polynomial, $i =1,2$. Then the construction of the free product is particularly simple, because $\mathcal{A}_{i}^{0}$ is not only a linear space but closed under multiplication. $(\mathcal{A}_{1}, \varphi_{1})\star(\mathcal{A}_{2}, \varphi_{2})$ will be the polynomial algebra $\mathbb{C}\langle X_{1}, X_{2}\rangle $ endowed with the similarly defined state.
\end{example}

Here we have constructed the free product in the category of algebras or $^{*}$-algebras. When one starts with $C^{*}$-algebras or von Neumann algebras, the above described free product should be endowed with a norm and a completion procedure should be carried out. There is another problem concerning positivity. When $\varphi_{1}$ and $\varphi_{2}$ are states, we want the free product functional $\omega$ to be positive. We do not go into the details here, but free product representations and free product von Neumann algebras will be explained in Sec.~\ref{ch07:sec7.1}.

\section{The free relation}
\label{ch02:sec2.2}

\noindent Let us recall that in the free product $(\mathcal{B}, \omega)=(\mathcal{A}_{1}, \varphi_{1})\star(\mathcal{A}_{2}, \varphi_{2})$ of noncommutative probability spaces the state $\omega$ is determined by the condition that it vanishes on all products $a_{1}a_{2}\cdots a_{n}$ such that $a_{k}\in \mathcal{A}_{i(k)}, \, i(k)\neq i(k+1)$, and all factors have $0$ expectation. The concept of \textit{free relation} emerges from this observation. The free relation, or \textit{freeness}, is a fundamental concept throughout this monograph.

Let $(\mathcal{A}, \varphi)$ be a noncommutative probability space and let $\mathcal{A}_{i}$ be subalgebras of $\mathcal{A}\ (i\in I)$. We say that the family $(\mathcal{A}_{i})_{i\in I}$ is \textit{in free relation} (or \textit{free}) with respect to $\varphi$ if, for every $n\in \mathbb{N}$ and $i(1), \ldots, i(n)\in I$ such that $i(k)\neq i(k+1)\ (1\leq k\leq n-1)$,
\begin{equation}
\varphi(a_{1}a_{2}\cdots a_{n})=0 \quad \mathrm{whenever} \quad a_{k}\in \mathcal{A}_{i(k)},\ \varphi(a_{k})=0,\ 1\leq k\leq n.
\label{ch02:eqn2.2.1}
\end{equation}
(We stress that in this definition non-neighboring $i(k)$'s are allowed to be equal.) The free relation can be formulated equivalently in terms of the free product. Let $(\mathcal{B}, \omega)=\star_{i\in I}(\mathcal{A}_{i}, \varphi|_{\mathcal{A}_{i}})$ and let $\iota_{i}:\mathcal{A}_{i}\rightarrow \mathcal{B}$ be the canonical embedding. Then the above definition is equivalent to
\begin{align*}
&\varphi(a_{1}a_{2}\cdots a_{n})=\omega(\iota_{i(1)}(a_{1})\iota_{i(2)}(a_{2})\cdots\iota_{i(n)}(a_{n})) \quad \mathrm{for\ every}\\
& \qquad \quad \ a_{k}\in \mathcal{A}_{i(k)},\ i(k)\in I,\ i(k)\neq i(k+1),\ 1\leq k\leq n-1,\ n\in \mathbb{N}.
\end{align*}

By saying that two families of noncommutative random variables $\{a_{i}:i\in I\}$ and $\{b_{j}:j\in J\}$ in $\mathcal{A}$ are in free relation, we mean that the subalgebra generated by $\{a_{i}:i\in I\}$ and that generated by $\{b_{j}:j\in J\}$ are free (with respect to $\varphi$). When $\mathcal{A}$ is a $^{*}$-algebra, the freeness of $\{a_{i}\}$ and $\{b_{j}\}$ is usually defined as above with the generated $^{*}$-subalgebras, and the term $^{*}$-\textit{freeness}\index{${}^{*}$-freeness} is sometimes referred to. The freeness of several families of random variables is defined similarly.

\begin{example}\label{ch02:exa2.2.1}
Let $h_{1}, h_{2}\in \mathcal{H}$, and let $\ell(h_{i})$ be the corresponding left creation operators on $\mathcal{F}(\mathcal{H})$. Then $\{\ell(h_{1}), \ell(h_{1})^{*}\}$ and $\{\ell(h_{2}), \ell(h_{2})^{*}\}$ are free with respect to the vacuum state if and only if $\langle h_{1}, h_{2}\rangle =0$.

First, if $\ell_{1} :=\ell(h_{1})$ and $\ell_{2} :=\ell(h_{2})$ are free, then $\langle h_{1},  h_{2}\rangle=\langle \ell_{2}^{*}\ell_{1}\Phi, \Phi\rangle=0$ since $\langle \ell_{1}\Phi, \Phi\rangle=\langle\ell_{2}\Phi, \Phi\rangle=0$.

To prove the converse we assume $\langle h_{1}, h_{2}\rangle=0$ and hence $\ell_{2}^{*}\ell_{1}=0$, and we want to verify directly that (\ref{ch02:eqn2.2.1}) holds. Since any monomial $p(\ell_{i}, \ell_{i}^{*})$ of $\ell_{i}$ and $\ell_{i}^{*}$ has $0$ expectation unless it reduces to $c\mathbf{1}$, we may restrict ourselves to the case when the $a_{k}$'s are monomials. Due to the relation $\ell_{i}^{*}\ell_{i}=\mathbf{1}$, we may assume that $a_{k}$ is of the form $\ell_{i}^{n}\ell_{i}^{*m}, \, n+m>0$. In this way we have reduced the statement to the verification of the relation
\begin{equation*}
\langle\cdots(\ell_{1}^{n(3)}\ell_{1}^{*m(3)})(\ell_{2}^{n(2)}\ell_{2}^{*m(2)})(\ell_{1}^{n(1)}\ell_{1}^{*m(1)})\Phi,\Phi\rangle=0
\end{equation*}
when $n(k)+m(k)>0$. It is elementary to analyze that this holds.

More generally, if $\{h_{i}:i\in I\}$ is an orthogonal family in the Hilbert space $\mathcal{H}$, then the families
\begin{equation*}
\{\ell(h_{i}),\ell(h_{i})^{*}\} \qquad (i\in I)
\end{equation*}
are in free relation with respect to the vacuum expectation. In particular, for any polynomials $p_{i}$ the operators $\ell(h_{i})^{*}+p_{i}(\ell(h_{i}))\ (i\in I)$ are $^{*}$-free.
\end{example}

Let $(\mathcal{A}_{i})_{i\in I}$ be a family of $^{*}$-subalgebras of $\mathcal{A}$ and assume that they are free with respect to a state $\varphi$ on $\mathcal{A}$. We need methods for the evaluation of $\varphi(a_{1}a_{2}\cdots a_{n})$ when $a_{k}\in \mathcal{A}_{i(k)}\ (i(1), \ldots, i(n) \in I)$ and $i(k)\neq i(k+1)$. We write $a_{k}=\varphi(a_{k})\mathbf{1}+a_{k}^{0}$, and
\begin{align}
& \varphi(a_{1}a_{2}\cdots a_{n}) \notag \\
& \qquad =\sum\varphi(a_{\pi(1)})\varphi(a_{\pi(2)})\cdots\varphi(a_{\pi(m)})\varphi(a_{\sigma(1)}^{0}a_{\sigma(2)}^{0}\cdots a_{\sigma(n-m)}^{0}) \label{ch02:eqn2.2.2}
\end{align}
is an identity if the summation is over all ordered partitions $(\{\pi(1), \pi(2), \ldots, \pi(m)\}, \{\sigma(1), \sigma(2), \ldots, \sigma(n-m)\})$ of the set $\{1, 2, \ldots, n\}$ and $0\leq m\leq n$. The term corresponding to $m=0$ vanishes, due to the assumption of freeness. So (\ref{ch02:eqn2.2.2}) gives a recursion. On its right-hand side $\varphi$ is on products of length less than $n$. One can obtain a slightly different formula by expanding
\begin{equation*}
\varphi((a_{1}-\varphi(a_{1})\mathbf{1})(a_{2}-\varphi(a_{2})\mathbf{1})\cdots(a_{n}-\varphi(a_{n})\mathbf{1})),
\end{equation*}
which vanishes under the assumption of the free relation. Hence
\begin{align}
\varphi(a_{1}a_{2}\cdots a_{n})=\sum_{r=1}^{n}\, \sum_{1\leq k_{1}<\ldots<k_{r}\leq n} & \quad (-1)^{r+1}\varphi(a_{k_{1}})\cdots\varphi(a_{k_{r}}) \label{ch02:eqn2.2.3}\\
& \quad \times\varphi(a_{1}\cdots \hat{a}_{k_{1}}\cdots\hat{a}_{k_{r}}\cdots a_{n})\, ,\notag
\end{align}
where \, $\hat{}$ \, indicates terms that are omitted.

Let $a$ and $b$ be in free relation with respect to $\varphi$. One can understand the freeness as a rule for computing the expectation of products of $a$ and $b$. From $\varphi((a-\varphi(a)\mathbf{1})(b-\varphi(b)\mathbf{1}))\, -\, 0$ we obtain $\varphi(ab)=\varphi(a)\varphi(b)$. The factorization of the expectation of free variables\index{noncommutative!random variables, independent} may remind us of independence\index{independence} in probability theory. However, the freeness behaves very differently for longer products. By means of the above formulas one computes
\begin{equation}
\varphi(abab)=\varphi(a^{2})\varphi(b)^{2}+\varphi(a)^{2}\varphi(b^{2})-\varphi(a)^{2}\varphi(b)^{2} \, ,
\label{ch02:eqn2.2.4}
\end{equation}
and
\begin{equation*}
\varphi(ab^{2}a)=\varphi(a^{2})\varphi(b^{2}) \, .
\end{equation*}
The latter identity shows that the free relation precludes commutativity. Indeed, when $ab=ba$ it follows that
\begin{equation*}
\varphi((a-\varphi(a)\mathbf{1})^{2})\varphi((b-\varphi(b)\mathbf{1})^{2})=0,
\end{equation*}
and the variance of $a$ or that of $b$ must vanish. Formulating this fact in the language of probability theory, we can say that $a$ or $b$ is constant ``with probability one''.

Let $\mathcal{A}_{1}$ and $\mathcal{A}_{2}$ be two $^{*}$-subalgebras of $\mathcal{A}$. In the algebraic language used here we would say that $\mathcal{A}_{1}$ and $\mathcal{A}_{2}$ are \textit{independent} with respect to a state $\varphi$ on $\mathcal{A}$, if $a_{1}a_{2}=a_{2}a_{1}$ and $\varphi(a_{1}a_{2})=\varphi(a_{1})\varphi(a_{2})$ whenever $a_{1}\in \mathcal{A}_{1}$ and $a_{2}\in \mathcal{A}_{2}$. The independence allows a simple computation of the joint distribution of $a_{1}$ and $a_{2}$. The same holds true when $\mathcal{A}_{1}$ and $\mathcal{A}_{2}$ are in free relation. However, for instance, the computation rule (\ref{ch02:eqn2.2.4}) is more complicated. On the other hand, independence and the free relation may hold at the same time only in trivial cases, as we discussed above.

In the next example (\ref{ch02:eqn2.2.4}) is extended.

\begin{example}\label{ch02:exa2.2.2}
Assume that $\{a_{1}, a_{2}, a_{3}\}$ and $\{b_{1}, b_{2}\}$ are in free relation with respect to a state $\varphi$. Then
\begin{align*}
\varphi(a_{1}b_{1}a_{2}b_{2}a_{3}) & \ = \ \varphi(a_{1}a_{3})\varphi(a_{2})\varphi(b_{1}b_{2})+\varphi(a_{1}a_{2}a_{3})\varphi(b_{1})\varphi(b_{2})\\
& \qquad \quad -\varphi(a_{1}a_{3})\varphi(a_{2})\varphi(b_{1})\varphi(b_{2}) \, .
\end{align*}
\end{example}

\begin{lemma}\label{ch02:lem2.2.3}
Let $(\mathcal{A}_{i})_{i\in I}$ be a family of $^{*}$-subalgebras of $\mathcal{A}$ and assume that they are free with respect to a state $\varphi$ on $\mathcal{A}$. If $a_{k}\in \mathcal{A}_{i(k)},\ i(1), \ldots, i(n) \in I,\ i(k)\neq i(k+1)$, and there is a $j$ such that $i(k)\neq i(j)$ for $k\neq j$, then
\begin{equation*}
\varphi(a_{1}a_{2}\cdots a_{n})=\varphi(a_{j})\varphi(a_{1}a_{2}\cdots a_{j-1}a_{j+1}\cdots a_{n}) \, .
\end{equation*}
\end{lemma}

\begin{proof2}
First we assume that $\varphi(a_{j})=0$ and show that $\varphi(a_{1}a_{2}\cdots a_{n})=0$. We use induction on $n$. The case $n=1$ is trivial. In the induction step we benefit from (\ref{ch02:eqn2.2.2}) and show that all summands on the right-hand side vanish. Indeed, $j\in\{\pi(1), \pi(2), \ldots, \pi(m)\}$, or $j\in\{\sigma(1), \sigma(2), \ldots, \sigma(n-m)\}$. In the first case we refer to $\varphi(a_{j})=0$; in the second case the induction hypothesis is referred to.

When $\varphi(a_{j})\neq 0$ we have
\begin{align*}
\varphi(a_{1}a_{2}\cdots a_{n}) & \ = \ \varphi(a_{j})\varphi(a_{1}a_{2}\cdots a_{j-1}a_{j+1}\cdots a_{n})\\
& \qquad \quad +\varphi(a_{1}a_{2}\cdots a_{j-1}(a_{j}-\varphi(a_{j})\mathbf{1})a_{j+1}\cdots a_{n}),
\end{align*}
and the second term on the right-hand side vanishes due to the previous case.
\end{proof2}

For example, a repeated application of Lemma~\ref{ch02:lem2.2.3} shows the following:

\begin{example}\label{ch02:exa2.2.4}
If $(a, b, c, d)$ is a free family of noncommutative random variables in $(\mathcal{A}, \varphi)$, then
\begin{equation*}
\varphi(cbaabcdc)=\varphi(a^{2})\varphi(b^{2})\varphi(d)\varphi(c^{3}) \, .
\end{equation*}
\end{example}

\begin{lemma}\label{ch02:lem2.2.5}
Let $(\mathcal{A}_{i})_{i\in I}$ be a family of $^{*}$-subalgebras of $\mathcal{A}$, and assume that they are free with respect to a state $\varphi$ on $\mathcal{A}$. If $x=a_{1}a_{2}\cdots a_{n}$ is a product such that $a_{k}\in \mathcal{A}_{i(k)}\ (i(k)\in I),\ i(k)\neq i(k+1)$, and $\varphi(a_{k})=0$, and if $y=b_{1}b_{2}\cdots b_{m}$ is such that $b_{k}\in \mathcal{A}_{j(k)}\ (j(k)\in I),\ j(k)\neq j(k+1)$, and $\varphi(b_{k})=0$, then
\begin{equation*}
\varphi(y^{*}x)=\delta_{nm}\delta_{i(1)j(1)}\delta_{i(2)j(2)}\cdots\delta_{i(n)j(n)}\varphi(b_{1}^{*}a_{1})\varphi(b_{2}^{*}a_{2})\cdots\varphi(b_{n}^{*}a_{n}) \, .
\end{equation*}
\end{lemma}

\begin{proof2}
We apply induction on $n+m$. Write
\begin{equation*}
\varphi(y^{*}x)=\varphi(b_{m}^{*}b_{m-1}^{*}\cdots b_{1}^{*}a_{1}a_{2}\cdots a_{n}) \, .
\end{equation*}
When $b_{1}$ and $a_{1}$ are in different subalgebras, that is, $i(1)\neq j(1)$, the very definition of the free relation gives $\varphi(y^{*}x)=0$, and the statement holds true. When they are in the same subalgebra, we write
\begin{equation*}
b_{1}^{*}a_{1}=\varphi(b_{1}^{*}a_{1})\mathbf{1}+(b_{1}^{*}a_{1}-\varphi(b_{1}^{*}a_{1})\mathbf{1})
\end{equation*}
and conclude that
\begin{equation*}
\varphi(y^{*}x)=\varphi(b_{1}^{*}a_{1})\varphi(b_{m}^{*}b_{m-1}^{*}\cdots b_{2}^{*}a_{2}\cdots a_{n}) \, .
\end{equation*}
Now the induction hypothesis works.
\end{proof2}

\begin{proposition}\label{ch02:pro2.2.6}
Let $(\mathcal{A}, \varphi)=\star_{i\in I}(\mathcal{A}_{i}, \varphi_{i})$. If $\varphi_{i}$ is a tracial state, that is, $\varphi_{i}(ab)=\varphi_{i}(ba)$ for every $i\in I$, then $\varphi$ is tracial.
\end{proposition}

\begin{proof2}
Let $x$ and $y$ be as in Lemma~\ref{ch02:lem2.2.5}. It is sufficient to prove that $\varphi(y^{*}x)= \varphi(xy^{*})$. This follows from Lemma~\ref{ch02:lem2.2.5}, because $\varphi(b_{i}^{*}a_{i})=\varphi(a_{i}b_{i}^{*})$ holds by assumption.
\end{proof2}

This proposition has a consequence. Dealing with a family of selfadjoint operators which are in free relation with respect to a state $\varphi$ of a $C^{*}$-algebra, we may always assume that $\varphi$ is tracial. Indeed, the joint distribution of our noncommutative random variables does not change if we pass to the free product of the commutative $C^{*}$-algebras generated by each of the selfadjoint operators. According to the proposition, we arrive at a tracial state in this way.

\begin{example}\label{ch02:exa2.2.7}
On the full Fock space $\mathcal{F}(\mathcal{H})$, consider the von Neumann algebra $\mathcal{M}$ generated by the operators $\ell(h)^{*}+\ell(h),\ h\in \mathcal{H}$. Then the vacuum expectation $\varphi$ is a tracial state on $\mathcal{M}$.

We may assume that $h$ runs over an orthogonal basis in $\mathcal{H}$. Then we have the freeness of the family $\ell(h)+\ell(h)^{*}$ of selfadjoint operators, and the previous proposition yields our statement.
\end{example}

\begin{proposition}\label{ch02:pro2.2.8}
Let $(\mathcal{A}_{i})_{i\in I}$ be a free family of $^{*}$-subalgebras in $(\mathcal{A}, \varphi)$. If $I= \bigcup_{j\in J}I_{j}$ is a partition and $\mathcal{B}_{j}$ is the $^{*}$-subalgebra generated by $\bigcup_{i\in I_{j}}\mathcal{A}_{i}$, then $(\mathcal{B}_{j})_{j\in J}$ is free.
\end{proposition}

\begin{proof2}
Let $\mathcal{A}_{i}^{0}:=\{a\in \mathcal{A}_{i}:\varphi(a)=0\}$ and $\mathcal{B}_{j}^{0} :=\{b\in \mathcal{B}_{j}:\varphi(b)=0\}$. It is easy to see that each element in $\mathcal{B}_{j}^{0}$ is written as a finite sum of products $a_{1}a_{2}\cdots a_{n}$ of $a_{k}\in \mathcal{A}_{i_{k}}^{0},\ i_{k}\in I_{j}$, with $i_{1}\neq i_{2}\neq\ldots\neq i_{n}$. This fact immediately yields the conclusion.
\end{proof2}

\begin{example}\label{ch02:exa2.2.9}
Let $I_{1}$ and $I_{2}$ be two disjoint intervals in $\mathbb{R}^{+}$, and let $\mathcal{H}$ be $L^{2}(\mathbb{R}^{+})$. The $^{*}$-algebra generated by $\{\ell(f):f\in L^{2}(\mathbb{R}), \ \mathrm{supp}\, f\subset I_{j}\}$ is denoted by $\mathcal{A}_{j}, j=1,2$. Then the algebras $\mathcal{A}_{1}$ and $\mathcal{A}_{2}$ are in free relation in $B(\mathcal{F}(L^{2}(\mathbb{R})))$ with respect to the vacuum state.

Although a direct proof is possible, we can choose an orthogonal basis $h_{i}^{(j)} (i=1,2, \ldots)$ in $L^{2}(I_{j})$. For the orthogonal family $\{h_{i}^{(1)}:i\geq 1\}\cup\{h_{i}^{(2)}:i\geq 1\}$ we apply Example~\ref{ch02:exa2.2.1}. Then we conclude from Proposition~\ref{ch02:pro2.2.8} that the families $\{\ell(h_{i}^{(1)}):i\geq 1\}$ and $\{\ell(h_{i}^{(2)}):i\geq 1\}$ are $^{*}$-free. (Stricly speaking, an additional continuity argument is still to be applied to complete our statement.)
\end{example}

\begin{example}\label{ch02:exa2.2.10}
Let $\mathcal{H} :=L^{2}(\mathbb{R}^{+})$ and consider the operators $X_{t}:=\ell(\chi_{[0,t)})^{*}+ \ell(\chi_{[0,t)})$ on the full Fock space $\mathcal{F}(L^{2}(\mathbb{R}^{+}))$ as in Example~\ref{ch01:exa1.2.8}. It follows from Example~\ref{ch02:exa2.2.9} that for $0\leq t(1)<t(2)\leq s(1)<s(2)$ the increments $X_{t(2)}-X_{t(1)}$ and $X_{s(2)}-X_{s(1)}$ are in free relation with respect to the vacuum expectation $\varphi.\ X_{t}$ is an example of the \textit{free Brownian motion}.\index{free!Brownian motion} Remember that $\varphi$ is tracial on the algebra generated by the $X_{t}$'s.
\end{example}

In the next example matrices built from creation operators are considered.

\begin{example}\label{ch02:exa2.2.11}
Let $C^{*}(\ell(\mathcal{H}))$ denote the $C^{*}$-algebra generated by left creation operators on the full Fock space $\mathcal{F}(\mathcal{H})$. Let $\{h_{ij}^{k}:1\leq i, j\leq n,\,k=1,2,\ldots\}$ be an orthonormal family of vectors in the Hilbert space $\mathcal{H}$. We consider the algebra $C^{*}(\ell(\mathcal{H}))\otimes M_{n}(\mathbb{C})$, the tensor product of $C^{*}(\ell(\mathcal{H}))$ and the $n\times n$ matrices. In order to view this algebra as a noncommutative probability space, we consider a product state $\psi=\phi \otimes\rho$, where $\phi$ is the vacuum expectation in $\mathcal{F}(\mathcal{H})$ and $\rho$ is a state on $M_{n}(\mathbb{C})$ having a diagonal density $\mathbf{Diag}(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})$. Put
\begin{equation*}
L_{k}:=\sum_{i,j=1}^{n}\sqrt{\lambda_{i}}\ell(h_{ij}^{k})\otimes e_{ij} \, ,
\end{equation*}
where $(e_{ij})$ is the usual system of matrix units in $M_{n}(\mathbb{C})$. We are going to show that $\mathbb{C}\mathbf{1}_{\mathcal{H}}\otimes M_{n}(\mathbb{C})$ and the $^{*}$-algebra generated by $L_{1}, L_{2}, \ldots$ are free in the algebra $C^{*}(\ell(\mathcal{H}))\otimes M_{n}(\mathbb{C})$ with respect to the state $\psi$.

The operators $L_{k}$ are regarded as $n\times n$ matrices with operator entries. For example, when $n=2$ we have
\begin{equation*}
L_{k}=\left[\begin{array}{ll}
\sqrt{\lambda_{1}}\ell(h_{11}^{k}) & \sqrt{\lambda_{1}}\ell(h_{12}^{k})\\
\sqrt{\lambda_{2}}\ell(h_{21}^{k}) & \sqrt{\lambda_{2}}\ell(h_{22}^{k})
\end{array}\right].
\end{equation*}
It is easy to see that $L_{s}^{*}L_{r}=\delta_{sr}\mathbf{1}$ by using the assumed orthogonality relations. If $w=L_{i(1)}L_{i(2)}\cdots L_{i(r)}L_{j(1)}^{*}L_{j(2)}^{*}\cdots L_{j(s)}^{*}$ is viewed as a matrix, then all of its entries have $0$ expectation under $\phi$; hence $\psi(w)=0$. This says that the joint distribution of the operators $L_{1}, L_{2}, \ldots$ with respect to $\psi$ is the same as that of $\ell(f_{1}), \ell(f_{2}), \ldots$ in a full Fock space with respect to the vacuum expectation, where the $f_{k}$'s are orthonormal vectors.

In order to show the above stated free relation, we have to prove that $\psi(W)=0$ when
\begin{equation*}
W=a_{0}w_{1}a_{1}\cdots w_{n}a_{n} \, ,
\end{equation*}
where $a_{k}=\mathbf{1}_{\mathcal{H}}\otimes e_{pq}-\delta_{pq}\lambda_{p}\mathbf{1}$ ($a_{0}, a_{n}$ may be $\mathbf{1}$) and $w_{k}$ is of the form of the above $w$. If $W$ does not contain a substring $L_{s}^{*}a_{k}L_{r}$, then all of its entries have zero expectation under $\phi$, and $\psi(W)=0$ holds. However, a simple computation yields that $L_{s}^{*}a_{k}L_{r}=0$, so $\psi(W)=0$ in any case.
\end{example}

\section{The free central limit theorem}
\label{ch02:sec2.3}

\noindent The classical central limit theorem says that if $a_{1}, a_{2}, \ldots$ are independent and identically distributed random variables with $E(a_{i})=0$ and $E(a_{i}^{2})=1$, then the distribution of
\begin{equation}
\frac{a_{1}+a_{2}+\cdots+a_{n}}{\sqrt{n}}
\label{ch02:eqn2.3.1}
\end{equation}
tends to the standard Gaussian law. The free analogue of this classical result is due to Voiculescu: When the assumption of independence is replaced by the free relation of the noncommutative random variables $a_{1}, a_{2}, \ldots$, the limit distribution of (\ref{ch02:eqn2.3.1}) is the semicircle law (under certain conditions).

The free central limit theorem is interesting in its own right, however, aside from the fact that the combinatorial proof gives an occasion to introduce the concept of non-crossing partition which is fundamental in the combinatorial approach to freeness. In order to show this result, we first obtain the moments of the semicircle law in a combinatorial form.

A partition $\mathcal{V}=\{V_{1}, V_{2}, \ldots, V_{s}\}$ of the set $[k]:=\{1,2, \ldots, k\}$ consists of nonempty, pairwise disjoint blocks $V_{1}, V_{2}, \ldots, V_{s}$ satisfying $\bigcup_{i=1}^{s}V_{i}=[k]$. For the sake of definiteness, we assume that the blocks are indexed in increasing order of their minimum elements, and elements of each block are increasingly ordered as well. For example, $1, 5, 7/2, 6/3, 8, 10/4, 9$ indicates a partition of [\citen{bib10}]. This is not the only way to represent partitions. We may plot the numbers $1, 2,\ldots, k$ horizontally, and successive elements of the same block are joined by arcs.

$\mathcal{V}$ is called a \textit{non-crossing partition}\index{non-crossing!partition}\index{partition!non-crossing} if for $V_{i}=\{v_{1}, v_{2}, \ldots, v_{p}\}$ and $V_{j} = \{w_{1}, w_{2}, \ldots, w_{q}\}$ we have $w_{m}<v_{1}<w_{m+1}$ if and only if $w_{m}<v_{p}<w_{m+1} (m=1,2, \ldots, q-1)$ . Non-crossing partitions are represented graphically, as shown in the figure.

\begin{figure}
\includegraphics{chap02-vend-scan-01.eps}
\caption{Representation of the non-crossing partition 1, 5, 7/2, 3,
4/6.} \label{ch02:fig2.1}
\end{figure}

Another way to represent partitions is the use of the \textit{restricted growth function}.\index{growth function, restricted}\index{restricted growth function} Let $\mathcal{V}$ be a partition of $[k]$. The restricted growth function $\omega : [k]\rightarrow[k]$ associates to $i$ the index of the block of $\mathcal{V}$ which contains $i$. For example, the restricted growth function of the partition 1, 5, 7/2, 3, 4/6 is 1, 2, 2, 2, 1, 3, 1. The number of crossings in a partition is the number of subwords in the restricted growth function which are of the form $a, b, a, b$ with $a<b$. In the previous example, there is no subword in that form and hence the partition is non-crossing. On the other hand, the partition given by the restricted growth function 1, 2, 1, 2, 3, 3, 2 has two crossings due to the subword 1, 2, 1, 2.

\begin{lemma}\label{ch02:lem2.3.1}
The number of non-crossing pair partitions of $[k]$ equals the $k$th moment $m_{k}$ of the standard semicircle law $w_{2}$ given in \emph{(\ref{ch01:eqn1.1.6})}.
\end{lemma}

\begin{proof2}
The case of odd $k$ is trivial, so we take $2k$ instead of $k$. Let $s_{k}$ be the number of non-crossing pair partitions of the set $[2k]$. The element 1 should be paired with an even element, say $2m$. The number of partitions containing $\{1, 2m\}$ is $s_{m-1}s_{k-m}$. Hence
\begin{equation}
s_{k}=s_{k-1}s_{0}+s_{k-2}s_{1}+\cdots+s_{0}s_{k-1}
\label{ch02:eqn2.3.2}
\end{equation}
$(s_{0}:=1)$. What we have to prove is that $s_{k}$ equals the Catalan number $c_{k}$ in (\ref{ch01:eqn1.1.7}). Due to the Taylor expansion we have
\begin{equation}
g(x) :=\frac{1}{2}(1-\sqrt{1-4x})=\sum_{k=0}^{\infty}\frac{x^{k+1}}{k+1} \left(\begin{array}{c}
2k\\
k\\
\end{array}\right),
\label{ch02:eqn2.3.3}
\end{equation}
which is a kind of generator function of the even moments of the semicircle law. The functional equation $g(x)^{2}=g(x)-x$ shows that the coefficients of the Taylor expansion must satisfy the recursion (\ref{ch02:eqn2.3.2}), and we conclude that $s_{k}=c_{k}$.
\end{proof2}

\begin{theorem}\label{ch02:the2.3.2}
Let $a_{1}, a_{2}, \ldots$ be a free sequence in the noncommutative probability space $(\mathcal{A}, \varphi)$. Assume that $\varphi(a_{i})=0$ and $\varphi(a_{i}^{2})=1$. Furthermore, assume that $\sup_{i}|\varphi(a_{i}^{k})|<+\infty$ for all $k\in \mathbb{N}$. Then the sequence $(a_{1}+a_{2}+\cdots+a_{n})/\sqrt{n}$ converges in distribution to the standard semicircle law $w_{2}$. That is, for every $k\in \mathbb{N}$ the following limit relation holds:
\begin{equation*}
\varphi\left(\frac{(a_{1}+a_{2}+\cdots+a_{n})^{k}}{n^{k/2}}\right)\rightarrow\left\{\begin{array}{ll}
0 & \mathit{if}\ k\ is\ odd,\\
\dfrac{1}{1+k/2}\left(\begin{array}{c}k\\ k/2 \\ \end{array} \right) & \mathit{if}\ k\ is\ even.
\end{array}\right.
\end{equation*}
\end{theorem}

\begin{proof2}
We have
\begin{equation*}
\varphi((a_{1}+a_{2}+\cdots+a_{n})^{k})=\sum\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)}),
\end{equation*}
and we are going to group the summands using partitions of $[k]=\{1,2, \ldots, k\}$. Fix a summand $\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})$, and say that $u\sim v$ if $i(u)=i(v)$. This equivalence relation determines a partition $\mathcal{V}=\{V_{1}, V_{2}, \ldots, V_{s}\}$, where $s$ depends on the summand we chose. ($s$ is the number of different factors in the product $a_{i(1)}a_{i(2)}\cdots a_{i(k)}$.) Many different terms give rise to the same partition. More precisely, a partition $\mathcal{V}=\{V_{1}, V_{2}, \ldots, V_{s}\}$ comes from $n(n-1)\cdots(n-s+1)$ terms. We write
\begin{equation}
\varphi\left(\frac{(a_{1}+a_{2}+\cdots+a_{n})^{k}}{n^{k/2}}\right)=\sum_{s=1}^{k}\sum_{\mathcal{V}}\sum\frac{\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})}{n^{k/2}} \, ,
\label{ch02:eqn2.3.4}
\end{equation}
where the second summation is over all partitions $\mathcal{V}=\{V_{1}, V_{2}, \ldots, V_{s}\}$ and the third is over all terms determining this partition in the above scheme.

By repeated use of (\ref{ch02:eqn2.2.3}) it is easy to see that $\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})$ is written as a polynomial in the variables $\varphi(a_{i}^{m}) \ (i\in\{i(1), \ldots, i(k)\},\ 1\leq m\leq k)$, and the polynomial depends only on the partition $\mathcal{V}$ corresponding to $i(1), \ldots, i(k)$. So the assumption of bounded moments implies that for each $k\in \mathbb{N}$ there is a bound $C_{k}$ such that $|\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})|\leq C_{k}$ for all choices of $i(1), \ldots, i(k)\in \mathbb{N}$.

We split the summation over $s$ in (\ref{ch02:eqn2.3.4}) into three parts: $s<k/2, \, s>k/2$, and $s=k/2$. For $s<k/2$,
\begin{equation*}
\left|\sum_{s<k/2}\sum_{\mathcal{V}}\sum\frac{\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})}{n^{k/2}}\right|\leq\sum_{s<k/2}\sum_{\mathcal{V}}\frac{n(n-1)\cdots(n-s+1)}{n^{k/2}}\, C_{k},
\end{equation*}
and this tends to $0$ as $ n\rightarrow\infty$. For $s>k/2$,
\begin{equation*}
\sum_{s>k/2}\sum_{\mathcal{V}}\sum\frac{\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})}{n^{k/2}}=0,
\end{equation*}
because in this case at least one of the $V_{i}$'s is a singleton and due to Lemma~\ref{ch02:lem2.2.3} we have $\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})=0$. If $k$ is odd, then $s<k/2$ and $s>k/2$ cover all possibilities and the limit of (\ref{ch02:eqn2.3.4}) is $0$. For an even $k$ the term
\begin{equation*}
\sum_{\mathcal{V}:s=k/2}\sum\frac{\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})}{n^{k/2}}
\end{equation*}
contributes to the limit. The summation $\sum_{\mathcal{V}:s=k/2}$ is over all pair partitions of $[k]$. Now let $i(1), i(2), \ldots, i(k)$ be a string of indices defining a pair partition $\mathcal{V}$ of $[k]$. By mathematical induction on $k$ we show that if $\mathcal{V}$ is crossing then $\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})=0$, and if $\mathcal{V}$ is non-crossing then
\begin{equation*}
\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})=\varphi(a_{j(1)}^{2})\varphi(a_{j(2)}^{2})\cdots\varphi(a_{j(k/2)}^{2}) \, ,
\end{equation*}
where $j(1), j(2), \ldots, j(k/2)$ are different indices appearing in $i(1), i(2), \ldots, i(k)$. If there is a $j$ such that $i(j)=i(j+1)$, then Lemma~\ref{ch02:lem2.2.3} implies that
\begin{equation*}
\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})=\varphi(a_{i(j)}^{2})\varphi(a_{i(1)}\cdots a_{i(j-1)}a_{i(j+2)}\cdots a_{i(k)}) \, ,
\end{equation*}
and the induction hypothesis works because the pair partition of $\{1,\ldots, j-1, j+ 2, \ldots, k\}$ defined by $i(1), \ldots, i(j-1), i(j+2), \ldots, i(k)$ is crossing if and only if $\mathcal{V}$ is crossing. If there is no $j$ such that $i(j)=i(j+1)$, then the very definition of the freeness gives $\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(k)})=0$, and $\mathcal{V}$ is crossing in this case.

What we proved above is that the only contribution to the limit of (\ref{ch02:eqn2.3.4}) is given as
\begin{equation}
\sum\sum_{(j(1),\ldots,j(k/2))}\frac{\varphi(a_{j(1)}^{2})\varphi(a_{j(2)}^{2})\cdots\varphi(a_{j(k/2)}^{2})}{n^{k/2}} \, ,
\label{ch02:eqn2.3.5}
\end{equation}
where the first summation is over non-crossing partitions of $[k]$ and the second is over all ordered $(j(1), j(2), \ldots, j(k/2))$ whose elements are different numbers from $\{1, 2, \ldots, n\}$. Finally, the assumption $\varphi(a_{i}^{2})=1$ is used to conclude that the limit of (\ref{ch02:eqn2.3.5}) is the number of non-crossing pair partitions of $[k]$, which is exactly the $k$th moment of the standard semicircle law due to Lemma~\ref{ch02:lem2.3.1}.
\end{proof2}

Since
\begin{equation*}
\frac{1}{n^{k/2}}\left|\left(\sum_{i=1}^{n}\varphi(a_{i}^{2})\right)^{k/2}-\sum_{(j(1),\ldots,j(k/2))}\varphi(a_{j(1)}^{2})\varphi(a_{j(2)}^{2})\cdots\varphi(a_{j(k/2)}^{2})\right|\rightarrow 0
\end{equation*}
as $ n\rightarrow\infty$, we observe that the conclusion of Theorem~\ref{ch02:the2.3.2} holds true even if the assumption $\varphi(a_{i}^{2})=1$ is weakened to $\lim_{n\rightarrow\infty}\tfrac{1}{n}\sum_{i=1}^{n}\varphi(a_{i}^{2})=1$.

It is noteworthy that the $k$th moment of the standard normal distribution is the number of all pair partitions of a $k$-element-set. The normal distribution is the limit distribution in the classical central limit theorem,\index{free!central limit theorem}\index{central limit theorem!free} say for independent identically distributed random variables. Theorem~\ref{ch02:the2.3.2} is analogous, and we call it the \textit{free central limit theorem}. It is natural to say that in free probability theory the semicircle law plays the role of the Gaussian law of classical theory. A different proof using $R$-series will be given at the end of Sec.~\ref{ch02:sec2.4}.

\section{Free convolution of measures}
\label{ch02:sec2.4}

The classical convolution of measures on the real line is strongly related to independence of random variables; namely, the convolution of distributions is the distribution of the sum of independent variables. The free convolution to be discussed in this section is determined by a noncommutative detour. The given distributions are represented by noncommutative random variables in free relation. Those are really noncommuting, because the free relation prevents commutativity. The distribution of their sum gives the additive free convolution. Since the relation of the classical convolution to the free convolution is the same as that of independence to freeness, it is not surprising that semicircle laws form a semigroup with respect to the free convolution. This is again a manifestation of the parallelism between the semicircle law and the Gaussian distribution.

The additive free convolution\index{free!convolution, additive}\index{convolution!additive free} of measures can be computed by means of certain formal power series, namely $R$-series. The concept of $R$-series is regarded as the analogue of the logarithm of the Fourier transform from the classical theory, since it linearizes the additive free convolution.\index{additive free convolution} The $R$-series is defined by a canonical representation of a measure (or more generally, of a noncommutative random variable) by means of a formal series of a creation operator on the Fack space. The convergence of the $R$-series will be established in the next chapter, where an analytic machinery for the free convolution will appear.

Let $\mu_{1}$ and $\mu_{2}$ be two compactly supported probability measures on $\mathbb{R}$. One can find a noncommutative probability space $(\mathcal{A}, \varphi)$ with two elements $a_{1}$ and $a_{2}$ in free relation such that the noncommutative random variables $a_{i}$ are distributed according to $\mu_{i}$, that is,
\begin{equation*}
\int x^{k}\,d\mu_{i}(x)=\varphi(a_{i}^{k}) \qquad (k\in \mathbb{N},\,i=1,2).
\end{equation*}
Consider the distribution $\mu$ of $a_{1}+a_{2}$. This measure is characterized by the property
\begin{equation}
\int x^{k}\,d\mu(x)=\varphi((a_{1}+a_{2})^{k}) \qquad (k\in \mathbb{N})\, ,
\label{ch02:eqn2.4.1}
\end{equation}
and it will be called the \textit{additive free convolution} of $\mu_{1}$ and $\mu_{2}$, in notation $\mu= \mu_{1} \boxplus \mu_{2}$. In other words, $\mu_{1} \boxplus \mu_{2}$ is the distribution measure of the noncommutative random variable $a_{1}+a_{2}$. Since multiplicative free convolution, another concept of free convolution, is practically not treated in this book, instead of additive free convolution we mostly use the shorter term ``free convolution''. Recall that the usual (or classical) convolution of $\mu_{1}$ and $\mu_{2}$ is achieved by choosing independent usual random variables $\xi_{1}$ and $\xi_{2}$ distributed according to $\mu_{1}$ and $\mu_{2}$, respectively, and then considering the distribution of $\xi_{1}+\xi_{2}$.

The objects $(\mathcal{A}, \varphi), a_{1}, a_{2}$ required in the definition of the free convolution always exist. Example~\ref{ch01:exa1.2.2} shows that any compactly supported measure can be represented as a noncommutative random variable, so we have $(\mathcal{A}_{i},\varphi_{i})$ and $a_{i}\in \mathcal{A}_{i}$ such that $\mu_{i}$ and $a_{i}$ have the same moments. Then the free product $(\mathcal{A}, \varphi)=(\mathcal{A}_{1}, \varphi_{1})\, \star\, (\mathcal{A}_{2}, \varphi_{2})$ contains $a_{1}$ and $a_{2}$. If we wish we can choose $\mathcal{A}_{i}$ to be a $C^{*}$-algebra, and then (after completion) $\mathcal{A}$ is a $C^{*}$-algebra as well. When the $a_{i}$ are selfadjoint, so is $a_{1}+a_{2}$, and the spectral theorem ensures the existence of the measure $\mu$ satisfying the condition (\ref{ch02:eqn2.4.1}). This argument shows also that $\mathrm{supp}\,\mu_{i}\subset[\alpha_{i}, \beta_{i}]$ implies $\mathrm{supp}(\mu_{1} \boxplus \mu_{2}) \subset[\alpha_{1}+\alpha_{2}, \beta_{1}+\beta_{2}]$. To see that $\mu$ is independent of the choice of $(\mathcal{A}, \varphi), a_{1}, a_{2}$, one has to recognize that (\ref{ch02:eqn2.4.1}) provides a rule for the computation of moments $m_{k}(\mu)=\int x^{k}\,d\mu(x)$ of $\mu$ from those of $\mu_{1}$ and $\mu_{2}$. In fact, there are universial polynomials $P_{n}(x_{1}, x_{2}, \ldots, x_{n}, y_{1}, y_{2}, \ldots, y_{n})$ such that, plugging the first $n$ moments of $\mu_{1}$ in place of the $x_{i}$'s and those of $\mu_{2}$ in place of the $y_{i}$'s, we obtain the $n$th moment of $\mu_{1} \boxplus \mu_{2}$. The polynomials are universal in the sense that they are independent of $\mu_{i}$ and $(\mathcal{A}, \varphi), a_{1}, a_{2}$. $P_{n}$ can be computed, at least in principle, by expanding $(a_{1}+a_{2})^{n}$ and then using the formula (\ref{ch02:eqn2.2.3}) of the free relation repeatedly. For example,
\begin{align*}
&m_{1} (\mu_{1} \boxplus \mu_{2}) =m_{1}(\mu_{1})+m_{1}(\mu_{2})\\
& \qquad (P_{1}(x_{1}, y_{1})=x_{1}+y_{1}) \, ,\\
&m_{2} (\mu_{1} \boxplus \mu_{2}) =m_{2}(\mu_{1})+m_{2}(\mu_{2})+2m_{1}(\mu_{1})m_{1}(\mu_{2})\\
& \qquad (P_{2}(x_{1}, x_{2}, y_{1}, y_{2})=x_{2}+y_{2}+2x_{1}y_{1}) \, ,\\
&m_{3}(\mu_{1} \boxplus \mu_{2}) =m_{3}(\mu_{1})+m_{3}(\mu_{2})+3m_{1}(\mu_{1})m_{2}(\mu_{2})+3m_{2}(\mu_{1})m_{1}(\mu_{2})\\
&\qquad (P_{3}(x_{1}, x_{2}, x_{3}, y_{1}, y_{2}, y_{3})=x_{3}+y_{3}+3x_{1}y_{2}+3x_{2}y_{1}) \, .
\end{align*}
Up through the third moment of $\mu_{1} \boxplus \mu_{2}$, the computation rule is the same as that for the moments of the usual convolution of $\mu_{1}$ and $\mu_{2}$. The difference appears only from the fourth moment:
\begin{align*}
&m_{4}(\mu_{1}\boxplus \mu_{2}) = m_{4}(\mu_{1})+\mu_{4}(\mu_{2})+4m_{3}(\mu_{1})m_{1}(\mu_{2})+4m_{1}(\mu_{1})m_{3}(\mu_{2})\\
& \qquad \qquad \qquad \quad + 4m_{2}(\mu_{1})m_{2}(\mu_{2})+2m_{2}(\mu_{1})m_{1}(\mu_{2})^{2}\\
& \qquad \qquad \qquad \quad + 2m_{1}(\mu_{1})^{2}m_{2}(\mu_{2})-2m_{1}(\mu_{1})^{2}m_{1}(\mu_{2})^{2}\\
& \quad \ \ (P_{4}(x_{1},\ldots,x_{4},y_{1},\ldots,y_{4})=x_{4}+y_{4}+4x_{3}y_{1}+4x_{1}y_{3}+4x_{2}y_{2}\\
& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \ \ + 2x_{2}y_{1}^{2}+2x_{1}^{2}y_{2}-2x_{1}^{2}y_{1}^{2}) \, .
\end{align*}

It is sometimes convenient to consider the notion of distributions\index{distribution} in the abstract sense. A \textit{distribution} $\mu$ is abstractly given as a linear functional $\mu : \mathbb{C}\langle X\rangle \rightarrow \mathbb{C}$ with $\mu(\mathbf{1})=1$, and it may be identified with the \textit{moment}\index{moment} sequence $m_{k}(\mu) :=\mu(X^{k})\ (k\in \mathbb{N})$. Given two distributions $\mu_{i}(i=1,2)$ in this abstract sense, we have $(\mathbb{C}\langle X_{1}\rangle, \mu_{1})\star(\mathbb{C}\langle X_{2}\rangle, \mu_{2})=(\mathbb{C}\langle X_{1}, X_{2}\rangle, \mu_{1}\, \star\, \mu_{2})$. Since $X_{1}$ and $X_{2}$ are of course free in this noncommutative probability space, the free convolution $\mu_{1} \boxplus \mu_{2}$ can be defined by
\begin{equation*}
m_{k} (\mu_{1} \boxplus \mu_{2}) =(\mu_{1}\star\mu_{2})\left((X_{1}+X_{2})^{k}\right) \qquad (k\in \mathbb{N}).
\end{equation*}
More information about moments will be given in the next section.

\begin{example}\label{ch02:exa2.4.1}
For point measures we have $\delta(x) \boxplus \delta(y)=\delta(x+y) \ (x, y\in \mathbb{R})$.

Choose an arbitrary noncommutative probability space $(\mathcal{A}, \varphi)$ and let $a_{1}=x\mathbf{1}, a_{2}=y\mathbf{1}$. Then $a_{1}$ and $a_{2}$ are trivially free, and the distribution of $a_{1}+a_{2}$ is $\delta(x+y)$.

Since $a$ and $\mathbf{1}$ are in free relation in any noncommutative probability space, we can obtain that $\delta(x) \boxplus \mu$ is the translation of the measure $\mu$ by $x\in \mathbb{R}$, similarly to the above argument.
\end{example}

\begin{example}\label{ch02:exa2.4.2}
For semicircle laws we have
\begin{equation*}
w_{m_{1},r_{1}}\boxplus w_{m_{2},r_{2}}=w_{m,r}, \quad \mathrm{where} \quad m=m_{1}+m_{2},\, r^{2}=r_{1}^{2}+r_{2}^{2}.
\end{equation*}

Theorem~\ref{ch01:the1.1.5} and Example~\ref{ch02:exa2.2.1} will be used. We choose $h_{i}\in \mathcal{H}$ such that $h_{1}$ and $h_{2}$ are orthogonal. So $\ell(h_{1})$ and $\ell(h_{2})$ are free with respect to the vacuum expectation in $\mathcal{F}(\mathcal{H})$. If $2\Vert h_{i}\Vert=r_{i}$, then the distribution of $\ell(h_{i})^{*}+\ell(h_{i})$ is $w_{r_{i}}$ and the distribution of $m_{i}\mathbf{1}+l(h_{i})^{*}+\ell(h_{i})$ is $w_{m_{i},r_{i}}$. These operators are free with respect to the vacuum, and
\begin{align}
& \left(m_{1}\mathbf{1}+\ell(h_{1})^{*}+\ell(h_{1})\right)+(m_{2}\mathbf{1}+\ell(h_{2})^{*}+\ell(h_{2})) \notag\\
& \qquad =(m_{1}+m_{2})\mathbf{1}+\ell(h_{1}+h_{2})^{*}+\ell(h_{1}+h_{2}) \, .
\label{ch02:eqn2.4.2}
\end{align}
Since $\Vert h_{1}+h_{2}\Vert^{2}=\Vert h_{1}\Vert^{2}+\Vert h_{2}\Vert^{2}$, (\ref{ch02:eqn2.4.2}) is distributed according to the stated semicircle law.
\end{example}

Let $\ell=\ell(h)$ be a creation operator on $\mathcal{F}(\mathcal{H})$ with $\Vert h\Vert=1$. If $\sum_{k=0}^{\infty}|\alpha_{k+1}|<\infty$, then the series
\begin{equation*}
\ell^{*}+\sum_{k=0}^{\infty}\alpha_{k+1}\ell^{k}
\end{equation*}
converges in norm and gives a bounded operator on the full Fock space. It is easy to see that
\begin{equation}
\left\langle\left(\ell^{*}+\sum_{k=0}^{\infty}\alpha_{k+1}\ell^{k}\right)^{n}\Phi, \Phi\right\rangle=\alpha_{n}+p_{n}(\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n-1}) \, ,
\label{ch02:eqn2.4.3}
\end{equation}
where the $p_{n}$ are certain polynomials. Indeed, in expanding the $n$th power, a contribution to the expectation value arises only in the case when the total sum of creations is equal to the number of annihilations, that is, the number of factors $\ell^{*}$. If $\ell^{k}$ is involved in an $n$-factor product and $k\geq n$, then we cannot take enough $\ell^{*}$ factors to get nonzero. All these terms may be simply neglected. Furthermore, among the permutations of $\ell^{n-1}$ with $n-1$ copies of $\ell^{*}$ factors, only the term $\langle(\ell^{*})^{n-1}\ell^{n-1}\Phi, \Phi\rangle$ contributes, and it has the coefficient $\alpha_{n}$. This argument justifies (\ref{ch02:eqn2.4.3}), and moreover it allows us to speak about
\begin{equation*}
\left\langle\left(\ell^{*}+\sum_{k=0}^{\infty}\alpha_{k+1}\ell^{k}\right)^{n}\Phi,\Phi\right\rangle
\end{equation*}
for any sequence $(\alpha_{k+1})$. In the general case $\ell^{*}+\sum_{k=0}^{\infty}\alpha_{k+1}\ell^{k}$ does not correspond to an operator on a Hilbert space, but it is rather regarded as a formal expression. To create a solid foundation of the formal manipulations, we introduce an algebra $\tilde{\mathcal{E}}$ as follows.

$\tilde{\mathcal{E}}$ is the unital algebra of formal infinite sums
\begin{equation*}
T=\sum_{n,m=0}^{\infty}c_{m,n}\ell^{m}\ell^{*n},
\end{equation*}
where the coefficients $c_{m,n}$ are complex numbers and we assume that there exists $N$ such that $c_{m,n}=0$ whenever $n>N$. The sum of two series is obviously defined. The multiplication is governed by the rule $\ell^{*}\ell=\mathbf{1}$. For example, $\ell^{3}\ell^{*2}\times\ell^{4}\ell^{*2}= \ell^{5}\ell^{*2}$. The product of two series $T$ and $T'$ is obtained by multiplying them term by term, then simplifying by means of $\ell^{*}\ell=\mathbf{1}$, and by collecting terms, which means finite addition due to the condition imposed on the coefficients. We consider the linear functional $\tilde{\omega}$ on $\tilde{\mathcal{E}}$ which sends a formal sum $T$ into its coefficient $c_{0,0}$. In this way, $(\tilde{\mathcal{E}},\tilde{\omega})$ is a noncommutative probability space while the involution is not defined on $\tilde{\mathcal{E}}$.

Let $\mu$ be a measure on $\mathbb{R}$ such that all moments of $\mu$ exist. The formal sum
\begin{equation*}
\ell^{*}+\sum_{k=0}^{\infty}\alpha_{k+1}\ell^{k}=\ell^{*}\left(\mathbf{1}+\sum_{k=1}^{\infty}\alpha_{k}\ell^{k}\right)\in\tilde{\mathcal{E}}
\end{equation*}
is called the \textit{canonical noncommutative random variable}\index{noncommutative!random variable, canonical}\index{canonical!noncommutative random variable} associated with $\mu$, if its moments under $\tilde{\omega}$ are the same as those of $\mu$, that is,
\begin{equation}
m_{n}(\mu)=\tilde{\omega}\left(\left(\ell^{*}+\sum_{k=0}^{\infty}\alpha_{k+1}\ell^{k}\right)^{n}\right) \qquad (n\in \mathbb{N}). \label{ch02:eqn2.4.4}
\end{equation}
In fact, according to (\ref{ch02:eqn2.4.3}), the coefficients of the canonical variable can be computed from the moment sequence of $\mu$ recursively. For example,
\begin{align}
\alpha_{1}&=m_{1}(\mu), \quad \alpha_{2}=m_{2}(\mu)-m_{1}(\mu)^{2}, \label{ch02:eqn2.4.5}\\
\alpha_{3}&=m_{3}(\mu)-3m_{2}(\mu)m_{1}(\mu)+2m_{1}(\mu)^{3}. \notag
\end{align}
(The general formula for an $m_{n}(\mu)$ in terms of the $\alpha_{k}$'s and its inverse formula will be given in the next section.) In a similar manner, we can speak of the canonical random variable associated with any noncommutative random variable $a$ in $(\mathcal{A}, \varphi)$ satisfying (\ref{ch02:eqn2.4.4}) with $\varphi(a^{n})$ in place of $m_{n}(\mu)$.

When the associated canonical random variable is given by the sequence $\alpha_{k}$, the formal power series
\begin{equation*}
R_{\mu}(z):=\sum_{k=1}^{\infty}\alpha_{k}z^{k}
\end{equation*}
is called the $R$-\textit{series} of the measure $\mu$. More generally, we can speak of the $R$-series $R_{a}(z)$ of any noncommutative random variable $a$. Here we warn that this differs by a factor of $z$ from Voiculescu's original $\mathcal{R}$-\textit{transform}\index{$\mathcal{R}$-transform}\index{$R$-series} $\mathcal{R}_{\mu}(z) :=\sum_{k=0}^{\infty}\alpha_{k+1}z^{k}$. This is why we use a slightly different term. The use of $R$-series is notationally a bit more convenient (particularly in the multivariable case, see Sec.~\ref{ch02:sec2.5}).

\begin{example}\label{ch02:exa2.4.3}
If $R_{a}(z)$ is the $R$-series of a noncommutative random variable $a$, then the $R$-series of $\lambda a(\lambda\in \mathbb{C})$ is $R_{a}(\lambda z)$.

Let $\sum_{k=1}^{\infty}\alpha_{k}z^{k}$ be the $R$-series of $a$. One has to see that $\lambda a$ and the formal sum
\begin{equation*}
\ell^{*}+\sum_{k=0}^{\infty}\lambda^{k+1}\alpha_{k+1}\ell^{k}\in\tilde{\mathcal{E}}
\end{equation*}
possess the same $n$th moments for every $n$. The argument used to justify (\ref{ch02:eqn2.4.3}) can be continued. In the expansion of the left-hand side of (\ref{ch02:eqn2.4.3}) we have terms like
\begin{align}
& \alpha_{k_{1}+1}\alpha_{k_{2}+1}\cdots\alpha_{k_{r}+1} \label{ch02:eqn2.4.6}\\
& \quad \ \ \times\langle(\mathrm{product\ of}\ \ell^{k_{1}}, \ell^{k_{2}}, \ldots, \ell^{k_{r}}\ \mathrm{and}\ s\ \mathrm{factors\ of}\ \ell^{*}) \Phi, \Phi\rangle \, . \notag
\end{align}
Here the total number $r+s$ of factors is $n$, and $k_{1}+k_{2}+\cdots+k_{r}=s$ is a necessary condition to have nonzero expectation (total number of creations equals the number of annihilations). Hence $(k_{1}+1)+(k_{2}+1)+\cdots+(k_{r}+1)=n$, and replacing $\alpha_{k_{i}+1}$ by $\lambda^{k_{i}+1}\alpha_{k_{i}+1}$ we observe that (\ref{ch02:eqn2.4.6}) is multiplied by a factor $\lambda^{n}$. In this way we have proved that
\begin{equation*}
p_{n}(\lambda\alpha_{1},\lambda^{2}\alpha_{2},\ldots,\lambda^{n-1}\alpha_{n-1})=\lambda^{n}p_{n}(\alpha_{1},\alpha_{2},\ldots,\alpha_{n-1}) \, ,
\end{equation*}
which is sufficient to conclude the statement. (It is noteworthy that the argument shows that $p_{n}$ has nonnegative integer coefficients.)
\end{example}

The proof in Example~\ref{ch02:exa2.4.2} benefits from the fact that the canonical variable associated with the semicircle measure is actually a finite sum, and therefore an operator representation on the Fock space is conveniently used. Moreover, in order to represent free random variables, we may consider the creation operators $\ell(h_{1})$ and $\ell(h_{2})$ with orthogonal unit vectors. To allow room for similar arguments in the case of arbitrary canonical variables, we need the two-variable extension of the algebra $\tilde{\mathcal{E}}$.

$\tilde{\mathcal{E}}_{2}$ consists of the infinite sums
\begin{align*}
T = \sum_{m,n=0}^{\infty}\,\sum_{i(1),\ldots,i(m);j(1),\ldots,j(n)} & \quad c_{i(1),i(2),\ldots, i(m);j(1),j(2),\ldots, j(n)}\\
& \quad \times \ell_{i(1)}\ell_{i(2)}\cdots \ell_{i(m)}\ell_{j(1)}^{*}\ell_{j(2)}^{*}\ldots \ell_{j(n)}^{*} \, ,
\end{align*}
where $c_{i(1),\ldots, i(m);j(1),\ldots, j(n)}\in \mathbb{C}$ for $i(1),\ldots, i(m), j(1), \ldots, j(n)\in\{1,2\}$, and there exists an $N\in \mathbb{N}$ such that $c_{i(1),\ldots,i(m);j(1),\ldots,j(n)}=0$ whenever $n>N$. The multiplication is governed by the computational rules $\ell_{1}^{*}\ell_{1}=\ell_{2}^{*}\ell_{2}=\mathbf{1}$ and $\ell_{1}^{*}\ell_{2}=\ell_{2}^{*}\ell_{1}=0$. The condition on the coefficients ensures that in the multiplication of two infinite sums we have to add only finitely many numbers. On $\tilde{\mathcal{E}}_{2}$ we consider the normalized linear functional $\tilde{\omega}_{2}$ sending each sum to its constant term.

Let $\mathcal{F}(\mathcal{H})$ be the full Fock space over the Hilbert space $\mathcal{H}$, and let $h_{1}$ and $h_{2}$ be two orthogonal unit vectors in $\mathcal{H}$. If $T\in\tilde{\mathcal{E}}_{2}$ is a finite sum, then we can obtain an operator $\mathcal{F}(T)$ acting on the Fock space by replacing each $\ell_{i}$ by $\ell(h_{i})$ and each $\ell_{i}^{*}$ by $\ell(h_{i})^{*}$. It is rather clear that
\begin{equation*}
\tilde{\omega}_{2}(T)=\langle \mathcal{F}(T)\Phi,\Phi\rangle,
\end{equation*}
and on this basis we sometimes say that $\tilde{\omega}_{2}(\,\cdot\,)$ is the vacuum expectation. As far as finite sums in $\tilde{\mathcal{E}}_{2}$ are concerned, it is a matter of convenience whether the formal setting of $\tilde{\mathcal{E}}_{2}$ or the operator setting on $\mathcal{F}(\mathcal{H})$ is considered.

Clearly $(\tilde{\mathcal{E}},\tilde{\omega})$ is embedded into $(\tilde{\mathcal{E}}_{2},\tilde{\omega}_{2})$ in two different ways. We can replace each $\ell$ in an element of $\tilde{\mathcal{E}}$ by $\ell_{1}$ or by $\ell_{2}$. The former embedding yields $\tilde{\mathcal{E}}_{(1)}\subset\tilde{\mathcal{E}}_{2}$ and the latter one gives $\tilde{\mathcal{E}}_{(2)}\subset\tilde{\mathcal{E}}_{2}$. The argument of Example~\ref{ch02:exa2.2.1} goes through, and we have

\begin{example}\label{ch02:exa2.4.4}
The subalgebras $\tilde{\mathcal{E}}_{(1)}$ and $\tilde{\mathcal{E}}_{(2)}$ are free in $\tilde{\mathcal{E}}_{2}$ with respect to the functional $\tilde{\omega}_{2}$.
\end{example}

\begin{theorem}\label{ch02:the2.4.5}
Let $\mu_{i}$ be probability measures on $\mathbb{R}$ with compact support, $i=1,2$. If $\mu :=\mu_{1} \boxplus \mu_{2}$, then
\begin{equation*}
R_{\mu}(z)=R_{\mu_{1}}(z)+R_{\mu_{2}}(z)
\end{equation*}
in the sense of formal power series.
\end{theorem}

\begin{proof2}
Let $R_{\mu_{1}}(z)=\sum_{k=1}^{\infty}\alpha_{k}z^{k}$ and $R_{\mu_{2}}(z)=\sum_{k=1}^{\infty}\beta_{k}z^{k}$. We canonically represent $\mu_{i}$ in the algebra $\tilde{\mathcal{E}}_{(i)}$, and use the freeness of those algebras to compute moments of the free convolution $\mu=\mu_{1} \boxplus \mu_{2}$ as
\begin{equation}
m_{n}(\mu)=\tilde{\omega}_{2}\left(\left(\ell_{1}^{*}+\ell_{2}^{*})+\sum_{k=0}^{\infty}\alpha_{k+1}\ell_{1}^{k}+\sum_{k=0}^{\infty}\beta_{k+1}\ell_{2}^{k}\right)^{n}\right).
\label{ch02:eqn2.4.7}
\end{equation}
What we need to prove is that this expectation coincides with
\begin{equation}
\tilde{\omega}\left(\left(\ell^{*}+\sum_{k=0}^{\infty}\alpha_{k+1}\ell^{k}+\sum_{k=0}^{\infty}\beta_{k+1}l^{k}\right)^{n}\right).
\label{ch02:eqn2.4.8}
\end{equation}
The expansions of (\ref{ch02:eqn2.4.7}) and (\ref{ch02:eqn2.4.8}) have a similar structure when $(\ell_{1}^{*}+\ell_{2}^{*})$ in (\ref{ch02:eqn2.4.7}) is treated as a whole. To each term of the expansion of (\ref{ch02:eqn2.4.7}) there corresponds a term of (\ref{ch02:eqn2.4.8}); they have the same scalar coefficients. The terms of (\ref{ch02:eqn2.4.7}) are monomials of factors $(\ell_{1}^{*}+\ell_{2}^{*}), \ \ell_{1}$ and $\ell_{2}$. If those factors are replaced by $\ell^{*}, \ \ell$ and $\ell$, respectively, then the corresponding term of (\ref{ch02:eqn2.4.8}) is obtained. It is sufficient to show that the expectation is invariant under this transformation of monomials. Any monomial of $\ell_{1}$ and $\ell_{2}$ has the vacuum expectation $0$, and so does the correponding monomial of (\ref{ch02:eqn2.4.8}), which is a power of $\ell$ in this case. The same can be said about a monomial of only $(\ell_{1}^{*}+\ell_{2}^{*})$. Hence we need to check the invariance of the vacuum expectation for monomials containing $(\ell_{1}^{*}+\ell_{2}^{*})$ and $\ell_{i} (i=1,2)$. This can be done by induction on the number of factors. The point is the transformation of the identities $(\ell_{1}^{*}+\ell_{2}^{*})\ell_{1}=\mathbf{1}$ and $(\ell_{1}^{*}+\ell_{2}^{*})\ell_{2}=\mathbf{1}$ into $\ell^{*}\ell=\mathbf{1}$. By the use of these the length of monomials can be reduced.
\end{proof2}

In fact, the above proof shows that if noncommutative random variables $a_{1}$ and $a_{2}$ in $(\mathcal{A}, \varphi)$ are in free relation, then
\begin{equation*}
R_{a_{1}+a_{2}}(z)=R_{a_{1}}(z)+R_{a_{2}}(z) \, .
\end{equation*}
In particular,
\begin{equation}
R_{a+\lambda \mathbf{1}}(z)=R_{a}(z)+\lambda z
\label{ch02:eqn2.4.9}
\end{equation}
for every noncommutative random variable $a$ and every $\lambda\in \mathbb{C}$.

\begin{example}\label{ch02:exa2.4.6}
The $R$-series of the semicircle law $w_{m,r}$ is
\begin{equation*}
R_{w_{m,r}}(z)=mz+\frac{r^{2}}{4}z^{2} \, .
\end{equation*}

Indeed, since $w_{2}$ is the distribution of $\ell^{*}+\ell$, we have $R_{w_{2}}(z)=z^{2}$. Apply Example~\ref{ch02:exa2.4.3} and (\ref{ch02:eqn2.4.9}) to get the $R$-series of $w_{m,r}$.
\end{example}

Normally it is difficult to find explicitly the canonical noncommutative random variable associated to a given probability distribution. Besides the semicircle law, in the next chapter we shall have the canonical representation for some other probability distributions.

\begin{example}\label{ch02:exa2.4.7}
Let $p\in \mathcal{A}$ be a projection which is free from the noncommutative random variable $a\in \mathcal{A}$ with respect to the expectation $\varphi$. Assume that $\varphi(p)\neq 0$, and let $R_{a}(z)=\sum_{n}\alpha_{n}z^{n}$ be the $R$-series of $a$. Then the $R$-series of \textit{pap} in the reduced algebra $p\mathcal{A}p$ with respect to the reduced state $\varphi'(pbp):=\varphi(pbp)/\varphi(p)$ is
\begin{equation*}
\sum_{n}\alpha_{n}\varphi(p)^{n-1}z^{n} \, .
\end{equation*}
Hence the $R$-series of $\varphi(p)^{-1} pap$ in $(p\mathcal{A}p, \, \varphi')$ is $\varphi(p)^{-1}R_{a}(z)$.

We are going to benefit from the matricial model discussed in Example~\ref{ch02:exa2.2.11}. Let
\begin{equation*}
L:=\sum_{i,j=1}^{2}\sqrt{\lambda_{i}}\ell(h_{ij})\otimes e_{ij},
\end{equation*}
where $\lambda_{1}:=\varphi(p),\lambda_{2}:=1-\varphi(p)$, and the $h_{ij}$ are orthonormal vectors. The noncommutative random variable $a$ is represented by the formal operator sum
\begin{equation*}
A:=L^{*}+\sum_{n=0}^{\infty}\alpha_{n+1}L^{n} \, .
\end{equation*}
(Precisely speaking, $A$ is defined as an element of the noncommutative probability space $(\tilde{\mathcal{E}}_{4}\otimes M_{2}(\mathbb{C}),\tilde{\omega}_{4}\otimes\rho)$; see Sec.~\ref{ch02:sec2.5} for $(\tilde{\mathcal{E}}_{k},\tilde{\omega}_{k}).)$ According to the quoted example, $\mathbf{1}\otimes e_{11}$ is free from $A$, and $\psi(\mathbf{1}\otimes e_{11})=\lambda_{1}=\varphi(p)$. Hence the distribution of \textit{pap} with respect to $\varphi'$ is the same as that of the $(1, 1)$ entry of the matrix $A$ with respect to the vacuum expectation. The $(1, 1)$ entry of $L^{n}$ is a linear combination of terms like $\ell(h_{1i(1)})\ell(h_{i(1)i(2)})\cdots \ell(h_{i(n-1)1})$, which have nonzero expectation only in the case $i(1)=i(2)=\ldots=i(n-1)=1$. Therefore, it follows that the $(1, 1)$ entry of $A$ has the same distribution as
\begin{equation*}
\sqrt{\lambda_{1}}\ell(h_{11})^{*}+\sum_{n=0}^{\infty}\alpha_{n+1}\sqrt{\lambda_{1}^{n}}\ell(h_{11})^{n}\, .
\end{equation*}
The distribution of this does not change if we multiply $\ell$ by $t$ and $\ell^{*}$ by $t^{-1}$. In particular, for $t=\sqrt{\lambda_{1}}=\sqrt{\varphi(p)}$ we have
\begin{equation*}
\ell(h_{11})^{*}+\sum_{n=0}^{\infty}\alpha_{n+1}\varphi(p)^{n}\ell(h_{11})^{n}\, .
\end{equation*}
From this representation the $R$-series is read out. The second assertion follows from Example~\ref{ch02:exa2.4.3}.
\end{example}

We end the section with another proof of Theorem~\ref{ch02:the2.3.2} by the method of $R$-series. In the following proof we shall use the fact that when $R_{a}(z)=\sum_{k=1}^{\infty}\alpha_{k}z^{k}$ is the $R$-series of a noncommutative random variable $a$ in $(\mathcal{A}, \varphi)$, each $\alpha_{k}$ is a universal polynomial in the moments $\varphi(a), \varphi(a^{2}), \ldots, \varphi(a^{k})$, and conversely $\varphi(a^{k})$ is a universal polynomial in $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{k}$. This is an immediate consequence of (\ref{ch02:eqn2.4.3}).

\begin{proof1}[Second proof of Theorem \ref{ch02:the2.3.2}]
Let $a_{1}, a_{2}, \ldots$ be a free sequence in $(\mathcal{A}, \varphi)$ such that $\varphi(a_{i})=0,\ \sup_{i}|\varphi(a_{i}^{k})|<+\infty$ for all $k\in \mathbb{N}$, and $\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{n}\varphi(a_{i}^{2})=1$. The $R$-series of $a_{i}$ is written as
\begin{equation*}
R_{a_{i}}(z)=\alpha_{i,2}z^{2}+\alpha_{i,3}z^{3}+\cdots,
\end{equation*}
where $\alpha_{i,2}=\varphi(a_{i}^{2})$ (see (\ref{ch02:eqn2.4.5})). In view of Theorem~\ref{ch02:the2.4.5} and Example~\ref{ch02:exa2.4.3}, the $R$-series of $(a_{1}+a_{2}+\cdots+a_{n})/\sqrt{n}$ is
\begin{equation*}
\sum_{i=1}^{n}R_{a_{i}}(z/\sqrt{n})=\sum_{k=2}^{\infty}\left(n^{-k/2}\sum_{i=1}^{n}\alpha_{i,k}\right)z^{k}.
\end{equation*}
Set $\alpha_{k}^{(n)} :=n^{-k/2}\sum_{i=1}^{k}\alpha_{i,k}$ for $k\geq 2$. By the fact remarked above and the boundedness assumption for the moments of the $a_{i}$'s, it is clear that $\sup_{i}|\alpha_{i,k}|<+\infty$ for all $k\geq 2$, so $\alpha_{k}^{(n)}$ converges to $0$ as $ n\rightarrow\infty$ for any $k\geq 3$. On the other hand, we have $\alpha_{2}^{(n)}=\frac{1}{n}\sum_{i=1}^{n}\varphi(a_{i}^{2})\rightarrow 1$ as $ n\rightarrow\infty$. Since $z^{2}$ is the $R$-series of the standard semicircle law $w_{2}$, the moments of $(a_{1}+a_{2}+\cdots+a_{n})/\sqrt{n}$ converge to those of $w_{2}$.
\end{proof1}

\section{Moments and cumulants}
\label{ch02:sec2.5}

\noindent Compactly supported probability measures on the real line can be described both by their moments and by their cumulants. The transformation between moments and cumulants is related to the lattice structure of partitions of finite sets. In the free probabilistic setting the coefficients of the $R$-series behave like cumulants; moreover the moment-cumulant and cumulant-moment formulas are strongly related to the lattice of non-crossing partitions. Non-crossing partitions appeared already in the combinatorial proof of the free central limit theorem. Their role cannot be overvalued. Besides the Fock space approach, the free cumulants\index{cumulant!classical}\index{classical cumulant} give an equally useful formalism for freeness.

Let $\xi_{i}$ be a classical random variable and $\mu_{i}$ its distribution measure on $\mathbb{R},i=1,2$. The \textit{classical convolution}\index{classical convolution}\index{convolution!classical} $\mu_{1}*\mu_{2}$ is the distribution measure of $\xi_{1}+\xi_{2}$ when $\xi_{1}$ and $\xi_{2}$ are chosen to be independent. The sequence of \textit{moments}\index{moment}
\begin{equation*}
m_{n}(\mu)=\int x^{n}\,d\mu(x)=E(\xi^{n})
\end{equation*}
is conveniently used to describe the convolution (whenever moments exist). The binomial formula allows one to express moments of $\mu_{1}*\mu_{2}$ in terms of those of $\mu_{1}$ and $\mu_{2}$. Another approach consists in use of the \textit{cumulant} sequence $s_{n}(\mu)$, given by
\begin{equation*}
\log E(e^{\mathrm{i}\,t\xi})=\sum_{n=1}^{\infty}\frac{(\mathrm{i}\,t)^{n}}{n!}s_{n}(\mu).
\end{equation*}
The convolution is linearized by the cumulant sequence:
\begin{equation*}
s_{n}(\mu_{1}*\mu_{2})=s_{n}(\mu_{1})+s_{n}(\mu_{2}) \, .
\end{equation*}
(Cumulants are also called \textit{semi-invariants}.)\index{semi-invariant} Probability theory knows the relation between the moment and cumulant\index{moment-cumulant formula, classical} sequences ([\citen{bib166}], Sec. II.12):
\begin{equation}
m_{n}(\mu)=\sum_{\mathcal{V}}\prod_{i=1}^{k}s_{|V_{i}|}(\mu) \, ,
\label{ch02:eqn2.5.1}
\end{equation}
where the summation is over all partitions $\mathcal{V}=\{V_{1}, V_{2}, \ldots, V_{k}\}$ of the set $[n]$ and $|V_{i}|$ denotes the number of elements in $V_{i}$. For example,
\begin{equation*}
m_{3}(\mu)=s_{3}(\mu)+3s_{2}(\mu)s_{1}(\mu)+s_{1}(\mu)^{3} \, .
\end{equation*}
The inverse relation is similar:
\begin{equation}
s_{n}(\mu)=\sum_{\mathcal{V}}(-1)^{k-1}(k-1)! \prod_{i=1}^{k}m_{|V_{i}|}(\mu) \, ,
\label{ch02:eqn2.5.2}
\end{equation}
for example
\begin{equation*}
s_{4}(\mu)=m_{4}(\mu)-4m_{3}(\mu)m_{1}(\mu)-3m_{2}(\mu)^{2}+12m_{2}(\mu)m_{1}(\mu)^{2}-6m_{1}(\mu)^{4} \, .
\end{equation*}

The partitions of the set $[n]$ form a \textit{lattice}\index{lattice of partitions} $L_{n}$. Recall that the partial order on $L_{n}$ is defined by saying $\mathcal{V}_{1}\leq \mathcal{V}_{2}$ whenever each block of $\mathcal{V}_{1}$ is contained in a block of $\mathcal{V}_{2}$. (In other words, one can get $\mathcal{V}_{1}$ by partitioning the blocks of $\mathcal{V}_{2}$.) The smallest element of $L_{n}$ is $\{\{1\}, \{2\}, \ldots, \{n\}\}$, and it will be denoted by $\mathbf{0}_{n}$. The largest element is $\mathbf{1}_{n} :=\{[n]\}$. The \textit{incidence algebra}\index{incidence algebra} is the algebra of real-valued functions on $\{(\mathcal{V}_{1}, \mathcal{V}_{2})\in L_{n}\times L_{n}:\mathcal{V}_{1}\leq \mathcal{V}_{2}\}$, where addition is the standard pointwise addition of functions and multiplication is defined by the convolution
\begin{equation*}
(F\cdot G)(\mathcal{V}_{1},\mathcal{V}_{2}):=\sum_{\mathcal{V}_{1}\leq \mathcal{V}\leq \mathcal{V}_{2}}F(\mathcal{V}_{1},\mathcal{V})G(\mathcal{V},\mathcal{V}_{2}) \, .
\end{equation*}
The unit of this algebra is the \textit{Kronecker $\delta$-function}\index{Kronecker $\delta$-function}
\begin{equation*}
\delta(\mathcal{V}_{1}, \mathcal{V}_{2}):=\left\{\begin{array}{l}
1\quad \mathrm{if}\ \mathcal{V}_{1}=\mathcal{V}_{2},\\
0\quad \mathrm{otherwise},
\end{array}\right.
\end{equation*}
and another fundamental function is the \textit{zeta function}\index{zeta function}
\begin{equation*}
\zeta(\mathcal{V}_{1}, \mathcal{V}_{2}):=1 \quad \mathrm{for \ all} \quad  \mathcal{V}_{1}\leq \mathcal{V}_{2}.
\end{equation*}
The zeta function has an inverse, which is called the \textit{M\"{o}bius function} and denoted by $\mu(\mathcal{V}_{1}, \mathcal{V}_{2})$. The \textit{M\"{o}bius inversion theorem} says that if $f$ and $g$ are functions on $L_{n}$, then the following two conditions are equivalent:
\begin{align}
g(\mathcal{V}_{0})&=\sum_{\mathcal{V}\leq \mathcal{V}_{0}} f(\mathcal{V}) \quad \mathrm{for \ every} \quad \mathcal{V}_{0}\in L_{n}, \label{ch02:eqn2.5.3}\\
f(\mathcal{V}_{0})&=\sum_{\mathcal{V}\leq \mathcal{V}_{0}}g(\mathcal{V})\mu(\mathcal{V}, \mathcal{V}_{0})\quad \mathrm{for\ every} \quad \mathcal{V}_{0}\in L_{n}. \label{ch02:eqn2.5.4}
\end{align}
In fact, put $F(\mathbf{0}_{n}, \mathcal{V})=f(\mathcal{V}), \ G(\mathbf{0}_{n}, \mathcal{V})=g(\mathcal{V})$, and $F(\mathcal{V}_{1}, \mathcal{V}_{2})=G(\mathcal{V}_{1}, \mathcal{V}_{2})=0$ for others. Then (\ref{ch02:eqn2.5.3}) and (\ref{ch02:eqn2.5.4}) are nothing but $G=F \cdot \zeta$ and $F=G \cdot \mu$, respectively, so the above M\"{o}bius inversion is a direct consequence of the relation $\zeta\cdot\mu=\mu\cdot\zeta=\delta$.

Given a probability measure $\mu$, set
\begin{equation*}
g(\mathcal{V}):=\prod_{V\in \mathcal{V}}m_{|V|}(\mu) \quad \mathrm{and} \quad f(\mathcal{V}):=\prod_{V\in \mathcal{V}}s_{|V|}(\mu) \, .
\end{equation*}
Then (\ref{ch02:eqn2.5.1}) tells us that (\ref{ch02:eqn2.5.3}) holds; that is,
\begin{equation*}
g(\mathcal{V}_{0})=\sum_{\mathcal{V}\leq \mathcal{V}_{0}}\prod_{V\in \mathcal{V}}s_{|V|}(\mu) \, ,
\end{equation*}
because if $\mathcal{V}_{0}\in L_{n}$ then one gets all $\mathcal{V}\leq \mathcal{V}_{0}$ by partitioning the blocks of $\mathcal{V}_{0}$. Since $s_{n}(\mu)=f(\mathbf{1}_{n})$, in order to deduce (\ref{ch02:eqn2.5.2}) from (\ref{ch02:eqn2.5.1}) we need to know the values $\mu(\mathcal{V}, \mathbf{1}_{n})$ of the M\"{o}bius function. It is known that
\begin{equation*}
\mu(\mathcal{V},\mathbf{1}_{n})=(-1)^{|\mathcal{V}|-1}(|\mathcal{V}|-1)!,
\end{equation*}
and the formula for $f(\mathbf{1}_{n})$ coming from the M\"{o}bius inversion\index{M\"{o}bius!inversion theorem}\index{M\"{o}bius!inversion process} agrees with (\ref{ch02:eqn2.5.2}).

The set $NC(n)$ of all non-crossing partitions\index{lattice of non-crossing partitions} forms a \textit{lattice} again, and we can consider the \textit{incidence algebra}\index{incidence algebra} over $NC(n)$. The \textit{M\"{o}bius inversion process} is very general, and it is still available in the setting of non-crossing partitions. Nevertheless, the M\"{o}bius function,\index{M\"{o}bius!function} which is defined again as the inverse of the zeta function, is different. It was computed by G. Kreweras.

Before we explain the M\"{o}bius inversion in the setting of the lattice $NC(n)$, it is convenient to introduce the Kreweras complementation on $NC(n)$. Let $\mathcal{V}_{1}$ and $\mathcal{V}_{2}$ be two partitions of $[n]$. By the transformation $i\mapsto 2i-1$ we copy $\mathcal{V}_{1}$ to the set $\{1, 3, \ldots, 2n-1\}$, and similarly we copy $\mathcal{V}_{2}$ to $\{2, 4, \ldots, 2n\}$ by the transformation $i\mapsto 2i$. Combining the two partitions copied on the odd numbers and on the even numbers, we have a partition of $[2n]$, which will be denoted by $\mathcal{V}_{1}\sqcup \mathcal{V}_{2}$. A partition of $[2n]$ is said to be \textit{parity preserving}\index{parity preserving partition}\index{partition!parity preserving} if every block contains only odd or only even numbers. The partiton $\mathcal{V}_{1}\sqcup \mathcal{V}_{2}$ constructed above is parity preserving.

\begin{figure}
\includegraphics{chap02-vend-scan-02.eps}
\caption{Picture of $(1, 2, 3/4) \sqcup (1,2/3,4)$.}
\label{ch02:fig2.2}
\end{figure}

Given $\mathcal{V}\in NC(n)$, there is the largest among all partitions $\mathcal{V}'$ of $[n]$ such that $\mathcal{V}\sqcup \mathcal{V}'$ is non-crossing. We denote this by $K(\mathcal{V})$ and call it the \textit{Kreweras complement}\index{Kreweras complement} of $\mathcal{V}$. Hence $\mathcal{V}\sqcup \mathcal{V}'\in NC(2n)$ if and only if $\mathcal{V}'\leq K(\mathcal{V})$. The Kreweras complementation is an order anti-automorphism of $NC(n)$; that is, the mapping $\mathcal{V}\mapsto K(\mathcal{V})$ is a bijection on $NC(n)$ and $\mathcal{V}_{1}\leq \mathcal{V}_{2}$ is equivalent to $K(\mathcal{V}_{2})\leq K(\mathcal{V}_{1})$.

\begin{figure}
\includegraphics{chap02-vend-scan-03.eps}
\caption{The Kreweras complement of $1, 3/2/4/5$ is $1, 2/3, 4, 5$.}
\label{ch02:fig2.3}
\end{figure}

For a function $F$ in the incidence algebra over $NC(n)$, the involution $\check{F}$ is defined as $\check{F}(\mathcal{V}_{1}, \mathcal{V}_{2}) :=F(K(\mathcal{V}_{2}), K(\mathcal{V}_{1}))$. Then one has $(F\cdot G)^{\check{}}=\check{G}\cdot\check{F}$. Since $\check{\delta}=\delta$ and $\check{\zeta}=\zeta$, the relation $\zeta\cdot\mu=\mu\cdot\zeta=\delta$ yields $\check{\mu}\cdot\zeta=\zeta\cdot\check{\mu}=\delta$, so $\check{\mu}=\mu$ follows. Here we do not give the formula for $\mu(\mathcal{V}_{1}, \mathcal{V}_{2})$ in full generality, but are content with some particular cases:
\begin{equation}
\mu(\mathbf{0}_{n},\mathcal{V})=\prod_{V\in \mathcal{V}}S_{|V|},\quad \mu(\mathcal{V},\mathbf{1}_{n})=\prod_{V\in K(\mathcal{V})}S_{|V|} \, ,
\label{ch02:eqn2.5.5}
\end{equation}
where
\begin{equation*}
S_{k}:=(-1)^{k-1}\frac{1}{k}\left(\begin{array}{c}
2k-2\\
k-1\\
\end{array}\right)=(-1)^{k-1} c_{k-1} \qquad (k\in \mathbb{N}).
\end{equation*}
(The $c_{k}$'s are the Catalan numbers.) The second formula in (\ref{ch02:eqn2.5.5}) is a consequence of the first, thanks to $\check{\mu}=\mu$. On the other hand, the first formula will be determined at the end of this section. The relation $\mu\cdot\zeta=\delta$ implies that
\begin{equation*}
\sum_{\mathcal{V}\leq \mathcal{V}_{0}}\mu(\mathbf{0}_{n},\mathcal{V})=\sum_{\mathcal{V}\leq \mathcal{V}_{0}}\prod_{V\in \mathcal{V}}S_{|V|}=0
\end{equation*}
if $\mathcal{V}_{0}$ is different from $\mathbf{0}_{n}$. Equivalently, this says that
\begin{equation}
\sum_{\mathcal{V} \in NC(n)} \prod_{V\in \mathcal{V}} S_{|V|}=0 \quad \mathrm{for\ every}\quad  n>1.
\label{ch02:eqn2.5.6}
\end{equation}
After the above detour about the M\"{o}bius inversion, we return to the $R$-series.

Since the $R$-series linearizes the additive free convolution, we call the coefficients of the $R$-series $R_{\mu}(z)$ the \textit{free cumulants} of the measure $\mu$. We write $\alpha_{n}(\mu)$ for the $n$th free cumulant, which is the coefficient of $z^{n}$ in the $R$-series $R_{\mu}(z)$. We note that Example~\ref{ch02:exa2.4.6} tells us that the free cumulants of the semicircle law with mean $m$ and variance $\sigma^{2}$ coincide with the semi-invariants of the Gaussian law $N(m, \sigma^{2})$.

We start from the formula (\ref{ch02:eqn2.4.4}) to express moments in terms of free cumulants. Let $\Pi_{n}$ be the set of all sequences $(\varepsilon_{1},\varepsilon_{2}, \ldots, \varepsilon_{n})$ such that $\varepsilon_{i}\in\{-1\}\cup \mathbb{Z}^{+}$ and $\sum_{i=1}^{k}\varepsilon_{i}\geq 0$ for every $1\leq k\leq n$, with $\sum_{i=1}^{k}\varepsilon_{i}=0$ for $k=n$. (\ref{ch02:eqn2.4.4}) shows that
\begin{equation}
m_{n}(\mu)=\sum\alpha_{\varepsilon_{1}+1}\alpha_{\varepsilon_{2}+1}\cdots\alpha_{\varepsilon_{n}+1}\langle\ell^{\varepsilon_{n}}\ell^{\varepsilon_{n-1}}\cdots \ell^{\varepsilon_{1}}\Phi,\Phi\rangle,
\label{ch02:eqn2.5.7}
\end{equation}
where $\varepsilon_{i} \quad  \in \quad  \{-1\} \ \, \cup \ \, \mathbb{Z}^{+}$ and $\ell^{-1}$ is for $\ell^{*}$. The vacuum expectation $\langle \ell^{\varepsilon_{n}}\ell^{\varepsilon_{n-1}}\cdots\ell^{\varepsilon_{1}}\Phi, \Phi\rangle$ is different from $0$ if and only if $(\varepsilon_{1}, \varepsilon_{2}, \ldots, \varepsilon_{n})\in\Pi_{n}$, and then the expectation is 1. Hence the sum (\ref{ch02:eqn2.5.7}) may run over $(\varepsilon_{1},\varepsilon_{2}, \ldots, \varepsilon_{n})\in\Pi_{n}$.

To each $(\varepsilon_{1},\varepsilon_{2}, \ldots, \varepsilon_{n})\in\Pi_{n}$ we can associate a polygonal line from $(0,0)$ to $(n, 0)$ going through the lattice points $(1, \varepsilon_{1}), (2, \varepsilon_{1}+\varepsilon_{2}), \ldots, (n-1,\varepsilon_{1}+\varepsilon_{2}+\cdots+ \varepsilon_{n-1})$. (The polygonal lines associated with $\pm 1$ sequences are familiar from the proof of Theorem~\ref{ch01:the1.1.5} in Chap.~\ref{ch01:chap01}.)

\begin{lemma}\label{ch02:lem2.5.1}
There is a one-to-one correspondence between $\Pi_{n}$ and $NC(n)$. The sequence $(\varepsilon_{1},\varepsilon_{2}, \ldots, \varepsilon_{n})\in\Pi_{n}$ associated with a non-crossing partition $\mathcal{V}= \{V_{1}, V_{2}, \ldots, V_{k}\}$ is described as follows:
\begin{equation*}
\varepsilon_{i}=\left\{\begin{array}{ll}
|V_{j}|-1 & if \ i\ is\ the\ smallest\ element\ of\ V_{j},\\
-1 & otherwise.\\
\end{array}\right.
\end{equation*}
\end{lemma}

\begin{proof2}
The proof is an elementary induction.
\end{proof2}

If we rewrite (\ref{ch02:eqn2.5.7}) by means of the bijection described in the previous lemma, then we arrive at
\begin{equation}
m_{n}(\mu)=\sum_{\mathcal{V}\in NC(n)}\prod_{\, i=1}^{k}\alpha_{|V_{i}|}(\mu),
\label{ch02:eqn2.5.8}
\end{equation}
which shows remarkable similarity to (\ref{ch02:eqn2.5.1}), but the summation here is over the non-crossing partitions $\mathcal{V}=\{V_{1}, V_{2}, \ldots, V_{k}\}$. We call (\ref{ch02:eqn2.5.8}) the \textit{free moment-cumulant formula}.\index{moment-cumulant formula, free}\index{free!moment-cumulant formula}\index{free!cumulant}\index{cumulant!free} Consider the functions
\begin{align*}
g(\mathcal{V}_{0})& \ := \ \prod_{V_{0}\in \mathcal{V}_{0}} m_{|V_{0}|}(\mu)=\prod_{V_{0}\in \mathcal{V}_{0}}\,\sum_{\, \mathcal{U}\in NC(|V_{0}|)}\,\prod_{\,U\in \mathcal{U}}\alpha_{|U|}(\mu)\\
& \ = \ \sum_{\mathcal{V}\leq \mathcal{V}_{0}}\,\prod_{V\in \mathcal{V}}\alpha_{|V|}(\mu)
\end{align*}
and
\begin{equation*}
f(\mathcal{V}):=\prod_{V\in \mathcal{V}}\alpha_{|V|}(\mu) \, ,
\end{equation*}
where $\mathcal{V}, \mathcal{V}_{0}$ and $\mathcal{U}$ are all non-crossing partitions. We observe that (\ref{ch02:eqn2.5.3}) holds, so the inverse of the free moment-cumulant formula (\ref{ch02:eqn2.5.8}) is
\begin{equation}
\alpha_{n}(\mu)=\sum_{\mathcal{V}\in NC(n)}\,\prod_{\, V\in \mathcal{V}}m_{|V|}(\mu)\prod_{U\in K(\mathcal{V})}S_{|U|},
\label{ch02:eqn2.5.9}
\end{equation}
as a consequence of the M\"{o}bius inversion and (\ref{ch02:eqn2.5.5}).

\begin{example}\label{ch02:exa2.5.2}
All semi-invariants of the classical Poisson distribution are the same. So we may call a probability measure $\mu$ on $\mathbb{R}$ the \textit{free Poisson distribution}\index{free!Poisson distribution}\index{distribution!free Poisson} if $\alpha_{n}(\mu)=\lambda>0$ for every $n$. From (\ref{ch02:eqn2.5.8}) we have
\begin{equation*}
m_{n}(\mu)=\sum_{i=1}^{n}Q_{n}^{(i)}\lambda^{i} \, ,
\end{equation*}
where $Q_{n}^{(i)}$ denotes the number of non-crossing partitions of $[n]$ into $i$ blocks, given by
\begin{equation}
Q_{n}^{(i)}=\frac{1}{n} \left(\begin{array}{l}
n\\
i\\
\end{array}\right)\left(\begin{array}{c}
n\\
i-1\\
\end{array}\right).
\label{ch02:eqn2.5.10}
\end{equation}
It is known that
\begin{equation}
\sum_{i=1}^{n}Q_{n}^{(i)}=\frac{1}{n+1}\left(\begin{array}{c}
2n\\
n\\ \end{array}
\right).
\label{ch02:eqn2.5.11}
\end{equation}
This means by Lemma~\ref{ch02:lem2.3.1} that the number of non-crossing partitions of $[n]$ is equal to the number of non-crossing pair partitions of $[2n]$. (Indeed, this fact will be seen in the proof of Proposition~\ref{ch02:pro2.6.3}.) Hence, if $\mu_{1}$ denotes the free Poisson distribution in the case $\lambda=1$, then we have $m_{n}(\mu_{1})=m_{2n}(w_{2}) \ (n\in \mathbb{N})$,  which shows that
\begin{equation}
\mu_{1}=\frac{w_{2}(\sqrt{x})}{\sqrt{x}}\chi_{(0,4]}(x)\,dx=\frac{\sqrt{4x-x^{2}}}{2\pi x}\chi_{(0,4]}(x)\,dx \,.
\label{ch02:eqn2.5.12}
\end{equation}
This says that if a random variable $\xi$ is distributed according to the standard semicircle law, then the distribution of $\xi^{2}$ is $\mu_{1}$.

The free Poisson distribution is one of the fundamental distributions of free probability theory. The density of the general free Poisson measure will be given in Example~\ref{ch03:exa3.3.5}. Similarly to the classical Poisson distribition, the parameter $\lambda$ is the mean and the variance at the same time. For parameter values $\lambda\geq 1$ the measure $\mu_{\lambda}$ is absolutely continuous with respect to the Lebesgue measure.
\end{example}

We want to consider vacuum expectations in $(\tilde{\mathcal{E}}_{2},\tilde{\omega}_{2})$ like
\begin{equation}
\tilde{\omega}_{2}(\ell_{1}^{\varepsilon_{n}}\ell_{2}^{\delta_{n}}\ell_{1}^{\varepsilon_{n-1}}\cdots \ell_{1}^{\varepsilon_{1}}\ell_{2}^{\delta_{1}})=\langle \ell_{1}^{\varepsilon_{n}}\ell_{2}^{\delta_{n}}\ell_{1}^{\varepsilon_{n-1}}\cdots \ell_{1}^{\varepsilon_{1}}\ell_{2}^{\delta_{1}}\Phi, \Phi\rangle \, ,
\label{ch02:eqn2.5.13}
\end{equation}
where $\varepsilon_{i},\delta_{i}\in\{-1\}\cup \mathbb{Z}^{+}(1\leq i\leq n)$ and $\ell_{j}^{-1}$ is for $\ell_{j}^{*}(j=1,2)$. (What we have to know about the operators $\ell_{1}$ and $\ell_{2}$ is the relations $\ell_{1}^{*}\ell_{1}=\ell_{2}^{*}\ell_{2}=\mathbf{1},\, \ell_{1}^{*}\ell_{2}=\ell_{2}^{*}\ell_{1}=0$, and $\ell_{1}^{*}\Phi=\ell_{2}^{*}\Phi=0.)$ Our aim is to find a necessary and sufficient condition for (\ref{ch02:eqn2.5.13}) to be nonzero. Reading a monomial of $\ell_{1}, \ell_{1}^{*}, \ell_{2}, \ell_{2}^{*}, \mathbf{1}$ from right to left, we think that $\ell_{1}$ creates a black particle, $\ell_{2}$ creates a white particle, $\ell_{1}^{*}$ annihilates a black particle, and $\ell_{2}^{*}$ annihilates a white particle. Moreover, the application of a factor $\mathbf{1}$ does not have any effect. Starting from the vacuum $\Phi$, we have to return to the vacuum after a procedure of creations and annihilations. The procedure stops if the annihilation of a particle of a certain colour was not preceded in a previous step by the creation of a particle of the same colour, or if a particle of the same colour was previously created but after that a particle of the different colour was created and not annihilated. Since particles of different colours do not interfere, a necessary condition for (\ref{ch02:eqn2.5.13}) to be nonzero is that
\begin{equation}
(\varepsilon_{1},\varepsilon_{2},\ldots,\varepsilon_{n}),\, (\delta_{1},\delta_{2},\ldots,\delta_{n})\in\Pi_{n} \, .
\label{ch02:eqn2.5.14}
\end{equation}
If this is fulfilled, then by Lemma~\ref{ch02:lem2.5.1} we can associate to $(\varepsilon_{1}, \varepsilon_{2}, \ldots, \varepsilon_{n})$ a non-crossing partition $\mathcal{V}_{\varepsilon}$ of $[n]$ and to $(\delta_{1}, \delta_{2}, \ldots, \delta_{n})$ another non-crossing partition $\mathcal{V}_{\delta}$ of $[n]$. Then we have the partition $\mathcal{V}_{\delta}\sqcup \mathcal{V}_{\varepsilon}$ of $[2n]$ which was introduced in defining the Kreweras complement. Recall that $\mathcal{V}_{\delta}$ is copied on $\{1, 3, \ldots, 2n-1\}$ while $\mathcal{V}_{\varepsilon}$ is copied on $\{2, 4, \ldots, 2n\}$.

\begin{lemma}\label{ch02:lem2.5.3}
The expectation value \emph{(\ref{ch02:eqn2.5.13})} is different from $0$ if and only if \emph{(\ref{ch02:eqn2.5.14})} holds and the partition $\mathcal{V}_{\delta}\sqcup \mathcal{V}_{\varepsilon}$ of $[2n]$ is non-crossing.
\end{lemma}

\begin{proof2}
We remarked already that $\ell_{1}^{\varepsilon_{n}}\ell_{2}^{\delta_{n}}\ell_{1}^{\varepsilon_{n-1}}\cdots\ell_{1}^{\varepsilon_{1}}\ell_{2}^{\delta_{1}}\Phi=\Phi$ implies (\ref{ch02:eqn2.5.14}) and we have the partition $\mathcal{V}_{\delta}\sqcup \mathcal{V}_{\varepsilon}$ at our disposal. We have to show that this partition is non-crossing.

Recall that the zeroth powers of $\ell_{1}$ and $\ell_{2}$ in the string $\ell_{1}^{\varepsilon_{n}}\ell_{2}^{\delta_{n}}\ell_{1}^{\varepsilon_{n-1}} \cdots \ell_{1}^{\varepsilon_{1}}\ell_{2}^{\delta_{1}}$ give singleton blocks in the partition $\mathcal{V} :=\mathcal{V}_{\delta}\sqcup \mathcal{V}_{\varepsilon}$. If we remove the zeroth powers (i.e. the identity \textbf{1}) from the string and the singletons from the partition $\mathcal{V}$, then the crossing number will not change. If the remaining string contains a factor $\ell_{1}^{*}$, then it contains also a positive power of $\ell_{1}$, and we find a substring $\ell_{1}^{*}\cdots \ell_{1}^{k}$ such that $k>0$ and there are no powers of $\ell_{1}$ in between. If we cannot find powers of $\ell_{2}$ in between, then actually $\ell_{1}^{*}$ and $\ell_{1}^{k}$ are neighbors and we have $\ell_{1}^{*}\ell_{1}^{k}$. In the corresponding partition the position of this $\ell_{1}^{k}$ is the first in a block of $k+1$ elements, and the position of $\ell_{1}^{*}$ is the second in this block. Now we remove $\ell_{1}^{*}$ from the string and its position from the corresponding partition, and replace $\ell_{1}^{k}$ by $\ell_{1}^{k-1}$. In this way the remaining string still corresponds to the new partition, and the change did not effect the crossing number. If there are $\ell_{2}$'s between $\ell_{1}^{*}$ and $\ell_{1}^{k}$, then we choose a substring $\ell_{2}^{*}\ell_{2}^{m}$ of $\ell_{1}^{*}\cdots\ell_{1}^{k}$ and act similarly to the previous step.

When originally $\ell_{1}^{\varepsilon_{n}}\ell_{2}^{\delta_{n}}\ell_{1}^{\varepsilon_{n-1}}\cdots\ell_{1}^{\varepsilon_{1}}\ell_{2}^{\delta_{1}}\Phi=\Phi$, we can continue the above procedure until the long product of $\ell_{1}$'s and $\ell_{2}$'s becomes empty. Since the crossing number of the partition does not change during the procedure, $\mathcal{V}$ was non-crossing.

The other direction of the assertion is obtained similarly by reversing the reasoning; the change of the string is governed by the partition $\mathcal{V}$. At each step we shrink a block of $\mathcal{V}$ possessing neighboring smallest elements.
\end{proof2}

\begin{proposition}\label{ch02:pro2.5.4}
Assume that $(\mathcal{A}, \varphi)$ is a noncommutative probability space and $a, b\in \mathcal{A}$ are in free relation. Then the moments of $ab$ are expressed by the free cumulants of $a$ and $b$ as follows:
\begin{equation}
m_{n}(ab)=\sum_{\mathcal{V}_{1},\mathcal{V}_{2}}\,\prod_{V_{1}\in \mathcal{V}_{1}}\alpha_{|V_{1}|}(a)\prod_{V_{2}\in \mathcal{V}_{2}}\alpha_{|V_{2}|}(b)\, ,
\label{ch02:eqn2.5.15}
\end{equation}
where the summation is over all partitions $\mathcal{V}_{1}$ and $\mathcal{V}_{2}$ of $[n]$ such that $\mathcal{V}_{1}\sqcup \mathcal{V}_{2}$ is non-crossing.
\end{proposition}

\begin{proof2}
Thanks to (\ref{ch02:eqn2.2.3}) it is immediate to see that each $m_{n}(ab)$ is written as a certain polynomial of the moments of $a$ and $b$. So we may assume that
\begin{equation*}
a=\sum_{k=-1}^{\infty}\alpha_{k+1}(a)\ell_{1}^{k} \quad \mathrm{and} \quad b=\sum_{k=-1}^{\infty}\alpha_{k+1}(b)\ell_{2}^{k}
\end{equation*}
are the canonical noncommutative random variables in $\tilde{\mathcal{E}}_{2}$. We use the notation of the previous lemma. (In particular, $\ell_{j}^{-1}$ stands for $\ell_{j}^{*}$ and $\alpha_{0}(a)=\alpha_{0}(b)=1$.) We have
\begin{align*}
m_{n}(ab) & \ = \ \left\langle\left(\sum_{k,m}\alpha_{k+1}(a)\alpha_{m+1}(b)\ell_{1}^{k}\ell_{2}^{m}\right)^{n}\Phi, \Phi\right\rangle\\
& \ = \ \sum\alpha_{\varepsilon_{1}+1}(a)\alpha_{\varepsilon_{2}+1}(a)\cdots\alpha_{\varepsilon_{n}+1}(a)\alpha_{\delta_{1}+1}(b)\alpha_{\delta_{2}+1}(b)\cdots\alpha_{\delta_{n}+1}(b)\\
& \qquad \qquad \ \times\langle \ell_{1}^{\varepsilon_{n}}\ell_{2}^{\delta_{n}}\ell_{1}^{\varepsilon_{n-1}}\cdots\ell_{1}^{\varepsilon_{1}}\ell_{2}^{\delta_{1}}\Phi,\Phi\rangle,
\end{align*}
where the sum is over all sequences $(\varepsilon_{1},\varepsilon_{2}, \ldots, \varepsilon_{n}), \, (\delta_{1}, \delta_{2}, \ldots, \delta_{n})$ described in the previous lemma. Invoking the lemma yields (\ref{ch02:eqn2.5.15}) when the summation is for $\mathcal{V}_{1}, \mathcal{V}_{2}$ such that $\mathcal{V}_{2}\sqcup \mathcal{V}_{1}$ is non-crossing. But the non-crossingness of $\mathcal{V}_{2}\sqcup \mathcal{V}_{1}$ is equivalent to that of $\mathcal{V}_{1}'\sqcup \mathcal{V}_{2}$ when $\mathcal{V}_{1}'$ is the transform of $\mathcal{V}_{1}$ by the cyclic permutation of $[n]$, and clearly $\prod_{V_{1}\in \mathcal{V}_{1}}\, \alpha_{|V_{1}|}(a)$ does not change under this transformation. Hence the conclusion follows.
\end{proof2}

Let $a$ and $b$ be as in the above proposition. Let $\mu_{1}$ be the distribution of $a$ and $\mu_{2}$ be that of $b$. Similarly to the idea of additive free convolution, Voiculescu introduced the \textit{multiplicative free convolution}\index{free!convolution, multiplicative}\index{convolution!multiplicative free} of $\mu_{1}$ and $\mu_{2}$ as the distribution $\mu$ (in the abstract sense in general) of $ab$ in $(\mathcal{A}, \varphi)$. When $\mu_{1}$ and $\mu_{2}$ are compactly supported measures on $\mathbb{R}$ and furthermore $\mu_{1}$ is supported in $\mathbb{R}^{+}$, the multiplicative free convolution\index{multiplicative free convolution} $\mu$ is a compactly supported measure on $\mathbb{R}$. Indeed, one can find a $C^{*}$-probability space $(\mathcal{A}, \tau)$ with a tracial state $\tau$ and elements $a\geq 0$ and $b=b^{*}$ in $\mathcal{A}$ having the distributions $\mu_{1}$ and $\mu_{2}$, respectively. Then $\mu$ can be obtained from the spectral measure of $a^{1/2}ba^{1/2}$ becuase $\tau((ab)^{n})=\tau((a^{1/2}ba^{1/2})^{n})$; here the uniqueness of $\mu$ is guaranteed by Proposition~\ref{ch02:pro2.5.4}. In particular, if both $\mu_{1}$ and $\mu_{2}$ are supported in $\mathbb{R}^{+}$, then so is $\mu$.

By means of the Kreweras complementation we can reformulate Proposition~\ref{ch02:pro2.5.4} as follows:
\begin{equation*}
m_{n}(ab)=\sum_{\mathcal{V}\in NC(n)}\,\prod_{\, V\in \mathcal{V}}\alpha_{|V|}(a)\sum_{\mathcal{V}'\leq K(\mathcal{V})}\,\prod_{\, V'\in \mathcal{V}'}\alpha_{|V'|}(b) \, .
\end{equation*}
From the moment-cumulant formula (\ref{ch02:eqn2.5.8}) it is immediate that the above sum over $\mathcal{V}'\leq K(\mathcal{V})$ is nothing else but the product $\prod_{U\in K(\mathcal{V})}m_{|U|}(b)$ of moments of $b$. So
\begin{equation}
m_{n}(ab)=\sum_{\mathcal{V}\in NC(n)}\,\prod_{V\in \mathcal{V}}\alpha_{|V|}(a)\prod_{U\in K(\mathcal{V})}m_{|U|}(b) \, .
\label{ch02:eqn2.5.16}
\end{equation}
It seems worthwhile to translate this formula into a context of formal power series. Let $F(X)=\sum_{n=1}^{\infty}a_{n}X^{n}$ and $G(X)=\sum_{n=1}^{\infty}b_{n}X^{n}$ be two formal power series, and set
\begin{equation*}
t_{n}:=\sum_{\mathcal{V}\in NC(n)}\,\prod_{\, V\in \mathcal{V}}a_{|V|}\prod_{U\in K(\mathcal{V})}b_{|U|} \qquad (n\in \mathbb{N}).
\end{equation*}
Then we call the series $(F\star G)(X) :=\sum_{n=1}^{\infty}t_{n}X^{n}$ the \textit{combinatorial convolution}\index{combinatorial convolution} of $F$ and $G$. We denote by $\mathbb{C}_{\infty}\langle X\rangle$ the algebra of all formal power series without constant term. The combinatorial convolution\index{convolution!combinatorial}\index{moment!generating series} is commutative and associative. The series $Sum(X) :=X$ is the unit; that is, $Sum \star F=F\star Sum=F$ for every $F\in \mathbb{C}_{\infty}\langle X\rangle$. (Our notation for $Sum(X) :=X$ might seem strange, but in the multivariable case $Sum(X_{1}, X_{2}, \ldots, X_{n}) :=X_{1}+X_{2}+\cdots+X_{n}$ is a rather natural notation, the case $n=1$ being a bit degenerate.)

In the new language the formula (\ref{ch02:eqn2.5.16}) reads simply
\begin{equation}
M_{ab}=R_{a}\star M_{b} \, ,
\label{ch02:eqn2.5.17}
\end{equation}
where the \textit{moment generating series} $M_{b}(X)\in \mathbb{C}_{\infty}\langle X\rangle$ is defined as
\begin{equation*}
M_{b}(X):=\sum_{n=1}^{\infty}m_{n}(b)X^{n} \, .
\end{equation*}
Further evidence of the relevance of the combinatorial convolution is supplied by the fact that the moment-cumulant formulas (\ref{ch02:eqn2.5.8}) and (\ref{ch02:eqn2.5.9}) admit a form of convolution:
\begin{equation}
M_{\mu}=R_{\mu}\star Zeta, \quad R_{\mu}=M_{\mu}\star Moeb,
\label{ch02:eqn2.5.18}
\end{equation}
where
\begin{equation*}
Zeta(X) :=\sum_{n=1}^{\infty}X^{n}, \quad Moeb(X) :=\sum_{n=1}^{\infty}(-1)^{n-1}c_{n-1}X^{n} \, .
\end{equation*}
The condition (\ref{ch02:eqn2.5.6}) is actually equivalent to the relation
\begin{equation}
Moeb \, \star \, Zeta=Sum \, ,
\label{ch02:eqn2.5.19}
\end{equation}
so \textit{Moeb} and \textit{Zeta} are the inverse of each other with respect to the combinatorial convolution.

From (\ref{ch02:eqn2.5.17}) and (\ref{ch02:eqn2.5.18}) we obtain the following:

\begin{theorem}\label{ch02:the2.5.5}
If $a$ and $b$ are free noncommutative random variables, then
\begin{equation*}
R_{ab}=R_{a}\star R_{b} \, .
\end{equation*}
\end{theorem}

The next theorem gives us a remarkable relation between $M_{\mu}$ and $R_{\mu}$, which will be useful in Sec.~\ref{ch03:sec3.2}.

\begin{theorem}\label{ch02:the2.5.6}
If $F, G\in \mathbb{C}_{\infty}\langle X\rangle$ satisfy $G=F\star Zeta$, then
\begin{equation*}
F(X(1+G(X)))=G(X) \quad and \quad G\left(\frac{X}{1+F(X)}\right)=F(X)
\end{equation*}
as formal power series.
\end{theorem}

\begin{proof2}
For coefficients $a_{n}$ of $F$ and $b_{n}$ of $G$ the assumption means that
\begin{equation}
b_{n}=\sum_{\mathcal{V}\in NC(n)}\,\prod_{\, V\in \mathcal{V}}a_{|V|} \qquad (n\in \mathbb{N}).
\label{ch02:eqn2.5.20}
\end{equation}
Let $V_{1}=\{v(1)=1, v(2), \ldots, v(k)\}$ be the first block of $\mathcal{V}\in NC(n)$. Then it is obvious that any $\mathcal{V}\in NC(n)$ containing $V_{1}$ is the combination of $\{V_{1}\}$ and partitions taken from $NC([v(j)+1, v(j+1)-1])$ for each $1\leq j\leq k$ such that $v(j+1)-v(j)>1$, where $v(k+1) :=n+1$. Since the data relevant to our question are only the block-lengths $|V|\ (V\in \mathcal{V})$, we may take $NC(v(j+1)-v(j)-1)$ in place of $NC([v(j)+1, v(j+1)-1])$. Hence by (\ref{ch02:eqn2.5.20}) we have
\begin{align*}
b_{n} & \ = \ \sum_{k=1}^{n}\,\sum_{v(1)=1<v(2)<\ldots <v(k)} a_{k} \prod_{\begin{subarray}{c}1\leq j\leq k\\v(j+1)-v(j)>1\\ \end{subarray}}
\left(\sum_{\mathcal{V}\in NC(v(j+1)-v(j)-1)}\,\prod_{V\in \mathcal{V}}a_{|V|}\right)\\
&\ = \ \sum_{k=1}^{n} a_{k}\,\sum_{v(1)=1< v(2) < \ldots < v(k)}\prod_{\begin{subarray}{c}1\leq j \leq k\\ v(j+1)-v(j)>1\\ \end{subarray} } b_{v(j+1)-v(j)-1}\\
& \ = \ \sum_{k=1}^{n} a_{k} \,\sum_{\begin{subarray}{c}i(1),\ldots, i(k)\geq 0\\ i(1)+\cdots+ i(k)=n-k\\ \end{subarray}} b_{i(1)}b_{i(2)}\cdots b_{i(k)},
\end{align*}
with the convention $b_{0}:=1$. The above expression implies that
\begin{align*}
G(X)& \ =\ \sum_{n=1}^{\infty}\left(\sum_{k=1}^{n}a_{k}\sum_{\begin{subarray}{c}i(1),\ldots,i(k)\geq 0\\ i(1)+\cdots +i(k)=n-k\\ \end{subarray}}b_{i(1)}b_{i(2)}\cdots b_{i(k)}\right)X^{n}\\
& \ = \ \sum_{k=1}^{\infty}a_{k}X^{k}\left(\sum_{i(1),\ldots, i(k)=0}^{\infty}b_{i(1)}b_{i(2)}\cdots b_{i(k)}\right)X^{i(1)+i(2)+\cdots+i(k)}\\
& \ =\ \sum_{k=1}^{\infty}a_{k}X^{k}\left(1+ \sum_{i=1}^{\infty}b_{i}X^{i}\right)^{k}\\
& \ = \ F(X(1+G(X))) \, .
\end{align*}
Next, put $Y:=X(1+G(X))$. Since $G(X)=F(Y)$ and $X=Y/(1+F(Y))$, we have $F(Y)=G(Y/(1+F(Y)))$, implying the second equation.
\end{proof2}

It is worth noting that one can use the previous theorem to determine the coefficients of \textit{Moeb}. Indeed, due to (\ref{ch02:eqn2.5.19}) the second equation of the theorem yields
\begin{equation*}
Moeb(X)^{2}+Moeb(X)-X=0,
\end{equation*}
and hence
\begin{equation*}
Moeb(X) =-\frac{1}{2}(1-\sqrt{1+4X})=\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n} \left(\begin{array}{c}
2n -2\\
n -1\\
\end{array}\right) X^{n}
\end{equation*}
by (\ref{ch02:eqn2.3.3}). On the other hand, this expression says that (\ref{ch02:eqn2.5.6}) holds true if $S_{n}= (-1)^{n-1}c_{n-1}$, and it determines the values of $\mu(\mathbf{0}_{n}, \mathcal{V})$ as in (\ref{ch02:eqn2.5.5}).

\section{Multivariables}
\label{ch02:sec2.6}

\noindent Random vectors are common in classical probability theory. The joint distribution of two classical random variables is the same as the distribution of the random vector formed by the two variables. For two noncommutative random variables $a_{1}$ and $a_{2}$ in the same noncommutative probability space $(\mathcal{A}, \varphi)$, their joint distribution was introduced in Sec.~\ref{ch01:sec1.2}. The aim of this section is to extend the concept of free cumulants and the moment-cumulant formula to a $k$-tuple $(a_{1}, a_{2}, \ldots, a_{k})$ of noncommutative random variables.

Let $\tilde{\mathcal{E}}_{k}$ consist of the infinite sums
\begin{align*}
T = \sum_{m,n=0}^{\infty}\,\sum_{i(1),\ldots,i(m);j(1),\ldots,j(n)} & \quad c_{i(1),i(2),\ldots, i(m);j(1),j(2),\ldots,j(n)}\\
& \quad \times\ell_{i(1)}\ell_{i(2)}\cdots  \ell_{i(m)}\ell_{j(1)}^{*}\ell_{j(2)}^{*}\ldots \ell_{j(n)}^{*},
\end{align*}
where $c_{i(1),\ldots,i(m);j(1),\ldots,j(n)}\in \mathbb{C}$ for $i(1), \ldots, i(m), j(1), \ldots, j(n)\in[k]$ and there exists an $N\in \mathbb{N}$ such that $c_{i(1),\ldots,i(m);j(1),\ldots,j(n)}=0$ whenever $n>N$. The multiplication is governed by the computational rules $\ell_{i}^{*}\ell_{i}=\mathbf{1}$ and $\ell_{i}^{*}\ell_{j}=0$ for $i\neq j$. The condition on the coefficients ensures that in the multiplication of two infinite sums we have to add only finitely many numbers. On $\tilde{\mathcal{E}}_{k}$ we consider a normalized linear functional $\tilde{\omega}_{k}$ sending each infinite sum to its constant term.

Let $(a_{1}, a_{2}, \ldots, a_{k})$ be a $k$-tuple of noncommutative random variables in a noncommutative probability space $(\mathcal{A}, \varphi)$. The infinite sum
\begin{equation}
T=\mathbf{1}+\sum\alpha(i(1),i(2),\ldots,i(m))\ell_{i(m)}\ell_{i(m-1)}\cdots\ell_{i(1)}\in\tilde{\mathcal{E}}_{k}
\label{ch02:eqn2.6.1}
\end{equation}
is said to be the \textit{canonical representation} of the $k$-tuple if $(a_{1}, a_{2}, \ldots, a_{k})$ and $(\ell_{1}^{*}T, \ell_{2}^{*}T, \ldots, \ell_{k}^{*}T)$ have the same joint distribution, that is, the equality
\begin{equation*}
\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(n)})=\tilde{\omega}_{k}(\ell_{i(1)}^{*}T\ell_{i(2)}^{*}T\cdots\ell_{i(n)}^{*}T)
\end{equation*}
holds for every $i(1), i(2), \ldots, i(n)\in[k]$ and $n\in \mathbb{N}$. (Later it will be shown that the canonical representation exists and is unique.)

\begin{example}\label{ch02:exa2.6.1}
A noncommutative random variable\index{noncommutative!random variable, semicircular}\index{canonical!representation} in a $C^{*}$-probability space is said to be (standard) \textit{semicircular}\index{semicircular!element} if it is selfadjoint and has the standard semicircle law. Let $a$ and $b$ be standard semicircular elements in free relation. Then the canonical representation of the pair $(a,b)$ is $T=\mathbf{1}+\ell_{1}^{2}+\ell_{2}^{2}$.

Indeed, $\ell_{i}^{*}T=\ell_{i}^{*}+\ell_{i}(i=1,2)$, and they are free and semicircularly distributed according to Theorem~\ref{ch01:the1.1.5} and Example~\ref{ch02:exa2.2.1}.
\end{example}

The use of infinite power series has proved convenient in the one-variable case, and we are now making the multivariable generalization. We denote by $\mathbb{C}_{\infty}\langle X_{1}, X_{2}, \ldots,  X_{k}\rangle$ the set of all infinite power series of noncommuting\index{noncommutative!random variable, circular} indeterminates $X_{1}, X_{2}, \ldots, X_{k}$ without constant term. When $F$ is such a series and $i(1), i(2), \ldots, i(n)\in[k]$, we use the notation
\begin{equation*}
\mathrm{coef}[i(1), i(2), \ldots, i(n)](F)
\end{equation*}
for the coefficient of $X_{i(1)}X_{i(2)}\cdots X_{i(n)}$. If $V=\{h_{1}<h_{2}<\ldots<h_{r}\}\subset[n]$, then we set
\begin{equation*}
[i(1),i(2),\ldots,i(n)|V]:=[i(h_{1}),i(h_{2}),\ldots,i(h_{r})].
\end{equation*}
If $\mathcal{V}$ is a partition of $[n]$, then we write
\begin{equation}
\mathrm{coef}[i(1), i(2), \ldots, i(n);\mathcal{V}](F) :=\prod_{V\in \mathcal{V}} \mathrm{coef}[i(1), i(2), \ldots, i(n)|V](F)\,.
\label{ch02:eqn2.6.2}
\end{equation}
We say that $M\in \mathbb{C}_{\infty}\langle X_{1}, X_{2}, \ldots, X_{k}\rangle$ is the \textit{moment generating series}\index{moment!generating series}\index{circular!element} of a $k$-tuple $(a_{1}, a_{2}, \ldots, a_{k})$ when the following relation holds for every $i(1), \ldots, i(n)\in[k]$ and $n\in \mathbb{N}$:
\begin{equation*}
\mathrm{coef}[i(1), i(2), \ldots, i(n)](M)=\varphi(a_{i(1)}a_{i(2)}\cdots a_{i(n)})\,.
\end{equation*}
We use the notation $M_{(a_{1},a_{2},\ldots,a_{k})}$ for this series. The $R$-\textit{series} $R_{(a_{1},a_{2},\ldots,a_{k})}$ of $(a_{1}, a_{2}, \ldots, a_{k})$ is determined by the relation
\begin{equation*}
\mathrm{coef}[i(1), i(2), \ldots, i(n)](R)=\alpha(i(1), i(2), \ldots, i(n)),
\end{equation*}
where $\alpha$ is from the canonical representation (\ref{ch02:eqn2.6.1}).

\begin{example}\label{ch02:exa2.6.2}
A noncommutative random variable $c$ is said to be (standard) \textit{circular} if it is written in the form $c=(a+\mathrm{i}\,b)/\sqrt{2}$, where $a$ and $b$ are free standard semicircular elements. For such $a, b, c$ the $R$-series of the pairs $(a, b)$ and $(c, c^{*})$ are $X_{1}^{2}+X_{2}^{2}$ and $X_{1}X_{2}+X_{2}X_{1}$, respectively. Moreover, we have $R_{c^{*}c}= Zeta$, so the distribution of $c^{*}c$ is the free Poisson measure $\mu_{1}$ in (\ref{ch02:eqn2.5.12}), and that of $|c|=(c^{*}c)^{1/2}$ is
\begin{equation}
\frac{1}{\pi}\sqrt{4-x^{2}}\chi_{[0,2]}(x)\,dx\,.
\label{ch02:eqn2.6.3}
\end{equation}

The pair $(a, b)$ was already treated in the previous example, and its $R$-series was known.

A representation of $a$ is $\ell(f)^{*}+\ell(f)$ on the full Fock space with a unit vector $f$. Similarly, $b$ is represented as $\ell(g)^{*}+\ell(g)$ with $\Vert g\Vert=1$. The condition $\langle f, g\rangle =0$ assures the free relation. So
\begin{equation*}
\frac{\ell(f)^{*}+\ell(f)}{\sqrt{2}}+\mathrm{i}\frac{\ell(g)^{*}+\ell(g)}{\sqrt{2}}=\ell\left(\frac{f-\mathrm{i}\,g}{\sqrt{2}}\right)^{*}+\ell\left(\frac{f+\mathrm{i}\,g}{\sqrt{2}}\right)
\end{equation*}
represents $c$. Since $(f+\mathrm{i}\,g)/\sqrt{2}$ and $(f-\mathrm{i}\,g)/\sqrt{2}$ are orthogonal unit vectors, this variable has the same distribution as $\ell_{1}^{*}+\ell_{2}$ in $\tilde{\mathcal{E}}_{2}$. So the pair $(c, c^{*})$ has the same distribution as $(\ell_{1}^{*}+\ell_{2}, \ell_{2}^{*}+\ell_{1})$. Now it is clear that the canonical representation of the pair is $\mathbf{1}+\ell_{1}\ell_{2}+\ell_{2}\ell_{1}$, and consequently $X_{1}X_{2}+X_{2}X_{1}$ is the $R$-series

For the second assertion, since Example~\ref{ch02:exa2.5.2} says that \textit{Zeta} is the $R$-series of $(\ell_{1}^{*}+\ell_{1})^{2}$, what we need to prove is
\begin{equation*}
\tilde{\omega}_{2}(((\ell_{1}+\ell_{2}^{*})(\ell_{1}^{*}+\ell_{2}))^{n})=\tilde{\omega}_{2}((\ell_{1}^{*}+\ell_{1})^{2n}) \qquad (n\in \mathbb{N}).
\end{equation*}
Now let $a_{1}=\ell_{1}\ell_{1}^{*},\,a_{2}=\ell_{1}\ell_{2},\,a_{3}=\ell_{2}^{*}\ell_{1}^{*}$ and $b_{1}=\ell_{1}\ell_{1}^{*},\,b_{2}=\ell_{1}^{2},\,b_{3}=\ell_{1}^{*2}$. Then it suffices to prove that for every $(i_{1}, \ldots, i_{n})\in\{1,2,3\}^{n}(n\in \mathbb{N})$
\begin{equation}
\tilde{\omega}_{2}(a_{i_{1}}a_{i_{2}}\cdots a_{i_{n}})=\tilde{\omega}_{2}(b_{i_{1}}b_{i_{2}}\cdots b_{i_{n}})\,.
\label{ch02:eqn2.6.4}
\end{equation}
The case $n=1$ is trivial. Let $n\geq 2$. If $(i_{k}, i_{k+1})=(1,2)$, then $a_{i_{k}}$ and $b_{i_{k}}$ can be removed in the respective left-hand and right-hand sides of (\ref{ch02:eqn2.6.4}), because $a_{1}a_{2}=a_{2}$ and $b_{1}b_{2}=b_{2}$. If $(i_{k}, i_{k+1})=(3,2)$, then $a_{i_{k}}a_{i_{k+1}}$ and $b_{i_{k}}b_{i_{k+1}}$ can be removed in (\ref{ch02:eqn2.6.4}), because $a_{3}a_{2}=b_{3}b_{2}=1$. Hence the induction hypothesis works in these cases. Otherwise, $(i_{k}, i_{k+1})\not\in\{(1,2), (3,2)\}$ for any $1\leq k\leq n-1$, which implies that both sides of (\ref{ch02:eqn2.6.4}) are zero. Thus we have shown that $c^{*}c$ has the distribution (\ref{ch02:eqn2.5.12}), and it immediately determines the distribution of $|c|$ as (\ref{ch02:eqn2.6.3}).
\end{example}

The measure (\ref{ch02:eqn2.6.3}) is called the \textit{quarter-circular distribution}.\index{quarter-circular!istribution} Moreover, a noncommutative random variable in a $C^{*}$-probability space is called a \textit{quarter-circular} element\index{quarter-circular!element} if it is positive and has the quarter-circular distribution.\index{distribution!quarter-circular}

\begin{proposition}\label{ch02:pro2.6.3}
Let $R$ and $M$ be the $R$-series and the moment generating series, respectively, of a $k$-tuple $(a_{1}, a_{2}, \ldots, a_{k})$ of noncommutative random variables. Then the following relation holds for every $1\leq i(1), i(2), \ldots, i(n)\leq k$ and $n\in \mathbb{N}$:
\begin{equation}
\mathrm{coef}[i(1), i(2), \ldots, i(n)](M)= \sum_{\mathcal{V}\in NC(n)} \mathrm{coef}[i(1), i(2), \ldots, i(n);\mathcal{V}](R)\,,
\label{ch02:eqn2.6.5}
\end{equation}
where the term in the right-hand side was given in \emph{(\ref{ch02:eqn2.6.2})}.
\end{proposition}

\begin{proof2}
We have to show that
\begin{equation}
\tilde{\omega}_{k}(\ell_{i(1)}^{*}T\ell_{i(2)}^{*}T\cdots \ell_{i(n)}^{*}T)
\label{ch02:eqn2.6.6}
\end{equation}
equals the right-hand side of (\ref{ch02:eqn2.6.5}). In order to show this, we plug in (\ref{ch02:eqn2.6.1}) in place of $T$ and arrive at the following sum:
\begin{equation}
\sum\alpha_{1}\alpha_{2}\cdots\alpha_{n}\tilde{\omega}_{k}(\ell_{i(1)}^{*}L_{1}\ell_{i(2)}^{*}L_{2}\cdots \ell_{i(n)}^{*}L_{n})\,,
\label{ch02:eqn2.6.7}
\end{equation}
where $L_{1}, L_{2}, \ldots, L_{n}$ are monomials in $\ell_{1}, \ell_{2}, \ldots, \ell_{k},\mathbf{1}$ and $\alpha_{i}$ is the coefficient of the term $L_{i}$ in (\ref{ch02:eqn2.6.1}). The sum is over all possibilities, but many of the terms vanish. The rest of the proof consists in analyzing the structure of the non-vanishing terms.

We are dealing with vacuum expectations
\begin{equation*}
\langle \ell_{i(1)}^{*}L_{1}\ell_{i(2)}^{*}L_{2}\cdots\ell_{i(n)}^{*}L_{n}\Phi,\Phi\rangle\,,
\end{equation*}
which contain $n$ annihilation operators. To be nonzero, a necessary condition is that the total number of creations must be $n$ as well. Moreover, $\ell_{i(1)}^{*}$ has to be coupled with a factor $\ell_{i(1)}$ included in $L_{1}$, or in $L_{2}, \ldots$, or in $L_{n},\,\ell_{i(2)}^{*}$ has to be coupled with a factor $\ell_{i(2)}$ from $L_{2}$, or from $L_{3}, \ldots$, or from $L_{n}$, etc. Those couplings establish a pair partition of $[2n],\ 2n$ being the number of factors of the long product and the position of $\ell_{i(j)}^{*}$ being paired with the position of $\ell_{i(j)}$ annihilated by $\ell_{i(j)}^{*}$. The vacuum expectation is non-vanishing if and only if this pair partition is non-crossing. Hence (\ref{ch02:eqn2.6.6}) is a sum over the non-crossing pair partitions of $[2n]$. Given such a partition, we can recover the corresponding term as follows: $\ell_{i(1)}^{*}$ is the first factor under the expectation; the pair of 1 is replaced by $\ell_{i(1)}$; then we choose the smallest among the smaller elements of the pairs, which will be replaced by $\ell_{i(2)}^{*}$, and the other element of this pair will be replaced by $\ell_{i(2)}$, etc. For example, the expectation
\begin{equation*}
\langle \ell_{i(1)}^{*}\ell_{i(2)}^{*}\ell_{i(3)}^{*}\ell_{i(3)}\ell_{i(4)}^{*}\ell_{i(4)}\ell_{i(2)}\ell_{i(5)}^{*}\ell_{i(5)}\ell_{i(1)}\Phi,\Phi\rangle
\end{equation*}
corresponds to the non-crossing partition $1,10/2,7/3,4/5,6/8,9$. The value of a non-vanishing vacuum expectation is simply 1; hence (\ref{ch02:eqn2.6.7}) reduces to $\sum\alpha_{1}\alpha_{2}\cdots\alpha_{n}$, where the summation is over all terms which define non-crossing pair partitions as above. Here $\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}$ are coefficients of the $R$-series; however, several of them are 1, corresponding to $L_{i}=\mathbf{1}$.

To complete the proof we need to rewrite the sum $\sum\alpha_{1}\alpha_{2}\cdots\alpha_{n}$ over non-crossing pair partitions of $[2n]$ in terms of non-crossing partitions of $[n]$, and we need to identify the term $\alpha_{1}\alpha_{2}\cdots\alpha_{n}$ with a term like $\mathrm{coef}[i(1), i(2), \ldots, i(n);\mathcal{V}](R)$. So the point is a one-to-one correspondence between the non-crossing pair partitions of $[2n]$ and $NC(n)$.

Let $\{x_{1}, y_{1}\}, \{x_{2}, y_{2}\}, \ldots, \{x_{n}, y_{n}\}$ be a non-crossing pair partition of $[2n]$, and assume that $x_{i} <y_{i}$ and $x_{1} <x_{2} < \cdots <x_{n}$. One can define an equivalence relation on $[n]$ by saying that $u\,\sim\,v$ if there is no $x_{i}$ between $y_{u}$ and $y_{v}$. One checks that, starting from a non-crossing pair partition, this equivalence relation generates a non-crossing partition of $[n]$. For example, $1, 10/2, 7/3, 4/5, 6/8, 9$ generates $1, 5/2, 4/3$. Mathematical induction on $n$ is convenient. When $\{x_{1}, y_{1}\}, \{x_{2}, y_{2}\}, \ldots, \{x_{n}, y_{n}\}$ is non-crossing, we can find $i$ with $x_{i}+1=y_{i}$. If we remove the pair $\{x_{i}, y_{i}\}$, then the crossing number will not change and after renumbering we can get a non-crossing pair partition of $[2n-2]$. On the other hand, $i$ must be the largest element in a block of the corresponding partition of $[n]$, and, removing $i$ and renumbering, also here we obtain a non-crossing partition of $[n-1]$.

The above procedure can be reversed. Let $\mathcal{V}$ be a non-crossing partition of $[n]$. For each block $V=\{h_{1}<h_{2}<\ldots<h_{r}\}$ set $L_{h_{r}} :=\ell_{i(h_{r})}\ell_{i(h_{r-1})}\cdots\ell_{i(h_{1})}$, and let $L_{i} :=\mathbf{1}$ when $i$ is not the largest number of any block of $\mathcal{V}$. Then one can easily see that the product $\ell_{i(1)}^{*}L_{1}\ell_{i(2)}^{*}L_{2}\cdots \ell_{i(n)}^{*}L_{n}$ is a non-vanishing term from (\ref{ch02:eqn2.6.7}), and the corresponding non-crossing pair partition of $[2n]$ determines the given $\mathcal{V}$. Moreover, the corresponding term $\alpha_{1}\alpha_{2}\cdots\alpha_{n}$ is equal to
\begin{equation*}
\prod_{V\in \mathcal{V}}\alpha (i(h_{1}), i(h_{2}), \ldots, i(h_{r}))= \mathrm{coef}[i(1), i(2), \ldots, i(n);\mathcal{V}](R)\,,
\end{equation*}
and (\ref{ch02:eqn2.6.5}) is obtained.
\end{proof2}

The coefficients $\alpha (i(1), i(2), \ldots, i(n))$ are called the \textit{free cumulants} of $(a_{1}, a_{2}, \ldots, a_{k})$, and (\ref{ch02:eqn2.6.5}) may be called the \textit{free moment-cumulant formula} for multivariables. Although we have proved neither the existence nor the uniqueness of the canonical representation yet, they immediately follow from the relation (\ref{ch02:eqn2.6.5}). Indeed, this relation can uniquely determine all coefficients of $R$ from those of $M$.

To paraphrase the moment-cumulant formula\index{moment-cumulant formula, free}\index{free!moment-cumulant formula} in terms of power series, we introduce the \textit{combinatorial convolution},\index{convolution!combinatorial}\index{combinatorial convolution}\index{free!cumulant} which extends the single-variable concept treated in the previous section. Let $F_{1}, F_{2}\in \mathbb{C}_{\infty}\langle X_{1}, X_{2}, \ldots, X_{k}\rangle$. Then the convolution $F_{1}\star F_{2}$ is determined as
\begin{align*}
&\mathrm{coef}[i(1), i(2), \ldots, i(n)](F_{1}\star F_{2})\\
&= \sum_{\mathcal{V}\in NC(n)} \mathrm{coef}[i(1), i(2), \ldots, i(n);\mathcal{V}](F_{1}) \mathrm{coef}[i(1), i(2), \ldots, i(n);K(\mathcal{V})](F_{2})\,,
\end{align*}
where $K$ is the Kreweras complement.

Let $Zeta\in \mathbb{C}_{\infty}\langle X_{1}, X_{2}, \ldots,  X_{k}\rangle$ be such that it is without constant term and all other coefficients are 1. Then the moment-cumulant\index{cumulant!free} formula takes the simple form
\begin{equation*}
M_{(a_{1},a_{2},\ldots, a_{k})}=R_{(a_{1},a_{2},\ldots,a_{k})} \star Zeta.
\end{equation*}
An important role is played also by the series
\begin{equation*}
Moeb (X_{1},X_{2},\ldots,X_{k}):=\sum_{n=1}^{\infty}\,\sum_{i(1),\ldots,i(n)=1}^{k}(-1)^{n-1}c_{n-1}X_{i(1)}X_{i(2)}\cdots X_{i(n)}
\end{equation*}
and
\begin{equation*}
Sum(X_{1}, X_{2}, \ldots, X_{k}) :=X_{1}+X_{2}+\cdots+X_{k}.
\end{equation*}
\textit{Sum} is a unit for the convolution. \textit{Zeta} and \textit{Moeb} are inverse of each other. (In the $k$-variable case the convolution is not commutative, but \textit{Zeta} and \textit{Moeb} are central elements.) Hence
\begin{equation*}
R_{(a_{1},a_{2},\ldots,a_{k})}=M_{(a_{1},a_{2},\ldots,a_{k})} \star Moeb,
\end{equation*}
and one can see again that the $R$-series and the canonical representation of a $k$-tuple always exist and are uniquely determined.

All the concepts and notations of convolution, $R$-series, \textit{Zeta}, \textit{Sum}, \textit{Moeb} are extensions of the one-variable case treated in the previous section.

The following is a main property of the multivariate $R$-series.

\begin{proposition}\label{ch02:pro2.6.4}
Let $(a_{1}, a_{2}, \ldots, a_{k})$ be a $k$-tuple and $(b_{1}, b_{2}, \ldots, b_{m})$ an $m$-tuple in the same noncommutative probability space. Then $\{a_{1}, a_{2}, \ldots, a_{k}\}$ and $\{b_{1}, b_{2}, \ldots, b_{m}\}$ are free if and only if the $R$-series of the multivariable $(a_{1}, a_{2}, \ldots, a_{k}, b_{1}, b_{2}, \ldots, b_{m})$ is
\begin{equation*}
R_{(a_{1},a_{2},\ldots,a_{k})}(X_{1},X_{2},\ldots,X_{k})+R_{(b_{1},b_{2},\ldots,b_{m})}(X_{k+1},X_{k+2},\ldots,X_{k+m})\,.
\end{equation*}
\end{proposition}

\begin{proof2}
Let
\begin{equation*}
T_{a}=\mathbf{1}+\sum\alpha(i(1),i(2),\ldots,i(n))\ell_{i(n)}\ell_{i(n-1)}\cdots \ell_{i(1)}
\end{equation*}
and
\begin{equation*}
T_{b}=\mathbf{1}+\sum\beta(i(1),i(2),\ldots,i(n))\ell_{k+i(n)}\ell_{k+i(n-1)}\cdots\ell_{k+i(1)}
\end{equation*}
be the canonical representations of $(a_{1}, a_{2}, \ldots, a_{k})$ and $(b_{1}, b_{2}, \ldots, b_{m})$, respectively. Set
\begin{align*}
T:=\mathbf{1} & \ +\ \sum\alpha(i(1),\ldots,i(n))\ell_{i(n)}\cdots \ell_{i(1)}\\
& \ + \ \sum \beta(i(1),\ldots,i(n))\ell_{k+i(n)}\cdots\ell_{k+i(1)}\,.
\end{align*}
Since
\begin{equation*}
\ell_{i}^{*}T=\left\{\begin{array}{ll}
\ell_{i}^{*}T_{a} & \mathrm{if}\ 1\leq i\leq k,\\
\ell_{i}^{*}T_{b} & \mathrm{if}\ k+1\leq i\leq k+m,\\
\end{array}\right.
\end{equation*}
$\{\ell_{1}^{*}T, \ldots, \ell_{k}^{*}T\}$ and $\{\ell_{k+1}^{*}T, \ldots, \ell_{k+m}^{*}T\}$ are free in $\tilde{\mathcal{E}}_{k+m}$. Hence, if $\{a_{1}, \ldots, a_{k}\}$ and $\{b_{1}, \ldots, b_{m}\}$ are free, then the joint distribution of $(\ell_{1}^{*}T, \ldots, \ell_{k}^{*}T, \ell_{k+1}^{*}T, \ldots, \ell_{k+m}^{*}T)$ is the same as that of $(a_{1}, \ldots, a_{k}, b_{1}, \ldots, b_{m})$, so $T$ is the canonical representation of $(a_{1}, \ldots, a_{k}, b_{1}, \ldots, b_{m})$.

The proof of the converse is a kind of trick. Choose $(\tilde{a}_{1}, \ldots,\tilde{a}_{k})$ and $(\tilde{b}_{1}, \ldots,\tilde{b}_{m})$ in another noncommutative probability space $(\tilde{\mathcal{A}},\tilde{\varphi})$ so that they are free and their joint distributions are the same as those of $(a_{1}, \ldots, a_{k})$ and $(b_{1}, \ldots, b_{m})$, respectively. (For instance, take $(\tilde{\mathcal{A}},\tilde{\varphi}):=(\mathcal{A}_{1}, \varphi|\mathcal{A}_{1})\star(\mathcal{A}_{2}, \varphi|\mathcal{A}_{2})$, where $\mathcal{A}_{1}$ and $\mathcal{A}_{2}$ are the subalgebras of $\mathcal{A}$ generated by $\{a_{1}, \ldots, a_{k}\}$ and $\{b_{1}, \ldots, b_{m}\}$, respectively.) We have
\begin{align*}
& R_{(a_{1},\ldots,a_{k},b_{1},\ldots, b_{m})}(X_{1},\ldots,X_{k},X_{k+1},\ldots,X_{k+m})\\
& \qquad =R_{(a_{1},\ldots,a_{k})}(X_{1},\ldots,X_{k})+R_{(b_{1},\ldots,b_{m})}(X_{k+1},\ldots, X_{k+m})\\
& \qquad =R_{(\tilde{a}_{1},\ldots, \tilde{a}_{k})}(X_{1},\ldots,X_{k})+R_{(\tilde{b}_{1},\ldots,\tilde{b}_{m})}(X_{k+1},\ldots,X_{k+m}) \\
& \qquad =R_{(\tilde{a}_{1},\ldots,\tilde{a}_{k},\tilde{b}_{1},\ldots,\tilde{b}_{m})}(X_{1},\ldots,X_{k},X_{k+1},\ldots,X_{k+m})\,.
\end{align*}
Here the first equality is by assumption and the third is by the first half of the proof. Hence the freeness follows because the $R$-series determines the joint distribution.
\end{proof2}

Another important result is the multivariable extension of Theorem~\ref{ch02:the2.4.5}.

\begin{proposition}\label{ch02:pro2.6.5}
Let $(a_{1}, a_{2}, \ldots, a_{k})$ and $(b_{1}, b_{2}, \ldots, b_{k})$ be two $k$-tuples in the same noncommutative probability space. If $\{a_{1}, a_{2}, \ldots, a_{k}\}$ and $\{b_{1}, b_{2}, \ldots, b_{k}\}$ are free, then the $R$-series of $(a_{1}+b_{1}, a_{2}+b_{2}, \ldots, a_{k}+b_{k})$ is
\begin{equation*}
R_{(a_{1},a_{2},\ldots,a_{k})}(X_{1},X_{2},\ldots,X_{k})+R_{(b_{1},b_{2},\ldots,b_{k})}(X_{1},X_{2},\ldots,X_{k})\,.
\end{equation*}
\end{proposition}

\begin{proof2}
The idea in the proof of Theorem~\ref{ch02:the2.4.5} can work in the mutivariable case too. Let
\begin{equation*}
T_{a}=\mathbf{1}+\sum\alpha(i(1),i(2),\ldots,i(n))\ell_{2i(n)-1}\ell_{2i(n-1)-1}\cdots \ell_{2i(1)-1}
\end{equation*}
and
\begin{equation*}
T_{b}=\mathbf{1}+\sum\beta(i(1),i(2),\ldots,i(n))\ell_{2i(n)}\ell_{2i(n-1)}\cdots\ell_{2i(1)}
\end{equation*}
be the canonical representations of $(a_{1}, a_{2}, \ldots, a_{k})$ and $(b_{1}, b_{2}, \ldots, b_{k})$, respectively. Set
\begin{equation*}
T:=\mathbf{1}+\sum(\alpha(i(1),\ldots,i(n))+\beta(i(1),\ldots,i(n)))\ell_{i(n)}\ell_{i(n-1)}\cdots\ell_{i(1)}.
\end{equation*}
By the freeness assumption, the joint distribution of $(a_{1}, \ldots, a_{k}, b_{1}, \ldots, b_{k})$ is the same as that of $(\ell_{1}^{*}T_{a}, \ldots, \ell_{2k-1}^{*}T_{a}, \ell_{2}^{*}T_{b}, \ldots, \ell_{2k}^{*}T_{b})$, so the joint distributions of $(a_{1}+b_{1}, \ldots, a_{k}+b_{k})$ and $(\ell_{1}^{*}T_{a}+\ell_{2}^{*}T_{b}, \ldots, \ell_{2k-1}^{*}T_{a}+\ell_{2k}^{*}T_{b})$ coincide. Furthermore, we can express
\begin{align*}
&\ell_{i}^{*}T=\ell_{i}^{*}\\
& \qquad \quad +\sum(\alpha(i(1),\ldots,i(n-1),i)+\beta(i(1),\ldots,i(n-1),i))\ell_{i(n-1)},\cdots,\ell_{i(1)}\,,
\end{align*}

\begin{align*}
\ell_{2i-1}^{*}T_{a}+\ell_{2i}^{*}T_{b}& \ = \ (\ell_{2i-1}^{*}+\ell_{2i}^{*})\\
& \qquad +\sum\alpha(i(1),\ldots,i(n-1),i)\ell_{2i(n-1)-1}\cdots\ell_{2i(1)-1}\\
& \qquad +\sum\beta(i(1),\ldots,i(n-1),i)\ell_{2i(n-1)}\cdots \ell_{2i(1)}\,.
\end{align*}

What we have to show is that
\begin{equation*}
\tilde{\omega}_{k}(\ell_{i(1)}^{*}T\cdots \ell_{i(n)}^{*}T)=\tilde{\omega}_{2k}((\ell_{2i(1)-1}^{*}T_{a}+\ell_{2i(1)}^{*}T_{b})\cdots(\ell_{2i(n)-1}^{*}T_{a}+\ell_{2i(n)}^{*}T_{b}))\,.
\end{equation*}
In view of the above expressions for $\ell_{i}^{*}T$ and $\ell_{2i-1}^{*}T_{a}+\ell_{2i}^{*}T_{b}$, it suffices to show that
\begin{equation*}
\tilde{\omega}_{k}(\ell_{i(1)}^{*}L_{1}\ldots \ell_{i(n)}^{*}L_{n})=\tilde{\omega}_{2k}((\ell_{2i(1)-1}^{*}+\ell_{2i(1)}^{*})\tilde{L}_{1}\cdots(\ell_{2i(n)-1}^{*}+\ell_{2i(n)}^{*})\tilde{L}_{n})\,,
\end{equation*}
where $L_{j}$ is a monomial of $\ell_{1}, \ldots, \ell_{k}, \mathbf{1}$ and $\tilde{L}_{j}$ is the corresponding monomial of either $\ell_{1}, \ldots, \ell_{2k-1},\mathbf{1}$ or $\ell_{2}, \ldots, \ell_{2k}, \mathbf{1}$. But one can easily check this because $\ell_{2i-1}^{*}+\ell_{2i}^{*}$ is canceled as a whole with $\ell_{2i-1}$ or $\ell_{2i}$ (cf. the proof of Theorem~\ref{ch02:the2.4.5}).
\end{proof2}

The linear transformation rule for the multivariate $R$-series is simply stated in the following proposition:

\begin{proposition}\label{ch02:pro2.6.6}
Let $(a_{1}, a_{2}, \ldots, a_{k})$ be a $k$-tuple of noncommutative random variables. If $A=[A_{ij}]_{1\leq i\leq m,\,1\leq j\leq k}$ is an $m\times k$ complex matrix and $b_{i}=\sum_{j=1}^{k}A_{ij}a_{j} (1\leq i\leq m)$, then
\begin{align*}
&R_{(b_{1},b_{2},\ldots,b_{m})}(X_{1}, X_{2},X_{m})\\
& \qquad =R_{(a_{1},a_{2},\ldots,a_{k})}\left(\sum_{i=1}^{m}A_{i1}X_{i},\sum_{i=1}^{m}A_{i2}X_{i},\ldots,\sum_{i=1}^{m}A_{ik}X_{i}\right),
\end{align*}
or, for short, $R_{Aa}(X)=R_{a}(A^{t}X)$, where $^{t}$ stands for the transpose.
\end{proposition}

\begin{proof2}
Let $R$ and $M$ be as in Proposition~\ref{ch02:pro2.6.3} for $(a_{1}, a_{2}, \ldots, a_{k})$. Let $\hat{M}$ be the moment generating series for $(b_{1}, b_{2}, \ldots, b_{m})$, and let
\begin{equation*}
\hat{R}(X_{1}, X_{2}, \ldots, X_{m}) :=R\left(\sum_{i=1}^{m}A_{i1}X_{i}, \sum_{i=1}^{m}A_{i2}X_{i}, \ldots, \sum_{i=1}^{m}A_{ik}X_{i}\right).
\end{equation*}
Let $1\leq i(1), \ldots, i(n)\leq m$. Since
\begin{align*}
&\mathrm{coef}[i(1), \ldots, i(n)](\hat{R})\\
& \qquad =\sum_{j(1),\ldots,j(n)=1}^{k}A_{i(1)j(1)}\cdots A_{i(n)j(n)}\ \mathrm{coef}[j(1), \ldots, j(n)](R)\,,
\end{align*}
the following holds for every partition $\mathcal{V}$ of $[n]$:
\begin{align*}
&\mathrm{coef}[i(1), \ldots, i(n);\mathcal{V}](\hat{R})\\
& \qquad =\sum_{j(1),\ldots,j(n)=1}^{k}A_{i(1)j(1)}\cdots A_{i(n)j(n)}\ \mathrm{coef}[j(1), \ldots, j(n);\mathcal{V}](R)\,.
\end{align*}
Therefore, by (\ref{ch02:eqn2.6.5}) we have
\begin{align*}
&\mathrm{coef}[i(1), \ldots, i(n)](\hat{M})\\
& \qquad =\sum_{j(1),\ldots,j(n)=1}^{k}A_{i(1)j(1)}\cdots A_{i(n)j(n)}\ \mathrm{coef}[j(1), \ldots, j(n)](M)\\
& \qquad = \sum_{\mathcal{V}\in NC(n)}\,\sum_{j(1),\ldots,j(n)=1}^{k}A_{i(1)j(1)}\cdots A_{i(n)j(n)}\ \mathrm{coef}[j(1), \ldots, j(n);\mathcal{V}](R) \\
& \qquad = \sum_{\mathcal{V}\in NC(n)} \mathrm{coef}[i(1), \ldots, i(n);\mathcal{V}](\hat{R})\,,
\end{align*}
which shows that $\hat{R}$ is the $R$-series of $(b_{1}, b_{2}, \ldots, b_{m})$.
\end{proof2}

\begin{example}\label{ch02:exa2.6.7}
Let $a$ and $b$ be free standard semicircular elements. For $-1 <\tau< 1$ set
\begin{equation*}
y:=\sqrt{\frac{1+\tau}{2}}a+\mathrm{i}\,\sqrt{\frac{1-\tau}{2}}b.
\end{equation*}
Then the $R$-series of $(y, y^{*})$ is $X_{1}X_{2}+X_{2}X_{1}+\tau X_{1}^{2}+\tau X_{2}^{2}$, and the distribution of $y^{*}y$ is the free Poisson measure $\mu_{1}$, as in the circular case.

An application of the previous transformation rule to Example~\ref{ch02:exa2.6.2} yields
\begin{equation*}
R_{(y,y^{*})}(X_{1}, X_{2})=\left(\sqrt{\frac{1+\tau}{2}}(X_{1}+X_{2})\right)^{2}+\left(\mathrm{i}\,\sqrt{\frac{1-\tau}{2}}(X_{1}-X_{2})\right)^{2},
\end{equation*}
which equals the required expression.

We can write $y=uc+vc^{*}$ with a circular element $c$ and
\begin{equation*}
u:=\frac{\sqrt{1+\tau}+\sqrt{1-\tau}}{2},\quad v:=\frac{\sqrt{1+\tau}-\sqrt{1-\tau}}{2}\,.
\end{equation*}
So, for the second assertion we may show that if $y=u(\ell_{1}^{*}+\ell_{2})+v(\ell_{1}+\ell_{2}^{*})$ in $\tilde{\mathcal{E}}_{2}$, then $\tilde{\omega}_{2}((y^{*}y)^{n})=\tilde{\omega}_{2}((\ell_{1}^{*}+\ell_{1})^{2n}) (n\ \in \mathbb{N})$. The left-hand side is expanded in the following sum:
\begin{equation*}
\sum u^{p}v^{2n-p}\tilde{\omega}_{2}(a_{1}a_{2}\cdots a_{2n-1}a_{2n})
\end{equation*}
where $a_{i}\in\{\ell_{1}, \ell_{1}^{*}, \ell_{2}, \ell_{2}^{*}\}$ and $p$ is the sum of the number of $i$ with $a_{2i-1}\in\{\ell_{1}, \ell_{2}^{*}\}$ and the number of $i$ with $a_{2i}\in\{\ell_{1}^{*}, \ell_{2}\}$. When $\tilde{\omega}_{2}(a_{1}a_{2}\cdots a_{2n})=1$ (otherwise this is $0$), the product $b_{1}b_{2}\cdots b_{2n}$ of $\ell_{1}, \ell_{1}^{*}$ is obtained by replacing all $\ell_{2}, \ell_{2}^{*}$ in $a_{1}a_{2}\cdots a_{2n}$ by $\ell_{1}, \ell_{1}^{*}$, respectively, and then $\tilde{\omega}_{2}(b_{1}b_{2}\cdots b_{2n})=1$. Any such term $a_{1}a_{2}\cdots a_{2n}$ is conversely obtained from $b_{1}b_{2}\cdots b_{2n}$ by replacing $\ell_{1}$ by $\ell_{2}$ and $\ell_{1}^{*}$ by $\ell_{2}^{*}$ in some positions. Here one can repeatedly replace $\ell_{1}$ in any position by $\ell_{2}$, but the corresponding position where $\ell_{1}^{*}$ must be replaced by $\ell_{2}^{*}$ is uniquely determined, and the coefficients of the replaced $\ell_{2}, \ell_{2}^{*}$ are both $u$ or both $v$. In this way, we observe that the sum of $u^{p}v^{2n-p}$ for all $a_{1}a_{2}\cdots a_{2n}$ corresponding to a fixed $b_{1}b_{2}\cdots b_{2n}$ is equal to
\begin{equation*}
\sum_{k=0}^{n}\left(\begin{array}{c}
n\\
k\\ \end{array}
\right)u^{2k}v^{2n-2k}=(u^{2}+v^{2})^{n}=1\,.
\end{equation*}
For example, the terms $u^{p}v^{6-p}a_{1}a_{2}\cdots a_{6}$ corresponding to $\ell_{1}^{*}\ell_{1}^{*}\ell_{1}^{*}\ell_{1}\ell_{\mathbf{1}}\ell_{1} (n =6)$ are
\begin{equation*}
\begin{array}{rrr}
u^{2}v^{4}\ell_{1}^{*}\ell_{1}^{*}\ell_{1}^{*}\ell_{1}\ell_{1}\ell_{1}, & u^{4}v^{2}\ell_{2}^{*}\ell_{1}^{*}\ell_{1}^{*}\ell_{1}\ell_{1}\ell_{2}, & v^{6}\ell_{1}^{*}\ell_{2}^{*}\ell_{1}^{*}\ell_{1}\ell_{2}\ell_{1},\\
u^{4}v^{2}\ell_{1}^{*}\ell_{1}^{*}\ell_{2}^{*}\ell_{2}\ell_{1}\ell_{1}, & u^{2}v^{4}\ell_{2}^{*}\ell_{2}^{*}\ell_{1}^{*}\ell_{1}\ell_{2}\ell_{2}, & u^{6}\ell_{2}^{*}\ell_{1}^{*}\ell_{2}^{*}\ell_{2}\ell_{1}\ell_{2},\\
u^{2}v^{4}\ell_{1}^{*}\ell_{2}^{*}\ell_{2}^{*}\ell_{2}\ell_{2}\ell_{1},& u^{4}v^{2}\ell_{2}^{*}\ell_{2}^{*}\ell_{2}^{*}\ell_{2}\ell_{2}\ell_{2},&\\
\end{array}
\end{equation*}
and the sum of these $u^{p}v^{6-p}$ is $(u^{2}+v^{2})^{3}=1$. Hence the equality
\begin{equation*}
\tilde{\omega}_{2}((y^{*}y)^{n})=\sum_{b_{i}\in\{\ell_{1},\ell_{1}^{*}\}}\tilde{\omega}_{2}(b_{1}b_{2}\cdots b_{2n})=\tilde{\omega}_{2}((\ell_{1}^{*}+\ell_{1})^{2n})
\end{equation*}
is obtained.
\end{example}
The noncommutative random variable $y$ in this example may be called an \textit{elliptic} element,\index{elliptic!element} which contains a circular element (when $\tau=0$) in Example~\ref{ch02:exa2.6.2} and also converges to a standard semicircular law (when $\tau\rightarrow 1$).

Let $(\mathcal{A}, \varphi)$ be a $C^{*}$-probability space such that $\varphi$ is a tracial state. A multi-variable $(a_{1}, a_{2}, \ldots, a_{k})$ of selfadjoint elements in $\mathcal{A}$ is said to be \textit{centered general semicircular}\index{centered general semicircular multivariable} if its $R$-series is of the form
\begin{equation*}
\sum_{i,j=1}^{k}A_{ij}X_{i}X_{j}\,.
\end{equation*}
Since $\varphi(a_{i})=\alpha(i)$ and $\varphi(a_{i}a_{j})=\alpha(i, j)+\alpha(i)\alpha(j)$ from the moment-cumulant formula, $a_{i}$ has $0$ expectation and the $k\times k$ matrix $A$ is exactly the covariance matrix. In particular, $A$ is symmetric and positive semidefinite. Thanks to Proposition~\ref{ch02:pro2.6.6} one can choose an orthogonal matrix $S$ and a free family $(b_{1}, b_{2}, \ldots, b_{k})$ such that
\begin{equation*}
a_{i}=\sum_{j=1}^{k}S_{ij}b_{j} \qquad (1\leq i\leq k)
\end{equation*}
and all $b_{j}$ are centered semicircular.

One can consider the centered general semicircular multivariables from the multivariate free central limit theorem. Assume that for each $n\in \mathbb{N}$ a multivariable $(b_{1}^{(n)}, b_{2}^{(n)}, \ldots, b_{k}^{(n)})$ of selfadjoint elements in $\mathcal{A}$ is given such that its joint distribution is independent of $n$, the expectation of $b_{i}^{(n)}$ is $0$ and the familiy of $\{b_{1}^{(n)}, b_{2}^{(n)}, \ldots, b_{k}^{(n)}\}\ (n \in \mathbb{N})$ is free. Then the joint distribution of
\begin{equation}
\left(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}b_{1}^{(i)}, \frac{1}{\sqrt{n}}\sum_{i=1}^{n}b_{2}^{(i)}, \ldots, \frac{1}{\sqrt{n}}\sum_{i=1}^{n}b_{k}^{(i)}\right)
\label{ch02:eqn2.6.8}
\end{equation}
converges as $n\rightarrow\infty$ to the joint distribution of the centered general semicircular multivariable determined by the covariance matrix of $(b_{1}^{(1)}, b_{2}^{(1)}, \ldots, b_{k}^{(1)})$. An easy proof of this statement comes from the calculation of the $R$-series of (\ref{ch02:eqn2.6.8}). Propositions~\ref{ch02:pro2.6.5} and \ref{ch02:pro2.6.6} imply that this $R$-series is equal to
\begin{equation*}
nR_{(b_{1}^{(1)},b_{2}^{(1)},\ldots,b_{k}^{(1)})}\left(\frac{1}{\sqrt{n}}X_{1},\frac{1}{\sqrt{n}}X_{2},\ldots,\frac{1}{\sqrt{n}}X_{k}\right),
\end{equation*}
and all coefficients go to $0$, except for the quadratic terms, which remain independent of $n$ (cf. the second proof of Theorem~\ref{ch02:the2.3.2} at the end of Sec.~\ref{ch02:sec2.4}). This means the desired convergence of the joint distribution of (\ref{ch02:eqn2.6.8}) due to the moment-cumulant formula (\ref{ch02:eqn2.6.5}).

Proposition~\ref{ch02:pro2.6.6} provides the transformation of the multivariate $R$-series under linear transformation of the noncommutative random variables. Next we treat compressions of noncommutative random variables and more general expressions in terms of matrix units. Contrary to the case of linear transformation, a hypothesis on a certain free relation will be imposed.

\begin{proposition}\label{ch02:pro2.6.8}
Let $(\mathcal{A}, \varphi)$ be a $C^{*}$-probability space and let $e_{ij}\ (1\leq i, j\leq n)$ be a system of matrix units in $\mathcal{A}$. Given $a\in \mathcal{A}$, set $a_{ij} :=e_{1i}ae_{j1}\in \mathcal{A}_{1} :=e_{11}\mathcal{A}e_{11}$. Assume that $\varphi(e_{ij})=n^{-1}\delta_{ij}$, and consider the family $a_{ij}\ (1\leq i,j\leq n)$ in the noncommutative probability space $(\mathcal{A}_{1}, \varphi_{1}), \varphi_{1} :=\varphi(e_{11})^{-1}\varphi|_{\mathcal{A}_{1}}$. If a is free from $\{e_{ij}:1\leq i, j\leq n\}$, then the multivariate $R$-series of the noncommutative random variables $(a_{ij}:1\leq i, j\leq n)$ is of the form
\begin{equation*}
R_{(a_{ij}:1\leq i,j\leq n)}((X_{ij})_{1\leq i,j\leq n})=n\mathrm{Tr}\,R_{a}(n^{-1}X)\,,
\end{equation*}
where $X_{ij}$ is the indeterminate corresponding to $a_{ij}$, the matrix $X$ is formed from the indeterminates, $n^{-1}X$ is formally put in the $R$-series, and $\mathrm{Tr}$ denotes the formal trace.
\end{proposition}

In full detail, the above $R$-series reads as
\begin{equation*}
\sum_{m}\sum_{i(1),i(2),\ldots,i(m)=1}^{n}\alpha_{m}n^{1-m}X_{i(1)i(2)}X_{i(2)i(3)}\cdots X_{i(m)i(1)},
\end{equation*}
if $R_{a}(z)=\sum_{m}\alpha_{m}z^{m}$. This proposition is not proven here, but a few important particular cases will be discussed.

\begin{example}\label{ch02:exa2.6.9} Let $p$ be a projection in a noncommutative probability space $(\mathcal{A}, \varphi)$, let $a\in \mathcal{A}$, and assume that $a$ and $p$ are free. When $\varphi(p)=1/n$ for an integer $n$, we may assume that we are in the setting of the previous proposition and $p=e_{11}$. (If necessary, the given probability space is enlarged by taking the free product with a matrix algebra, or the construction of Example~\ref{ch02:exa2.2.11} can be applied.) To compute the $R$-series of \textit{pap} with respect to the reduced state $\varphi_{1}$, we have to put $0$ in place of the indeterminates $X_{ij}$ whenever $i$ or $j$ is different from 1. In this way,
\begin{equation*}
R_{pap}(X_{11})=\sum_{m}\alpha_{m}n^{1-m}X_{11}^{m}.
\end{equation*}

In particular, if $a$ is free Poisson distributed with parameter $\lambda$, then $\varphi(p)^{-1}$ \textit{pap} is free Poisson as well with parameter $n\lambda$. This can be extended to the case when $n$ is not an integer; in fact, Example~\ref{ch02:exa2.4.7} contains our statement. The advantage of the method used there is that it extends to several random variables. Indeed, from Example~\ref{ch02:exa2.2.11} one can deduce that $\{pa_{1}p, pa_{2}p, \ldots, pa_{n}p\}$ is a free family if $a_{1}, a_{2}, \ldots, a_{n}, p$ are free and $p$ is a projection.
\end{example}

\begin{example}\label{ch02:exa2.6.10}
In the setting of Proposition~\ref{ch02:pro2.6.8}, the $R$-series of the elements $a_{1}:=e_{11}ae_{21}$ and $a_{2}:=e_{12}ae_{11}$ with respect to $\varphi_{1}$ is
\begin{equation*}
R_{(a_{1},a_{2})}(X_{1},X_{2})=\sum_{k}\alpha_{2k}n^{1-2k}\left((X_{1}X_{2})^{k}+(X_{2}X_{1})^{k}\right),
\end{equation*}
where the $\alpha_{k}$'s are the free cumulants of $a$.

Indeed, if a product $X_{i(1)i(2)}X_{i(2)i(3)}\cdots X_{i(m)i(1)}$ is formed only from $X_{12}$ and $X_{21}$, then the only possibility is an alternating sequence of $X_{12}$ and $X_{21}$ with $m=2k$. We replace $X_{12}$ and $X_{21}$ by $X_{1}$ and $X_{2}$, respectively.
\end{example}

It is sometimes useful to consider cumulants as multilinear functionals. Let $(\mathcal{A}, \varphi)$ be a noncommutative probability space and $a_{1}, a_{2}, \ldots, a_{n}\in \mathcal{A}$. The value $\phi_{n}(a_{1}, a_{2}, \ldots, a_{n})$ of the \textit{cumulant functional}\index{cumulant!functional} $\phi_{n}$ is defined as the coefficient of $X_{1}X_{2}\cdots X_{n}$ of the $R$-series of the $n$-tuple $(a_{1}, a_{2}, \ldots, a_{n})$; that is,
\begin{equation*}
\phi_{n}(a_{1}, a_{2}, \ldots, a_{n}) := \mathrm{coef}[1, 2, \ldots,n](R_{(a_{1},a_{2},\ldots,a_{n})})\,.
\end{equation*}
Actually, by Proposition~\ref{ch02:pro2.6.6} one has
\begin{equation}
\phi_{k}(a_{i(1)}, a_{i(2)}, \ldots, a_{i(k)})= \mathrm{coef}[i(1), i(2), \ldots, i(k)](R_{(a_{1},a_{2},\ldots,a_{n})})
\label{ch02:eqn2.6.9}
\end{equation}
for all $k\in \mathbb{N}$ and $1\leq i(1),i(2), \ldots, i(k)\leq n$. Furthermore, the cumulant functional
\begin{equation*}
(a_{1},a_{2},\ldots,a_{n})\mapsto\phi_{n}(a_{1},a_{2},\ldots,a_{n})
\end{equation*}
is multilinear on $\mathcal{A}^{n}$. Indeed, if $b_{i}=\sum_{j=1}^{k}A_{ij}a_{j}(1\leq i\leq n)$, then Proposition~\ref{ch02:pro2.6.6} shows that
\begin{align*}
&\phi_{n}(b_{1}, b_{2}, \ldots, b_{n})\\
&= \mathrm{coef}[1, 2, \ldots, n]\left(R_{(a_{1},a_{2},\ldots,a_{k})}\left(\sum_{i=1}^{n}A_{i1}X_{i}, \sum_{i=1}^{n}A_{i2}X_{i}, \ldots, \sum_{i=1}^{n}A_{ik}X_{i}\right)\right)\\
&=\sum_{j(1),\ldots,j(n)=1}^{k}A_{1j(1)}A_{2j(2)} \cdots A_{nj(n)}\mathrm{coef}[j(1),j(2), \ldots,j(n)](R_{(a_{1},a_{2},\ldots,a_{k})})\\
& =\sum_{j(1),\ldots,j(n)=1}^{k}A_{1j(1)}A_{2j(2)}\cdots A_{nj(n)}\phi_{n}(a_{j(1)}, a_{j(2)}, \ldots, a_{j(n)})
\end{align*}
by (\ref{ch02:eqn2.6.9}). According to the moment-cumulant formula (\ref{ch02:eqn2.6.5}), the cumulant functional are determined by the recursion
\begin{equation}
\varphi(a_{1}a_{2}\cdots a_{n})=\sum_{\mathcal{V}\in NC(n)}\,\prod_{V\in \mathcal{V}}\phi_{|V|}(a_{1}, a_{2}, \ldots, a_{n}|V)\,,
\label{ch02:eqn2.6.10}
\end{equation}
where $(a_{1}, a_{2}, \ldots, a_{n}|V) :=(a_{i(1)}, a_{i(2)}, \ldots, a_{i(r)})$ for $V=\{i(1)<i(2)<\cdots < i(r)\}\subset[n]$. It follows from Proposition~\ref{ch02:pro2.6.4} that if $\{a_{1}, \ldots, a_{k}\}$ and $\{a_{k+1}, \ldots, a_{n}\}$ are free, then
\begin{equation*}
\phi_{n}(a_{1}, a_{2}, \ldots, a_{n})=0.
\end{equation*}

\begin{lemma}\label{ch02:lem2.6.11}
The cumulant functionate have the following property for a product:
\begin{align*}
& \phi_{n-1}(a_{1}, \ldots, a_{k-1}, a_{k}a_{k+1}, a_{k+2}, \ldots a_{n})\\
& \qquad =\phi_{n}(a_{1}, \ldots, a_{k-1}, a_{k}, a_{k+1}, \ldots a_{n})\\
& \qquad \qquad +\sum_{\mathcal{V}}\prod_{V\in \mathcal{V}}\phi_{|V|}(a_{1}, \ldots, a_{k-1}, a_{k}, a_{k+1}, \ldots a_{n}|V),
\end{align*}
where the summation is over all $\mathcal{V}\in NC(n)$ such that $|\mathcal{V}|=2$ and $k, k+1$ belong to different blocks.
\end{lemma}

\begin{proof2}
We proceed by induction on $n$. For $n=2$ we get by (\ref{ch02:eqn2.6.10})
\begin{equation*}
\phi_{1}(a_{1}a_{2})=\varphi(a_{1}a_{2})=\phi_{2}(a_{1},a_{2})+\phi_{1}(a_{1})\phi_{1}(a_{2})\,.
\end{equation*}
Now let $n\geq 3$ and assume the property for all $n'<n$. Then it is straightforward to see that for every $\mathcal{U}\in NC(n)$ with $\mathcal{U}\neq \mathbf{1}_{n-1}$
\begin{align*}
& \prod_{U\in \mathcal{U}}\phi_{|U|}(a_{1},\ldots,a_{k-1},a_{k}a_{k+1},a_{k+2},\ldots,a_{n}|U)\\
& \qquad =\sum_{\begin{subarray}{c}\mathcal{V}\in NC (n)\\ \mathcal{V}|_{k=k+1}=\mathcal{U}\\ \end{subarray}}
\prod_{V\in \mathcal{V}}\phi_{|V|}(a_{1},\ldots,a_{k-1},a_{k},a_{k+1},\ldots,a_{n}|V),
\end{align*}
where $\mathcal{V}|_{k=k+1}\in NC(n-1)$ is the partition obtained by identifying $k, k+1$ and replacing $k+2, \ldots, n$ by $k+1, \ldots, n-1$ (here two blocks containing $k$ and $k+1$ must merge if $k, k+1$ belong to different blocks of $\mathcal{V}$). Hence by (\ref{ch02:eqn2.6.10}) we have
\begin{align*}
& \phi_{n-1}(a_{1},\ldots,a_{k-1},a_{k}a_{k+1},a_{k+2},\ldots,a_{n}) \\
& \qquad =\varphi(a_{1}\cdots a_{k-1}a_{k}a_{k+1}\cdots a_{n})\\
& \qquad \qquad - \sum_{\begin{subarray}{c}\mathcal{U}\in N\,C(n-1)\\ \mathcal{U}\neq 1_{n-1}\\ \end{subarray}}\,\prod_{U\in\mathcal{U}}\phi_{|U|} (a_{1}, \ldots, a_{k-1}, a_{k}a_{k+1}, a_{k+2}, \ldots, a_{n}|U)\\
& \qquad =\varphi(a_{1}\cdots a_{k-1}a_{k}a_{k+1}\cdots a_{n})\\
&\qquad \qquad - \sum_{\begin{subarray}{c}\mathcal{U}\in N\,C(n-1)\\ \mathcal{U}\neq 1_{n-1}\\ \end{subarray}}\,\sum_{\begin{subarray}{c}\mathcal{V} \in N\,C (n)\\ \mathcal{V}|_{k=k+1}=\mathcal{U} \\ \end{subarray}}\,\prod_{V\in\mathcal{V}}\phi_{|V|}(a_{1}, \ldots, a_{k-1}, a_{k},a_{k+1}, \ldots, a_{n}|V)\\
& \qquad =\varphi(a_{1}\cdots a_{k-1}a_{k}a_{k+1}\cdots a_{n})\\
& \qquad \qquad - \sum_{\begin{subarray}{c}\mathcal{V}\in N\,C(n)\\ \mathcal{V}|_{k=k+1}\neq 1_{n-1}\\ \end{subarray}} \prod_{V\in\mathcal{V}}\phi_{|V|} (a_{1}, \ldots, a_{k-1} a_{k},a_{k+1}, \ldots, a_{n}|V)\\
& \qquad \qquad - \sum_{\begin{subarray}{c}\mathcal{V}\in N\,C(n)\\ \mathcal{V}|_{k=k+1}=1_{n-1}\\ \end{subarray}} \prod_{V\in\mathcal{V}}\phi_{|V|} (a_{1}, \ldots, a_{k-1}, a_{k},a_{k+1}, \ldots, a_{n}|V),
\end{align*}
which is the desired property for $n$.
\end{proof2}

\begin{example}\label{ch02:exa2.6.12}
Let $u$ be a Haar unitary. Then the $R$-series of the pair $(u, u^{*})$ is
\begin{equation*}
R_{(u,u^{*})}(X_{1},X_{2})=\sum_{n=1}^{\infty}(-1)^{n-1}c_{n}-1\left((X_{1}X_{2})^{n}+(X_{2}X_{1})^{n}\right).
\end{equation*}

We compute by recursion that
\begin{align}
\phi_{2n-1} (u, u^{*}, \ldots, u, u^{*}, u)=\phi_{2n-1}(u^{*}, u, \ldots, u^{*}, u, u^{*})=0\,, \label{ch02:eqn2.6.11}\\
\phi_{2n}(u, u^{*}, \ldots, u, u^{*})=\phi_{2n}(u^{*}, u, \ldots, u^{*}, u)=(-1)^{n-1}c_{n-1}\,.
\label{ch02:eqn2.6.12}
\end{align}
For $n=1$ we get $\phi_{1}(u)=\varphi(u)=0=\varphi(u^{*})=\phi_{1}(u^{*})$ and $\phi_{2}(u, u^{*})=\varphi(uu^{*})- \varphi(u)\varphi(u^{*})=1=c_{0}=\phi_{2}(u^{*}, u)$ obviously.

Now assume that the above formulas hold for all $\phi_{2n'-1}$ and $\phi_{2n'}$ when $n'\leq n$. We consider $\phi_{2n+1} (u, u^{*}, \ldots, u, u^{*}, u)$. Applying Lemma~\ref{ch02:lem2.6.11}, we have
\begin{align*}
&\phi_{2n+1}(u,u^{*},\ldots,u,u^{*},u)\\
& \qquad =\phi_{2n}(\mathbf{1},u,\ldots,u,u^{*},u)-\sum_{\mathcal{V}}\prod_{V\in \mathcal{V}}\phi_{|V|}(u,u^{*},\ldots,u,u^{*},u|V) \\
& \qquad =-\sum_{\mathcal{V}}\prod_{V \in \mathcal{V}}\phi_{|V|}(u,u^{*},\ldots,u,u^{*},u|V),
\end{align*}
where $\phi_{2n}(\mathbf{1}, u, \ldots, u, u^{*}, u)=0$ because $\mathbf{1}$ is in free relation to anything, and the summation is over all $\mathcal{V}\in N\,C(2n+1)$ such that $|\mathcal{V}|=2$ and $1, 2$ belong to different blocks. Such a partition $\mathcal{V}$ must be of the form
\begin{equation*}
\mathcal{V}=\{\{1, t, t+1, \ldots, 2n+1\}, \{2,3, \ldots, t-1\}\}\,.
\end{equation*}
One of the two blocks gives an alternating sequence of $u, u^{*}$ of odd length, and the corresponding cumulant functional vanishes according to the induction hypothesis. Hence $\phi_{2n+1} (u, u^{*}, \ldots, u, u^{*}, u)=0$.

Next we deal with $\phi_{2n+2} (u, u^{*}, \ldots, u, u^{*})$. Similarly to the above argument, we form the product of $u$ and $u^{*}$ in the first two places and obtain
\begin{align*}
& \phi_{2n+2}(u,u^{*},\ldots,u,u^{*})\\
& \qquad =-\sum_{\mathcal{V}=\{V_{1},V_{2}\}}\phi_{|V_{1}|}(u,u^{*},\ldots,u,u^{*}|V_{1})\phi_{|V_{2}|}(u,u^{*},\ldots,u,u^{*}|V_{2})\,,
\end{align*}
where the summation is over all partitions $\mathcal{V}=\{\{1, t, t+1, \ldots, 2n+2\}, \{2, 3, \ldots, t- 1\}\}$ for $t=3,4, \ldots, 2n+3$. Due to the induction hypothesis only the even $t=2m$ survives in the sum, because the others correspond to vanishing cumulants. Hence
\begin{align*}
& \phi_{2n+2}(u, u^{*}, \ldots, u, u^{*})\\
& \qquad =-\sum_{m=2}^{n+1}\phi_{2(n-m+2)}\ (u, u^{*}, \ldots, u, u^{*})\phi_{2(m-1)}(u, u^{*}, \ldots, u, u^{*})\\
& \qquad =-\sum_{m=2}^{n+1}(-1)^{n-m+1}c_{n-m+1}(-1)^{m-2}c_{m-2}=(-1)^{n}c_{n}\,.
\end{align*}
In the last step the recursion formula (\ref{ch02:eqn2.3.2}) for the Catalan numbers was used.

We have shown that (\ref{ch02:eqn2.6.11}) and (\ref{ch02:eqn2.6.12}) hold, and we now turn to the coefficients of $R_{(u,u^{*})}$. In the light of (\ref{ch02:eqn2.6.9}) the coefficients of the alternating products of $X_{1}$ and $X_{2}$ are already settled, and it is remarkable that up to this point only the condition $\varphi(u)=0$ was used from the assumption of $u$ being a Haar unitary. Since the expectations of the product of different numbers of $u$'s and $u^{*}$'s are zero, it is a consequence of the moment-cumulant formula that the coefficient of a monomial of different numbers of $X_{1}$ and $X_{2}$ factors is $0$. It remains to prove that $\phi_{2n}(u_{1}, \ldots, u_{2n})=0$ if $(u_{1}, u_{2}, \ldots, u_{2n})$ is not an alternating sequence of $u, u^{*}$ and the number of $u$ terms is $n$. This can be done by induction on $n$. For such $(u_{1}, \ldots, u_{2n})$ choose $1\leq k<2n$ such that $u_{k}=u$ and $u_{k+1}=u^{*}$ (or $u_{k}=u^{*}$ and $u_{k+1}=u)$. Lemma~\ref{ch02:lem2.6.11} tells us that
\begin{align*}
& 0= \phi_{2n}(u_{1},u_{2},\ldots,u_{2n})\\ &
\qquad \quad + \sum_{\mathcal{V}=\{V_{1},V_{2}\}}\phi_{|V_{1}|}(u_{1},u_{2},\ldots,u_{2n}|V_{1})\phi_{|V_{2}|}(u_{1},u_{2},\ldots,u_{2n}|V_{2})\,,
\end{align*}
where the summation is over all non-crossing partitions $\mathcal{V}=\{V_{1}, V_{2}\}$ such that $k\in V_{1}$ and $k+1\in V_{2}$. One can see that under the constraints it is impossible that both $(u_{1}, u_{2}, \ldots, u_{2n}|V_{1})$ and $(u_{1}, u_{2}, \ldots, u_{2n}|V_{2})$ are alternating even sequences. So at least one of the corresponding cumulants is $0$, according to the induction hypothesis. This gives $\phi_{2n}(u_{1}, u_{2}, \ldots, u_{2n})=0$.
\end{example}

A pair $(a, b)$ of noncommutative random variables is called an $R$-\textit{diagonal pair} if its $R$-series is of the form
\begin{equation*}
R_{(a,b)}(X_{1},X_{2})=\sum_{n=1}^{\infty}\alpha_{n}\left((X_{1}X_{2})^{n}+(X_{2}X_{1})^{n}\right)\,.
\end{equation*}
In this case the one-variable series $\sum_{n=1}^{\infty}\alpha_{n}X^{n}$ is called the \textit{determining series}\index{determining series} of the $R$-diagonal pair $(a,b)$. An element $a$ in a $C^{*}$-probability space is called an $R$-\textit{diagonal element}\index{$R$-diagonal!element} if $(a, a^{*})$ is an $R$-diagonal pair. We know from Examples~\ref{ch02:exa2.6.12} and \ref{ch02:exa2.6.2} that $R$-diagonal elements are common generalizations of Haar unitaries and circular elements. It follows from Proposition~\ref{ch02:pro2.6.5} that the sum of free $R$-diagonal pairs\index{$R$-diagonal!pair} is $R$-diagonal again. In particular, the sum $a+b$ is $R$-diagonal when $a, b$ are so and moreover $\{a, a^{*}\}$ and $\{b, b^{*}\}$ are in free relation. Example~\ref{ch02:exa2.6.10} shows an $R$-diagonal pair in the setting of compressions.

In the following results, due to Nica and Speicher, $(\mathcal{A}, \varphi)$ is assumed to be a noncommutative probability space such that $\varphi$ is tracial.

\begin{theorem}\label{ch02:the2.6.13}
Let $a, b,p_{1},p_{2}$ be in $(\mathcal{A}, \varphi)$. If $(a, b)$ is an $R$-diagonal pair and $\{p_{1}, p_{2}\}$ is free from $\{a, b\}$, then $(ap_{1}, p_{2}b)$ is an $R$-diagonal pair.
\end{theorem}

\begin{proposition}\label{ch02:pro2.6.14}
If $(a, b)$ is an $R$-diagonal pair in $(\mathcal{A}, \varphi)$, then the determining series of $(a, b)$ is $R_{ab}\star Moeb$. Hence the distribution of $(a, b)$ is determined by that of $ab$.
\end{proposition}

The above theorem and proposition are not proven here, but they have interesting consequences. If $a$ is an $R$-diagonal element and moreover $\{a, a^{*}\}$ and $\{b, b^{*}\}$ are in free relation, then $ab$ is $R$-diagonal. In particular, if $u$ is a Haar unitary and $h=h^{*}$ is free from $\{u, u^{*}\}$ in a $C^{*}$-probability space, then $uh$ is an $R$-diagonal element. In the case where $h$ is quarter-circular, we get a circular element. Furthermore, $R$-\textit{diagonal elements} are characterized as follows.

\begin{corollary}\label{ch02:cor2.6.15}
An element a in $(\mathcal{A}, \varphi)$ is $R$-diagonal if and only if there exist a Haar unitary $u$ and a positive element $h$ $($in another $C^{*}$-probability space$)$ such that $h$ is free from $\{u, u^{*}\}$ and the distributions of $(a, a^{*})$ and $(uh, hu^{*})$ are the same.
\end{corollary}

\begin{proof2}
For any $R$-diagonal element $a$, choose a Haar unitary $u$ and a positive $h$ such that $h$ is free from $\{u, u^{*}\}$ and the distribution of $h^{2}$ is equal to that of $aa^{*}$. Then $uh$ is $R$-diagonal by Theorem~\ref{ch02:the2.6.13}, as noted above. By Proposition~\ref{ch02:pro2.6.14} both determining series of $(a, a^{*})$ and $(uh, hu^{*})$ are $R_{\mu}\star Moeb$, where $\mu$ is the same distribution of $aa^{*}$ and $h^{2}$. Hence the distribution of $(a, a^{*})$ is equal to that of $(uh, hu^{*})$. The converse is clear.
\end{proof2}

It will be shown in Sec.~\ref{ch04:sec4.4} that $R$-diagonal elements admit a convenient random matrix model.

\textbf{Notes and Remarks.}The definition of free relation was introduced in [\citen{bib196}]. Earlier Wai-Mee Ching constructed the free product of von Neumann algebras with cyclic and separating trace vectors in [\citen{bib50}]. The free product of $C^{*}$-algebras with designated states was first given in [\citen{bib8}] and [\citen{bib196}] independently. In our language they constructed the free product of noncommutative $C^{*}$-probability spaces. In [\citen{bib8}] the notion of free relation is implicit, but a sufficient condition for the free product $C^{*}$-algebra being simple is important. In the algebraic construction given in Sec.~\ref{ch02:sec2.1} one has to check that the functional $\omega$ is positive. This comes automatically when one works with representations. Results of [\citen{bib13}] show that the free product of von Neumann algebras is often of type III. This is so even for the free product of matrix algebras with respect to non-tracial states; see [\citen{bib65}].

The free central limit theorem (Theorem~\ref{ch02:the2.3.2}) is due to Voiculescu [\citen{bib196}]. Two proofs are supplied; the first, in Sec.~\ref{ch02:sec2.2}, is based on a combinatorial method, and the second, given at the end of Sec.~\ref{ch02:sec2.4}, is Voiculescu's original proof using $R$-series. Previously, Bo\.{z}ejko [\citen{bib36}] got a free central limit in a particular case arising from free generators in a discrete group, where the Catalan numbers were found as the limit moments. (A discussion similar to Bo\.{z}ejko's is in the second section of the Overview.)

The number of non-crossing partitions of $[n]$ is equal to the number of noncrossing pair partitions of $[2n]$. This fact is stated in Example~\ref{ch02:exa2.5.2} and shown in the proof of Proposition~\ref{ch02:pro2.6.3}. Indeed, the number (\ref{ch02:eqn2.5.10}) of non-crossing partitions of $[n]$ as well as the identity (\ref{ch02:eqn2.5.11}) were given in [\citen{bib112}]. The paper [\citen{bib171}] is about non-crossing partitions and contains further references on the subject. The M\"{o}bius inversion process grew out of the classical inclusion-exclusion in the lattice of all subsets of a finite set; see [\citen{bib5}] for the M\"{o}bius inversion theorem and the M\"{o}bius function in the lattice $L_{n}$, and see the monograph [\citen{bib180}] for a comprehensive discussion on incidence algebras. Multiplicative functions on the lattice $NC(n)$ were studied by Speicher in [\citen{bib176}], and the form of the M\"{o}bius function was computed in [\citen{bib112}] (also [\citen{bib176}]). The value of the M\"{o}bius function $\mu(\mathcal{V}_{1}, \mathcal{V}_{2})$ at a general pair $\mathcal{V}_{1}\leq \mathcal{V}_{2}$ is expressed by decomposing the sublattice $\{\mathcal{V} \in NC(n):\mathcal{V}_{1}\leq \mathcal{V}\leq \mathcal{V}_{2}\}$ into products of lattices $NC(k)$. Theorem~\ref{ch02:the2.5.6} is from [\citen{bib176}].

The additive free convolution, along with the $R$-series (or $\mathcal{R}$-transform) and the canonical random variables, was introduced in [\citen{bib197}], and the multiplicative one in [\citen{bib198}]. (For the sake of simplicity, our discussion was restricted here to compactly supported probability measures, where the method of moments is conveniently used.) The notion of $\mathcal{S}$-\textit{transform}\index{$\mathcal{S}$-transform} plays a role for the multiplicative free convolution similar to the one the $R$-series plays for the additive free convolution. In [\citen{bib96}] Haagerup made an essential simplification in the description of the multiplicative free convolution. In [\citen{bib138}] Nica and Speicher gave an alternative approach for the $\mathcal{S}$-transform via the ``Fourier transform'' of multiplicative functions on $NC(n)$.

It is worth mentioning that the notion of $R$-series was also discovered in a particular situation from study of random walks on free product groups (see [\citen{bib48}], [\citen{bib215}]).

The multivariate $R$-series and Propositions~\ref{ch02:pro2.6.3}--\ref{ch02:pro2.6.6} are from [\citen{bib132}], while [\citen{bib176}] contains some similar results for multivariables. The combinatorial convolution of formal power series was studied in [\citen{bib136}]. Examples~\ref{ch02:exa2.2.11} and \ref{ch02:exa2.4.7} are from [\citen{bib168}]. The unpublished formula in Proposition~\ref{ch02:pro2.6.8} was communicated to the authors by A. Nica and R. Speicher. It can be proven by the methods of the papers [\citen{bib133}] and [\citen{bib168}].

The concept of $R$-diagonal was introduced in [\citen{bib137}]; Theorem~\ref{ch02:the2.6.13} is a main result there. Proposition~\ref{ch02:pro2.6.14} and Corollary~\ref{ch02:cor2.6.15} were also shown there. Computation of the $R$-series of $(u, u^{*})$ for a Haar unitary $u$ is from [\citen{bib178}], Sec. 3.4. A power of an $R$-diagonal element is $R$-diagonal again; see [\citen{bib97}], [\citen{bib116}] and [\citen{bib111}]. In particular, if $c$ is a circular element, then $c^{k}$ is $R$-diagonal and
\begin{equation*}
\alpha_{n}=\frac{1}{(k-2)n+1}\left(\begin{array}{c}
(k-1)n\\
n-1\\
\end{array}\right),
\end{equation*}
the \textit{generalized Catalan numbers}\index{Catalan number!generalized} (see [\citen{bib141}] for the details).

The $n$th moment $m_{n}$ of the standard $q$-Gaussian distribution that appeared in Example~\ref{ch01:exa1.1.8} can be expressed as follows:
\begin{equation*}
m_{n}=\sum_{\mathcal{V}}q^{\mathrm{cr}(\mathcal{V})}=(1-q)^{-n}\sum_{k=-n}^{n}\left(\begin{array}{c}
2n\\
n+k\\
\end{array}\right)(-1)^{k}q^{k(k-1)/2},
\end{equation*}
where the first summation is over all pair partitions $\mathcal{V}$ of $[n]$ and $\mathrm{cr}(\mathcal{V})$ stands for the reduced left crossing number of $\mathcal{V}$ defined in [\citen{bib131}]. (This paper contains the $q$-version of Lemma~\ref{ch02:lem2.5.1}.) The second expression is from [\citen{bib106}].

The notion of free products extends to those with amalgamation. The amalgamated free product for $C^{*}$-algebras was introduced by Voiculescu [\citen{bib196}], where $C^{*}$-algebras $\mathcal{A}_{i}$ have a common $C^{*}$-subalgebra $\mathcal{B}$ with conditional expectations $\Phi_{i} : \mathcal{A}_{i}\rightarrow \mathcal{B}$. The free product of $C^{*}$-probability spaces is the case when $\mathcal{B}$ is the scalars $\mathbb{C}\mathbf{1}$ and so the conditional expectations $\Phi_{i}$ are states. As free probability theory corresponds to free products, the amalgamated free probability theory is developed for \textit{amalgamated free products}.\index{free!product, amalgamated} Assume that the algebras $\mathcal{A}_{i}$ are contained in the algebra $\mathcal{A}, \mathcal{B}$ is a common subalgebra of the $\mathcal{A}_{i}$'s, and there is a conditional expectation $E_{\mathcal{B}}:\mathcal{A}\rightarrow \mathcal{B}$. The subalgebras $\mathcal{A}_{i}$ are called \textit{free with amalgamation}\index{freeness!with amalgamation} over $\mathcal{B}$ if, for every $n \in \mathbb{N}$ and $i(1)\neq i(2)\neq\ldots\neq i(n)$,
\begin{equation*}
E_{\mathcal{B}}(a_{1}a_{2}\cdots a_{n})=0 \quad \mathrm{whenever}\quad a_{k}\in \mathcal{A}_{i(k)},\ E_{B}(a_{k})=0,1\leq k\leq n.
\end{equation*}
In this generalization moments and cumulants of noncommutative random variables are operator-valued, $\mathcal{B}$-valued in the above situation. See [\citen{bib204}] and [\citen{bib178}] for the details.


\chapter{Analytic Function Theory and Infinitely Divisible Laws}
\label{ch03:chap03}

\noindent This chapter is mostly devoted to the analytic machinery used to deal with free convolutions of measures. When a probability measure on $\mathbb{R}$ is considered as the distribution of a noncommutative random variable, one can associate to it some analytic functions which behave similarly to the logarithm of the Fourier transform in classical probability theory. In the previous chapter the $R$ series (or $\mathcal{R}$ transform) of a distribution measure was introduced as a formal power series which linearizes the additive free convolution. It will turn out that this formal series is related to the Cauchy transform of the distribution measure, and moreover the series is actually convergent in a complex domain whenever the measure is compactly supported. This relation between the $R$-series and the Cauchy transform was discovered by Voiculescu, and his original proof used analytic tools with the Toeplitz operator calculus. Two proofs are supplied here; the first, due to Speicher, comes from the combinatorics developed in the previous chapter, and the second is a simpler proof due to Haagerup. Since a probability measure on $\mathbb{R}$ can be recovered from its Cauchy transform by means of the well-known Stieltjes inversion formula, the above relation provides an effective device to determine free convolutions of measures.

In most of the chapter compactly supported measures are treated. Although many of the results hold true without this restriction, the proofs become essentially more transparent for compactly supported measures. By the method of analytic functions, examples will show the apparently strange behavior of the free convolution when one has a comparison with the classical convolution in mind: For example, the free convolution of atomic measures can be continuous.

The aim of the latter half of this chapter is to characterize infinitely divisible laws (with respect to the free convolution). The free Poisson distribution and the semicircle law are typical infinitely divisible measures. The theory of Nevanlinna-Pick functions plays an important role in the description of infinitely divisible laws, as it does in the classical L\'{e}vy-Hin\v{c}in formula. The compound free Poisson distribution is a natural generalization of the free Poisson distribution, and it is also infinitely divisible.

\section{Cauchy transform, Poisson integral, and Hilbert transform}
\label{ch03:sec3.1}

\noindent This section is a brief survey, for the reader's convenience, on the subjects of the title, which are basic ingredients in potential theory and some aspects of harmonic analysis.

Let $\mu$ be a probability measure on $\mathbb{R}$. Its \textit{Cauchy transform}\index{Cauchy transform}
\begin{equation}
G_{\mu}(z) :=\int_{-\infty}^{\infty}\frac{d\mu(t)}{z-t}
\label{ch03:eqn3.1.1}
\end{equation}
is defined when $z$ lies in the upper half-plane $\mathbb{C}^{+}:=\{z\in \mathbb{C}:\mathrm{Im}\,z>0\}$ (and also in the lower half-plane $\mathbb{C}^{-} :=\{z\in \mathbb{C}:\mathrm{Im}\,z<0\})$. The transform $G_{\mu}(z)$ is an analytic function in $\mathbb{C}^{+}$ possessing the following properties:
\begin{equation}
G_{\mu}(\mathbb{C}^{+})\subset \mathbb{C}^{-} \quad \mathrm{and}\quad |G_{\mu}(z)|\leq\frac{1}{\mathrm{Im}\, z}\,.
\label{ch03:eqn3.1.2}
\end{equation}
Let the support of $\mu$ be bounded. Then $G_{\mu}(z)$ is analytic in a neighborhood of $\infty$. Since $(z-t)^{-1}=\sum_{k=0}^{\infty}t^{k}z^{-k-1}$, it is obvious that $G_{\mu}(z)$ has the following expansion at $z=\infty$:
\begin{equation}
G_{\mu}(z)=z^{-1}+\sum_{k=1}^{\infty}m_{k}(\mu)z^{-k-1},
\label{ch03:eqn3.1.3}
\end{equation}
where $m_{k}(\mu)=\int t^{k}\,d\mu(t)\ (k\in \mathbb{Z}^{+})$.

The \textit{Poisson kernel}\index{Poisson!kernel} $P_{y}$ is given as
\begin{equation*}
P_{y}(x):=\frac{y}{\pi(x^{2}+y^{2})} \qquad (x\in \mathbb{R},\,y>0),
\end{equation*}
and it has the following properties:
\begin{enumerate}
\item[(1)] $P_{y}(x)>0$ and $\int_{-\infty}^{\infty}P_{y}(x)\,dx=1$.

\item[(2)] $P_{y}\in\bigcap_{1\leq p\leq\infty}L^{p}(\mathbb{R})$.

\item[(3)] $P_{y_{1}}\ * \ P_{y_{2}}=P_{y_{1}+y_{2}}$ for $y_{1}, y_{2}>0$, where $*$ means the usual convolution.
\end{enumerate}
Let $\nu$ be a finite positive (or, more generally, signed) measure on $\mathbb{R}$. The \textit{Poisson integral}\index{Poisson!integral} $P_{y}*\nu$ is the convolution product of $P_{y}$ and $\nu$; that is,
\begin{equation}
P_{y}*\nu(x)=\frac{1}{\pi}\int_{-\infty}^{\infty}\frac{y}{(x-t)^{2}+y^{2}}\,d\nu(t)\qquad (x\in \mathbb{R},\,y>0).
\label{ch03:eqn3.1.4}
\end{equation}
Then $P_{y}*\nu$ is an integrable function and $\Vert P_{y}*\nu\Vert_{1}\leq\Vert\nu\Vert$, where $\Vert\nu\Vert$ is the total variation of $\nu$. For every $f\in L^{p}(\mathbb{R})$, the Poisson integral $P_{y}*f$ is defined as (\ref{ch03:eqn3.1.4}) with $f(t)\,dt$ instead of $d\nu(t)$, and it satisfies $\Vert P_{y}*f\Vert_{p}\leq\Vert f\Vert_{p}, \,1\leq p\leq\infty$.

The following are some basic facts concerning the Poisson integral.
\begin{enumerate}
\item[(4)] $F(x+\mathrm{i}\,y) :=(P_{y}*\nu)(x)$ (also $F(x+\mathrm{i}\,y) :=(P_{y}*f)(x)$ for $f\in L^{p}(\mathbb{R}))$ is a harmonic function in $\mathbb{C}^{+}$.

\item[(5)] If $f\in L^{p}(\mathbb{R})$ with $1\leq p<\infty$, then $\Vert P_{y}*f-f\Vert_{p}\rightarrow 0$ and $(P_{y}*f)(x)\rightarrow f(x)$ a.e. as $y\rightarrow+0$.

\item[(6)] If $f$ is a bounded continuous function on $\mathbb{R}$, then $P_{y}*f\rightarrow f$ as $y\rightarrow+0$ uniformly on every compact set.

\item[(7)] If $\nu $ is a finite signed measure, then $ P_{y}*\nu \rightarrow\nu$ as $y\rightarrow+0$ in the $\mathrm{w}^{*}$-topology on the dual space of $C_{0}(\mathbb{R})$.
\end{enumerate}

The \textit{Hilbert transform} $H\,f$ of a function $f$ is given by the principal value integral
\begin{equation*}
Hf(x):=\lim_{\varepsilon\rightarrow+0}\frac{1}{\pi}\int_{|x-t|\geq\varepsilon}\frac{f(t)}{x-t}\,dt,
\end{equation*}
whenever the limit exists for a.e. $x\in \mathbb{R}$. Also let us define $\tilde{P}_{y}f$ by
\begin{equation*}
\tilde{P}_{y}f(x) :=\frac{1}{\pi}\int_{-\infty}^{\infty}\frac{(x-t)f(t)}{(x-t)^{2}+y^{2}}\,dt \qquad (x\in \mathbb{R}, y>0).
\end{equation*}
The Hilbert transform\index{Hilbert transform} has, among others, the following properties.
\begin{enumerate}
\item[(8)] If $f\in L^{p}(\mathbb{R})$ with $1<p<\infty$, then $H\,f$ is defined and $\Vert H\,f\Vert_{p}\leq A_{p}\Vert f\Vert_{p}$, where $A_{p}$ is a constant depending on $p$.

\item[(9)] If $f$ is as in (8), then $\tilde{P}_{y}f=P_{y}*H\,f$, and hence $\Vert\tilde{P}_{y}f-Hf\Vert_{p}\rightarrow 0$ and $\tilde{P}_{y}f(x)\rightarrow Hf(x)$ a.e. as $y\rightarrow+0$.
\end{enumerate}

Let $\mu$ be a compactly supported probability measure on $\mathbb{R}$. Writing $z=x+\mathrm{i}\,y$, one can split the Cauchy transform (\ref{ch03:eqn3.1.1}) into its real and imaginary parts:
\begin{equation*}
G_{\mu}(x+\mathrm{i}\,y)=\int_{-\infty}^{\infty}\frac{x-t}{(x-t)^{2}+y^{2}}\,d\mu(t)-\mathrm{i}\int_{-\infty}^{\infty}\frac{y}{(x-t)^{2}+y^{2}}\,d\mu(t).
\end{equation*}
Comparing this with (\ref{ch03:eqn3.1.4}) and with the property (7) of the Poisson integral, one can observe that the limit of the imaginary part of $G_{\mu}(x+\mathrm{i}\,y)$ recovers $\mu$ up to a factor $-\pi$. More precisely,
\begin{equation*}
\mu=\mathrm{w}^{*}\hbox{-}\lim_{y\rightarrow+0}\left[-\frac{1}{\pi}\mathrm{Im}\,G_{\mu}(x+\mathrm{i}\,y)\,dx\right].
\end{equation*}
(This relation sometimes bears the name \textit{Stieltjes inversion formula}.)\index{Stieltjes inversion formula} Also, note that $t_{0}\in \mathbb{R}$ is an isolated point of the support of $\mu$ if and only if $z=t_{0}$ is a simple pole of $G_{\mu}(z)$. Moreover, $\mu(\{t_{0}\})$ is the residue of $G_{\mu}(z)$ at $t_{0}$. When $\mu$ has a continuous derivative $f=d\mu/dx$, we obtain
\begin{equation}
f(x)=-\frac{1}{\pi} \lim_{y\rightarrow +0} \mathrm{Im}\,G_{\mu}(x+\mathrm{i}\,y)
\label{ch03:eqn3.1.5}
\end{equation}
and
\begin{equation*}
Hf(x)=\frac{1}{\pi}\lim_{y\rightarrow +0} \mathrm{Re}\,G_{\mu}(x+\mathrm{i}\,y),
\end{equation*}
due to property (9) of the Hilbert transform.

\begin{example}\label{ch03:exa3.1.1}
The Cauchy transform of the semicircle law $w_{r}$ is
\begin{equation}
G_{w_{r}}(z)=\frac{2}{r^{2}}(z-\sqrt{z^{2}-r^{2}})\,.
\label{ch03:eqn3.1.6}
\end{equation}
(Here the branch of $\sqrt{z^{2}-r^{2}}$ is taken in accordance with (\ref{ch03:eqn3.1.2}).)

The proof of Lemma~\ref{ch02:lem2.3.1} contains a formula for the generator function of the even moments of the standard semicircle law. Benefitting from that, we have
\begin{equation*}
\frac{1}{2}(1-\sqrt{1-4x})=\sum_{k=0}^{\infty}\frac{m_{2k}(w_{r})}{(r/2)^{2k}}x^{k+1},
\end{equation*}
which holds if $|x|$ is small. Replacing $x$ by $(r/2)^{2}z^{-2}$, we arrive at
\begin{equation*}
\frac{2}{r^{2}}(z-\sqrt{z^{2}-r^{2}})=\sum_{k=0}^{\infty}m_{2k}(w_{r})z^{-2k-1}
\end{equation*}
for large $|z|$. Since the left-hand side is analytic in $\mathbb{C}^{+}$, we get the above expression (\ref{ch03:eqn3.1.6}) in the light of (\ref{ch03:eqn3.1.3}).
\end{example}

We can easily take the limits as $y\rightarrow+0$ of the real and imaginary parts of (\ref{ch03:eqn3.1.6}):
\begin{align*}
\lim_{y\rightarrow+0}\mathrm{Re}\,G_{r}(x+\mathrm{i}\,y) & = \left\{\begin{array}{ll}
\dfrac{2}{r^{2}}x & \mathrm{if}\ |x|\leq r,\\
\dfrac{2}{r^{2}}(x-\sqrt{x^{2}-r^{2}}) & \mathrm{if}\ |x|>r,
\end{array}\right.\\
\lim_{y\rightarrow+0}\mathrm{Im}\,G_{r}(x+\mathrm{i}\,y) & = \left\{\begin{array}{llll}
-\dfrac{2}{r^{2}}\sqrt{r^{2}-x^{2}} & & & \mathrm{if}\ |x|\leq r,\\
0 & & & \mathrm{if}\ |x|>r.
\end{array}\right.
\end{align*}
The latter limit is in accordance with (\ref{ch03:eqn3.1.5}).

The $R$-series of $w_{r}$ is $R_{w_{r}}(z)=r^{2}z^{2}/4$ by Example~\ref{ch02:exa2.4.6}. One can easily check that $G_{w_{r}}(z)$ and $z^{-1}(1+R_{w_{r}}(z))$ are the inverse of each other. Indeed, this is a general fact that will be proved in the next section.

\section{Relation between Cauchy transform and $R$-series}
\label{ch03:sec3.2}

\noindent The $R$-series introduced in Sec.~\ref{ch02:sec2.4} contains all information of the distribution of a noncommutative random variable, and it is an analogue of the logarithm of the Fourier transform in classical probability theory. The essence of this section is the convergence of the formal series in a certain domain and its relation to the Cauchy transform of a measure.

As we mentioned in Sec.~\ref{ch02:sec2.4} (see the paragraph before Example~\ref{ch02:exa2.4.1}), it is convenient to consider abstract distributions which are given as linear functionals $\mu : \mathbb{C}\langle X\rangle \rightarrow \mathbb{C}$ with $\mu(\mathbf{1})=1$. The set of such distributions is denoted by $\Sigma$. If $\mu$ is a probability measure on $\mathbb{R}$ all of whose moments exist, then $\mu$ is considered as an element of $\Sigma$ via $\mu(X^{k})=m_{k}(\mu)\ (k\in \mathbb{Z}^{+})$. Note that (\ref{ch03:eqn3.1.3}) is meaningful for any $\mu\in\Sigma$; that is, one can define
\begin{equation}
G_{\mu}(z) :=z^{-1}(1+M_{\mu}(z^{-1}))=z^{-1}+\sum_{k=1}^{\infty}\mu(X^{k})z^{-k-1}
\label{ch03:eqn3.2.1}
\end{equation}
as a formal power series, where $M_{\mu}(z)=\sum_{k=1}^{\infty}\mu(X^{k})z^{k}$ is the moment generating series in (\ref{ch02:eqn2.5.17}). Also, the definition of the $R$-series $R_{\mu}(z)$ in Sec.~\ref{ch02:sec2.4} can be applied to any $\mu\in\Sigma$, and a formal power series $K_{\mu}(z)$ is defined by
\begin{equation}
K_{\mu}(z) :=z^{-1}(1+R_{\mu}(z))=z^{-1}+\sum_{k=0}^{\infty}\alpha_{k+1}z^{k}.
\label{ch03:eqn3.2.2}
\end{equation}
Then we have the following important result.

\begin{theorem}\label{ch03:the3.2.1}
For every $\mu\in\Sigma,\,G_{\mu}(z)$ and $K_{\mu}(z)$ given in \emph{(\ref{ch03:eqn3.2.1})} and \emph{(\ref{ch03:eqn3.2.2})} are the inverse of each other as formal power series. Thus, in particular, if $\mu$ is a compactly supported probability measure on $\mathbb{R}$, then $K_{\mu}(z)$ is a univalent analytic function from a neighborhood of $0$ to a neighborhood of $\infty$ whose inverse is the Cauchy transform $G_{\mu}(z)$.
\end{theorem}

\begin{proof2}
Theorem~\ref{ch02:the2.5.6} implies that
\begin{equation*}
R_{\mu}(G_{\mu}(z))=M_{\mu}(z^{-1}) \quad \mathrm{and}\quad M_{\mu}\left(\frac{1}{K_{\mu}(z)}\right)=R_{\mu}(z).
\end{equation*}
Hence we have
\begin{equation*}
K_{\mu}(G_{\mu}(z))=\frac{1+M_{\mu}(z^{-1})}{G_{\mu}(z)}=z
\end{equation*}
and
\begin{equation*}
G_{\mu}(K_{\mu}(z))=\frac{1+R_{\mu}(z)}{K_{\mu}(z)}=z,
\end{equation*}
so the first assertion is shown. Now the second is immediate, because $G_{\mu}(z)$ is analytic and univalent in a neighborhood of $\infty$ for any compactly supported measure $\mu$.
\end{proof2}

The above proof is a direct reformulation of Theorem~\ref{ch02:the2.5.6}, which is essentially combinatorial, based on the free moment-cumulant formula (\ref{ch02:eqn2.5.6}). It is worth emphasizing that free probability theory possesses both analytic and combinatorial aspects. Voiculescu's original proof of the above theorem is analytic, using Toeplitz operators. The proof presented below is due to Haagerup.

\begin{proof2}[Second Proof of Theorem~\ref{ch03:the3.2.1}]
Let $\mu\in\Sigma$ and $R_{\mu}(z)=\sum_{k=1}^{\infty}\alpha_{k}z^{k}$. Then $\alpha_{k}$ (resp. $\mu(X^{k})$) is a polynomial in $\mu(X),\ldots, \mu(X^{k})$ (resp. $\alpha_{1}, \ldots, \alpha_{k})$. The formal inverse of the power series $K_{\mu}(z)=z^{-1}+\sum_{k=0}^{\infty}\alpha_{k+1}z^{k}$ is of the form $z^{-1}+ \sum_{k=1}^{\infty}\mu_{k}z^{-k-1}$, where $\alpha_{k}$ (resp. $\mu_{k}$) is a polynomial in $\mu_{1}, \ldots, \mu_{k}$ (resp. $\alpha_{1}, \ldots, \alpha_{k})$. Based on these facts, it is enough to assume that $R_{\mu}(z)$ is a polynomial.

Let $\ell=\ell(h)$ be a creation operator on the full Fock space $\mathcal{F}(\mathcal{H})$, where $h$ is a unit vector, and let $\Phi$ be the vacuum vector and $\varphi$ the vacuum expectation, as usual. Set
\begin{equation*}
\Phi(z):=\Phi+\sum_{k=1}^{\infty}z^{k}h^{\otimes k}
\end{equation*}
for a family of vectors parametrized by complex numbers $z$ with $|z|<1$. Assume that $z^{-1}R_{\mu}(z)$ is a polynomial $P(z)=\sum_{k=0}^{N}\alpha_{k+1}z^{k}$, and set
\begin{equation*}
S:=\ell+P(\ell^{*})\,.
\end{equation*}
Note that $S^{*}=\ell^{*}+\sum_{k=1}^{N}\overline{\alpha}_{k+1}\ell^{k}$ is the canonical noncommutative random variable associated with $\mu$, except that the coeffcients are changed to complex-conjugate. This shows that $\mu(X^{k})=\overline{\varphi(S^{*k})}=\varphi(S^{k})$, and hence $G_{\mu}(z)=\sum_{k=0}^{\infty}\varphi(S^{k})z^{-k-1}$. Now compute
\begin{equation*}
S\Phi(z)=z^{-1}(\Phi(z)-\Phi)+P(z)\Phi(z)
\end{equation*}
and
\begin{equation*}
[(z^{-1}+P(z))\mathbf{1}-S]\Phi(z)=z^{-1}\Phi\,.
\end{equation*}
Therefore, we have
\begin{equation*}
[(z^{-1}+P(z))\mathbf{1}-S]^{-1}\Phi=z\Phi(z)
\end{equation*}
if $|z|>0$ is so small that $|z^{-1}+P(z)|>\Vert S\Vert$. Taking the inner product with $\Phi$, we obtain $\sum_{k=0}^{\infty}(z^{-1}+P(z))^{-k-1}\varphi(S^{k})=z$; that is, $G_{\mu}(z^{-1}+P(z))=z$. This is what we wanted, since $z^{-1}+P(z)=K_{\mu}(z)$.

Note that $\Phi(z)$ is a sort of exponential vector in the context of the full Fock space, cf. (\ref{ch01:eqn1.1.3}).
\end{proof2}

\begin{example}\label{ch03:exa3.2.2}
Let $\nu :=2^{-1}(\delta(-1/2)+\delta(1/2))$. Then the free convolution $\mu :=\nu \boxplus \nu$ has probability density
\begin{equation*}
f_{\mu}(x):=\left\{\begin{array}{ll}
\dfrac{1}{\pi\sqrt{1-x^{2}}} & \mathrm{if}\ |x|<1,\\
\\
0 & \mathrm{otherwise}.\\
\end{array}\right.
\end{equation*}
(This is called the \textit{arcsine law}.)\index{arcsine law}\index{law!arcsine} This example shows that the free convolution of atomic measures can be absolutely continuous. In particular, the free convolution is not distributive with respect to the addition of measures.

First we compute the $R$-series of $\nu$. The Cauchy transform of $\nu$ is $G_{\nu}(z)= 4z/(4z^{2}-1)$, and to get $R_{\nu}$ we have to solve the equation
\begin{equation}
G_{\nu}\left(\frac{1+R_{\nu}(u)}{u}\right)=u.
\label{ch03:eqn3.2.3}
\end{equation}
This is easy, and we obtain
\begin{equation*}
R_{\nu}(u)=\frac{-1+\sqrt{u^{2}+1}}{2}.
\end{equation*}
The $R$-series of $\mu$ is $R_{\mu}=2R_{\nu}$, and the Cauchy transform $G_{\mu}$ is the solution of (\ref{ch03:eqn3.2.3}) when $\nu$ is replaced by $\mu$. We arrive at
\begin{equation*}
G_{\mu}(z)=\frac{\sqrt{z^{2}-1}}{z^{2}-1}\,,
\end{equation*}
and
\begin{equation*}
\lim_{y\rightarrow+0}\mathrm{Im}\,G_{\mu}(x+\mathrm{i}\,y)=-\pi f_{\mu}(x)\,.
\end{equation*}
\end{example}

The last example is somewhat special; the convolution of atomic measures generally possesses both absolutely continuous and atomic parts.

\begin{example}\label{ch03:exa3.2.3}
Let $\nu :=(1-\alpha)\delta(-1/2)+\alpha\delta(1/2)$. Then the free convolution $\mu:=\nu \boxplus \nu$ is
\begin{align}
&\frac{\sqrt{4\alpha(1-\alpha)-x^{2}}}{\pi(1-x^{2})}\chi(x)\,dx \label{ch03:eqn3.2.4}\\
& \qquad \quad +\max\{1-2\alpha, 0\}\delta(-1)+\max\{2\alpha-1,0\}\delta(1), \notag
\end{align}
where $\chi$ is the characteristic function of the interval $[-2\sqrt{\alpha(1-\alpha)}, 2\sqrt{\alpha(1-\alpha)}]$.

The computation goes along the lines of the previous example. The Cauchy transform
\begin{equation*}
G_{\mu}(z)=\frac{2\alpha-1+\sqrt{z^{2}-4\alpha(1-\alpha)}}{z^{2}-1}
\end{equation*}
has a simple pole at $z=-1$ for $0\leq\alpha<1/2$ and at $z=1$ for $1/2<\alpha\leq 1$. The corresponding residues are $ 1-2\alpha$ and $2\alpha-1$, respectively. In this way, (\ref{ch03:eqn3.2.4}) is obtained.
\end{example}

Improving the above computation, it is not difficult to show the following fact: Let $\mu, \nu$ be compactly supported probability measures and $a, b, c$ the maximum of the supports of $\mu, \nu, \mu \boxplus \nu$, respectively. Then
\begin{equation*}
\left\{\begin{array}{l}
c=a+b\quad \mathrm{if}\ \mu(\{a\})+\nu(\{b\})\geq 1,\\
c<a+b\quad \mathrm{otherwise}.\\
\end{array}\right.
\end{equation*}
This is another feature of the free convolution which is very different from the classical one.

\section{Infinitely divisible laws}
\label{ch03:sec3.3}

\noindent Infinitely divisible laws with respect to the additive free convolution will be discussed in this section. In the classical theory (related to the ordinary convolution) the theory of Pick-Nevanlinna functions plays an important role, and the L\'{e}vy-Hin\v{c}in formula is the highlight. The free analogue seems rather similar. Parallel to the Gaussian and Poisson laws that are typical in the classical case, typical infinitely divisible laws in the free case are the Wigner and free Poisson ones.

We first give a short survey on Pick functions. An analytic function $f$ in $\mathbb{C}^{+}$ such that $\mathrm{Im}\,f(z)\geq 0$ for all $z\in \mathbb{C}^{+}$ is called a \textit{Pick function}.\index{Pick function} Note that a Pick function $f$ satisfies $f(\mathbb{C}^{+})\subset \mathbb{C}^{+}$ unless $f$ is a constant. For $a, b\in \mathbb{R},\,a<b$, we denote by $\mathcal{P}(a, b)$ the set of Pick functions $f$ which have analytic continuation to $(\mathbb{C}\,\backslash\,\mathbb{R})\cup(a, b)$ so that $f(\overline{z})=\overline{f(z)}$. The following is a kind of Pick-Nevanlinna theorem, which is tailor-made for our purpose.

\begin{theorem}\label{ch03:the3.3.1}
Let $D$ be a domain in $\mathbb{C}^{+}$. For a function $f : D\rightarrow \mathbb{C}$ the following conditions are equivalent:
\begin{enumerate}
\item[(i)] $f$ extends to a Pick function.

\item[(ii)] For every $z_{1}, \ldots, z_{n}\in D$, the matrix
\begin{equation*}
\left[\frac{f(z_{i})-\overline{f(z_{j})}}{z_{i}-\overline{z}_{j}}\right]_{i,j=1}^{n}
\end{equation*}
is positive semidefinite.

\item[(iii)] There exist $\alpha\in \mathbb{R},\,\beta\geq 0$, and a positive finite measure $\nu$ on $\mathbb{R}$ such that
\begin{equation}
f(z) =\alpha\ +\beta z +\int_{-\infty}^{\infty}\frac{1+xz}{x-z}\,d\nu(x) \qquad (z \in D).
\label{ch03:eqn3.3.1}
\end{equation}
\end{enumerate}

Moreover, if the above conditions hold, then $\alpha, \beta$, and $\nu$ in \emph{(iii)} are unique, and $f$ extends to a Pick function in $\mathcal{P}(a, b)$ if and only if $\nu$ is supported on $\mathbb{R}\,\backslash\,(a, b)$.
\end{theorem}

\begin{corollary}\label{ch03:cor3.3.2}
 If $f$ is an analytic function in $\mathbb{C}^{+}$, then the following conditions are equivalent:
\begin{enumerate}
\item[(i)] $f$ extends to a Pick function in $\mathcal{P}(-\varepsilon ,\varepsilon)$ for some $\varepsilon >0$; that is, $f$ $($extended$)$ is analytic in a neighborhood of $(\mathbb{C}\,\backslash\,\mathbb{R})\cup\{0\},\,f(\overline{z})=\overline{f(z)}$, and $\mathrm{Im}\,f(z)\geq 0$ for $z\in \mathbb{C}^{+}$.

\item[(ii)] There exist $\alpha\in \mathbb{R}$ and a positive finite measure $\nu$ on $\mathbb{R}$ with compact support such that
\begin{equation*}
f(z)=\alpha+\int_{-\infty}^{\infty}\frac{z}{1-xz}\,d\nu(x).
\end{equation*}

In this case, $\alpha$ and $\nu$ in \emph{(ii)} are unique and $\nu$ is supported on $[-1/\varepsilon, 1/\varepsilon]$.
\end{enumerate}
\end{corollary}

\begin{proof2}
(ii)$\,\Rightarrow\,$(i) is obvious. For (i)$\,\Rightarrow\,$(ii), by Theorem~\ref{ch03:the3.3.1} there exist $\alpha'\in \mathbb{R}, \beta\geq 0$ and a positive finite measure $\nu'$ on $\mathbb{R}\,\backslash\,(-\varepsilon, \varepsilon)$ for which (\ref{ch03:eqn3.3.1}) holds. Let $\nu''$ be the transform of $\nu'$ by $x\mapsto x^{-1}$, and define a finite measure $\nu$ on $[-1/\varepsilon,\,1/\varepsilon]$ by
\begin{equation*}
\nu :=\beta\delta(0)+(1+x^{2})\nu''.
\end{equation*}
Then we have
\begin{align*}
f(z) & \ = \ \alpha'+\beta z+\int\frac{1+x^{-1}z}{x^{-1}-z}\,d \nu''(x)\\
& \ = \ \alpha'+\beta z+\int\left(x+\frac{(1+x^{2})z}{1-xz}\right)d\nu''(x)\\
& \ = \ \alpha+\int\frac{z}{1-xz}\,d\nu(x)
\end{align*}
with $\alpha :=\alpha'+\int x\,d\nu''(x)$. The last assertion is easily seen from Theorem~\ref{ch03:the3.3.1}.
\end{proof2}

From now on we shall apply the Pick-Nevanlinna theory to characterize infinitely divisible laws with respect to the free convolution. Let $\mathcal{M}_{0}(\mathbb{R})$ denote the set of compactly supported probability measures on $\mathbb{R}$. A measure $\mu\in \mathcal{M}_{0}(\mathbb{R})$ is said to be $\boxplus$-\textit{infinitely divisible}\index{$\boxplus$-infinitely divisible} if for each $n\in \mathbb{N}$ there exists $\mu_{n}\in \mathcal{M}_{0}(\mathbb{R})$ such that
\begin{equation*}
\mu=\mu_{n} \boxplus \cdots \boxplus \mu_{n} \quad (n \ \mathrm{times}).
\end{equation*}
In this case, $\mu_{n}$ is unique because $R_{\mu_{n}}=n^{-1}R_{\mu}$ by Theorem~\ref{ch02:the2.4.5}.

\begin{example}\label{ch03:exa3.3.3}
The semicircle law $w_{m,r}$ is $\boxplus$-infinitely divisible because
\begin{equation*}
w_{m,r}=(w_{m/n,r/\sqrt{n}})^{\boxplus n},
\end{equation*}
see Example~\ref{ch02:exa2.4.2}.
\end{example}

\begin{lemma}\label{ch03:lem3.3.4}
Let $(\mu_{n})$ be a sequence in $\mathcal{M}_{0}(\mathbb{R})$. Then the following conditions are equivalent:
\begin{enumerate}
\item[(i)] The supports of the $\mu_{n}$ are uniformly bounded, and $(\mu_{n})$ converges in the $w^{*}$-topology to a measure $\mu\in \mathcal{M}_{0}(\mathbb{R})$.

\item[(ii)] $(R_{\mu_{n}})$ converges uniformly in some neighborhood of $0$ to a function $R$.
\end{enumerate}

In this case, $R=R_{\mu}$.
\end{lemma}

\begin{proof2}
(i)$\,\Rightarrow\,$(ii). Assume that the supports of $\mu_{n}$ (and hence $\mu$) are included in $[-\alpha, \alpha]$. Let
\begin{equation*}
f_{n}(z):=G_{\mu_{n}}(z^{-1})=\int_{-\alpha}^{\alpha}\frac{z}{1-xz}\,d\mu_{n}(x).
\end{equation*}
Since $f_{n}'(z)=\int_{-\alpha}^{\alpha}1/(1-xz)^{2}\,d\mu_{n}(x)$, there exists $\varepsilon >0$ such that $\mathrm{Re}\,f_{n}'(z)>0$ and $|f_{n}(z)|>|z|/2$ for $|z|<\varepsilon$ and all $n$. Since
\begin{equation*}
\mathrm{Re}\,\frac{f_{n}(z_{2})-f_{n}(z_{1})}{z_{2}-z_{1}}=\int_{0}^{1}\mathrm{Re}\,f_{n}'(z_{1}+t(z_{2}-z_{1}))\,dt>0 \quad \mathrm{for} \quad |z_{1}|, |z_{2}|<\varepsilon,
\end{equation*}
all $f_{n}(z)$ are univalent in $|z|<\varepsilon$. Furthermore, we have $\{f_{n}(z):|z|<\varepsilon\}\supset\{|z|< \varepsilon /2\}$. Thus, if $U :=\{|z|>\varepsilon^{-1}\}\cup\{\infty\}$ and $V :=\{|z|<\varepsilon/2\}$, then all $G_{\mu_{n}}$ are univalent in $U$ and $G_{\mu_{n}}(U)\supset V$, that is, $K_{\mu_{n}}(V)\subset U$. This shows that $(K_{\mu_{n}})$ in $V$ is a normal family. By (i) we have $m_{k}(\mu_{n})\rightarrow m_{k}(\mu)$ as $ n\rightarrow\infty$ for all $k\in \mathbb{N}$. Hence the Taylor coefficients of $R_{\mu_{n}}$ converge to the corresponding ones of $R_{\mu}$, because the coefficient of $R_{\mu_{n}}$ for $z^{k}$ is a universal polynomial in $m_{j}(\mu_{n}),\,1\leq j\leq k$. This implies that $(K_{\mu_{n}})$ converges uniformly in $V$ to $K_{\mu}$, as does $(R_{\mu_{n}})$ to $R_{\mu}$.

(ii)$\,\Rightarrow\,$(i). Let
\begin{equation*}
g_{n}(z):=\frac{1}{K_{\mu_{n}}(z)}=\frac{z}{1+R_{\mu_{n}}(z)}.
\end{equation*}
Since $g_{n}'(z)=(1+R_{\mu_{n}}(z)-zR_{\mu_{n}}'(z))/(1+R_{\mu_{n}}(z))^{2}$, there exists $\varepsilon >0$ such that $\mathrm{Re}\,g_{n}'(z)>0$ and $|g_{n}(z)|>|z|/2$ for $|z|<\varepsilon$ and all $n$. As in the above proof, all $K_{\mu_{n}}$ are univalent in $U,\ K_{\mu_{n}}(U)\supset V$ and $G_{\mu_{n}}(V)\subset U$, where $U :=\{|z|<\varepsilon\}$ and $V :=\{|z|>2/\varepsilon\}$. So it is easy to see that $G_{\mu_{n}}(z^{-1})\in \mathcal{P}(-\varepsilon/2, \varepsilon/2)$. This implies by Corollary~\ref{ch03:cor3.3.2} that all $\mu_{n}$ are supported on $[-2/\varepsilon, 2/\varepsilon]$. Now let $\mu$ be any $\mathrm{w}^{*}$-limit point of $(\mu_{n})$. Then the above (i)$\,\Rightarrow\,$(ii) shows that $R_{\mu}=R$. Thus $\mu$ is a unique $\mathrm{w}^{*}$-limit point of $(\mu_{n})$, and so $\mu_{n}\rightarrow\mu$ in the $\mathrm{w}^{*}$-topology.
\end{proof2}

As a consequence of Lemma~\ref{ch03:lem3.3.4}, we observe that if $(\mu_{n})$ is a sequence of $\boxplus$-infinitely divisible measures in $\mathcal{M}_{0}(\mathbb{R})$ with uniformly bounded supports and it converges in the $\mathrm{w}^{*}$-topology to $\mu\in \mathcal{M}_{0}(\mathbb{R})$, then $\mu$ is $\boxplus$-infinitely divisible too.

In Example~\ref{ch02:exa2.5.2} the form of the free Poisson distribution $\mu_{1}$ was determined by a combinatorial argument. For the general free Poisson distribution we have

\begin{example}\label{ch03:exa3.3.5}
Let $\lambda >0$, and let $\mu_{\lambda}$ be the free Poisson distribution introduced in Example~\ref{ch02:exa2.5.2}, that is, the distribution\index{distribution!free Poisson} of the canonical noncommutative random variable $\ell^{*}+\sum_{k=0}^{\infty}\lambda\ell^{k}$. In order to imitate the Poisson limit theorem, let
\begin{equation*}
\mu^{(n)}:=\left(\left(1-\frac{\lambda}{n}\right)\negthickspace\delta(0)+\frac{\lambda}{n}\delta(1)\right)^{\boxplus n}.
\end{equation*}
Then $(\mu^{(n)})$ converges to $\mu_{\lambda}$ in the $\mathrm{w}^{*}$-topology. Recall that according to the classical Poisson limit theorem\index{free!Poisson distribution} ([\citen{bib76}], X.6) this sequence converges to the Poisson law when the free convolution $\boxplus$ is replaced by the classical one. This free analogue might be called the \textit{free Poisson limit theorem}.\index{free!Poisson limit theorem} Moreover, the exact form of $\mu_{\lambda}$ is
\begin{equation}
\mu_{\lambda}=\left\{\begin{array}{ll}
\dfrac{\sqrt{4\lambda-(x-1-\lambda)^{2}}}{2\pi x}\chi(x)\,dx & \mathrm{if}\ \lambda\geq 1,\\
(1-\lambda)\delta(0)+\dfrac{\sqrt{4\lambda-(x-1-\lambda)^{2}}}{2\pi x}\chi(x)\,dx & \mathrm{if}\ 0<\lambda<1,
\end{array}\right.
\label{ch03:eqn3.3.2}
\end{equation}
where $\chi$ stands for the characteristic function of the interval $[(1-\sqrt{\lambda})^{2}, (1+\sqrt{\lambda})^{2}]$.

The $R$-series of $\mu_{\lambda}$ is
\begin{equation*}
R_{\mu_{\lambda}}(z)=\sum_{k=1}^{\infty}\lambda z^{k}=\frac{\lambda z}{1-z},
\end{equation*}
and the Cauchy transform of $\nu_{n}:=(1-\lambda/n)\delta(0)+(\lambda/n)\delta(1)$ is
\begin{equation*}
G_{\nu_{n}}(z)=\left(1-\frac{\lambda}{n}\right)\negthickspace\frac{1}{z}+\frac{\lambda}{n}\,\frac{1}{z-1}.
\end{equation*}
Hence on the basis of Theorem~\ref{ch03:the3.2.1} we obtain the Cauchy transform of $\mu_{\lambda}$ and the $R$-series of $\nu_{n}$ as follows:
\begin{align*}
G_{\mu_{\lambda}}(z)&=\frac{z+(1-\lambda)-\sqrt{(z-1-\lambda)^{2}-4\lambda}}{2z},\\
R_{\nu_{n}}(z)&=\frac{z-1-\sqrt{(z-1)^{2}+4z\lambda/n}}{2}=\frac{\lambda z}{n(1-z)}+O(n^{-2})z.
\end{align*}
Since $R_{\mu^{(n)}}(z)=nR_{\nu_{n}}(z)$ has the uniform limit $R_{\mu_{\lambda}}(z)$ as $n\rightarrow\infty$ in a neighborhood of $0$, we observe by Lemma~\ref{ch03:lem3.3.4} that $(\mu^{(n)})$ has uniformly bounded supports and converges to $\mu_{\lambda}$ in the $\mathrm{w}^{*}$-topology; in particular, $\mu_{\lambda}$ is compactly supported. From the above formula for $G_{\mu_{\lambda}}(z)$ we recover the measure $\mu_{\lambda}$ by the Stieltjes inversion formula and arrive at the expression (\ref{ch03:eqn3.3.2}). Here, note that, when $0<\lambda<1, G_{\mu_{\lambda}}(z)$ has a pole at $z=0$ and therefore $\mu_{\lambda}$ has an atom at $0$.

Furthermore, one can get the exact form of $\mu^{(n)}$. The Cauchy transform of $\mu^{(n)}$ is computed by Theorem~\ref{ch03:the3.2.1} as follows:
\begin{equation*}
G_{\mu^{(n)}}(z)=\frac{(n-2)z+n(1-\lambda)-\sqrt{n^{2}z^{2}-2n(n+(n-2)\lambda)z+n^{2}(1-\lambda)^{2}}}{2(nz-z^{2})}.
\end{equation*}
When $0<\lambda<1$ this has a pole at $z=0$ with residue $1-\lambda$, but $z=n$ is not a pole if $n$ is large. Hence for large $n$ the Stieltjes inversion formula yields
\begin{equation}
\mu^{(n)}=\frac{\sqrt{4\lambda\negthickspace\left(1-\frac{1}{n}\right)\negthickspace\left(1-\frac{\lambda}{n}\right)-\left(x-1-\left(1-\frac{2}{n}\right)\negthickspace\lambda\right)^{2}}}{2\pi x(1-\frac{x}{n})}\chi_{n}(x)\,dx
\label{ch03:eqn3.3.3}
\end{equation}
if $\lambda\geq 1$ and
\begin{equation}
(1-\lambda)\delta(0)+\frac{\sqrt{4\lambda(1-\frac{1}{n})(1-\frac{\lambda}{n})-(x-1-(1-\frac{2}{n})\lambda)^{2}}}{2\pi x(1-\frac{x}{n})}\chi_{n}(x)\,dx
\label{ch03:eqn3.3.4}
\end{equation}
when $0<\lambda<1$, where $\chi_{n}$ is the characteristic function of the interval
\begin{align*}
& \left[\left(1-\sqrt{\lambda(1-\tfrac{1}{n})(1-\tfrac{\lambda}{n})}\right)^{2}-\tfrac{\lambda}{n}+\left(1-\tfrac{1}{n}\right)\tfrac{\lambda^{2}}{n},\right.\\
& \qquad \qquad \qquad \quad \ \left.\left(1+\sqrt{\lambda(1-\tfrac{1}{n})(1+\tfrac{\lambda}{n})}\right)^{2}-\tfrac{\lambda}{n}+(1-\tfrac{1}{n})\tfrac{\lambda^{2}}{n}\right].
\end{align*}
In this way, one can observe that the interval of $\chi_{n}$ converges to that of $\chi$ and the density function in (\ref{ch03:eqn3.3.3}) and (\ref{ch03:eqn3.3.4}) uniformly converges to that in (\ref{ch03:eqn3.3.2}). So the convergence $\mu^{(n)}\rightarrow\mu_{\lambda}$ is much better than stated above.
\end{example}

For the free Poisson distribution\index{distribution!Marchenko-Pastur} $\mu_{\lambda}$ the term \textit{Marchenko-Pastur distribution}\index{Marchenko-Pastur distribution} is also used in connection with random matrices (see Sec.~\ref{ch04:sec4.1}). Since $\mu_{\lambda}$ is a limit of certain free convolutions according to the free Poisson limit theorem in the above example, one may expect $\mu_{\lambda}$ to be infinitely divisible. In fact, $\mu_{\lambda} \boxplus \mu_{\lambda'}=\mu_{\lambda+\lambda'}$, because $R_{\mu_{\lambda}}+R_{\mu_{\lambda'}} =R_{\mu_{\lambda+\lambda'}}$. The free Poisson distribution will be described in Example~\ref{ch03:exa3.3.10} in a slightly modified form.

A one-parameter family $(\mu_{t})_{t\geq 0}$ of measures in $\mathcal{M}_{0}(\mathbb{R})$ is called a $\boxplus$-\textit{semigroup} if $\mu_{t} \boxplus \mu_{s}=\mu_{t+s}$ for all $t, s\geq 0$ (in particular, $\mu_{0}=\delta(0)$). The above $\mu_{\lambda}$'s form a $\mathrm{w}^{*}$-continuous $\boxplus$-semigroup. Also, the family $(w_{2\sqrt{t}})_{t\geq 0}$ of semicircle laws $(w_{0}=\delta(0))$ is a $\mathrm{w}^{*}$-continuous $\boxplus$-semigroup by Example~\ref{ch02:exa2.4.2}. In Examples~\ref{ch01:exa1.2.8} and \ref{ch02:exa2.2.10} we considered a noncommutative process $(X_{t})_{t\geq 0}$ on the full Fock space. This is distributed according to $(w_{2\sqrt{t}})_{t\geq 0}$ and can be regarded as a \textit{free analogue of Brownian motion.}.\index{free!Brownian motion}

The next theorem characterizes the $\boxplus$-infinitely divisible laws in $\mathcal{M}_{0}(\mathbb{R})$. Up to now we have used the $R$-series $R_{\mu}(z)$, which differs by a factor $z$ from Voiculescu's $\mathcal{R}$-\textit{transform} $\mathcal{R}_{\mu}(z)\negthickspace:R_{\mu}(z)=z\mathcal{R}_{\mu}(z)$. To state the theorem the $\mathcal{R}$-transform\index{$\mathcal{R}$-transform} $\mathcal{R}_{\mu}$ seems to be more convenient than $R_{\mu}$, and in the rest of this section $\mathcal{R}_{\mu}$ will be preferred to $R_{\mu}$.

\begin{theorem}\label{ch03:the3.3.6}
The following conditions for $\mu\in \mathcal{M}_{0}(\mathbb{R})$ are equivalent:
\begin{enumerate}
\item[(i)] $\mu$ is $\boxplus$-infinitely divisible.

\item[(ii)] There exists a $w^{*}$-continuous $\boxplus$-semigroup\index{$\boxplus$-semigroup} $(\mu_{t})_{t\geq 0}$ such that $\mu_{1}=\mu$.

\item[(iii)] $\mathcal{R}_{\mu}$ extends to a Pick function in $\mathcal{P}(-\varepsilon, \varepsilon)$ for some $\varepsilon >0$.

\item[(iv)] There exist $\alpha\in \mathbb{R}$ and a positive finite measure $\nu$ on $\mathbb{R}$ with compact support such that
\begin{equation}
\mathcal{R}_{\mu}(z)=\alpha+\int_{-\infty}^{\infty}\frac{z}{1-xz}\,d\nu(x)
\label{ch03:eqn3.3.5}
\end{equation}
for all $z$ in a neighborhood of $(\mathbb{C}\,\backslash\,\mathbb{R})\cup\{0\}$.
\end{enumerate}

Moreover, if the above conditions hold, then $\alpha$ and $\nu$ in \emph{(iv)} are unique and the following hold:
\begin{enumerate}
\item[(1)] $\mathcal{R}_{\mu_{t}}(z)=t\mathcal{R}_{\mu}(z),\,t\geq 0$;

\item[(2)] $\alpha=m_{1}(\mu)=\lim_{\varepsilon\rightarrow+0}m_{1}(\mu_{\varepsilon})/\varepsilon$;

\item[(3)] $\nu=\mathrm{w}^{*}\hbox{-}\lim_{\varepsilon\rightarrow+0}\nu_{\varepsilon}$ where $d\nu_{\varepsilon}:=(x^{2}/\varepsilon)d\mu_{\varepsilon}$;

\item[(4)] $\nu(\mathbb{R})=m_{2}(\mu)-m_{1}(\mu)^{2}$, the variance of $\mu$.
\end{enumerate}
\end{theorem}

In classical probability theory, when $\mu_{1}, \mu_{2}$ are probability measures on $\mathbb{R}$ and $\mu=\mu_{1}\,*\,\mu_{2}$ (the usual convolution), we have $\Phi_{\mu}(x)=\Phi_{\mu_{1}}(x)\Phi_{\mu_{2}}(x)$, where $\Phi_{\mu}$ is the characteristic function (or the Fourier transform) of $\mu$, i.e. $\Phi_{\mu}(x)=\int e^{\mathrm{i}\,tx}\,d\mu(t), x\in \mathbb{R}$. If $\mu$ is infinitely divisible in the classical sense, then the following \textit{L\'{e}vy-Hin\v{c}in formula}\index{L\'{e}vy-Hin\v{c}in formula} is known:
\begin{equation*}
\log\Phi_{\mu}(x)=\mathrm{i}\,\alpha x+\int_{-\infty}^{\infty}\left(e^{\mathrm{i}\,tx}-1-\frac{\mathrm{i}\,tx}{1+t^{2}}\right)\frac{1+t^{2}}{t^{2}}\,d\nu(t),
\end{equation*}
where $\alpha\in \mathbb{R}$ and $\nu$ is a positive finite measure on $\mathbb{R}$. If the $\mathcal{R}$-transform $\mathcal{R}_{\mu}$ is regarded as the free version of the logarithm of the characteristic function $\Phi_{\mu}$, then the expression (\ref{ch03:eqn3.3.5}) is the free analogue of the L\'{e}vy-Hin\v{c}in formula.

We need some lemmas to prove Theorem~\ref{ch03:the3.3.6}.

\begin{lemma}\label{ch03:lem3.3.7}
Let $\mathcal{R}(z)=\sum_{k=0}^{\infty}\alpha_{k+1}z^{k}$ be a formal power series. Then the following conditions are equivalent:
\begin{enumerate}
\item[(i)] There exists $\mu\in \mathcal{M}_{0}(\mathbb{R})$ such that $\mathcal{R}=\mathcal{R}_{\mu}$.

\item[(ii)] $\mathcal{R}$ is convergent in a neighborhood of $0$ with $\mathcal{R}(\overline{z})=\overline{\mathcal{R}(z)}$, and for every $z_{1}, \ldots, z_{n}\in \mathbb{C}^{+}$ in a neighborhood of $\infty$, the matrix
\begin{equation*}
\left[\frac{z_{i}-\overline{z}_{j}}{z_{i}-\overline{z}_{j}+\mathcal{R}(z_{i}^{-1})-\mathcal{R}(\overline{z}_{j}^{-1})}\right]_{i,j=1}^{n}
\end{equation*}
is positive semidefinite.
\end{enumerate}

Furthermore, if $\mathcal{R}$ is a Pick function in $\mathcal{P}(-\varepsilon, \varepsilon)$ for some $\varepsilon >0$, then the above conditions hold.
\end{lemma}

\begin{proof2}
(i)$\,\Rightarrow\,$(ii). Let $\mu\in \mathcal{M}_{0}(\mathbb{R})$ and $\mathcal{R}=\mathcal{R}_{\mu}$. Then $F(z)=1/G_{\mu}(z)$ is analytic in a neighborhood of $(\mathbb{C}\,\backslash\, \mathbb{R})\cup\{\infty\}$. Theorem~\ref{ch03:the3.2.1} implies that $\mathcal{R}$ is analytic in a neighborhood of $0$ and $F$ is univalent in a neighborhood $U$ of $\infty$ with the inverse $K_{\mu}(z^{-1})=z+\mathcal{R}(z^{-1})$. Since $G_{\mu}(\overline{z})=\overline{G_{\mu}(z)}$, we get $F(\overline{z})=\overline{F(z)}$ and $K_{\mu}(\overline{z})=\overline{K_{\mu}(z)}$. Furthermore, since $G_{\mu}(\mathbb{C}^{+})\subset \mathbb{C}^{-}$, we get $F(\mathbb{C}^{+})\subset \mathbb{C}^{+}$, so that $F$ is a Pick function. Let $z_{1}, \ldots, z_{n}\in F(U)\cap \mathbb{C}^{+}$. Then for some $\zeta_{j}\in U\cap \mathbb{C}^{+}$ we have $z_{j}=F(\zeta_{j})$, and so $\zeta_{j}=z_{j}+\mathcal{R}(z_{j}^{-1})$. Therefore we have
\begin{equation}
\left[\frac{z_{i}-\overline{z}_{j}}{z_{i}-\overline{z}_{j}+\mathcal{R}(z_{i}^{-1})-\mathcal{R}(\overline{z}_{j}^{-1})}\right]_{i,j}=\left[\frac{F(\zeta_{i})-F(\overline{\zeta}_{j})}{\zeta_{i}-\overline{\zeta}_{j}}\right]_{i,j},
\label{ch03:eqn3.3.6}
\end{equation}
which is positive semidefinite by Theorem~\ref{ch03:the3.3.1}.

(ii)$\,\Rightarrow\,$(i). Since $\mathcal{R}$ is analytic in a neighborhood of $0$, it is obvious that $\hat{K}(z) := z+\mathcal{R}(z^{-1})$ is a univalent analytic function in a neighborhood $V$ of $\infty$. Let $F$ be the inverse of $\hat{K}|_{V}$. A domain $D\subset V\cap \mathbb{C}^{+}$ can be chosen so that $\hat{K}(D)\subset \mathbb{C}^{+}$ and the matrix in the condition (ii) is positive semidefinite for every $z_{1}, \ldots, z_{n}\in D$. If $\zeta_{1}, \ldots, \zeta_{n}\in\hat{K}(D)$ and so $\zeta_{j}=z_{j}+\mathcal{R}(z_{i}^{-1})$ for some $z_{j}\in D$, then we have (\ref{ch03:eqn3.3.6}) again, and therefore the matrix
\begin{equation*}
\left[\frac{F(\zeta_{i})-F(\overline{\zeta}_{j})}{\zeta_{i}-\overline{\zeta}_{j}}\right]_{i,j}
\end{equation*}
is positive semidefinite. Thus $F$ extends to a Pick function by Theorem~\ref{ch03:the3.3.1}. Now $\mathrm{sst}\ G(z) :=1/F(z)$. Then, since $\mathcal{R}(\overline{z})=\overline{\mathcal{R}(z)}$ in a neighborhood of $0$, it follows that $G(\overline{z})=\overline{G(z)}$ in a neighborhood of $\infty$, so $G(z^{-1})\in \mathcal{P}(-\varepsilon, \varepsilon)$ for some $\varepsilon >0$. Hence by Corollary~\ref{ch03:cor3.3.2} there exist $\alpha\in \mathbb{R}$ and a positive finite measure $\mu$ on $\mathbb{R}$ with compact support such that
\begin{equation*}
G(z^{-1})=\alpha+\int_{-\infty}^{\infty}\frac{z}{1-xz}\,d\mu(x)\,,
\end{equation*}
that is,
\begin{equation*}
G(z)=\alpha+\int_{-\infty}^{\infty}\frac{d\mu(x)}{z-x}\,.
\end{equation*}
But, since $\lim_{z\rightarrow\infty}zG(z)=\lim_{z\rightarrow\infty}\hat{K}(z)/z=1$, we have $\alpha=0$ and $\mu\in \mathcal{M}_{0}(\mathbb{R})$, implying $G=G_{\mu}$. Since $G_{\mu}$ is the inverse of $\hat{K}(z^{-1})=z^{-1}+\mathcal{R}(z)$, Theorem~\ref{ch03:the3.2.1} shows that $\mathcal{R}=\mathcal{R}_{\mu}$.

Next, assume that $\mathcal{R}$ belongs to $\mathcal{P}(-\varepsilon, \varepsilon )$ for some $\varepsilon >0$. Since $\mathrm{Im}\,z>0$ implies $\mathrm{Im}\,\mathcal{R}(z^{-1})<0, -\mathcal{R}(z^{-1})$ is a Pick function. A neighborhood $U$ of $\infty$ can be chosen so that
\begin{equation*}
\left|\frac{\mathcal{R}(z_{1}^{-1})-\mathcal{R}(\overline{z}_{2}^{-1})}{z_{1}-\overline{z}_{2}}\right|=\frac{1}{|z_{1}z_{2}|}\left|\frac{\mathcal{R}(z_{1}^{-1})-\mathcal{R}(\overline{z}_{2}^{-1})}{z_{1}^{-1}-\overline{z}_{2}^{-1}}\right|<\frac{1}{2}
\end{equation*}
for all $z_{1}, z_{2}\in U$. Then for every $z_{1}, \ldots, z_{n}\in U\cap \mathbb{C}^{+}$, the matrix
\begin{equation*}
\left[\frac{z_{i}-\overline{z}_{j}}{z_{i}-\overline{z}_{j}+\mathcal{R}(z_{i}^{-1})-\mathcal{R}(\overline{z}_{j}^{-1})}\right]_{i,j}=\left[1+\sum_{k=1}^{\infty}
\left(-\frac{\mathcal{R}(z_{i}^{-1})-\mathcal{R}(\overline{z}_{j}^{-1})}{z_{i}-\overline{z}_{j}}\right)^{k}\right]_{i,j}
\end{equation*}
is positive semidefinite by Theorem~\ref{ch03:the3.3.1}. Here we used the famous Schur theorem that the Hadamard (or entrywise) product of positive semidefinite matrices is positive semidefinite. Hence $\mathcal{R}$ satisfies (ii).
\end{proof2}

\begin{lemma}\label{ch03:lem3.3.8}
Let $(\mu_{t})_{t\geq 0}$ be a $w^{*}$-continuous $\boxplus$-semigroup in $\mathcal{M}_{0}(\mathbb{R})$, and let $\phi(z)=\mathcal{R}_{\mu_{1}}(z)$. Then $\mathcal{R}_{\mu_{t}}(z)=t\phi(z)$ for all $t\geq 0$, and the supports of $\mu_{t}\ (0\leq t\leq T)$ are uniformly bounded for any $T>0$.
\end{lemma}

\begin{proof2}
If $r=m/n$ is rational, then Theorem~\ref{ch02:the2.4.5} gives $n\mathcal{R}_{\mu_{r}}(z)=\mathcal{R}_{\mu_{m}}(z)= m\phi(z)$, and so $\mathcal{R}_{\mu_{r}}(z)=r\phi(z)$. For any $t\geq0$ let $r_{n}$ be rational with $r_{n}\rightarrow t$. Since $\mathcal{R}_{\mu_{r_{n}}}(z) =r_{n}\phi(z)\rightarrow t\phi(z)$ uniformly in a neighborhood of $0$ and $\mu_{r_{n}}\rightarrow\mu_{t}$ in the $\mathrm{w}^{*}$-topology, Lemma~\ref{ch03:lem3.3.4} implies that $\mathcal{R}_{\mu_{t}}(z)=t\phi(z)$. Hence the first assertion is shown. The second is immediate from Lemma~\ref{ch03:lem3.3.4} again.
\end{proof2}

The next lemma is given in a slightly more general setting than we need here.

\begin{lemma}\label{ch03:lem3.3.9}
Let $(\mu_{t})_{t\geq 0}$ be a $w^{*}$-continuous $\boxplus$-semigroup in $\mathcal{M}_{0}(\mathbb{R}),\ \nu\, \in\, \mathcal{M}_{0}(\mathbb{R}),\, G(t, z) :=G_{\nu \boxplus \mu_{t}}(z)$ and $\phi(z) :=\mathcal{R}_{\mu_{1}}(z)$. Then for each $T>0$ there exists a neighborhood $V$ of $\infty$ such that the equality
\begin{equation}
\frac{\partial G}{\partial t}(t, z)+\phi(G(t, z))\frac{\partial G}{\partial z}(t, z)=0
\label{ch03:eqn3.3.7}
\end{equation}
holds for all $0\leq t\leq T$ and $z\in V$.
\end{lemma}

\begin{proof2}
Define $\psi(z):=\mathcal{R}_{\nu}(z)$ and $K(t, z):=K_{\nu \boxplus\mu_{t}}(z)$. Then $\mathcal{R}_{\nu \boxplus\mu_{t}}(z)=\psi(z)+ t\phi(z)$ by Lemma~\ref{ch03:lem3.3.8}, and hence $K(t, z)=z^{-1}+\psi(z)+t\phi(z)$. For any $T>0$, as in the proof (ii)$\,\Rightarrow\,$(i) of Lemma~\ref{ch03:lem3.3.4}, there exists $\varepsilon_{T}>0$ such that $K(t, z) (0\leq t\leq T)$ are univalent in $U:=\{|z|<\varepsilon_{T}\}$ and
\begin{equation}
\bigcap_{0\leq t\leq T}K(t, U)\supset V :=\{|z|>2/\varepsilon_{T}\}.
\label{ch03:eqn3.3.8}
\end{equation}
Since $G(t, \cdot)$ is the inverse of $K(t, \cdot)|_{U}$ for each $0\leq t\leq T$, it follows that $G(t, z)$ is differentiable in $t\in[0, T]$ for every $z\in K(t, U)$, and
\begin{equation*}
G(t, K(t, \zeta))=\zeta \qquad (0\leq t\leq T,\, \zeta\in U).
\end{equation*}
Differentiating the above with respect to $t$ yields
\begin{equation*}
\frac{\partial G}{\partial t}(t, K(t, \zeta))+\frac{\partial G}{\partial z}(t, K(t, \zeta))\phi(\zeta)=0\qquad (0\leq t\leq T,\, \zeta\in U).
\end{equation*}
For every $0\leq t\leq T$ and $z\in V$, since $z=K(t,\zeta)$ and $\zeta=G(t, z)$ for some $\zeta\in U$ by (\ref{ch03:eqn3.3.8}), we get
\begin{equation*}
\frac{\partial G}{\partial t}(t, z)+\frac{\partial G}{\partial z}(t, z)\phi(G(t, z))=0 \qquad (0\leq t\leq T,\,z\in V).
\end{equation*}
\end{proof2}

\begin{proof2}[Proof of Theorem~\ref{ch03:the3.3.6}]
(ii)$\,\Rightarrow\,$(i) is obvious, and (iii)$\,\Leftrightarrow\,$(iv) was given in Corollary~\ref{ch03:cor3.3.2}. In the following let $\phi :=\mathcal{R}_{\mu}$.

(iii)$\,\Rightarrow\,$(ii). If (iii) is satisfied, then Lemma~\ref{ch03:lem3.3.7} implies that for any $t>0$ there exists $\mu_{t}\in \mathcal{M}_{0}(\mathbb{R})$ such that $t\phi=\mathcal{R}_{\mu_{t}}$. Since $\mathcal{R}_{\mu_{s}\boxplus \mu_{t}}=(s+t)\phi=\mathcal{R}_{\mu_{s+t}}$ for $s, t>0$, it follows that $(\mu_{t})_{t\geq 0}$ with $\mu_{0}=\delta(0)$ is a $\boxplus$-semigroup, whose $\mathrm{w}^{*}$-continuity is guaranteed by Lemma~\ref{ch03:lem3.3.4}.

(i)$\,\Rightarrow\,$(ii). Let $r=m/n$ be rational. By assumption there exists $\mu_{1/n}\in \mathcal{M}_{0}(\mathbb{R})$ such that $\phi=n\mathcal{R}_{\mu_{1/n}}$. Define
\begin{equation*}
\mu_{r}:=\mu_{1/n} \boxplus \cdots \boxplus \mu_{1/n} \quad (m\ \mathrm{times}).
\end{equation*}
Then $\mathcal{R}_{\mu_{r}}=m\mathcal{R}_{\mu_{1/n}}=r\phi$. For any $t>0$, taking rationals $r_{n}$ with $r_{n}\rightarrow t$ and using Lemma~\ref{ch03:lem3.3.4}, one can obtain $\mu_{t}\in \mathcal{M}_{0}(\mathbb{R})$ such that $t\phi=\mathcal{R}_{\mu_{t}}$. Then (ii) is seen as in the proof of (iii)$\,\Rightarrow\,$(ii).

(ii)$\,\Rightarrow\,$(iv). Let $G(t, z) :=G_{\mu_{t}}(z)$. Then Lemma~\ref{ch03:lem3.3.9} implies that
\begin{equation*}
\frac{\partial G}{\partial t}(0, z^{-1})+\phi(G(0, z^{-1}))\frac{\partial G}{\partial z}(0, z^{-1})=0
\end{equation*}
in a neighborhood of $0$. Since $G(0, z)=z^{-1}$ and $(\partial G/\partial z)(0, z) =-z^{-2}$, the above equation says that $(\partial G/\partial t)(0, z^{-1})-z^{2}\phi(z)=0$, and we get
\begin{align*}
\phi(z) & \ = \ z^{-2}\lim_{\varepsilon\rightarrow+0}\frac{1}{\varepsilon}(G(\varepsilon , z^{-1})-G(0, z^{-1}))\\
& \ = \ z^{-2}\lim_{\varepsilon\rightarrow+0}\frac{1}{\varepsilon}
\left(\int_{-\infty}^{\infty}\frac{z}{1-xz}\,d\mu_{\varepsilon}(x)-z\right)\\
& \ = \ \lim_{\varepsilon\rightarrow+0}\frac{1}{\varepsilon}\int_{-\infty}^{\infty}\frac{x}{1-xz}\,d\mu_{\varepsilon}(x).
\end{align*}
Hence $\phi(0)=\lim_{\varepsilon\rightarrow+0}m_{1}(\mu_{\varepsilon})/\varepsilon$ exists. Since $x/(1-xz)=x+x^{2}z/(1-xz)$, we have
\begin{equation}
\phi(z)=\alpha+\lim_{\varepsilon\rightarrow+0}\int_{-\infty}^{\infty}\frac{z}{1-xz}\,d\nu_{\varepsilon}(x)\,,
\label{ch03:eqn3.3.9}
\end{equation}
where $\alpha :=\phi(0)$ and $d\nu_{\varepsilon}:=(x^{2}/\varepsilon)d\mu_{\varepsilon}$. By Lemma~\ref{ch03:lem3.3.8} the supports of $\nu_{\varepsilon}\ (0< \varepsilon \leq 1)$ are uniformly bounded. Since (\ref{ch03:eqn3.3.9}) gives
\begin{equation*}
\frac{\phi(z)-\phi(0)}{z}=\lim_{\varepsilon\rightarrow+0}\int_{-\infty}^{\infty}\frac{d\nu_{\varepsilon}(x)}{1-xz}\,,
\end{equation*}
the total variations $\nu_{\varepsilon}(\mathbb{R})$ are bounded as $\varepsilon \rightarrow+0$. So one can choose $\varepsilon_{n}\rightarrow+0$ such that $(\nu_{\varepsilon_{n}})$ converges in the $\mathrm{w}^{*}$-topology to some positive finite measure $\nu$ on $\mathbb{R}$ with compact support, for which
\begin{equation*}
\phi(z)=\alpha+\int_{-\infty}^{\infty}\frac{z}{1-xz}\,d\nu(x)=\alpha+\sum_{k=0}^{\infty}\left(\int_{-\infty}^{\infty}x^{k}\,d\nu(x)\right)z^{k+1}
\end{equation*}
in a neighborhood of $0$. This shows that $\nu$ is determined by $\phi$ only. Hence $\nu$ is a unique $\mathrm{w}^{*}$-limit point of $\nu_{\varepsilon}$ as $\varepsilon \rightarrow+0$, so we obtain $\nu=\mathrm{w}^{*}\hbox{-}\lim_{\varepsilon\rightarrow+0}\nu_{\varepsilon}$ and (\ref{ch03:eqn3.3.5}).

For the second part of the theorem, since $\alpha=\mathcal{R}_{\mu}(0)$ and $\nu(\mathbb{R})=\mathcal{R}_{\mu}'(0)$ by (\ref{ch03:eqn3.3.5}), the first equality of (2) and (4) are immediately checked. On the other hand, (1), the second equality of (2), and (3) were shown in the course of the above proof (ii)$\,\Rightarrow\,$(iv).
\end{proof2}

In the setting of Lemma~\ref{ch03:lem3.3.9}, Theorem~\ref{ch03:the3.3.6} just proved says that $\phi(z)= \mathcal{R}_{\mu_{1}}(z)$ is a Pick function in $\mathcal{P}(-\varepsilon, \varepsilon)$ for some $\varepsilon >0$, and this implies that all terms in the left-hand side of (\ref{ch03:eqn3.3.7}) are analytic in $\mathbb{C}^{+}$. In this way we observe that the partial differential equation (\ref{ch03:eqn3.3.7}) actually holds for all $t\geq 0$ and $z\in \mathbb{C}^{+}$.

\begin{example}\label{ch03:exa3.3.10}
In Theorem~\ref{ch03:the3.3.6}, if $\nu=\gamma\delta(0)$ and so $\mathcal{R}_{\mu}(z)=\alpha+\gamma z$, then $\mu$ is the semicircle law $w_{\alpha,2\sqrt{\gamma}}$, which is a free analogue of the Gaussian law. On the other hand, if $\alpha=0$ and $\nu =\gamma\delta(t)\ (t\neq 0)$, then the corresponding $\mu\in \mathcal{M}_{0}(\mathbb{R})$ satisfies $\mathcal{R}_{\mu}(z)=\gamma z/(1-tz)$, which is regarded as a free analogue of the Poisson law. In fact, since $\mathcal{R}_{\mu}(z)=t\mathcal{R}_{\mu_{\lambda}}(tz)-\gamma/t$, where $\mu_{\lambda}$ is given in (\ref{ch03:eqn3.3.2}) with $\lambda=\gamma/t^{2}$, one can get the exact form of this $\mu$ as follows:
\begin{equation*}
\mu=\left\{\begin{array}{ll}
\dfrac{\sqrt{4\gamma-(x-t)^{2}}}{2\pi(tx+\gamma)}\chi(x)\,dx & \mathrm{if}\ \gamma\geq t^{2},\\
\left(1-\dfrac{\gamma}{t^{2}}\right)\negthickspace\delta(-\gamma/t)+\dfrac{\sqrt{4\gamma-(x-t)^{2}}}{2\pi(tx+\gamma)}\chi(x)\,dx & \mathrm{if}\ 0<\gamma<t^{2},
\end{array}\right.
\end{equation*}
where $\chi$ is the characteristic function of the interval $[t-2\sqrt{\gamma}, t+2\sqrt{\gamma}]$. Analogously to the classical case, Theorem~\ref{ch03:the3.3.6} says that any $\boxplus$-infinitely divisible law is a $\mathrm{w}^{*}$-limit of certain free convolutions of a Wigner law and a finite number of modified free Poisson laws.
\end{example}

The notion of free Poisson distributions\index{compound!free Poisson distribution} is generalized in the following way. For any $\rho\in \mathcal{M}_{0}(\mathbb{R})$ and $\lambda>0$, let
\begin{equation}
\mathcal{R}(z) :=\lambda\int\frac{x}{1-xz}\,d\rho(x)=\lambda\int x\,d\rho(x)+\lambda \int\frac{x^{2}z}{1-xz}\,d\rho(x).
\label{ch03:eqn3.3.10}
\end{equation}
From Corollary~\ref{ch03:cor3.3.2} and Lemma~\ref{ch03:lem3.3.7} it follows that $\mathcal{R}=\mathcal{R}_{\mu}$ for some $\mu\in \mathcal{M}_{0}(\mathbb{R})$. We call this measure $\mu$ a \textit{compound free Poisson distribution}\index{distribution!compound free Poisson} and denote it by $\pi_{\rho,\lambda}$. In fact, $\pi_{\rho,\lambda}$ is the free Poisson distribution $\mu_{\lambda}$ when $\rho=\delta(1)$. Note that $\mu=\pi_{\rho,\lambda}$ is $\boxplus$-infinitely divisible by Theorem~\ref{ch03:the3.3.6}, and its $R$-series is $R_{\mu}=\lambda M_{\rho}$, so that the $\boxplus$-semigroup $(\mu_{t})_{t\geq 0}$ in Theorem~\ref{ch03:the3.3.6} is given as $\mu_{t}=\pi_{\rho,\lambda t}\ (t>0)$.

The \textit{free Poisson limit theorem}\index{free!Poisson limit theorem} in Example~\ref{ch03:exa3.3.5} is extended as follows.

\begin{proposition}\label{ch03:pro3.3.11}
For any $\rho\in \mathcal{M}_{0}(\mathbb{R})$ and $\lambda >0$, define
\begin{equation*}
\mu^{(n)}:=\left(\left(1-\frac{\lambda}{n}\right)\negthickspace\delta(0)+\frac{\lambda}{n}\rho\right)^{\boxplus n}.
\end{equation*}
Then $(\mu^{(n)})$ has uniformly bounded supports, and converges in the $w^{*}$-topology to the compound free Poisson distribution $\pi_{\rho,\lambda}$.
\end{proposition}

\begin{proof2}
The Cauchy transform of $\nu_{n}:=(1-\lambda/n)\delta(0)+(\lambda/n)\rho$ is
\begin{align*}
G_{\nu_{n}}(z) & \ = \ \left(1-\frac{\lambda}{n}\right)\frac{1}{z}+\frac{\lambda}{n}G_{\rho}(z)\\
& \ = \ \left(1-\frac{\lambda}{n}\right)\frac{1}{z}+\frac{\lambda}{nz}\left(1+M_{\rho}\left(\frac{1}{z}\right)\right)\\
& \ = \ \frac{1}{z}+\frac{\lambda}{nz}M_{\rho}\left(\frac{1}{z}\right).
\end{align*}
Since $R_{\nu_{n}}(G_{\nu_{n}}(z))=M_{\nu_{n}}(z^{-1})$, we get
\begin{equation*}
R_{\nu_{n}}\left(z+\frac{\lambda}{n}zM_{\rho}(z)\right)=M_{\nu_{n}}(z)=\frac{\lambda}{n}M_{\rho}(z),
\end{equation*}
and hence
\begin{equation*}
R_{\mu^{(n)}}\left(z+\frac{1}{n}zR_{\mu}(z)\right)=R_{\mu}(z),
\end{equation*}
where $\mu:=\pi_{\rho,\lambda}$. From this we can easily see that the coefficients of $R_{\mu^{(n)}}(z)$ converge to the corresponding ones of $R_{\mu}(z)$ as $ n\rightarrow\infty$. Hence for each $k\in \mathbb{N}$ the $k$th moment of $\mu^{(n)}$ converges to that of $\mu$.

Now let $\alpha, \beta$ be the minimum and the maximum of $\mathrm{supp}\, \rho$, and set
\begin{equation*}
\mu_{\alpha}^{(n)}:=\left(\left(1-\frac{\lambda}{n}\right)\negthickspace\delta(0)+\frac{\lambda}{n}\delta(\alpha)\right)^{\boxplus n},
\end{equation*}
and similarly $\mu_{\beta}^{(n)}$ with $\beta$ in place of $\alpha$. Then it is clear that
\begin{equation*}
\min\,\mathrm{supp}\, \mu_{\alpha}^{(n)}\leq \min \mathrm{supp}\,\mu^{(n)}\leq \max\, \mathrm{supp}\, \mu^{(n)}\leq \max\,\mathrm{supp}\, \mu_{\beta}^{(n)}.
\end{equation*}
It is seen from Example~\ref{ch03:exa3.3.5} that the supports of the $\mu_{\alpha}^{(n)}$'s and $\mu_{\beta}^{(n)}$'s are uniformly bounded. Hence so are the supports of the $\mu^{(n)}$'s, and the result follows.
\end{proof2}

According to the form (\ref{ch03:eqn3.3.10}) we observe that a $\boxplus$-infinitely divisible $\mu\in \mathcal{M}_{0}(\mathbb{R})$ having the $\mathcal{R}$-transform (\ref{ch03:eqn3.3.5}) is a compound free Poisson distribution if and only if
\begin{equation*}
\int x^{-2}\,d\nu(x)<+\infty,\quad \alpha=\int x^{-1}\,d\nu(x) .
\end{equation*}
This says that the set of compound free Poisson distributions is rather small in the $\boxplus$-infinitely divisible laws; for example, a semicircle law is not a compound free Poisson distribution. Nevertheless, we have

\begin{proposition}\label{ch03:pro3.3.12}
The following conditions for $\mu\in \mathcal{M}_{0}(\mathbb{R})$ are equivalent:
\begin{enumerate}
\item[(i)] $\mu$ is $\boxplus$-infinitely divisible.

\item[(ii)] $\mu$ is the $w^{*}$-limit of a sequence of compound free Poisson distributions with uniformly bounded supports.
\end{enumerate}
\end{proposition}

\begin{proof2}
(ii)$\,\Rightarrow\,$(i) follows from the fact stated after Lemma~\ref{ch03:lem3.3.4}. To prove the converse, assume that $\mu$ has the $\mathcal{R}$-transform (\ref{ch03:eqn3.3.5}) and $(\mu_{t})_{t\geq 0}$ is as in Theorem~\ref{ch03:the3.3.6}. Since the supports of the $\mu_{t}\ (0\leq t\leq 1)$ are uniformly bounded by Lemma~\ref{ch03:lem3.3.8}, by (2) and (3) of Theorem~\ref{ch03:the3.3.6} we have
\begin{align*}
\mathcal{R}_{\mu}(z) & \ = \ \lim_{\varepsilon\rightarrow+0}\left(\frac{1}{\varepsilon}\int x\,d\mu_{\varepsilon}(x)+\frac{1}{\varepsilon}\int\frac{x^{2}}{1-xz}\,d\mu_{\varepsilon}(x)\right)\\
& \ = \ \lim_{\varepsilon\rightarrow+0}\frac{1}{\varepsilon}\int\frac{x}{1-xz}\,d\mu_{\varepsilon}(x)
\end{align*}
uniformly in a neighborhood of $z=0$. This implies by Lemma~\ref{ch03:lem3.3.4} that $\pi_{\mu_{\varepsilon},1/\varepsilon}$ converges to $\mu$ in the $\mathrm{w}^{*}$-topology, so (ii) follows.
\end{proof2}

For instance, let $(\rho_{j})$ be a sequence of symmetric measures in $\mathcal{M}_{0}(\mathbb{R})$ such that $\lambda_{j}^{-1} :=\int x^{2}\,d\rho_{j}(x)>0$ and $\mathrm{supp}\,\rho_{j}$ tends to $\{0\}$. Then we notice that $\mathcal{R}_{\pi_{\rho_{j},\lambda_{j}}}(z)\rightarrow z$ uniformly in a neighborhood of $0$, and hence $\pi_{\rho_{j},\lambda_{j}}$ converges in the $\mathrm{w}^{*}$-topology to $w_{2}$.

\textbf{Notes and Remarks.}Concerning the boundary values of the imaginary and real parts of the Cauchy tranform, as well as properties of the Hilbert transform, the interested reader may consult [\citen{bib110}], Chap. VI, or [\citen{bib183}], for example.

The relation between the Cauchy transform and the $R$-series given in Theorem~\ref{ch03:the3.2.1} was established in [\citen{bib197}], and the combinatorial proof is from [\citen{bib176}]. Another proof of Haagerup is from [\citen{bib96}]. In Voiculescu's papers the concept of $R$-series is different from ours, and it coincides with the $\mathcal{R}$-transform here. (Nica's work on the multivariate case supports very strongly the revised concept of $R$-series, used also in our discussions.)

It may be worth mentioning that the Pick-Nevanlinna theory of analytic functions plays a fundamental role also in theory of monotone operator functions (see [\citen{bib57}]). For details on the Pick-Nevanlinna theory, see [\citen{bib2}] and [\citen{bib57}]. The classical theory of infinitely divisible distributions is found in standard texts such as [\citen{bib58}] and [\citen{bib92}]. Theorem~\ref{ch03:the3.3.6} was shown in [\citen{bib197}] and [\citen{bib17}]. This was generalized to distributions with unbounded support in [\citen{bib119}] for the finite variance case and [\citen{bib18}] for the general case. In the unboundedly supported case, the transform $\varphi_{\mu}(z)=F_{\mu}^{-1}(z)-z$, with $F_{\mu}^{-1}$ being the right inverse of $F_{\mu}=1/G_{\mu}$ in some domain, is more convenient than $\mathcal{R}_{\mu}$.

The paper [\citen{bib119}] describes the reciprocal Cauchy transform and the Voiculescu transform of probability measures of finite variance, and also contains the exact form of the modified free Poisson distribution in Example~\ref{ch03:exa3.3.10}.

A fine analysis of the analytic aspects of the free central limit theorem was given in [\citen{bib19}]. Let $a_{1}, a_{2}, \ldots$ be a sequence of identically distributed noncommutative random variables in free relation, and let $\mu_{n}$ be the distribution measure of $(a_{1}+a_{2}+\cdots+a_{n})/\sqrt{n}$. Then it turns out that the support of $\mu_{n}$ is an interval $[a_{n}, b_{n}]$, and, for $n$ large, the density of $\mu_{n}$ is analytic and tends uniformly to the semicircle.

We did not treat in detail the multiplicative free convolution introduced in [\citen{bib198}] when the definition appeared in Sec.~\ref{ch02:sec2.4}. The papers [\citen{bib17}] and [\citen{bib18}] treated also the characterizations of infinitely divisible distributions on the circle $\mathbb{T}$ and on $\mathbb{R}_{+}$ with respect to the multiplicative free convolution.

Example~\ref{ch03:exa3.2.3} gives the distribution of $p+q-\mathbf{1}$ when $p, q$ are a free pair of projections having the expectation $\varphi(p)=\varphi(q)=\alpha$. The distributions of some linear combinations of a free family of projections were computed in [\citen{bib3}].

The notion of compound free Poisson distributions was introduced in [\citen{bib178}] in the framework of operator-valued (or amalgamated) free probability theory. Propositions~\ref{ch03:pro3.3.11} and \ref{ch03:pro3.3.12} are given there. For the classical compound Poisson distribution, see [\citen{bib76}]. The free Poisson distribution (or the Marchenko-Pastur distribution) in Example~\ref{ch03:exa3.3.5} as well as its compound generalization will be represented in the next chapter as the limiting eigenvalue distribution of certain random matrices.

The papers [\citen{bib172}], [\citen{bib40}] and [\citen{bib91}] are about free white noise.\index{free!white noise}

The differential equation (a complex Burger equation) in Lemma~\ref{ch03:lem3.3.9} (also the remark after the proof of Theorem~\ref{ch03:the3.3.6}) was found by Voiculescu. In the case of $\mu_{t}=w_{2\sqrt{t}}$ the equation is $\tfrac{\partial G}{\partial t}+G\frac{\partial G}{\partial z}=0$, which can be regarded as a free analogue of the heat equation. In the language of this chapter Pastur studied the free convolution $\mu \boxplus w_{r}$, and he called it the \textit{deformed Wigner law}.\index{deformed!Wigner law}\index{law!deformed Wigner} For example, a functional equation $G_{1}(z)=G_{0}(z-G_{1}(z))$ was deduced for the Cauchy transform $G_{1}$ of the free convolution, where $G_{0}$ denotes the Cauchy transform of $\mu$ (cf. (B.31) in the book [\citen{bib146}]). This functional equation is easily obtained following the proof of Lemma~\ref{ch03:lem3.3.9}. Let us start with the equation $K(0, z)=K(t, z)-t\phi(z)$, put $G(t,z)$ in place of $z$ and apply $G(0, z)$ to both sides of the equation. In this way we arrive at the equation $G(t, z)=G(0, z-t\phi(G(t, z)))$. For $t=1$ and $\phi(z)=z$ this is Pastur's relation.


\chapter{Random Matrices and Asymptotically Free Relation}
\label{ch04:chap04}

\noindent Random matrices have been a part of advanced multivariate statistical analysis since the end of the 1920's. When Eugene Wigner proposed in quantum mechanics to replace the selfadjoint Hamiltonian operator in an infinite dimensional Hilbert space by an ensemble of very large Hermitian matrices, and the statistics of experimentally measured energy levels of nuclei were explained in terms of the eigenvalues of random matrices, random matrix theory entered physics.

The space of $n\times n$ random matrices admits a natural linear functional $\tau_{n}$, defined as
\begin{equation*}
\tau_{n}(X)=\frac{1}{n}\sum_{i=1}^{n}E(X_{ii})
\end{equation*}
for an arbitrary random matrix $X$ (whenever the expectations exist). The random matrices whose entries have all moments form a noncommutative probability space with the tracial functional $\tau_{n}$. This is the space under study in this chapter, in particular in the limit as $n\rightarrow\infty$.

The classical Wigner theorem tells us that the mean eigenvalue density of random symmetric matrices tends to the semicircle law if the matrix size goes to infinity. This result says, in our language, that the distribution of random symmetric matrices converges to the semicircle law. The concept of convergence in distribution appears here. Not only the semicirle law admits a random matrix model; Wishart matrices converge to the Marchenko-Pastur distribution law. Beyond the above mentioned matrix ensembles, there exist further examples, and they all produce particular distributions as the asymptotic eigenvalue density; in particular, random unitaries play important roles. Furthermore, it is noteworthy that the eigenvalue densities of these random matrix ensembles are known to converge not only in expectation but in the almost sure sense for the empirical density. It will be the subject of the next chapter to show that the above mentioned matrix ensembles provide a rich ground for large deviation theorems.

The Wigner theorem is not the only contact point to the semicircle law; in Chap.~\ref{ch02:chap02} this law appeared in the free central limit theorem. Hence random matrices have to do with the free relation. The very aim of this chapter is to show that the really pure algebraic concept of free relation of noncommutative random variables can also be modeled by random matrix ensembles if the matrix size tends to infinity. If $U(n)$ and $V(n)$ are independent $n\times n$ random unitaries distributed according to the Haar measure, then they are free asymptotically; that is, the relations of freeness hold not for a finite $n$ but rather in the limit when $n$ goes to infinity. Another example is supplied by a combination of diagonal and symmetric (or selfadjoint) random matrices. If $D(n)$ is a sequence of $n\times n$ diagonal random matrices with the asymptotic spectral density $\mu$ and $T(n)$ is a sequence of independent symmetric Gaussian matrices whose asymptotic eigenvalue distribution is the semicircle law $w_{2}$, then the sequences $D(n)$ and $T(n)$ are asymptotically free and the asymptotic eigenvalue distribution of $D(n)+T(n)$ is the free convolution $\mu \boxplus w_{2}$. This fact, recognized by Voiculescu, explains why the semicircle law shows up in two apparently different settings.

Voiculescu proved his asymptotic freeness results first for standard Gaussian matrices, and then he turned to standard unitary matrices together with constant matrices. His difficult proofs can be simplified by working directly with unitary matrices. This approach enables us to prove the results for rather general unitarily invariant random matrix models (together with constant matrices). Moreover, we discuss almost sure convergence, improving Voiculescu's original results.

It is worth emphasizing that the asymptotic freeness of symmetric Gaussian matrices is a far-reaching extension of the Wigner theorem. The latter tells about the limit of the moments of a single Gaussian matrix, and the former describes the limit of joint moments of several independent Gaussian matrices.

\section{Random matrices and their eigenvalues}
\label{ch04:sec4.1}

\noindent In this section we introduce the most frequently used Gaussian matrix ensembles, compute their eigenvalue densities, and study the asymptotics as the matrix size tends to infinity. The selfadjoint Gaussian matrix with independent entries provides a model for the semicircle law. The free Poisson distribution is the limiting eigenvalue distribution of the Wishart matrix. The method of moments and the use of the Fourier transform will be shown.

Although randon matrices were encountered in multivariate mathematical statistics in the 30's, intensive interest in the subject began with the work of Wigner in nuclear physics. Wigner's program was pursued by Porter and Rosenzweig, by Mehta and by Dyson. It seems that today the interest in random matrices is stronger in physics than in mathematics.

First of all, we need to fix some general notation. Let $M_{n}(\mathbb{C})$ be the space of $n\times n$ complex matrices. We denote by $\mathrm{tr}_{n}$ the normalized trace on $M_{n}(\mathbb{C})$ , and by Tr the usual (non-normalized) trace on matrices. We write $M_{n}(\mathbb{R})^{sa}$ for the space of $n\times n$ real symmetric matrices and $M_{n}(\mathbb{C})^{sa}$ for the space of $n\times n$ complex selfadjoint matrices.

Given a probability space $(\Omega, \mathbf{Prob})$, we shall treat random matrices $X$ whose entries $X_{ij}$ have all moments or belong to $\bigcap_{1<p<\infty}L^{p}(\Omega, \mathbf{Prob})$. The set of those $n\times n$ random matrices forms an algebra, which is actually a $^{*}$-algebra. It becomes a noncommutative probability space endowed with the tracial state
\begin{equation*}
\tau_{n}(X) :=\frac{1}{n}\sum_{i=1}^{n}E(X_{ii})=E(\mathrm{tr}_{n}(X)).
\end{equation*}
We shall mostly speak of random matrices without specifying the underlying probability space.

An $n\times n$ random matrix $X$ has $n$ (random) eigenvalues $\lambda_{1}(X),\,\lambda_{2}(X), \ldots, \lambda_{n}(X)$. The random atomic measure
\begin{equation*}
\frac{1}{n}(\delta(\lambda_{1}(X))+\delta(\lambda_{2}(X))+\cdots+\delta(\lambda_{n}(X)))
\end{equation*}
is called the \textit{empirical eigenvalue distribution}\index{eigenvalue distribution!empirical}\index{empirical!eigenvalue distribution} of $X$, and the expectation value
\begin{equation*}
\mu_{X}:=\frac{1}{n}E(\delta(\lambda_{1}(X))+\delta(\lambda_{2}(X))+\cdots+\delta(\lambda_{n}(X)))
\end{equation*}
is the \textit{mean eigenvalue distribution}\index{eigenvalue distribution!mean}\index{mean eigenvalue distribution} of $X$. The empirical eigenvalue distribution is a random probability measure on $\mathbb{C}$, and in probability theory such a measure is often regarded as a distribution on the space of probability measures.

In case of a symmetric real (or selfadjoint complex) $n\times n$ random matrix $T$, the real eigenvalues are usually ordered increasingly, $\lambda_{1}(T)\leq\lambda_{2}(T)\leq\ldots\leq\lambda_{n}(T)$. So $\lambda$ can be regarded as a mapping
\begin{equation*}
\lambda:T\mapsto(\lambda_{1}(T), \lambda_{2}(T), \ldots, \lambda_{n}(T))\in \mathbb{R}_{\leq}^{n},
\end{equation*}
where
\begin{equation*}
\mathbb{R}_{\leq}^{n}:=\{(x_{1}, x_{2}, \ldots, x_{n})\in \mathbb{R}^{n}:x_{1}\leq x_{2}\leq\ldots\leq x_{n}\}.
\end{equation*}
The moments of the mean eigenvalue distribution are identical to those of $T$ with respect to $\tau_{n}$; that is,
\begin{equation}
\int x^{m}\,d\mu_{T}(x)=\tau_{n}(T^{m})\qquad (m\in \mathbb{N}).
\label{ch04:eqn4.1.1}
\end{equation}

\begin{example}\label{ch04:exa4.1.1}
By an $n\times n$ \textit{standard symmetric Gaussian matrix}\index{standard!symmetric Gaussian matrix}\index{Gaussian matrix!standard symmetric, density of eigenvalues} we mean a symmetric matrix $T(n)=[T_{ij}(n)]$ such that $\{T_{ij}(n):1\leq i\leq j\leq n\}$ is a family of independent real Gaussian random variables and
\begin{equation*}
E(T_{ij}(n))=0,\quad E(T_{ij}(n)^{2})=\frac{1+\delta_{ij}}{n+1}.
\end{equation*}
The variances of the entries $T_{ij}(n)$ are chosen such a way that $\tau_{n}(T(n)^{2})=1$. With respect to the Lebesgue measure
\begin{equation}
dT:=\prod_{i\leq j}dT_{ij} \quad \mathrm{on}\quad M_{n}(\mathbb{R})^{sa}\cong \mathbb{R}^{n(n+1)/2},
\label{ch04:eqn4.1.2}
\end{equation}
the density of a standard symmetric Gaussian\index{Gaussian matrix!standard symmetric} is
\begin{equation}
p(T) :=C_{n}\exp\left(-\frac{n+1}{4} \mathrm{Tr}\,T^{2}\right),
\label{ch04:eqn4.1.3}
\end{equation}
where
\begin{equation}
C_{n}=\left(\frac{4\pi}{n+1}\right)^{-n/2}\left(\frac{2\pi}{n+1}\right)^{-n(n-1)/4}=2^{-n/2}\left(\frac{2\pi}{n+1}\right)^{-n(n+1)/4}.
\label{ch04:eqn4.1.4}
\end{equation}
The measure $p(T)\,dT$ is called the \textit{standard Gaussian measure} on the space of symmetric matrices. In the literature the name \textit{Gaussian orthogonal ensemble}\index{Gaussian orthogonal ensemble} (GOE)\index{GOE} is also used.
\end{example}

It is remarkable that this measure is invariant under the transformation $T\mapsto OTO^{t}$ if $O$ is an orthogonal matrix with transpose $O^{t}$. In fact, the compact orthogonal group $\mathcal{O}(n)$ has an action $T\mapsto OTO^{t}$ on the space $M_{n}(\mathbb{R})^{sa}$. The orthogonal invariance of the standard Gaussian measure follows from the invariance of the measure (\ref{ch04:eqn4.1.2}) and the similarity invariance of the trace.

As it was shown above, the standard Gaussian measure\index{standard!Gaussian measure} is invariant under the orthogonal transformation and the matrix elements are independent with respect to it. These important features almost characterize the standard Gaussian measure. Namely, any orthogonally invariant measure $q(T)\,dT$ possessing independence of entries is in the form
\begin{equation*}
q(T)=\exp(-a\mathrm{Tr}\,T^{2}+b\mathrm{Tr}\,T+c)
\end{equation*}
with certain constants $a, b, c\ (a, b, c\in \mathbb{R},\,a>0)$ (cf. [154]).

Let $\mu$ be a measure on the space of $n\times n$ real symmetric matrices such that it has a density. The measure $\mu$ is invariant under the the action of the orthogonal group $\mathcal{O}(n)$ if and only if the density may be expressed in terms of the eigenvalues:
\begin{equation*}
p(T_{11}, T_{12}, \ldots, T_{1n}, T_{22}, \ldots, T_{2n}, \ldots, T_{nn})=g(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}),
\end{equation*}
where $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$ is the sequence of increasingly ordered eigenvalues which belongs to $\mathbb{R}_{\leq}^{n}$. There exists a measure $\bar{\mu}$ on $\mathbb{R}_{\leq}^{n}$ whose inverse image under the mapping $\lambda$ is the original measure $\mu$. We want to find out the relation between the densities of $\mu$ and $\bar{\mu}$, and we consider first the Lebesgue measure (\ref{ch04:eqn4.1.2}).

\begin{lemma}\label{ch04:lem4.1.2}
The measure on $\mathbb{R}_{\leq}^{n}$ $($the space of eigenvalues$)$ induced from the measure \emph{(\ref{ch04:eqn4.1.2})} has the density
\begin{equation*}
\frac{\pi^{n(n+1)/4}}{\prod_{j=1}^{n}\Gamma(j/2)}\prod_{i<j}|\lambda_{i}-\lambda_{j}|.
\end{equation*}
\end{lemma}

\begin{proof2}
We recall the diagonalization $T=ODO^{t}$, where $D := \mathbf{Diag} (\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})$ is a diagonal matrix and $O$ is an orthogonal matrix. When $\lambda_{1}\,<\,\lambda_{2}\,<\,\ldots\,<\,\lambda_{n}$ (the other degenerate case is of measure $0$ and so it is negligible), $O$ is unique if we assume that its first row is nonnegative. Differentiate $T=ODO^{t}$ and write
\begin{align*}
dT & \ = \  dO\cdot DO^{t}+O\cdot dD\cdot O^{t}+OD\cdot dO^{t}\\
& \ = \ (dD+D\cdot dO^{t} \cdot O+O^{t}\cdot dO\cdot D)O^{t}\\
& \ = \ O(dD+D\cdot dM-dM\cdot D)O^{t},
\end{align*}
where we take an infinitesimal matrix $dM :=-O^{t}\cdot dO$ so that $O^{t}O=I$ gives $dM= dO^{t}\cdot O=-dM^{t}$ (i.e., $dM$ is skew-symmetric). From the orthogonal invariance of the measure (\ref{ch04:eqn4.1.2}) we have
\begin{equation*}
\prod_{i\leq j}dT_{ij}=\prod_{i\leq j}(dD+D\cdot dM-dM\cdot D)_{ij}=\prod_{i<j}|\lambda_{i}-\lambda_{j}|\prod_{i=1}^{n}d\lambda_{i}\prod_{i<j}dM_{ij}.
\end{equation*}
We may integrate this with respect to $\prod_{i<j}dM_{ij}$ to conclude that the induced measure on $\mathbb{R}_{\leq}^{n}$ is
\begin{equation*}
C_{n}'\prod_{i<j}|\lambda_{i}-\lambda_{j}|\prod_{i=1}^{n}d\lambda_{i}.
\end{equation*}
Consider Example~\ref{ch04:exa4.1.1} to determine $C_{n}'$. The density of the standard Gaussian measure in (\ref{ch04:eqn4.1.3}) is
\begin{equation*}
p(T)=C_{n}\exp\left(-\frac{n+1}{4}\sum_{i=1}^{n}\lambda_{i}^{2}\right).
\end{equation*}
Hence the following must hold:
\begin{equation*}
\frac{C_{n}C_{n}'}{n!}\int_{\mathbb{R}^{n}}\exp\left(-\frac{n+1}{4}\sum_{i=1}^{n}\lambda_{i}^{2}\right)\prod_{i<j}|\lambda_{i}-\lambda_{j}|\,d\lambda=1,
\end{equation*}
where a factor $n!$ arises because the integration is over the whole $\mathbb{R}^{n}$ instead of $\mathbb{R}_{\leq}^{n}$. Below we shall see how to evaluate such an integral, and the constant $C_{n}C_{n}'$ is obtained in this way. Since $C_{n}$ is known to us from (\ref{ch04:eqn4.1.4}), the proof may be concluded.
\end{proof2}

Here we interrupt the discussion on random matrices for a while and treat integrals of
\begin{equation}
\Delta(x)\equiv\Delta(x_{1}, x_{2}, \ldots, x_{n}):=\prod_{i<j}(x_{i}-x_{j}).
\label{ch04:eqn4.1.5}
\end{equation}
Such integrals appear often when we pass from a random matrix to its eigenvalue density as in the previous lemma. In 1967 Mehta conjectured that
\begin{equation*}
\int_{\mathbb{R}^{n}}e^{-\Vert x\Vert^{2}/2}|\Delta(x)|^{2\beta}\,dx=(2\pi)^{n/2}\prod_{j=1}^{n}\frac{\Gamma(1+j\beta)}{\Gamma(1+\beta)},
\end{equation*}
where $\beta$ is a complex number with $\mathrm{Re}\,\beta>0,\ dx\equiv dx_{1}dx_{2}\cdots dx_{n}$ and $\Vert x\Vert^{2} := \sum_{i=1}^{n}x_{i}^{2}$. It turned out that his conjecture follows from the \textit{Selberg integral}\index{Selberg integral} (see [\citen{bib121}], Sec. 17.1) obtained in 1944:
\begin{align}
& \int_{0}^{1}\cdots\int_{0}^{1}|\Delta(y)|^{2\beta}\prod_{i=1}^{n}y_{i}^{a-1}(1-y_{i})^{b-1}\,dy \notag\\
& \qquad \ =\prod_{j=0}^{n-1}\frac{\Gamma(1+(j+1)\beta)\Gamma(a+j\beta)\Gamma(b+j\beta)}{\Gamma(1+\beta)\Gamma(a+b+(n+j-1)\beta)},
\label{ch04:eqn4.1.6}
\end{align}
where $\mathrm{Re}\,a>0,\ \mathrm{Re}\,b>0$ and $\mathrm{Re}\,\beta>-\min\{1/n, \mathrm{Re}\,a/(n-1), \mathrm{Re}\,b/(n-1)\}$. Indeed, we replace $y_{i}$ by $1/2+x_{i}/2L$ and choose $a=b=\alpha L^{2}/2+1\ (\alpha>0)$ in (\ref{ch04:eqn4.1.6}) to get
\begin{align*}
&\int_{-L}^{L}\cdots\int_{-L}^{L}|\Delta(x)|^{2\beta}\prod_{i=1}^{n}\left(1-\frac{x_{i}^{2}}{L^{2}}\right)^{\alpha L^{2}/2}\,dx\\
& \quad =2^{n(\alpha L^{2}+(n-1)\beta+1)}L^{n((n-1)\beta+1)}\prod_{j=0}^{n-1}\frac{\Gamma(1+(j+1)\beta)\Gamma(1+\alpha L^{2}/2+j\beta)^{2}}{\Gamma(1+\beta)\Gamma(2+\alpha L^{2}+(n+j-1)\beta)}.
\end{align*}
The Stirling formula
\begin{equation*}
\frac{\Gamma(1+x)}{\sqrt{2\pi}\,x^{x+1/2}e^{-x}}\rightarrow 1 \quad \mathrm{as}\quad x\rightarrow\infty
\end{equation*}
implies that as $L\rightarrow\infty$
\begin{align*}
& \frac{\Gamma(1+\alpha L^{2}/2+j\beta)^{2}}{\Gamma(2+\alpha L^{2}+(n+j-1)\beta)}\\
& \quad \approx\frac{\sqrt{2\pi}(\alpha L^{2}/2+j\beta)^{\alpha L^{2}+2j\beta+1}e^{-2j\beta}}{(\alpha L^{2}+(n+j-1)\beta+1)^{\alpha L^{2}+(n+j-1)\beta+3/2}e^{-((n+j-1)\beta+1)}}\\
& \quad \approx\frac{\sqrt{2\pi}(\alpha L^{2}/2)^{\alpha L^{2}+2j\beta+1}}{(\alpha L^{2})^{\alpha L^{2}+(n+j-1)\beta+3/2}}=\sqrt{2\pi}\,2^{-(\alpha L^{2}+2j\beta+1)}(\alpha L^{2})^{-((n-j-1)\beta+1/2)}.
\end{align*}
Hence the limit as $L\rightarrow\infty$ yields
\begin{equation}
\int_{\mathbb{R}^{n}}e^{-\alpha\Vert x\Vert^{2}/2}|\Delta(x)|^{2\beta}dx=(2\pi)^{n/2}\alpha^{-n((n-1)\beta+1)/2}\prod_{j=1}^{n}\frac{\Gamma(1+j\beta)}{\Gamma(1+\beta)},
\label{ch04:eqn4.1.7}
\end{equation}
which is another form of the Selberg integral. In the case $\beta=1/2$ (or $\beta=1$) this formula is related to the real symmetric (or complex selfadjoint) Gaussian matrices. (General $\beta>0$ will appear in Chap.~\ref{ch05:chap05} in some large deviation theorems.)

Similarly, we replace $y_{i}$ by $x_{i}/L$ and choose $b=L+1$ in (\ref{ch04:eqn4.1.6}); then the limit as $L\rightarrow\infty$ yields
\begin{align}
& \int_{(\mathbb{R}^{+})^{n}}\prod_{i=1}^{n}x_{i}^{a-1}\exp\left(-\sum_{i=1}^{n}x_{i}\right)|\Delta(x)|^{2\beta}\,dx \notag\\
& \quad \ \ =\prod_{j=0}^{n-1}\frac{\Gamma(1+(j+1)\beta)\Gamma(a+j\beta)}{\Gamma(1+\beta)},
\label{ch04:eqn4.1.8}
\end{align}
whenever $\mathrm{Re}\,a>0$ and $\mathrm{Re}\,\beta>0$.

The next proposition follows directly from Lemma~\ref{ch04:lem4.1.2}.

\begin{proposition}\label{ch04:pro4.1.3}
If an $n\times n$ random symmetric matrix $T$ has a probability density\index{density of eigenvalues!standard symmetric Gaussian matrix} $g(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})$ with respect to $\prod_{i\leq j}dT_{ij}$ which is expressed in terms of the increasingly ordered eigenvalues $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}$, then the joint probability density function of the eigenvalues of $T$ on $\mathbb{R}_{\leq}^{n}$ is
\begin{equation*}
\frac{\pi^{n(n+1)/4}}{\prod_{j=1}^{n}\Gamma(j/2)}g(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})\prod_{i<j}|\lambda_{i}-\lambda_{j}|.
\end{equation*}
\end{proposition}

In particular, let $T(n)$ be the $n\times n$ standard symmetric Gaussian matrix. On the convex domain $\mathbb{R}_{\leq}^{n}\subset \mathbb{R}^{n}$ the \textit{density of the eigenvalues} of $T(n)$ is
\begin{equation*}
\frac{2^{-n(n+3)/4}(n+1)^{n(n+1)/4}}{\prod_{j=1}^{n}\Gamma(j/2)}\exp\left(-\frac{n+1}{4}\sum_{i=1}^{n}\lambda_{i}^{2}\right)\prod_{i<j}|\lambda_{i}-\lambda_{j}|.
\end{equation*}
If the density is taken on the whole $\mathbb{R}^{n}$, then it is
\begin{equation}
\frac{2^{-n(n+3)/4}(n+1)^{n(n+1)/4}}{n!\prod_{j=1}^{n}\Gamma(j/2)}\exp\left(-\frac{n+1}{4}\sum_{i=1}^{n}\lambda_{i}^{2}\right)\prod_{i<j}|\lambda_{i}-\lambda_{j}|.
\label{ch04:eqn4.1.9}
\end{equation}
Writing $p_{n}$ for the density (\ref{ch04:eqn4.1.9}), we have
\begin{align*}
\tau_{n}(T(n)^{m}) & \ = \ \frac{1}{n}\int_{\mathbb{R}^{n}}(x_{1}^{m}+x_{2}^{m}+\cdots+x_{n}^{m})p_{n}(x_{1},\ x_{2},\ \ldots,\ x_{n})\,dx\\
& \ = \ \int_{\mathbb{R}^{n}}x_{1}^{m}p_{n}(x_{1}, x_{2}, \ldots, x_{n})\,dx
\end{align*}
bacause of the symmetry. Now, comparing this with (\ref{ch04:eqn4.1.1}), we see that the probability density function of $\mu_{T(n)}$ is
\begin{equation*}
\sigma_{n}(t) :=\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}p_{n}(t, x_{2}, x_{3}, \ldots, x_{n})\,dx_{2}\,dx_{3}\cdots dx_{n}\,.
\end{equation*}

It is a form of the \textit{Wigner theorem}\index{Wigner theorem} that $\sigma_{n}(t)$ tends to the density of the semicircle law $w_{2}$ as $ n\rightarrow\infty$. In other words, the mean eigenvalue distribution of standard symmetric Gaussian matrices is the standard semicircle law asymptotically. We shall use the name Wigner theorem for more general statements as well when the limiting eigenvalue distribution of random matrices of independent entries becomes the semicircle law. The asymptotics of $\sigma_{n}(t)$ can be treated by very powerful methods from the theory of orthogonal polynomials; however, below we take another route and apply the method of moments.

\begin{theorem}\label{ch04:the4.1.4}
For $n\in \mathbb{N}$ let $\xi_{ij}(n)\ (1\leq i\leq j\leq n)$ be independent real-valued random variables with finite moments. Assume that $E(\xi_{ij}(n))=0$ and $E(\xi_{ij}(n)^{2})= 1$ for $1\leq i<j\leq 1$. If
\begin{equation*}
C_{k}(n):=\sup_{1\leq i\leq j\leq n}E(|\xi_{ij}(n)|^{k})=O(1) \quad as \quad n\rightarrow\infty \quad (k\in \mathbb{N}),
\end{equation*}
then the mean eigenvalue distribution of the random symmetric matrix $T(n)$ defined by $T_{ij}(n)=\xi_{ij}(n)/\sqrt{n}$ tends to the standard semicircle law $w_{2}$ as $ n\rightarrow\infty$.
\end{theorem}

\begin{proof2}
We put $\xi_{ij}$ in place of $\xi_{ij}(n)$ for the sake of simpler writing, and use the convention $\xi_{ji}=\xi_{ij}$. The $k$th moment of the mean eigenvalue distribution is given as
\begin{equation}
\tau_{n}(T(n)^{k})=\frac{1}{n^{k/2+1}}\sum E(\xi_{m_{1}m_{2}}\xi_{m_{2}m_{3}}\cdots\xi_{m_{k}m_{1}})\,,
\label{ch04:eqn4.1.10}
\end{equation}
where the summation is over all $1\leq m_{1}, m_{2}, \ldots, m_{k}\leq n$. We shall prove by a combinatorial argument that the limit of the $k$th moment of the mean eigenvalue distributions is the known $k$th moment of the semicircle law. We shall rely on the fact that the $k$th moment of the standard semicircle law equals the number of non-crossing pair partitions of $[k]$ (see Lemma 2.2.1).

We group the terms $E(\xi_{m_{1}m_{2}}\xi_{m_{2}m_{3}}\cdots\xi_{m_{k}m_{1}})$ according to the cardinality of the set $\{m_{1}, m_{2}, \ldots, m_{k}\}$. The number of terms such that $\#\{m_{1}, m_{2}, \ldots, m_{k}\}=l$ is less than
\begin{equation*}
\left(\begin{array}{l}
n\\
l\\
\end{array}\right) l^{k},
\end{equation*}
and due to the H\"{o}lder inequality
\begin{align}
&|E(\xi_{m_{1}m_{2}}(n)\cdots\xi_{m_{k}m_{1}}(n))| \notag\\
& \qquad \leq E(|\xi_{m_{1}m_{2}}(n)|^{k})^{1/k}\cdots E(|\xi_{m_{k}m_{1}}(n)|^{k})^{1/k}\leq C_{k}(n).
\label{ch04:eqn4.1.11}
\end{align}
Hence for the sum we have
\begin{equation*}
\frac{1}{n^{k/2+1}}\left|\sum_{(l)}E(\xi_{m_{1}m_{2}}\xi_{m_{2}m_{3}}\cdots\xi_{m_{k}m_{1}})\right|\leq\left(\begin{array}{l}
n\\
l\\
\end{array}\right) \frac{l^{k}C_{k}(n)}{n^{k/2+1}},
\end{equation*}
which goes to $0$ whenever $l<k/2+1$. On the other hand,
\begin{equation*}
E(\xi_{m_{1}m_{2}}\xi_{m_{2}m_{3}}\cdots\xi_{m_{k}m_{1}})=0
\end{equation*}
if $\#\{m_{1},\ m_{2}, \ldots, m_{k}\}>k/2+1$, because in this case there must be a factor $\xi_{m_{i}m_{i+1}}$ (with $m_{k+1} :=m_{1}$) such that $m_{i}\neq m_{i+1}$ which appears without repetition. This fact can be shown by induction on $k$. Indeed, there is a subscript $p$ such that $m_{p}$ is without repetition in the sequence $m_{1}, m_{2}, \ldots, m_{k}$. When $m_{p-1}\neq m_{p+1}$ either $\xi_{m_{p-1}m_{p}}$ or $\xi_{m_{p}m_{p+1}}$ is a desired factor, where the neighbors of $m_{p}$ is understood in the cyclic order (i.e. $\mathrm{mod}\,k$) if $p=1$ or $p=k$. When $m_{p-1}=m_{p+1}$ we may apply the induction assumption to a shorter sequence obtained by removing $m_{p-1}, m_{p}$ from $m_{1}, m_{2}, \ldots, m_{k}$. In particular, for an odd $k$ the limit of (\ref{ch04:eqn4.1.10}) is $0$. In the rest of our argument we consider even $k$ and replace $k$ by $2k$.

We have to show that
\begin{equation}
\frac{1}{n^{k+1}}\sum_{(a),(b)}E(\xi_{m_{1}m_{2}}\xi_{m_{2}m_{3}}\cdots\xi_{m_{2k}m_{1}})\rightarrow\frac{1}{k+1} \left(\begin{array}{c}
2k\\
k\\
\end{array}\right) \quad \mathrm{as}\quad n\rightarrow\infty,
\label{ch04:eqn4.1.12}
\end{equation}
when the summation is over all $1\leq m_{1}, m_{2}, \ldots,\ m_{2k}\leq n$ satisfying the following:
\begin{enumerate}
\item[(a)] $\#\{m_{1}, m_{2}, \ldots, m_{2k}\}=k+1$, and

\item[(b)] every element of the sequence
\begin{equation}
\{m_{1}, m_{2}\}, \{m_{2}, m_{3}\}, \ldots, \{m_{2k}, m_{1}\}
\label{ch04:eqn4.1.13}
\end{equation}
appears at least twice.
\end{enumerate}
In order to prove (\ref{ch04:eqn4.1.12}), we show that if the sequence $m_{1}, m_{2}, \ldots, m_{2k}$ satisfies (a) and (b), then $m_{i}\neq m_{i+1}$ for $1\leq i\leq 2k$ (with $m_{2k+1} :=m_{1}$), each (unordered) pair appears in (\ref{ch04:eqn4.1.13}) exactly twice, and the pair partition $\mathcal{V}$ of [2k] defined as $\{i, j\}\in \mathcal{V}\Leftrightarrow\{m_{i}, m_{i+1}\}=\{m_{j}, m_{j+1}\}$ is non-crossing. This can be done by induction on $k$. The case $k=1$ is obvious. Suppose that our statement is true for $k-1$. There is a subscript $p$ such that $m_{p}$ is without repetition in the sequence $m_{1}, m_{2}, \ldots, m_{2k}$. Then $m_{p-1}=m_{p+1}\neq m_{p}$ must hold. Removing $m_{p-1}, m_{p}$ from $m_{1}, m_{2}, \ldots, m_{2k}$, we get a shorter sequence $n_{1}, n_{2}, \ldots, n_{2k-2}$ which contains $k$ different elements. Then the induction assumption can be applied to this shorter sequence, and the pair partition combined by $\{p-1,p\}$ and the non-crossing one corresponding to the shorter sequence is again non-crossing. Hence the statement is true for $k$ as well. Conversely, for any non-crossing pair partition $\mathcal{V}$ of $\{1, 2, \ldots, 2k\}$ there are sequences $m_{1}, m_{2}, \ldots, m_{2k}$ which induce $\mathcal{V}$ as above, and the number of such sequences is $n(n-1)\cdots(n-k)$ for each $\mathcal{V}$. (Prove by induction.)

We thus have
\begin{equation*}
\frac{1}{n^{k+1}}\sum_{(a),(b)}E(\xi_{m_{1}m_{2}}\xi_{m_{2}m_{3}}\cdots\xi_{m_{2k}m_{1}})=\frac{1}{k+1} \left(\begin{array}{c}
2k\\
k\\
\end{array}\right) \frac{n(n-1)\cdots(n-k)}{n^{k+1}}\,,
\end{equation*}
which proves (\ref{ch04:eqn4.1.12}).
\end{proof2}

The following interpretation of Theorem~\ref{ch04:the4.1.4} is useful for us. For every $n\in \mathbb{N}$ the random matrix $T(n)$ is a noncommutative random variable with a certain distribution, which is formally a linear functional $\varphi_{n}$ on the polynomial algebra $\mathbb{C}\langle X\rangle$ (see Sec.~\ref{ch01:sec1.2}). If $n\rightarrow\infty$, then $\varphi_{n}$ goes to the semicircular distribution. In particular, the standard symmetric Gaussian matrices constitute a \textit{random matrix model}\index{random matrix!model} for the semicircular distribution.

An $n\times n$ matrix is a noncommutative random variable with respect to the normalized trace functional $\mathrm{tr}_{n}$. The sequence of standard symmetric Gaussian matrices forms a model for the semicircle law in the almost sure sense: For almost all realizations of the sequence of those matrices we have convergence in distribution to the semicircle law. This is the content of our next result.

\begin{theorem}\label{ch04:the4.1.5}
Let $T(n)=[\xi_{ij}(n)/\sqrt{n}]$ be as in the previous theorem, with the same assumption. Then the empirical eigenvalue distribution of $T(n)$ converges in distribution almost surely to the semicircle law $w_{2}$ as $n\rightarrow\infty$; that is,
\begin{equation*}
\lim_{n\rightarrow\infty}\mathrm{tr}_{n}(T(n)^{k})=\frac{1}{2\pi}\int_{-2}^{2}x^{k}\sqrt{4-x^{2}}\,dx
\end{equation*}
almost everywhere for every $k\in \mathbb{N}$.
\end{theorem}

\begin{proof2}
It is sufficient to show that
\begin{equation*}
E\left(\sum_{n=1}^{\infty}[\mathrm{tr}_{n}(T(n)^{k})-\tau_{n}(T(n)^{k})]^{2}\right)<+\infty
\end{equation*}
for $k\in \mathbb{N}$. Indeed, this condition implies that $\sum_{n=1}^{\infty}[\mathrm{tr}_{n}(T(n)^{k})-\tau_{n}(T(n)^{k})]^{2}$ is finite almost everywhere, and therefore $\mathrm{tr}_{n}(T(n)^{k})-\tau_{n}(T(n)^{k})$ converges to $0$ almost surely.

We write
\begin{align*}
& E([\mathrm{tr}_{n}(T(n)^{k})-\tau_{n}(T(n)^{k})]^{2})\\
& \quad \ \ =E([\mathrm{tr}_{n}(T(n)^{k})]^{2})-[\tau_{n}(T(n)^{k})]^{2}\\
& \quad \ \ =\frac{1}{n^{k+2}}\sum Q_{n}(m_{1},\ldots,m_{k};m_{k+1},\ldots,m_{2k}),
\end{align*}
where the summation is over all $1\leq m_{1}, \ldots, m_{k}, m_{k+1}, \ldots, m_{2k}\leq n$ and
\begin{align*}
&Q_{n}(m_{1}, \ldots, m_{k};m_{k+1}, \ldots, m_{2k})\\
& \qquad :=E(\xi_{m_{1}m_{2}}\xi_{m_{2}m_{3}}\cdots\xi_{m_{k}m_{1}}\xi_{m_{k+1}m_{k+2}}\xi_{m_{k+2}m_{k+3}}\cdots\xi_{m_{2k}m_{k+1}})\\
& \qquad \qquad -E(\xi_{m_{1}m_{2}}\xi_{m_{2}m_{3}}\cdots\xi_{m_{k}m_{1}})E(\xi_{m_{k+1}m_{k+2}}\xi_{m_{k+2}m_{k+3}}\cdots\xi_{m_{2k}m_{k+1}})
\end{align*}
($\xi_{ij}$ being short for $\xi_{ij}(n)$). As in (\ref{ch04:eqn4.1.11}) we get the estimate
\begin{equation}
|Q_{n}(m_{1}, \ldots, m_{k};m_{k+1}, \ldots, m_{2k})|\leq C_{2k}(n)+C_{k}(n)^{2}=O(1)
\label{ch04:eqn4.1.14}
\end{equation}
as $n \rightarrow\infty$.

In order that $Q_{n}(m_{1}, \ldots, m_{k};m_{k+1}, \ldots, m_{2k})$ be nonzero, it is necessary that every factor $\xi_{ij}\ (i\neq j)$ of
\begin{equation*}
\xi_{m_{1}m_{2}}\xi_{m_{2}m_{3}}\cdots \xi_{m_{k}m_{1}}\xi_{m_{k+1}m_{k+2}}\xi_{m_{k+2}m_{k+3}}\cdots \xi_{m_{2k}m_{k+1}}
\end{equation*}
must have an equal pair, and at least one of the first $k$ factors has its pair among the last $k$ factors. Indeed, if there is a $\xi_{ij}\ (i\neq j)$ which appears only once, then $E(\xi_{ij})=0$ is factored out from both terms, and each of the two terms vanishes. If none of the first $k$ factors $\xi_{ij}$ appears among the second $k$, then
\begin{align*}
&E(\xi_{m_{1}m_{2}}\cdots\xi_{m_{k}m_{1}}\xi_{m_{k+1}m_{k+2}}\cdots\xi_{m_{2k}m_{k+1}})\\
& \qquad =E(\xi_{m_{1}m_{2}}\cdots\xi_{m_{k}m_{1}})E(\xi_{m_{k+1}m_{k+2}}\cdots\xi_{m_{2k}m_{k+1}})
\end{align*}
due to the independence of the $\xi_{ij}$'s. Therefore, there are $p\in\{1, \ldots, k\}$ and $q\in\{k+1, \ldots, 2k\}$ such that $\{m_{p}, m_{p+1}\}=\{m_{q}, m_{q+1}\}$, where $m_{p+1}$ and $m_{q+1}$ are taken in the cyclic order of $(m_{1}, \ldots, m_{k})$ and $(m_{k+1}, \ldots, m_{2k})$, respectively.

Given a nonzero term, we can associate a partition $\mathcal{V}$ of $[2k],\,p\in\{1, \ldots, k\}, q\in\{k+1, \ldots, 2k\}$ and a mapping $\pi : [2k]\rightarrow\{\pm 1\}$ such that
\begin{enumerate}
\item[(1)] $m_{i}=m_{i+1}$ if $\{i\}$ is a singleton block of $\mathcal{V}$,

\item[(2)] $\{m_{i}, m_{i+1}\}=\{m_{j}, m_{j+1}\}$ if $i$ and $j$ belong to the same block of $\mathcal{V}$,

\item[(3)] $m_{i}\leq m_{i+1}$ if $\pi(i)=1$ and $m_{i}\geq m_{i+1}$ if $\pi(i)=-1$,

\item[(4)] $\{m_{p}, m_{p+1}\}=\{m_{q}, m_{q+1}\}$.
\end{enumerate}
For any quadruple $(\mathcal{V}, \pi;p, q)$ as above we denote by $\Xi_{n}(\mathcal{V}, \pi;p, q)$ the set of all $(m_{1}, \ldots, m_{2k})\in[n]^{2k}$ satisfying $(1)$--$(4)$.

What we want to show is
\begin{equation}
\# \Xi_{n}(\mathcal{V}, \pi;p, q)\leq n^{k}.
\label{ch04:eqn4.1.15}
\end{equation}
For this we may assume, in view of the cyclicity of $(m_{1}, \ldots, m_{k})$ and $(m_{k+1}, \ldots, m_{2k})$, that $p=1$ and $q=k+1$, so that $m_{1}=m_{k+1}$ and $m_{2}=m_{k+2}$. It is convenient to consider the graph $G$ with $2k-2$ vertices $1\ (=k+1), 2(=k+2), 3,\ldots, k, k+3, \ldots, 2k$ and $2k$ edges $[1, 2],\ldots, [k-1, k], [k, 1], [k+1, k+2], \ldots, [2k- 1,2k], [2k, k+1]$. It has a double edge joining the vertices 1, 2.

\begin{figure}
\includegraphics{chap04-vend-scan-01.eps}
\caption{Picture of the graph $G$.} \label{ch04:fig4.1}
\end{figure}

Let $\tilde{G}$ be the quotient graph obtained by the following procedure: If $\{i\}$ is a singleton block of $\mathcal{V}$, delete the edge $[i, i+1]$ and identify the vertices $i$ and $i+1$; if $i$ and $j$ belong to the same block of $\mathcal{V}$, identify the edges $[i, i+1]$ and $[j, j+1]$ with orientation preserved if $\pi(i)\pi(j)=1$ and with orientation reversed if $\pi(i)\pi(j)=-1$.

Let $l_{0}$ be the number of singleton blocks of $\mathcal{V}$ and $l_{1}$ the number of other blocks of $\mathcal{V}$. Then $l_{1}\leq(2k-l_{0})/2\leq k$, and the number of edges of $\tilde{G}$ is equal to $l_{1}$. If $l_{1}$ is less than $k$, then $\tilde{G}$ has at most $k$ vertices (because $\tilde{G}$ is connected). Otherwise, $l_{1}=k$ and $l_{0}=0$ (so $\mathcal{V}$ is a pair partition), and $\tilde{G}$ has a loop passing through the vertex 1. This implies again that $\tilde{G}$ still has at most $k$ vertices. In this way (\ref{ch04:eqn4.1.15}) is proven, because the freedom for choosing $(m_{1}, \ldots, m_{2k})$ from $\Xi_{n}(\mathcal{V}, \pi;p, q)$ subject to $(1)$--$(4)$ is equal to the number of vertices of $\tilde{G}$.

From (\ref{ch04:eqn4.1.14}) and (\ref{ch04:eqn4.1.15}) we have
\begin{align*}
&E([\mathrm{tr}_{n}(T(n)^{k})-\tau_{n}(T(n)^{k})]^{2})\\
& \qquad =O(n^{-k-2})\sum_{\mathcal{V},\pi,p,q}\#\Xi_{n}(\mathcal{V}, \pi;p, q)=O(n^{-2})\quad \mathrm{as}\quad  n\rightarrow\infty,
\end{align*}
implying that
\begin{equation*}
E\left(\sum_{n=1}^{\infty}[\mathrm{tr}_{n}(T(n)^{k})-\tau_{n}(T(n)^{k})]^{2}\right)<+\infty,
\end{equation*}
which was our goal.
\end{proof2}

Functional analysis prefers complex matrices to real ones. An $n\times n$ complex selfadjoint random matrix $H(n)$ is called a \textit{standard selfadjoint Gaussian matrix}\index{Gaussian matrix!standard selfadjoint}\index{standard!selfadjoint Gaussian matrix} if
\begin{enumerate}
\item[(i)] $\{\mathrm{Re}\,H_{ij}(n):1\leq i\leq j\leq n\}\cup\{\mathrm{Im}\,H_{ij}(n):1\leq i<j\leq n\}$ is an independent family of Gaussian random variables, and

\item[(ii)] $E(H_{ij}(n))=0$ for $1\leq i\leq j\leq n,\ E(H_{ii}(n)^{2})=1/n$ for $1\leq i\leq n$, and $E((\mathrm{Re}\, H_{ij}(n))^{2})=E((\mathrm{Im}\,H_{ij}(n))^{2})=1/2n$ for $1\leq i<j\leq n$.
\end{enumerate}
We use the word ``standard'' because $\tau_{n}(H(n))=0$ and $\tau_{n}(H(n)^{2})=1$. The variance of the entries is chosen in such a way that the distribution of $H(n)$ on $M_{n}(\mathbb{C})^{sa}$ is invariant under the unitary conjugation $A\mapsto UAU^{*}$ for $U\in \mathcal{U}(n)$, where $\mathcal{U}(n)$ is the unitary group. This is clear also from the density function
\begin{equation}
C_{n}\exp\left(-\frac{n}{2}\mathrm{Tr}\,A^{2}\right) \quad \mathrm{with}\quad C_{n}=2^{-n/2}\left(\frac{\pi}{n}\right)^{-n^{2}/2}
\label{ch04:eqn4.1.16}
\end{equation}
with respect to the Lebesgue measure
\begin{equation}
dA:=\prod_{i=1}^{n}dA_{ii}\prod_{i<j}d(\mathrm{Re}\,A_{ij})\,d(\mathrm{Im}\,A_{ij}) \quad \mathrm{on}\quad M_{n}(\mathbb{C})^{sa}\cong \mathbb{R}^{n^{2}}.
\label{ch04:eqn4.1.17}
\end{equation}
The unitary invarince of the distribution is the reason for the nomenclature \textit{Gaussian unitary ensemble}\index{Gaussian unitary ensemble} (GUE).\index{GUE}

An analogue of Proposition~\ref{ch04:pro4.1.3} holds for complex selfadjoint random matrices. The next lemma is enough to show it. The proof is similar to that of Lemma~\ref{ch04:lem4.1.2} when we take the diagonalization $A=UDU^{*}\ (U\in \mathcal{U}(n))$ and examine the density (\ref{ch04:eqn4.1.16}) making use of the Selberg integral (\ref{ch04:eqn4.1.7}).

\begin{lemma}\label{ch04:lem4.1.6}
The measure on $\mathbb{R}_{\leq}^{n}$ $($the space of eigenvalues$)$ induced from the measure \emph{(\ref{ch04:eqn4.1.17})} has the density
\begin{equation*}
\frac{\pi^{n(n-1)/2}}{\prod_{j=1}^{n-1}j!}\prod_{i<j}(\lambda_{i}-\lambda_{j})^{2}.
\end{equation*}
If the density is taken on the whole $\mathbb{R}^{n}$, then $\prod_{j=1}^{n-1}j!$ in the above is replaced by $\prod_{j=1}^{n}j!$.
\end{lemma}

In particular, when $H(n)$ is the standard selfadjoint Gaussian matrix,\index{density of eigenvalues!standard selfadjoint Gaussian matrix} the joint probability \textit{density of the eigenvalues} of $H(n)$ on $\mathbb{R}^{n}$ is
\begin{equation}
\frac{(2\pi)^{-n/2}n^{n^{2}/2}}{\prod_{j=1}^{n}j!}\exp\left(-\frac{n}{2}\sum_{i=1}^{n}\lambda_{i}^{2}\right)\prod_{i<j}(\lambda_{i}-\lambda_{j})^{2}. \label{ch04:eqn4.1.18}
\end{equation}

The standard selfadjoint Gaussian matrices provide another random matrix model for the semicircular distribution; in other words, the mean eigenvalue distribution goes to $w_{2}$ as the matrix size goes to infinity. First we present an analytic proof for this fact, and then the combinatorial background will be discussed.

We consider an $n\times n$ random matrix $H_{0}+H(n)$, where $H_{0}$ is a fixed selfadjoint matrix and $H(n)$ is a standard selfadjoint Gaussian matrix. The density of $H_{0}+ H(n)$ with respect to the measure (\ref{ch04:eqn4.1.17}) is
\begin{equation*}
C_{H_{0}}\exp\left(-\frac{n}{2}\mathrm{Tr}\,A^{2}+n\mathrm{Tr}\,H_{0}A\right).
\end{equation*}
In the following computation, assume that $H_{0}$ has the eigenvalues $\delta_{1}>\delta_{2}>\ldots> \delta_{n}$. The Fourier transform of the mean eigenvalue distribution is
\begin{equation*}
F(t) :=\frac{C_{H_{0}}}{n}\int \mathrm{Tr}\,e^{\mathrm{i}\,tA}\exp\left(-\frac{n}{2} \mathrm{Tr}\,A^{2}+n\mathrm{Tr}\,H_{0}A\right)dA.
\end{equation*}
Since the measure is unitarily invariant, we can first integrate with respect to the Haar probability $dU$ over $\mathcal{U}(n)$:
\begin{equation*}
F(t)=\frac{C_{H_{0}}}{n}\int \mathrm{Tr}\, e^{\mathrm{i}\,tA}\exp\left(-\frac{n}{2}\mathrm{Tr}\,A^{2}\right)\left(\int\exp (n\mathrm{Tr}\,U\,AU^{*}H_{0})\,dU\right)dA.
\end{equation*}
In this way we are in a position to use an integral formula (see [\citen{bib121}], A.5 and also [\citen{bib21}], Theorem 7.24 for a more general formula attributed to Harish-Chandra):
\begin{equation*}
\int\exp(\mathrm{Tr}\,U\,AU^{*}B)\,dU=\frac{\det[\exp(\lambda_{i}\rho_{j})]}{\Delta(\lambda)\Delta(\rho)},
\end{equation*}
where $A, B$ are $n\times n$ selfadjoint matrices, the $\lambda_{i}$'s are the eigenvalues of $A$, the $\rho_{j}$'s are those of $B$, and $\Delta(\lambda)\equiv\Delta(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})$ is in (\ref{ch04:eqn4.1.5}). So by Lemma~\ref{ch04:lem4.1.6},
\begin{align*}
F(t) & \ = \ \frac{C_{H_{0}}}{n}\int \mathrm{Tr}\,e^{\mathrm{i}\,tA}\exp\left(-\frac{n}{2} \mathrm{Tr}\,A^{2}\right)\frac{\det[\exp(n\lambda_{i}\delta_{j})]}{\Delta(\lambda)\Delta(\delta)}\,dA\\
& \ = \ \frac{C_{H_{0}}'}{n}\int\left(\sum_{j}e^{\mathrm{i}\,t\lambda_{j}}\right)\exp\left(-\frac{n}{2}\sum_{i}\lambda_{i}^{2}\right)\Delta(\lambda)^{2}\frac{\det[\exp(n\lambda_{i}\delta_{j})]}
{\Delta(\lambda)\Delta(\delta)}\,d\lambda\\
& \ = \ \frac{C_{H_{0}}'}{n\Delta(\delta)}\int\left(\sum_{j}e^{\mathrm{i}\,t\lambda_{j}}\right)\exp\left(-\frac{n}{2}\sum_{i}\lambda_{i}^{2}\right)
\Delta(\lambda)\det[\exp(n\lambda_{i}\delta_{j})]\,d\lambda,
\end{align*}
where the latter integrals are over $\mathbb{R}_{\leq}^{n}$. Integration over $\mathbb{R}^{n}$ gives a factor $n!$. We expand the determinant by summing over all permutations $\sigma$ of $[n]$:
\begin{equation*}
\frac{C_{H_{0}}'}{n!\,n\Delta(\delta)}\sum_{\sigma}\int\left(\sum_{j}e^{\mathrm{i}\,t\lambda_{j}}\right)\exp\left(-\frac{n}{2}\sum_{i}\lambda_{i}^{2}\right)
\Delta(\lambda)(-1)^{|\sigma|}\prod_{i}\exp(n\lambda_{i}\delta_{\sigma(i)})\,d\lambda,
\end{equation*}
and all summands are the same, due to the fact that $\Delta$ changes sign when two of its arguments are exchanged. Hence we arrive at
\begin{equation*}
F(t)=\frac{C_{H_{0}}'}{n\Delta(\delta)}\int\left(\sum_{j}e^{\mathrm{i}\,t\lambda_{j}}\right)\exp\left(-\frac{n}{2}\sum_{i}\lambda_{i}^{2}+n\sum_{i}\lambda_{i}\delta_{i}\right)
\Delta(\lambda)\,d\lambda.
\end{equation*}
Now we proceed by means of the integral
\begin{equation*}
\int\exp\left(-\frac{n}{2}\sum_{i}(\lambda_{i}-a_{i})^{2}\right)\Delta(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})\,d\lambda =C(n)\Delta(a_{1}, a_{2}, \ldots, a_{n})
\end{equation*}
and conclude that
\begin{align*}
F(t) & \ = \ \frac{C_{H_{0}}''}{n}\exp\left(\frac{n}{2}\sum_{i}\delta_{i}^{2}\right)\exp\left(-\frac{t^{2}}{2n}\right)\\
& \quad \ \qquad \times\sum_{j}e^{\mathrm{i}\,\delta_{j}t}\frac{\Delta(\delta_{1},\ldots,\delta_{j-1},\delta_{j}+\mathrm{i}\,t/n,\delta_{j+1},\ldots,\delta_{n})}{\Delta(\delta_{1},\delta_{2},\ldots,\delta_{n})}\\
& \ = \ \frac{1}{n}\exp\left(-\frac{t^{2}}{2n}\right)\sum_{j}e^{\mathrm{i}\,\delta_{j}t}\prod_{i\neq j}\frac{\delta_{i}-\delta_{j}-\mathrm{i}\,t/n}{\delta_{i}-\delta_{j}},
\end{align*}
using the normalization $F(0)=1$. Since
\begin{equation*}
\prod_{k}\frac{z-\delta_{k}+\mathrm{i}\,t/n}{z-\delta_{k}}=1+\frac{\mathrm{i}\,t}{n}\sum_{j}\frac{1}{z-\delta_{j}}\prod_{i\neq j}\frac{\delta_{i}-\delta_{j}-\mathrm{i}\,t/n}{\delta_{i}-\delta_{j}},
\end{equation*}
the above sum can be represented by a contour integral on the complex plane:
\begin{equation*}
F(t)=\frac{1}{\mathrm{i\,}t}\exp\left(-\frac{t^{2}}{2n}\right)\oint\frac{dz}{2\pi \mathrm{i}}e^{\mathrm{i}\,tz}\prod_{k}\frac{z-\delta_{k}+\mathrm{i}\,t/n}{z-\delta_{k}}
\end{equation*}
if the contour encloses all the eigenvalues $\delta_{i}$. In this representation we can easily take the limit as $\delta_{i}\rightarrow 0$, and get
\begin{equation*}
F_{0}(t)=\frac{1}{\mathrm{i}\,t}\exp\left(-\frac{t^{2}}{2n}\right)\oint\frac{dz}{2\pi \mathrm{i}}e^{\mathrm{i}\,tz}\left(1+\frac{\mathrm{i}\,t}{nz}\right)^{n}.
\end{equation*}
This integral is evaluated by means of the residue theorem, and the limit as $n\rightarrow\infty$ is
\begin{equation*}
\lim_{n\rightarrow\infty}\exp\left(-\frac{t^{2}}{2n}\right)\sum_{k=0}^{n-1} \left(\begin{array}{c}
n\\
k+1\\
\end{array}\right) \frac{(\mathrm{i}\,t)^{2k}}{k!\,n^{k+1}}=\sum_{k=0}^{\infty}\frac{1}{k!\,(k+1)!}(\mathrm{i}\,t)^{2k}.
\end{equation*}
Since the moments of the semicircle law are known to us (see Lemma~\ref{ch02:lem2.3.1}), we recognize that this is the Fourier transform of the semicircle law, which must be the limiting measure. In this way we have shown that the limit distribution of the standard selfadjoint Gaussian matrices is $w_{2}$. An explicit combinatorial formula for the moments of the standard selfadjoint Gaussian matrix\index{moment!of standard selfadjoint Gaussian matrix} leads to the same conclusion.

\textit{Non-crossing pair partitions}\index{non-crossing!pair partition}\index{partition!non-crossing pair} of the set $\{1,2,\ldots,2k\}$ are in one-to-one correspondence with those permutations in the \textit{symmetric group}\index{group!symmetric}\index{symmetric group} $\mathbf{S}_{2k}$ which are of order two and do not have any fixed points. Let $\Gamma_{k}$ be the set of all those permutations. For $\gamma\in\Gamma_{k}$ one defines an equivalence relation on $\{1,2,\ldots,2k\}$ by saying that $j\sim_{\gamma}\gamma(j)+1$, where addition is $\mathrm{mod}\,2k$. Let $d(\gamma)$ stand for the number of equivalence classes of $\sim_{\gamma}$. Voiculescu showed that $d(\gamma)\leq k+1$, and Shlyakhtenko proved that $d(\gamma)=k+1$ if and only if the partition corresponding to $\gamma$ is non-crossing. (Both facts are easily proven by induction.) Now we are ready to state a formula for the \textit{moments of the standard selfadjoint Gaussian matrix} $H(n)$:
\begin{equation}
\tau_{n}(H(n)^{2k})=\sum_{\gamma\in\Gamma_{k}}n^{d(\gamma)-k-1}
\label{ch04:eqn4.1.19}
\end{equation}
If $n\rightarrow\infty$, then the limit is the number of non-crossing pair partitions.

The next theorem is a stronger result for more general selfadjoint random matrices.

\begin{theorem}\label{ch04:the4.1.7}
For $n\in \mathbb{N}$ let $[\xi_{ij}(n)]$ be a selfadjoint random matrix such that the $\xi_{ij}(n)$ are independent for $1\leq i\leq j\leq n$. Assume that $E(\xi_{ij}(n))=0$ and $E(|\xi_{ij}(n)|^{2})=1$ for $1\leq i<j\leq n$. Assume further that
\begin{equation*}
C_{k}(n):=\sup_{1\leq i\leq j\leq n}E(|\xi_{ij}(n)|^{k})=O(1) \quad \mathrm{as}\quad n\rightarrow\infty \qquad (k\in \mathbb{N}).
\end{equation*}
Then the distribution of the random matrix $H(n)$ defined by $H_{ij}(n) :=\xi_{ij}(n)/\sqrt{n}$ tends to the standard semicircle law $w_{2}$ with respect to the functional $\tau_{n}$ as $ n\rightarrow\infty$. Moreover, the convergence in distribution holds also with respect to the functional $\mathrm{tr}_{n}$ almost everywhere; that is, the empirical eigenvalue distribution of $H(n)$ converges in distribution almost surely to $w_{2}$.
\end{theorem}

The proof of the first assertion of this theorem may follow the lines of the combinatorial proof of Theorem~\ref{ch04:the4.1.4}. When the $\xi_{ij}$ are complex valued, the argument in the case where $\#\{m_{1}, m_{2}, \ldots, m_{k}\}<k/2+1$ or $>k/2+1$ is the same. But (\ref{ch04:eqn4.1.12}) is to be shown a bit more carefully. The summation is over $(a)$ and $(b)$, which excludes the possibility of diagonal entries. It turned out that the property $(b)$ holds true in the stronger form that every unordered pair $\{m_{1}, m_{2}\}, \{m_{2}, m_{3}\}, \ldots, \{m_{2k}, m_{1}\}$ appears exactly twice. However, the fact that each ordered pair $(m_{1}, m_{2}), (m_{2}, m_{3}), \ldots, (m_{2k}, m_{1})$ shows up exactly once comes also from an induction argument. Hence any factor $\xi_{m_{i}m_{i+1}}$ may be coupled to another factor $\xi_{m_{i+1}m_{i}}$. Since the random matrix $H(n)$ is selfadjoint with independent entries, the proof still works. Furthermore, in the same way as in the proof of Theorem~\ref{ch04:the4.1.5} one can show that
\begin{equation*}
E(|\mathrm{tr}_{n}(H(n)^{k})-\tau_{n}(H(n)^{k})|^{2})=O(n^{-2}) \quad \mathrm{as}\quad n\rightarrow\infty,
\end{equation*}
which yields the second assertion.

An $n\times n$ random matrix $W$ is called a \textit{Wishart matrix} of $p$ degrees of freedom if $W=R^{t}R$, where $R$ is a real $p\times n$ random matrix with independent standard Gaussian entries. If $p<n$ then $W$ is singular; the statistical literature is mostly confined to the case $p\geq n$. In fact, this is not an essential restriction, because the nonzero eigenvalues of the matrices $R^{t}R$ and $RR^{t}$ are the same.

The next lemma contains the \textit{moment generating function}\index{Wishart matrix!moment generating function} of the Wishart matrix.

\begin{lemma}\label{ch04:lem4.1.8}
Let $A$ be a real $n\times n$ symmetric matrix such that $A\leq I/2$. Then
\begin{equation}
E(\exp(\mathrm{Tr}\,AW))=\det(I-2A)^{-p/2}
\label{ch04:eqn4.1.20}
\end{equation}
for the $n\times n$ regular Wishart matrix\index{Wishart matrix} $W$ of $p$ degrees of freedom.
\end{lemma}

\begin{proof2}
Due to the orthogonal invariance of the Wishart matrix, we may assume that $A= \mathbf{Diag} (a_{1}, a_{2}, \ldots, a_{n})$. Then $\mathrm{Tr}\,AW=\sum_{i}a_{i}W_{ii}$. The random variables $W_{ii}$ are independent and $\chi^{2}$-distributed with $p$ degrees of freedom. The problem is reduced to computing the moment generating function of $\chi^{2}$-distributed variables, which is well-known. So
\begin{align*}
E\left(\exp\sum_{i=1}^{n}a_{i}W_{ii}\right) & =  \prod_{i=1}^{n}E(\exp(a_{i}W_{ii}))\\
&  = \prod_{i=1}^{n}(1-2a_{i})^{-p/2}=\det(I-2A)^{-p/2}.
\end{align*}
\end{proof2}

It follows from this lemma that the density of the Wishart matrix $W$ with respect to $dT=\prod_{i\leq j}\,dT_{ij}$ is
\begin{equation}
p(T) :=C_{np}\exp\left(-\frac{1}{2}\mathrm{Tr}\,T\right)(\det T)^{(p-n-1)/2},
\label{ch04:eqn4.1.21}
\end{equation}
which is supported on the space $M_{n}(\mathbb{R})^{+}$ of $n\times n$ positive semidefinite real symmetric matrices. Indeed, if we compute the moment generating function
\begin{equation*}
\int\exp(\mathrm{Tr}\,AT)p(T)\,dT=C_{np}\int\exp\left(-\frac{1}{2}\mathrm{Tr}\,(I-2A)T\right)(\det T)^{(p-n-1)/2}\,dT,
\end{equation*}
we need to arrive at the right-hand side of (\ref{ch04:eqn4.1.20}). Let us use the substitution $S=BTB$ with $B=(I-2A)^{1/2}$. Due to the orthogonal invariance of $dT$, to compute the Jacobian of this transformation, it is enough to assume that $B=\mathbf{Diag} (b_{1}, b_{2}, \ldots, b_{n})$. Then it is easy to check that
\begin{equation*}
\det\frac{\partial S}{\partial T}=(b_{1}b_{2}\cdots b_{n})^{n+1}=(\det B)^{n+1}.
\end{equation*}
So we have
\begin{align*}
& \int\exp(\mathrm{Tr}\,AT)p(T)\,dT\\
& \qquad =C_{np}\int(\det B^{-1}SB^{-1})^{(p-n-1)/2}\exp\left(-\frac{1}{2}\mathrm{Tr}\,S\right)(\det B)^{-(n+1)}\,dS\\
& \qquad =(\det B)^{-p}=\det(I-2A)^{-p/2}.
\end{align*}

Now from Proposition~\ref{ch04:pro4.1.3} and (\ref{ch04:eqn4.1.21}) we conclude that the joint probability \textit{density of the eigenvalues}\index{density of eigenvalues!Wishart matrix} of the $n\times n$ regular Wishart matrix of $p$ degrees of freedom is
\begin{equation}
C_{np}\exp\left(-\frac{1}{2}\sum_{i=1}^{n}\lambda_{i}\right)\prod_{i=1}^{n}\lambda_{i}^{(p-n-1)/2}\prod_{i<j}|\lambda_{i}-\lambda_{j}|,
\label{ch04:eqn4.1.22}
\end{equation}
supported on $(\mathbb{R}^{+})^{n}\ (p\geq n)$. The normalizing constant $C_{np}$ can be determined from the Selberg integral (\ref{ch04:eqn4.1.8}).

The \textit{Marchenko-Pastur distribution}\index{Marchenko-Pastur distribution} $\mu_{\lambda}$ appeared historically as the limiting mean eigenvalue distribution\index{distribution!Marchenko-Pastur} of the $n\times n$ random matrices
\begin{equation*}
B_{p}(n):=\sum_{i=1}^{p}P_{i}\,,
\end{equation*}
where the $P_{i}$'s are independent rank one projections with unitarily invariant distribution and $p/n\rightarrow\lambda$. It is evident that, for $\lambda<1$, zero is in the spectrum of $B_{p}(n)$ with multiplicity $n-p$, and this yields the atomic part of the limiting distribution. The measure $\mu_{\lambda}$ coincides with the free Poisson distribution treated already in Chap.~\ref{ch02:chap02} and Chap.~\ref{ch03:chap03}; see (\ref{ch03:eqn3.3.2}) for the concrete form of the distribution.

\begin{theorem}\label{ch04:the4.1.9}
Assume that the entries of a real $p(n)\times n$ random matrix $T(n)$ are independent and identically distributed with mean $0$ and variance $1$ and with all moments bounded. Moreover, assume that $\lim_{n\rightarrow\infty}p(n)/n=\lambda$. Then the limiting mean eigenvalue distribution of the matrix $T(n)^{t}T(n)/n$ is the Marchenko-Pastur distribution $\mu_{\lambda}$.
\end{theorem}

The proof of this result can be given by the combinatorial method of moments, similarly to the lines of the proof of Theorem~\ref{ch04:the4.1.4}. The key point is the moment formula of the free Poisson distribution given in Example~\ref{ch02:exa2.5.2}, where the coefficient of $\lambda^{i}$ is the number of non-crossing partitions of $[k]$ into $i$ blocks. By a refinement of the above combinatorial method the moments of the limit distribution can be computed as
\begin{equation*}
\lim_{n\rightarrow\infty}\tau_{n}((n^{-1}T(n)^{t}T(n))^{k})=\sum_{i=1}^{k}\frac{1}{k} \left(\begin{array}{c}
k\\
i\\
\end{array}\right)\left(\begin{array}{c}
k\\
i-1\\
\end{array}\right)\negthickspace \lambda^{i} \qquad (k\in \mathbb{N}).
\end{equation*}

Theorem~\ref{ch04:the4.1.9} tells us that the Wishart matrices with appropriate normalization and scaling form a random matrix model for the Marchenko-Pastur distribution.\index{Marchenko-Pastur distribution}

An $n\times n$ \textit{standard non-selfadjoint Gaussian matrix}\index{Gaussian matrix!standard non-selfadjoint}\index{standard!non-selfadjoint Gaussian matrix} $X(n)$ is defined in the following way:
\begin{enumerate}
\item[(i)] $\{\mathrm{Re}\,X_{ij}(n):1\leq i, j\leq n\}\cup\{\mathrm{Im}\,X_{ij}(n):1\leq i, j\leq n\}$ is an independent family of Gaussian random variables, and

\item[(ii)] $E(X_{ij}(n))=0$ for $1\leq i, j\leq n$, and $E((\mathrm{Re}\,X_{ij}(n))^{2})=E((\mathrm{Im}\,X_{ij}(n))^{2})= 1/2n$ for $1\leq i, j\leq n$.
\end{enumerate}
Equivalently, one can define $X(n)$ as
\begin{equation*}
X(n)=\frac{H^{(1)}(n)+\mathrm{i}\,H^{(2)}(n)}{\sqrt{2}}\,,
\end{equation*}
where $H^{(1)}(n)$ and $H^{(2)}(n)$ are independent standard selfadjoint Gaussian matrices.\index{distribution!Marchenko-Pastur}

Let $u, v\in \mathbb{R}$ be such that $u^{2}+v^{2}=1$, and for $n\in \mathbb{N}$ set
\begin{equation}
Y(n) :=uX(n)+vX(n)^{*},
\label{ch04:eqn4.1.23}
\end{equation}
which is what we call an \textit{elliptic Gaussian matrix}.\index{Gaussian matrix!elliptic}\index{elliptic!Gaussian matrix} This is a kind of interpolation between the standard selfadjoint Gaussian and the standard non-selfadjoint Gaussian cases. Namely, the choices $u=v$ and $v=0$ recover those examples. The elliptic Gaussian matrix contains some correlations. The correlation coefficient between $\mathrm{Re}\,Y_{ij}(n)$ and $\mathrm{Re}\,Y_{ji}(n)$ is $\tau :=2uv\ (-1\leq\tau\leq 1)$ when $i\neq j$. One can get the elliptic Gaussian matrix as
\begin{equation*}
Y(n)=\sqrt{\frac{1+\tau}{2}}H^{(1)}(n)+\mathrm{i}\,\sqrt{\frac{1-\tau}{2}}H^{(2)}(n)
\end{equation*}
as well, where $H^{(1)}(n)$ and $H^{(2)}(n)$ are independent standard selfadjoint Gaussian matrices.

We shall need the joint distribution of the eigenvalues\index{density of eigenvalues!elliptic Gaussian matrix} of $Y(n)$. The probability measure on $M_{n}(\mathbb{C})$ induced by $X(n)$ has the density
\begin{equation}
C_{n}\exp(-n\mathrm{Tr}\,X^{*}X) \quad \mathrm{with}\quad C_{n}=\left(\frac{\pi}{n}\right)^{-n^{2}}
\label{ch04:eqn4.1.24}
\end{equation}
with respect to the Lebesgue measure
\begin{equation}
dX:=\prod_{i,j=1}^{n}d(\mathrm{Re}\,X_{ij})\,d(\mathrm{Im}\,X_{ij}) \quad \mathrm{on} \quad M_{n}(\mathbb{C})\cong \mathbb{R}^{2n^{2}}.
\label{ch04:eqn4.1.25}
\end{equation}
The measure induced by $Y(n)$ is the image measure of the above via the transformation $Y=uX+vX^{*}$. In the case $u\neq\pm v$ this is a non-singular linear transformation on $M_{n}(\mathbb{C})\cong \mathbb{R}^{2n^{2}}$, whose inverse is $X=aY+bY^{*}$, where
\begin{equation*}
a:=\frac{u}{u^{2}-v^{2}},\quad b:=-\frac{v}{u^{2}-v^{2}}.
\end{equation*}
Since the Jacobian $\det(\partial X/\partial Y)$ is clearly a constant, the measure on $M_{n}(\mathbb{C})$ induced by $Y(n)$ has the density
\begin{align}
&C_{n}\exp(-n\mathrm{Tr}\,|aX+bX^{*}|^{2})\notag\\
& \qquad =C_{n}\exp\left(-\frac{n}{1-\tau^{2}}\mathrm{Tr}\,(X^{*}X-\tau\mathrm{Re}\,X^{2})\right),
\label{ch04:eqn4.1.26}
\end{align}
where $C_{n}$ is a new normalizing constant (including the Jacobian).

Now we determine the form of the joint distribution on $\mathbb{C}^{n}$ (the space of eigenvalues) induced from the distribution (\ref{ch04:eqn4.1.26}).

\begin{lemma}\label{ch04:lem4.1.10}
Assume $u\neq\pm v$ and hence $-1<\tau<1$. Then, with respect to $d\zeta\equiv d\zeta_{1}d\zeta_{2}\cdots d\zeta_{n}\ (d\zeta_{i}$ is the Lebesgue measure on $\mathbb{C})$, the joint probability density of the eigenvalues of $Y(n)$ is
\begin{equation*}
C_{n}\exp\left[-n\sum_{i=1}^{n}\left(\frac{(\mathrm{Re}\,\zeta_{i})^{2}}{1+\tau}+\frac{(\mathrm{Im}\,\zeta_{i})^{2}}{1-\tau}\right)\right]
\prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2}.
\end{equation*}
\end{lemma}

\begin{proof2}
The standard case $\tau=0$ was first discussed by Dyson (see [\citen{bib121}], A.35), and the general case follows this pattern. Take a unitary $U$ such that $U^{*}XU=T$ is upper triangular. Differentiating $X=UTU^{*}$, we get
\begin{equation}
dX=U(dT+\mathrm{i}\,(dM\cdot T-T\cdot dM))U^{*},
\label{ch04:eqn4.1.27}
\end{equation}
where $dT$ is triangular and $dM :=-\mathrm{i}\,U^{*}dU$ is Hermitian. From the $n$-dimensional freedom of $U$ we may impose $n$ conditions on the variations of $U$:
\begin{equation*}
dM_{ii}=-\mathrm{i}\,(U^{*}dU)_{ii}=0 \qquad (1\leq i\leq n).
\end{equation*}
Put $dY :=U^{*} \cdot dX \cdot U$, and write (\ref{ch04:eqn4.1.27}) in the matrix elements as follows: for $1\leq i\leq j\leq n$
\begin{equation}
dY_{ij}=dT_{ij}+\mathrm{i}\sum_{k\leq j}T_{kj}dM_{ik}-\mathrm{i}\sum_{l\geq i}T_{il}dM_{lj}\,,
\label{ch04:eqn4.1.28}
\end{equation}
and for $1\leq i<j\leq n$
\begin{equation}
dY_{ji}=\mathrm{i}\,(T_{ii}-T_{jj})\overline{dM}_{ij}+\mathrm{i}\sum_{k<i}T_{ki}\overline{dM}_{kj}-\mathrm{i}\sum_{l>j}T_{jl}\overline{dM}_{il}.
\label{ch04:eqn4.1.29}
\end{equation}
These give a linear correspondence between $(\mathrm{Re}\,dY_{ij}, \mathrm{Im}\,dY_{ij})_{1\leq i,j\leq n}$ and
\begin{equation}
(\mathrm{Re}\,dT_{ij}, \mathrm{Im}\,dT_{ij})_{1\leq i\leq j\leq n},\quad (\mathrm{Re}\,dM_{ij}, \mathrm{Im}\,dM_{ij})_{1\leq i<j\leq n}. \label{ch04:eqn4.1.30}
\end{equation}
When $\prod_{i,j=1}^{n}(\mathrm{Re}\,dY_{ij})(\mathrm{Im}\,dY_{ij})$ is expanded into products of terms in (\ref{ch04:eqn4.1.30}), one can neglect products other than those where every term in (\ref{ch04:eqn4.1.30}) appears once for each. The element $dT_{ij}$ can arise only from the equation (\ref{ch04:eqn4.1.28}) of $dY_{ij}$, and so the elements $dM_{ij}$ cannot come from (\ref{ch04:eqn4.1.28}). Then, looking at (\ref{ch04:eqn4.1.29}) successively in the order of $(i,j)=(1, n), (1, n-1), \ldots, (1, 2), (2, n), \ldots, (2, 3), \ldots, (n-1, n)$, one can see that the element $dM_{ij}$ must arise only from the equation (\ref{ch04:eqn4.1.29}) of $dY_{ji}$. In this way, to compute $\prod(\mathrm{Re}\,dY_{ij})(\mathrm{Im}\, dY_{ij})$, one may replace (\ref{ch04:eqn4.1.28}) by
\begin{equation*}
dY_{ij}=dT_{ij}
\end{equation*}
and (\ref{ch04:eqn4.1.29}) by
\begin{equation*}
dY_{ji}=\mathrm{i}\,(T_{ii}-T_{jj})\overline{dM}_{ij}.
\end{equation*}
Hence we have
\begin{align*}
& \prod_{i,j=1}^{n}(\mathrm{Re}\,dX_{ij})(\mathrm{Im}\,dX_{ij})=\prod_{i,j=1}^{n}(\mathrm{Re}\,dY_{ij})(\mathrm{Im}\,dY_{ij})\\
& \qquad =\prod_{i\leq j}(\mathrm{Re}\,dT_{ij})(\mathrm{Im}\,dT_{ij})\prod_{i<j}|T_{ii}-T_{jj}|^{2}(\mathrm{Re}\,dM_{ij})(\mathrm{Im}\,dM_{ij})\\
& \qquad =\prod_{i<j}|\zeta_{i}-\zeta_{i}|^{2}\prod_{i=1}^{n}d\zeta_{i}\prod_{i<j}(\mathrm{Re}\,dT_{ij})(\mathrm{Im}\,dT_{ij})(\mathrm{Re}\, dM_{ij})(\mathrm{Im}\,dM_{ij})\,,
\end{align*}
where $\zeta_{i} :=T_{ii}$ are the eigenvalues of $X$.

Furthermore, we compute
\begin{align*}
& \mathrm{Tr}\, |aX+bX^{*}|^{2}=\frac{1}{1-\tau^{2}} (\mathrm{Tr}\ T^{*}T-\tau\,\mathrm{Re}\ \mathrm{Tr}\ T^{2})\\
& \qquad =\frac{1}{1-\tau^{2}}\left(\sum_{i=1}^{n}|\zeta_{i}|^{2}+\sum_{i<j}|T_{ij}|^{2}-\tau\,\mathrm{Re}\sum_{i=1}^{n}\zeta_{i}^{2}\right)\\
& \qquad =\frac{1}{1+\tau}\sum_{i=1}^{n}(\mathrm{Re}\,\zeta_{i})^{2}+\frac{1}{1-\tau}\sum_{i=1}^{n}(\mathrm{Im}\,\zeta_{i})^{2}+\frac{1}{1-\tau^{2}}\sum_{i<j}|T_{ij}|^{2}.
\end{align*}
Therefore, the measure (\ref{ch04:eqn4.1.26}) is
\begin{align*}
&C_{n}\exp\left[-n\sum_{i=1}^{n}\left(\frac{(\mathrm{Re}\,\zeta_{i})^{2}}{1+\tau}+\frac{(\mathrm{Im}\,\zeta_{i})^{2}}{1-\tau}\right)\right]
\prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2}d\zeta\\
& \quad \times\exp\left(-\frac{n}{1-\tau^{2}}\sum_{i<j}|T_{ij}|^{2}\right)\prod_{i<j}(\mathrm{Re}\,dT_{ij})(\mathrm{Im}\,dT_{ij})\prod_{i<j}(\mathrm{Re}\, dM_{ij})(\mathrm{Im}\,dM_{ij}).
\end{align*}
Integrating with respect to $\prod(\mathrm{Re}\,dT_{ij})(\mathrm{Im}\,dT_{ij})$ and $\prod(\mathrm{Re}\,dM_{ij})(\mathrm{Im}\,dM_{ij})$, we conclude that the induced measure on $\mathbb{C}^{n}$ is written as
\begin{equation*}
 C_{n}'\exp\left[-n\sum_{i=1}^{n}\left(\frac{(\mathrm{Re}\,\zeta_{i})^{2}}{1+\tau}+\frac{(\mathrm{Im}\,\zeta_{i})^{2}}{1-\tau}\right)\right]
 \prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2}d\zeta.
\end{equation*}
\end{proof2}

In particular, the standard non-selfadjoint Gaussian matrix $X(n)$ (in the case $\tau=0)$ has the joint eigenvalue density
\begin{equation}
\frac{\pi^{-n}n^{n(n+1)/2}}{\prod_{j=1}^{n}j!}\exp\left(-n\sum_{i=1}^{n}|\zeta_{i}|^{2}\right)\prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2}
\label{ch04:eqn4.1.31}
\end{equation}
(see [\citen{bib121}], Sec. 15.1, for a computation of the above normalizing constant).

Results about the asymptotic freeness in Sec.~\ref{ch04:sec4.3} show that the distribution of the elliptic Gaussian matrix $Y(n)$ converges to that of an \textit{elliptic element}\index{randam matrix!model for elliptic element}\index{elliptic!element} discussed in Example~\ref{ch02:exa2.6.7}. It is worthwhile to form the following definition. Assume that $x$ is a noncommutative random variable and for every $n\in \mathbb{N}$ an $n\times n$ complex matrix $T(n)$ is given. It is said that $T(n)$ is a \textit{random matrix model}\index{random matrix!model} of $x$ if the distribution of $(T(n), T(n)^{*})$ (with respect to $\tau_{n}$) converges to the distribition of $(x, x^{*})$ as the matrix size goes to infinity. (Earlier we used the concept of random matrix model in the selfadjoint case, and that usage is compatible with this more general definition.) Hence $Y(n)$ is a \textit{random matrix model of the elliptic element}, and in particular the standard non-selfadjoint Gaussian matrices $X(n)$ form a model of the circular element. The elliptic Gaussian matrix will appear also in a large deviation result discussed in Chap.~\ref{ch05:chap05}.

Unitary random matrices are also important; we shall devote the whole next section to them.

\section{Random unitary matrices and asymptotic freeness}
\label{ch04:sec4.2}

\noindent Let $\mathcal{U}(n)$ be the compact group of $n\times n$ unitary matrices. The Haar probability measure $\gamma_{n}$ on $\mathcal{U}(n)$ is two-sided invariant. An $n\times n$ unitary random matrix is said to be \textit{standard} if its distribution is $\gamma_{n}$.

First, we determine the form of the joint distribution of the eigenvalues of the $n \times n$ standard unitary random matrix.\index{density of eigenvalues!standard unitary matrix} Since the eigenvalues of a unitary matrix\index{standard!unitary random matrix} are on the unit circle $\mathbb{T}$, the joint probability of the eigenvalues is supported on $\mathbb{T}^{n}$. Most of this section is spent in studying the multiple joint moments of independent standard random unitaries. The concept of asymptotic freeness will emerge from the calculations.

\begin{lemma}\label{ch04:lem4.2.1}
The measure on $\mathbb{T}^{n}$ induced from the Haar measure $\gamma_{n}$ on $\mathcal{U}(n)$ has the density
\begin{equation*}
\frac{1}{n!}\prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2}=\frac{1}{n!}\prod_{i<j}|e^{\mathrm{i}\,\theta_{i}}-e^{\mathrm{i}\,\theta_{j}}|^{2}
\end{equation*}
with respect to $d\zeta_{1}d\zeta_{2}\cdots d\zeta_{n}$, where $ d\zeta_{i}=d\theta_{i}/2\pi$ for $\zeta_{i}=e^{\mathrm{i}\,\theta_{i}}$.
\end{lemma}

\begin{proof2}
Consider the differential $dU$ at $U\in \mathcal{U}(n)$; then $dL :=-\mathrm{i}\,U^{*}dU$ is an infinitesimal Hermitian matrix. Applying the invariance of $\gamma_{n}$ to $dU=U(\mathrm{i}\,dL)= U(e^{\mathrm{i}\,dL}-I)$, one can see that
\begin{equation*}
\gamma_{n}(dU)=C_{n}\prod_{i=1}^{n}dL_{ii}\prod_{i<j}(\mathrm{Re}\,dL_{ij})(\mathrm{Im}\,dL_{ij}).
\end{equation*}
Diagonalize $U$ in the form $U\, =\, VDV^{*}$, where $V$ is a unitary and $D\, =\,\mathbf{Diag} (e^{\mathrm{i}\,\theta_{1}}, e^{\mathrm{i}\,\theta_{2}}, \ldots, e^{\mathrm{i}\,\theta_{n}})$. As in the proof of Lemma~\ref{ch04:lem4.1.10} we may require that $dM_{ii}=0\ (1\leq i\leq n)$ for $dM :=-\mathrm{i}\,V^{*}dV$. Since
\begin{equation*}
dL=-\mathrm{i}\,U^{*}dU=V(-\mathrm{i}\,D^{*}dD+D^{*}dMD-dM)V^{*},
\end{equation*}
we have
\begin{equation*}
(V^{*}dLV)_{ij}=\left\{\begin{array}{ll}
d\theta_{i} & \mathrm{if}\ i=j,\\
(e^{\mathrm{i}\,(\theta_{j}-\theta_{i})}-1)dM_{ij} & \mathrm{if}\ i<j,\\
\end{array}\right.
\end{equation*}
and hence
\begin{align*}
& \prod_{i=1}^{n}dL_{ii}\prod_{i<j}(\mathrm{Re}\,dL_{ij})(\mathrm{Im}\,dL_{ij})\\
& \qquad =\prod_{i<j}|e^{\mathrm{i}\,\theta_{i}}-e^{\mathrm{i}\,\theta_{j}}|^{2}\prod_{i=1}^{n}d\theta_{i}\prod_{i<j}(\mathrm{Re}\,dM_{ij})(\mathrm{Im}\, dM_{ij}).
\end{align*}
This shows that the induced measure on $\mathbb{T}^{n}$ has the density
\begin{equation*}
C_{n}'\prod_{i<j}|e^{\mathrm{i}\,\theta_{i}}-e^{\mathrm{i}\,\theta_{j}}|^{2}.
\end{equation*}

To determine $C_{n}'$ we need to compute the integral
\begin{equation}
\frac{1}{(2\pi)^{n}}\int_{0}^{2\pi}\cdots\int_{0}^{2\pi}\prod_{i<j}|e^{\mathrm{i}\,\theta_{i}}-e^{\mathrm{i}\,\theta_{j}}|^{2}\,d\theta_{1}\cdots d\theta_{n}\,.
\label{ch04:eqn4.2.1}
\end{equation}
Since
\begin{align*}
\prod_{i<j}|e^{\mathrm{i}\,\theta_{i}}-e^{\mathrm{i}\,\theta_{j}}|^{2} & \ = \ \prod_{i<j}(e^{\mathrm{i}\,\theta_{i}}-e^{\mathrm{i}\,\theta_{j}})\prod_{i<j}(e^{-\mathrm{i}\,\theta_{i}}-e^{-\mathrm{i}\,\theta_{j}})\\
& \ = \ \det[e^{\mathrm{i}\,k\theta_{i}}]_{1\leq i\leq n,\,0\leq k\leq n-1}\det[e^{-\mathrm{i}\,k\theta_{j}}]_{0\leq k\leq n-1,\,1\leq j\leq n}\\
& \ = \ \det\left[\sum_{k=0}^{n-1}e^{ik(\theta_{i}-\theta_{j})}\right]_{1\leq i,j\leq n},
\end{align*}
one can write
\begin{align*}
\prod_{i<j}|e^{\mathrm{i}\,\theta_{i}}-e^{\mathrm{i}\,\theta_{j}}|^{2} & \ = \  \det[S_{n}(\theta_{i}-\theta_{j})]_{1\leq i,j\leq n}\\
& \ = \ \sum_{k=1}^{n}\sum_{\sigma:\sigma(k)=n}\mathrm{sign}(\sigma)\prod_{i=1}^{n}S_{n}(\theta_{i}-\theta_{\sigma(i)})\,,
\end{align*}
where $S_{n}(\theta) :=\sum_{k=0}^{n-1}e^{\mathrm{i}\,k\theta}$ and the summation over permutations $\sigma$ of $[n]$ is decomposed according to $\sigma(k)=n$. Integration with respect to $\theta_{n}$ gives
\begin{align*}
&\frac{1}{2\pi}\int_{0}^{2\pi}\det[S_{n}(\theta_{i}-\theta_{j})]_{1\leq i,j\leq n}\,d\theta_{n}\\
& \qquad =S_{n}(0)\sum_{\sigma:\sigma(n)=n}\mathrm{sign}(\sigma)\prod_{i=1}^{n-1}S_{n}(\theta_{i}-\theta_{\sigma(i)})\\
& \qquad \qquad +\sum_{k=1}^{n-1}\sum_{\sigma:\sigma(k)=n}\mathrm{sign}(\sigma)\prod_{\begin{subarray}{l}i=1\\i\neq k\\ \end{subarray}}^{n-1} S_{n}(\theta_{i}-\theta_{\sigma(i)})\\
& \qquad \qquad \qquad \qquad \qquad \times\frac{1}{2\pi}\int_{0}^{2\pi}S_{n}(\theta_{k}-\theta_{n})S_{n}(\theta_{n}-\theta_{\sigma(n)})\,d\theta_{n}\\
&\qquad =n\,\det[S_{n}(\theta_{i}-\theta_{j})]_{1\leq i,j\leq n-1}\\
& \qquad \qquad +\sum_{k=1}^{n-1}\sum_{\sigma:\sigma(k)=n}\mathrm{sign}(\sigma)\prod_{\begin{subarray}{l}i=1\\ i\neq k\\ \end{subarray}}^{n-1}S_{n}(\theta_{i}-\theta_{\sigma(i)})\cdot S_{n}(\theta_{k}-\theta_{\sigma(n)})\,,
\end{align*}
because $S_{n}(0)=n$ and
\begin{equation*}
\frac{1}{2\pi}\int_{0}^{2\pi}S_{n}(\theta_{1}-\theta)S_{n}(\theta-\theta_{2})\,d\theta=S_{n}(\theta_{1}-\theta_{2})\,.
\end{equation*}
For $\sigma$ such that $\sigma(k)=n$ with $1\leq k\leq n -1$, take a permutation $\sigma'$ of $[n-1]$ as $\sigma'(i)=\sigma(i)$ for $i\in[n-1],\,i\neq k$, and $\sigma'(k)=\sigma(n)$. Then, since $\mathrm{sign}(\sigma)= -\mathrm{sign}(\sigma')$, we obtain
\begin{align*}
&\sum_{\sigma:\sigma(k)=n}\mathrm{sign}(\sigma)\prod_{\begin{subarray}{l}i=1\\i\neq k\\ \end{subarray}}^{n-1}S_{n}(\theta_{i}-\theta_{\sigma(i)}) \cdot S_{n}(\theta_{k}-\theta_{\sigma(n)})\\
& \qquad =-\sum_{\sigma'}\mathrm{sign}(\sigma')\prod_{i=1}^{n-1}S_{n}(\theta_{i}-\theta_{\sigma'(i)})=-\det[S_{n}(\theta_{i}-\theta_{j})]_{1\leq i,j\leq n-1},
\end{align*}
so that
\begin{align*}
&\frac{1}{2\pi}\int_{0}^{2\pi}\det[S_{n}(\theta_{i}-\theta_{j})]_{1\leq i,j\leq n}\,d\theta_{n}\\
&\qquad =(n-(n-1))\det[S_{n}(\theta_{i}-\theta_{j})]_{1\leq i,j\leq n-1}.
\end{align*}
Repeating the above computation yields that the integral (\ref{ch04:eqn4.2.1}) is equal to
\begin{equation*}
(n-(n-1))(n-(n-2))\cdots(n-1)S_{n}(0)=n!\,,
\end{equation*}
as required.
\end{proof2}

For a measurable function $f : \mathcal{U}(n)\rightarrow \mathbb{C}$ we have the expectation
\begin{equation*}
E(f):=\int f(U)\,d\gamma_{n}(U)
\end{equation*}
whenever this integral is defined. The standard unitary random matrix $U$ is a noncommutative random variable with respect to the functional $\tau_{n}$. Due to the invariance of $\gamma_{n},\, e^{\mathrm{i}\,\theta}U$ is standard for any $\theta\in \mathbb{R}$, and it follows that the moments $\tau_{n}(U^{k})$ vanish for all $k\in \mathbb{Z}\,\backslash \,\{0\}$. Namely, $U$ is a Haar unitary in the sense of Chap.~\ref{ch01:chap01} (see Example~\ref{ch01:exa1.2.4}).

Before we turn to the joint moments of two independent standard random unitaries, we start with the correlation of the matrix entries of a single standard random unitary.

Let $U=[U_{ij}]$ be an $n \times n$ standard unitary matrix. The two-sided invariance of the Haar measure on $\mathcal{U}(n)$ yields that $E(f(U))=E(f(VUW))$ for every measurable function $f$ and for all unitaries $V$ and $W$. When $V= \mathbf{Diag}(e^{\mathrm{i}\,\theta_{1}},\ldots,e^{\mathrm{i}\,\theta_{n}})$ and $W= \mathbf{Diag} (e^{\mathrm{i}\,\psi_{1}}, \ldots, e^{\mathrm{i}\,\psi_{n}})$, we have
\begin{equation}
E(f)=E(f([e^{\mathrm{i}\,(\theta_{i}+\psi_{j})}U_{ij}]_{i,j=1}^{n}))
\label{ch04:eqn4.2.2}
\end{equation}
for every $\theta_{i}, \psi_{j}\in \mathbb{R}$. Moreover, choosing permutation matrices $V$ and $W$, we obtain
\begin{equation}
E(f)=E(f([U_{\pi(i)\sigma(j)}]_{i,j=1}^{n}))
\label{ch04:eqn4.2.3}
\end{equation}
for all permutations $\pi, \sigma$ of $[n]$. The next lemma says that most of the multiple moments of the elements $U_{ij}$ vanish.

\begin{lemma}\label{ch04:lem4.2.2}
Let $l\in \mathbb{N},\ i_{1}, \ldots, i_{l}, j_{1}, \ldots, j_{l}\in[n]$ and $k_{1}, \ldots, k_{l}, m_{1}, \ldots, m_{l}\in \mathbb{Z}^{+}$. If either $\sum_{i_{r}=i}(k_{r}-m_{r})\neq 0$ for some $1\leq i\leq n$ or $\sum_{j_{r}=j}(k_{r}-m_{r})\neq 0$ for some $1\leq j\leq n$, then
\begin{equation}
E((U_{i_{1}j_{1}}^{k_{1}}\bar{U}_{i_{1}j_{1}}^{m_{1}})(U_{i_{2}j_{2}}^{k_{2}}\bar{U}_{i_{2}j_{2}}^{m_{2}})\cdots(U_{i_{l}j_{l}}^{k_{l}}\bar{U}_{i_{l}j_{l}}^{m_{l}}))=0\,.
\label{ch04:eqn4.2.4}
\end{equation}
In particular, if $\sum_{r=1}^{l}(k_{r}-m_{r})\neq 0$ $($this is the case when $\sum_{r=1}^{l}(k_{r}+m_{r})$ is odd$)$, then \emph{(\ref{ch04:eqn4.2.4})} holds.
\end{lemma}

\begin{proof2}
Suppose that $h:=\sum_{i_{r}=i}(k_{r}-m_{r})\neq 0$. We can apply (\ref{ch04:eqn4.2.2}) above and get
\begin{equation*}
E((U_{i_{1}j_{1}}^{k_{1}}\bar{U}_{i_{1}j_{1}}^{m_{1}})\cdots(U_{i_{l}j_{l}}^{k_{l}}\bar{U}_{i_{l}j_{l}}^{m_{l}}))=e^{\mathrm{i}\,h\theta}E((U_{i_{1}j_{1}}^{k_{1}}\bar{U}_{i_{1}j_{1}}^{m_{1}})\cdots(U_{i_{l}j_{l}}^{k_{l}}\bar{U}_{i_{l}j_{l}}^{m_{l}}))
\end{equation*}
for every $\theta\in \mathbb{R}$. This gives the statement.
\end{proof2}

The following are examples of multiple moments of the matrix elements $U_{ij}$ of a standard unitary. It is immediate from Lemma~\ref{ch04:lem4.2.2} that other multiple moments of $U_{ij}$'s up to the fourth order are all $0$.

\begin{proposition}\label{ch04:pro4.2.3}
For a standard unitary matrix $U=[U_{ij}]$ the following hold:
\begin{align}
&E(|U_{ij}|^{2})=\frac{1}{n} \qquad (1 \leq i, j\leq n), \label{ch04:eqn4.2.5}\\
&E(|U_{ij}|^{4})=\frac{2}{n(n+1)} \qquad (1 \leq i, j\leq n), \label{ch04:eqn4.2.6}\\ &E(|U_{ij}|^{2}|U_{i'j}|^{2})=E(|U_{ij}|^{2}|U_{ij'}|^{2})=\frac{1}{n(n+1)} \qquad (i\neq i',\ j\neq j'), \label{ch04:eqn4.2.7}\\ &E(|U_{ij}|^{2}|U_{i'j'}|^{2})=\frac{1}{n^{2}-1} \qquad (i\neq i',\ j\neq j'), \label{ch04:eqn4.2.8}\\ &E(U_{ij}U_{i'j'}\bar{U}_{ij'}\bar{U}_{i'j})=-\frac{1}{n(n^{2}-1)} \qquad (i\neq i',\ j\neq j'). \label{ch04:eqn4.2.9}
\end{align}
\end{proposition}

\begin{proof2}
The random variables $U_{ij}$ are identically distributed according to (\ref{ch04:eqn4.2.3}). This yields (\ref{ch04:eqn4.2.5}) because $\sum_{j=1}^{n}|U_{ij}|^{2}=1$.

Since the $(1, 1)$ entries of $U$ and
\begin{equation*}
\left(\left[\begin{array}{cl}
\cos \theta & \sin \theta\\
-\sin\theta & \cos \theta\\
\end{array}\right] \oplus I_{n-2}\right)U
\end{equation*}
are identically distributed, we have
\begin{align*}
& E(|U_{11}|^{4})\\
& \quad =E(|U_{11}\cos\theta+U_{21}\sin\theta|^{4})\\
& \quad =E((|U_{11}|^{2}\cos^{2}\theta+|U_{21}|^{2}\sin^{2}\theta+(U_{11}\bar{U}_{21}+\bar{U}_{11}U_{21})\cos\theta\sin\theta)^{2})\\
& \quad =E(|U_{11}|^{4})\cos^{4}\theta+E(|U_{21}|^{4})\sin^{4}\theta+4E(|U_{11}|^{2}|U_{21}|^{2})\cos^{2}\theta\sin^{2}\theta\\
& \quad =E(|U_{11}|^{4})(\cos^{4}\theta+\sin^{4}\theta)+4E(|U_{11}|^{2}|U_{21}|^{2})\cos^{2}\theta\sin^{2}\theta\,.
\end{align*}
Above we used $E(|U_{11}|^{2}U_{11}\bar{U}_{21})=0$, etc., according to Lemma~\ref{ch04:lem4.2.2}. Hence
\begin{equation*}
E(|U_{11}|^{4})=2E(|U_{11}|^{2}|U_{21}|^{2})=2E(|U_{i1}|^{2}|U_{i'1}|^{2}) \qquad (i\neq i').
\end{equation*}
Since $\sum_{i,i'=1}^{n}|U_{i1}|^{2}|U_{i'1}|^{2}=1$, this yields
\begin{align*}
1 & \ = \ \sum_{i=1}^{n}E(|U_{i1}|^{4})+\sum_{i\neq i'}E(|U_{i1}|^{2}|U_{i'1}|^{2})\\
& \ = \ nE(|U_{11}|^{4})+\frac{n(n-1)}{2}E(|U_{11}|^{4})=\frac{n(n+1)}{2}E(|U_{11}|^{4})\,,
\end{align*}
so $E(|U_{11}|^{4})=2/n(n+1)$ and $E(|U_{11}|^{2}|U_{21}|^{2})=1/n(n+1)$. In this way (\ref{ch04:eqn4.2.6}) and (\ref{ch04:eqn4.2.7}) are derived. Apply (\ref{ch04:eqn4.2.7}) to $\sum_{i,i'=1}^{n}|U_{i1}|^{2}|U_{i'2}|^{2}=1$ to get (\ref{ch04:eqn4.2.8}). The proof of (\ref{ch04:eqn4.2.9}) is similar to that of (\ref{ch04:eqn4.2.6}), and it is left as an exercise.
\end{proof2}

By Proposition~\ref{ch04:pro4.2.3} the correlation coefficient between $|U_{ij}|^{2}$ and $|U_{i'j'}|^{2}$ is computed as follows:
\begin{align*}
&\rho(|U_{ij}|^{2}, |U_{i'j}|^{2})=\rho(|U_{ij}|^{2}, |U_{ij'}|^{2})=-\frac{1}{n-1} \qquad (i\neq i',\ j\neq j'),\\
&\rho(|U_{ij}|^{2}, |U_{i'j'}|^{2})=\frac{1}{(n-1)^{2}} \qquad (i\neq i',\ j\neq j').
\end{align*}

\begin{lemma}\label{ch04:lem4.2.4}
For every $k\in \mathbb{Z}^{+}$,
\begin{equation*}
E(|U_{ij}|^{2k})=\left(\begin{array}{c}
n+k -1\\
n-1 \\
\end{array}\right)^{-1} \quad \ (1\leq i,j\leq n).
\end{equation*}
Furthermore, the distribution\index{beta distribution}\index{distribution!beta} of $U_{ij}$ is
\begin{equation}
\frac{n-1}{\pi}(1-r^{2})^{n-2}r\,dr\cdot d\theta \qquad (\zeta=re^{\mathrm{i}\,\theta},\ 0\leq r\leq 1,\ 0\leq\theta\leq 2\pi).
\label{ch04:eqn4.2.10}
\end{equation}
\end{lemma}

\begin{proof2}
First note that the variables $\langle Ug,  h\rangle$ (in particular $U_{ij}$) have the same distribution for all unit vectors $g, h\in \mathbb{C}^{n}$. Let $(\zeta_{1}, \zeta_{2}, \ldots, \zeta_{n})^{t}$ be a column of the Haar distributed random unitary matrix. The unitary transformations act on the sphere
\begin{equation*}
\left\{(z_{1}, z_{2}, \ldots, z_{n})\in \mathbb{C}^{n}:\sum_{i}|z_{i}|^{2}=1\right\}
\end{equation*}
transitively. The distribution measure induced on this space by the random unit vector $(\zeta_{1}, \zeta_{2}, \ldots, \zeta_{n})$ is invariant under the action of the unitary transformations. The invariant measure is unique, so we may assume that
\begin{equation*}
\zeta_{j}=\frac{\xi_{2j-1}+\mathrm{i}\,\xi_{2j}}{\sqrt{\sum_{i=1}^{2n}\xi_{i}^{2}}}\,,
\end{equation*}
where $\xi_{1}, \xi_{2}, \ldots, \xi_{2n}$ are independent Gaussian variables of $N(0, 1/2)$. We can write $\zeta_{j}$ in a slightly different form as
\begin{equation*}
\zeta_{j}=\frac{u_{j\sqrt{\eta_{j}}}}{\sqrt{\sum_{i=1}^{n}\eta_{i}}}
\end{equation*}
with independent variables $u_{1}, \ldots, u_{n}, \eta_{1}, \ldots, \eta_{n}$, where $\eta_{j}:=\xi_{2j-1}^{2}+\xi_{2j}^{2}$ and $u_{j}\sqrt{\eta_{j}}=\xi_{2j-1}+\mathrm{i}\,\xi_{2j}$. Note that $\eta_{j}$ is exponentially distributed with the density $e^{-x}\ (x>0)$ and $u_{j}$ is uniformly distributed on the unit circle $\mathbb{T}$. So we write $|\zeta_{1}|^{2}=\eta_{1}/(\eta_{1}+\eta)$, where $\eta$ is independent of $\eta_{1}$ and has the density $x^{n-2}e^{-x}/(n-2)$! $(x>0)$. Since
\begin{align*}
\mathbf{Prob} (|\zeta_{1}|^{2}\geq x) & \ =\ \mathbf{Prob} (\eta_{1}\geq\frac{x}{1-x}\eta)\\
& \ = \ \int_{0}^{\infty}\frac{t^{n-2}}{(n-2)!}e^{-t}\left(\int_{\frac{x}{1-x}t}^{\infty}e^{-s}\,ds\right)dt\\
& \ = \ (1-x)^{n-1},
\end{align*}
we conclude that the density of $|\zeta_{1}|^{2}$ is $(n-1)(1-x)^{n-2}$ and the density of $|\zeta_{1}|$ is $2(n-1)(1-x^{2})^{n-2}x$. Hence the distribution (\ref{ch04:eqn4.2.10}) is obtained.
\end{proof2}

The variable $|\zeta_{1}|^{2}$ in the above proof has a beta distribution (\ref{ch04:eqn4.2.18}) with $\alpha=1$ and $\beta=n-1$; see the end of this section. Furthermore, one can see that the joint distribution of $|\zeta_{1}|^{2}, |\zeta_{2}|^{2}, \ldots, |\zeta_{n-1}|^{2}$ is uniform on the set
\begin{equation*}
\left\{(x_{1}, x_{2}, \ldots, x_{n-1})\in(\mathbb{R}^{+})^{n-1}:\sum_{i}x_{i}\leq 1\right\}.
\end{equation*}
From this fact, for example, the correlation between $|\zeta_{i}|^{2}$ and $|\zeta_{j}|^{2}$ is easily computable.

When $U(n)$ is an $n\times n$ standard unitary matrix, it is important for the proof of the next proposition to notice that Lemma~\ref{ch04:lem4.2.4} yields
\begin{equation}
E(|U_{ij}(n)|^{2k})=O(n^{-k}) \quad \mathrm{as} \quad n\rightarrow\infty.
\label{ch04:eqn4.2.11}
\end{equation}
This order is the same as the $2k$th moment of the normal distribution $N(0, 1/n)$ with variance $1/n$. Moreover, it is worth noting that
\begin{equation*}
\mathbf{Prob} (|\sqrt{n}\,U_{ij}(n)|^{2}\geq x)=\left(1-\frac{x}{n}\right)^{n-1}\rightarrow e^{-x} \quad \mathrm{as} \quad n\rightarrow\infty,
\end{equation*}
and hence the distribution of $\sqrt{n}\,U_{ij}(n)$ tends to the standard complex Gaussian measure (i.e. the distribution of the above $\xi_{1}+\mathrm{i}\,\xi_{2}$). More generally, for each $k\in \mathbb{N}$, if
\begin{equation*}
T_{ij}(n)=U_{ij}(n)\quad \mathrm{for} \quad 1\leq i, j\leq k,
\end{equation*}
then $\sqrt{n}T(n)$ converges in distribution to the $k\times k$ standard Gaussian matrix as $n \rightarrow\infty$.

\begin{proposition}\label{ch04:pro4.2.5}
Let $U(n)$ and $V(n)$ be independent $n\times n$ standard unitary random matrices. Then
\begin{equation*}
\lim_{n\rightarrow\infty}\tau_{n}(U(n)^{m_{1}}V(n)^{m_{2}}\cdots U(n)^{m_{2l-1}}V(n)^{m_{2l}})=0
\end{equation*}
whenever $m_{1}, m_{2}, \ldots, m_{2l}\in \mathbb{Z}$ and the matrix under the trace is different from the identity.
\end{proposition}

\begin{proof2}
Obviously we may assume that the integers $m_{r}$ are all nonzero, so we have the product of $k$ unitaries under the trace, $k:=|m_{1}|+|m_{2}|+\cdots+|m_{2l}|$. We write
\begin{align*}
u_{ij}(-1,1,n) & \ := \ U_{ij}(n)\,,\\
u_{ij}(-1, -1,n) & \ := \ \bar{U}_{ji}(n)\,,\\
u_{ij}(1,1, n) & \ := \ V_{ij}(n)\,,\\
u_{ij}(1, -1, n) & \ := \ \bar{V}_{ji}(n)\,,
\end{align*}
and express the trace of the product of the random unitaries in terms of the matrix elements, keeping the order of the factors:
\begin{equation}
\frac{1}{n}\sum_{i_{1,\ldots,}i_{k}=1}^{n}E\left(\prod_{h=1}^{k}u_{i_{h}j_{h}}(s(h), \varepsilon(h), n)\right),
\label{ch04:eqn4.2.12}
\end{equation}
where $j_{h}=i_{h+1}\ (1\leq h\leq k-1)$ and $j_{k}=i_{1}$.

When we group together entries of $U(n)$ (indexed by $s(h)=-1$) and those of $V(n)$ (indexed by $s(h)=1$) in each summand of (\ref{ch04:eqn4.2.12}), the expectation will factor because entries of $U(n)$ are independent of those of $V(n)$. In this way, many of the summands of (\ref{ch04:eqn4.2.12}) vanish according to Lemma~\ref{ch04:lem4.2.2}; for example, for an odd $k$, Lemma~\ref{ch04:lem4.2.2} implies that the whole expression is zero. We can restrict ourselves to the case of an even $k$. When $k$ is even, using the H\"{o}lder inequality and (\ref{ch04:eqn4.2.11}), we can estimate
\begin{align}
&\left|E\left(\prod_{h=1}^{k}u_{i_{h}j_{h}}(s(h), \varepsilon(h), n)\right)\right| \notag\\
& \qquad \,\leq\prod_{h=1}^{k}E(|u_{i_{h}j_{h}}(s(h), \varepsilon(h), n)|^{k})^{1/k}=O(n^{-k/2})
\label{ch04:eqn4.2.13}
\end{align}
uniformly for $i_{1}, j_{1}, \ldots, i_{k}, j_{k}$ as $n\rightarrow\infty$.

We want to analyze the structure of the nonzero terms of (\ref{ch04:eqn4.2.12}). When the term $E(\prod_{h=1}^{k}u_{i_{h}j_{h}}(s(h), \varepsilon(h), n))$ is nonzero, we can apply Lemma~\ref{ch04:lem4.2.2} to two factored expectations of products for $s(h)=-1$ and for $s(h)=1$, and we have, for each $1\leq i, j\leq n$ and $\delta\in\{-1,1\}$,
\begin{align*}
\#\{h:i_{h}=i,\,s(h)=\delta,\,\varepsilon(h)=1\} & \ = \ \#\{h:j_{h}=i,\,s(h)=\delta,\,\varepsilon(h)=-1\}\,,\\
\#\{h:j_{h}=j,\,s(h)=\delta,\,\varepsilon(h)=1\} & \ = \ \#\{h:i_{h}=j,\,s(h)=\delta,\,\varepsilon(h)=-1\}\,.
\end{align*}
Thus, two pair partitions $\mathcal{U}$ and $\mathcal{V}$ can be chosen so that if $\{h, h'\}\in \mathcal{U}$ then
\begin{equation*}
s(h)=s(h')\,, \quad \varepsilon(h)=1\,,\quad \varepsilon(h')=-1\,,\quad i_{h}=j_{h'}\,(=i_{h'+1})\,,
\end{equation*}
and if $\{h, h'\}\in \mathcal{V}$ then
\begin{equation*}
s(h)=s(h')\,, \quad \varepsilon(h)=-1\,,\quad \varepsilon(h')=1\,,\quad i_{h}=j_{h'}\,(=i_{h'+1})\,,
\end{equation*}
where either $h<h'$ or $h>h'$ may be chosen. For $\mathcal{U}, \mathcal{V}$ as above let $\mathcal{R}(\mathcal{U}, \mathcal{V})$ denote the equivalence relation on $[k]$ generated by $h\sim h'+1$ when $\{h, h'\}\in \mathcal{U}, \varepsilon(h)=1,\,\varepsilon(h')=-1$ or when $\{h, h'\}\in \mathcal{V},\,\varepsilon(h)=-1,\,\varepsilon(h')=1$. Also, let $\Xi_{n}(\mathcal{U}, \mathcal{V})$ denote the set of $(i_{1}, \ldots, i_{k})\in[n]^{k}$ subject to the structure determined by $\mathcal{U}$ and $\mathcal{V}$ above. Note that if $(i_{1}, \ldots, i_{k})\in\Xi_{n}(\mathcal{U}, \mathcal{V})$, then $i_{h}$ must be the same for all $h$ from an equivalence class of $\mathcal{R}(\mathcal{U}, \mathcal{V})$. Therefore $\#\Xi_{n}(\mathcal{U}, \mathcal{V})=n^{k_{0}}$, where $k_{0}$ is the number of equivalence classes of $\mathcal{R}(\mathcal{U}, \mathcal{V})$. Suppose that $\{h\}$ is a singleton equivalence class of $\mathcal{R}(\mathcal{U}, \mathcal{V})$. Then we must have both $\{h, h-1\}\in \mathcal{U},\, \varepsilon(h)=1,\,\varepsilon(h-1)=-1$ and $\{h, h-1\}\in \mathcal{V},\,\varepsilon(h)=-1,\, \varepsilon(h-1)=1$ (where $h-1$ is meant $\mathrm{mod}\ k$); however these two conditions cannot happen at the same time. So there is no singleton equivalence class in $\mathcal{R}(\mathcal{U}, \mathcal{V})$. This shows that $k_{0}\leq k/2$.

From the above argument together with (\ref{ch04:eqn4.2.13}) we obtain
\begin{equation*}
\sum_{(i_{1,\ldots},i_{k})\in\Xi_{n}(\mathcal{U},\mathcal{V})}\left|E\left(\prod_{h=1}^{k}u_{i_{h}j_{h}}(s(h), \varepsilon(h), n)\right)\right|\leq n^{k/2}O(n^{-k/2})=O(1).
\end{equation*}
Summing up for $\mathcal{U}, \mathcal{V}$ (the choice of $\mathcal{U}, \mathcal{V}$ being independent of $n$) yields
\begin{equation*}
\sum_{i_{1},\ldots,i_{k}=1}^{n}\left|E\left(\prod_{h=1}^{k}u_{i_{h}j_{h}}(s(h), \varepsilon(h), n)\right)\right|=O(1)\,,
\end{equation*}
showing that (\ref{ch04:eqn4.2.12}) is $O(n^{-1})$ as $n\rightarrow\infty$.
\end{proof2}

Let $u$ and $v$ be free (more precisely, $^{*}$-free) Haar unitaries.\index{asymptotically free!Haar unitaries} Moreover, for every $n \in \mathbb{N}$ let $U(n)$ and $V(n)$ be independent $n\times n$ standard unitary random matrices. Proposition~\ref{ch04:pro4.2.5} tells us that $(U(n), V(n))$ converges in distribution to $(u, v)$. Since $u$ and $v$ are in free\index{free!asymptotically} relation, we say that $U(n)$ and $V(n)$ are \textit{asymptotically free}.\index{asymptotically free!relation} More generally, if $(X(s, n))_{1\leq s\leq k}$ is a sequence of $k$-tuples of noncommutative random variables converging in distribution to $(X_{s})_{1\leq s\leq k}$, then $(X(s, n))_{1\leq s\leq k}$ is said to be asymptotically free if the limiting family $(X_{s})_{1\leq s\leq k}$ is free. The definition of asymptotic freeness extends also to an infinite family; however an infinite family is asymtotically free if and only if all finite subsets are so.

The result and the proof of Proposition~\ref{ch04:pro4.2.5} extend to arbitrarily many independent unitaries. Let $S$ be a set, and for $n\in \mathbb{N}$ let $(U(s, n))_{s\in S}$ be an independent family of $n\times n$ standard unitary random matrices. Then, a slight modification of the above proof shows that
\begin{equation*}
\lim_{n\rightarrow\infty}\tau_{n}(U(s_{1}, n)^{m_{1}}U(s_{2}, n)^{m_{2}}\cdots U(s_{l}, n)^{m_{l}})=0
\end{equation*}
for every $m_{1}, m_{2}, \ldots, m_{l}\in \mathbb{Z}\,\backslash\, \{0\}$ whenever $s_{1}, s_{2}, \ldots, s_{l}\in S$ satisfy $s_{1}\neq s_{2}\neq \ldots \neq s_{l}$. Thus, the distribution of $(U(s, n))_{s\in S}$ converges to that of free Haar unitaries, so that $(U(s, n))_{s\in S}$ is asymptotically free.

Both the concept of asymptotic freeness and the asymptotic freeness of independent Haar distributed unitaries are due to Voiculescu. This aspect of random matrix theory came into being under the influence of free probability.

Let $a$ and $b$ be free random variables in a $C^{*}$-probability space $(\mathcal{A}, \varphi)$. If $(d_{n})$ is a sequence converging to $0$ in norm, then $a+d_{n}$ and $b$ are asymptotically free as $n \rightarrow\infty$ (obviously). Such a perturbation of the free relation can lead to asymptotic freeness in a trivial way. The critical point in random matrix models is that the freeness comes into being only in the limit, and we do not have the perturbation of some a priori free variables. Random matrices are noncommutative random variables with respect to the functional $\tau_{n}$, but on the other hand we can study them almost everywhere with respect to the normalized trace $\mathrm{tr}_{n}$. This happened already in Theorem~\ref{ch04:the4.1.5}. In a similar spirit we are going to extend Proposition~\ref{ch04:pro4.2.5} as follows.

\begin{theorem}\label{ch04:the4.2.6}
Let $(U(s, n))_{s\in S}$ be an independent family of $n\times n$ standard unitary random matrices. Let $s_{1}, s_{2}, \ldots, s_{l}\in S$ be such that $s_{1}\neq s_{2}\neq\ldots\neq s_{l}$. Then, for every $m_{1}, m_{2}, \ldots, m_{l}\in \mathbb{Z}\,\backslash\, \{0\}$ and every $ 1\leq p<\infty$,
\begin{equation}
E(|\mathrm{tr}_{n}(U(s_{1}, n)^{m_{1}}U(s_{2}, n)^{m_{2}}\cdots U(s_{l}, n)^{m_{l}})|^{p})^{1/p}=O(n^{-1}),
\label{ch04:eqn4.2.14}
\end{equation}
and moreover
\begin{equation}
\mathrm{tr}_{n}(U(s_{1}, n)^{m_{1}}U(s_{2}, n)^{m_{2}}\cdots U(s_{l}, n)^{m_{l}})\rightarrow 0
\label{ch04:eqn4.2.15}
\end{equation}
almost everywhere as $n\rightarrow\infty$.
\end{theorem}

\begin{proof2}
First we prove (\ref{ch04:eqn4.2.14}) for the case $p=2$. As in the proof of Proposition~\ref{ch04:pro4.2.5}, let $k:=|m_{1}|+\cdots+|m_{l}|$, and when $|m_{1}|+\cdots+|m_{r-1}|+1\leq h\leq |m_{1}|+\cdots+|m_{r}|\ (1\leq r\leq l)$ set
\begin{equation*}
s(h):=s_{r}\,,\quad \varepsilon(h):=\left\{\begin{array}{ll}
1 & \mathrm{if}\ m_{r}>0,\\
-1 & \mathrm{if}\ m_{r}<0.\\
\end{array}\right.
\end{equation*}
Moreover, set $s(k+h) :=s(h)$ and $\varepsilon(k+h) :=-\varepsilon(h)$ for $1\leq h\leq k$. If we write
\begin{equation*}
u_{ij}(s, \varepsilon, n):=\left\{\begin{array}{l}
U_{ij}(s,n) \quad \mathrm{if}\ \varepsilon=1,\\
\bar{U}_{ji}(s,n) \quad \mathrm{if}\ \varepsilon=-1, \\
\end{array}\right.
\end{equation*}
then the square of the $L^{2}$-norm in (\ref{ch04:eqn4.2.14}) is given as
\begin{equation}
\left(\frac{1}{n}\right)^{2}\sum_{i_{1},\ldots,i_{k},i_{k+1}\ldots,i_{2k}=1}^{n}E\left(\prod_{h=1}^{2k}u_{i_{h}j_{h}}(s(h), \varepsilon(h), n)\right),
\label{ch04:eqn4.2.16}
\end{equation}
where
\begin{equation*}
\left\{\begin{array}{l}
j_{h}=i_{h+1}\ \,(1\leq h\leq k-1)\,,\quad j_{k}=i_{1}\,,\\
i_{h}=j_{h+1}\ \,(k+1\leq h\leq 2k-1)\,,\quad i_{2k}=j_{k+1}\,.
\end{array}\right.
\end{equation*}

Now we can proceed as in the proof of Proposition~\ref{ch04:pro4.2.5}. For any nonzero term in (\ref{ch04:eqn4.2.16}) we can choose two pair partitions $\mathcal{U}$ and $\mathcal{V}$ of $[2k]$ such that if $\{h, h'\}\in \mathcal{U}$ then $s(h)=s(h'),\,\varepsilon(h)=1,\,\varepsilon(h')=-1,\, i_{h}=j_{h'}$, and if $\{h, h'\}\in \mathcal{V}$ then $s(h)=s(h'),\,\varepsilon(h)=-1,\,\varepsilon(h')=1,\,i_{h}=j_{h'}$. These pair partitions cause many equalities among $i_{1}, \ldots, i_{2k}$ and define the equivalence relation $\mathcal{R}(\mathcal{U}, \mathcal{V})$ on $[2k]$ so that $i_{h}=i_{h'}$ whenever $h, h'$ are in the same equivalence class of $\mathcal{R}(\mathcal{U}, \mathcal{V})$. It follows that the number of equivalence classes of $\mathcal{R}(\mathcal{U}, \mathcal{V})$ is not more than $k$, because $\mathcal{R}(\mathcal{U}, \mathcal{V})$ contains no singleton equivalence classes. Thus (\ref{ch04:eqn4.2.16}) is $n^{-2}n^{k}O(n^{-k})= O(n^{-2})$ as $n\rightarrow\infty$, showing (\ref{ch04:eqn4.2.14}) for $p=2$.

What we have just shown implies that
\begin{equation*}
E\left(\sum_{n=1}^{\infty}|\mathrm{tr}_{n}(U(s_{1},\,n)^{m_{1}}\cdots U(s_{l}, n)^{m_{l}})|^{2}\right)<+\infty\,.
\end{equation*}
This yields the almost sure convergence (\ref{ch04:eqn4.2.15}); see the argument at the beginning of the proof of Theorem~\ref{ch04:the4.1.5}.

Finally, to prove (\ref{ch04:eqn4.2.14}) for general $p$, it is enough to show it when $p$ is an even integer, say $2d$, because $E(|\cdot |^{p})^{1/p}\leq E(|\cdot |^{2d})^{1/2d}$ for $p\leq 2d$. But the above argument works for this case as well, and we can get
\begin{equation*}
E(|\mathrm{tr}_{n}(U(s_{1}, n)^{m_{1}}\cdots U(s_{l}, n)^{m_{l}})|^{2d})=\left(\frac{1}{n}\right)^{2d}n^{dk}O(n^{-dk})=O(n^{-2d})
\end{equation*}
as $n\rightarrow\infty$.
\end{proof2}

Independent Haar distributed orthogonal matrices are \textit{asymptotically free}\index{asymptotically free!orthogonal matrices} as well. Our discussion on Haar distributed unitary matrices presented in this section can be repeated for Haar distributed orthogonal real matrices. Let $O=[x_{ij}]_{i,j=1}^{n}$ be an $n \times n$ \textit{standard orthogonal random matrix}\index{standard!orthogonal random matrix} distributed according to the Haar probability measure on $\mathcal{O}(n)$. The lemma taking the place of Lemma~\ref{ch04:lem4.2.2} says that if $i_{1}, \ldots, i_{l}, j_{1}, \ldots, j_{l}\in[n]$ and $E(x_{i_{1}j_{1}}x_{i_{2}j_{2}}\cdots x_{i_{k}j_{k}})\neq 0$, then $\#\{r:i_{r}=i\}$ and $\#\{r:j_{r}=j\}$ are even for every $i, j\in[n]$. Lemma~\ref{ch04:lem4.2.4} is modified as
\begin{equation}
E(x_{ij}^{2k})=\prod_{j=0}^{k-1}\frac{1+2j}{n+2j}=O(n^{-k})\quad \mathrm{as}\quad n\rightarrow\infty.
\label{ch04:eqn4.2.17}
\end{equation}
Indeed, to determine the distribution of $x_{ij}^{2}$ we may assume that
\begin{equation*}
x_{ij}^{2}=\frac{\xi_{1}^{2}}{\sum_{k=1}^{n}\xi_{k}^{2}}
\end{equation*}
with independent standard Gaussian variables $\xi_{j}$. Hence
\begin{equation*}
x_{ij}^{2}=\frac{\eta_{1}}{\eta_{1}+\eta_{2}},
\end{equation*}
where $\eta_{1}$ has the $\chi^{2}$-\textit{distribution}\index{$\chi^{2}$-distribution}\index{distribution!$\chi^{2}$} with 1 degree of freedom, $\eta_{2}$ has the same with $n-1$ degrees of freedom, and moreover they are independent. This implies ([\citen{bib161}], Chap.~\ref{ch03:chap03}a) that $x_{ij}^{2}$ has a \textit{beta distribution}\index{distribution!beta}\index{beta distribution}
\begin{equation}
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}
\label{ch04:eqn4.2.18}
\end{equation}
with parameters $\alpha=1/2$ and $\beta=(n-1)/2$. Consequently, (\ref{ch04:eqn4.2.17}) is obtained.

Now the proofs of Proposition~\ref{ch04:pro4.2.5} and Theorem~\ref{ch04:the4.2.6} work well for an independent Haar distributed family $(O(s, n))_{s\in S}$ of $n \times n$ random orthogonal matrices. In this way, the distribution of $(O(s, n))_{s\in S}$ converges (in the strong sense for Theorem~\ref{ch04:the4.2.6}) to that of free Haar unitaries.

In the next section we shall give more extended results on the asymptotic freeness of standard unitary random matrices, and also some selfadjoint or nonselfadjoint random matrices.

\section{Asymptotic freeness of some random matrices}
\label{ch04:sec4.3}

\noindent The main achievement of the previous section was the asymptotic freeness of sequences of independent Haar distributed unitary (or orthogonal) matrices as the matrix size goes to infinity. In this section similar results are established for some other random matrices: Independent standard unitary matrices and deterministic matrices are asymptotically free when the latter admit the limit distribution; moreover in place of standard unitary matrices we can put selfadjoint or non-selfadjoint random matrices with a unitarily invariant distribution (in particular, standard selfadjoint or standard non-selfadjoint Gaussians are so).

Let $S$ be a set. For $n\in \mathbb{N}$ let $(X(s, n))_{s\in S}$ be a family of $n\times n$ random matrices. $(X(s, n))_{s\in S}$ is said to have the \textit{limit distribution}\index{limit distribution} $\mu$ as $n \rightarrow\infty$ if $\mu$ is a distribution\index{distribution!limit} on $\mathbb{C}\langle X_{s}|s\in S\rangle$ and
\begin{equation*}
\mu(X_{s_{1}}X_{s_{2}}\cdots X_{s_{m}})=\lim_{n\rightarrow\infty}\tau_{n}(X(s_{1}, n)X(s_{2}, n)\cdots X(s_{m}, n))
\end{equation*}
for all $s_{1}, \ldots, s_{m}\in S$, where $\mathbb{C}\langle X_{s}|s\in S\rangle$ denotes the algebra of polynomials in noncommuting indeterminates $X_{s}\ (s\in S)$ over $\mathbb{C}$. (Recall that the joint distribition of noncommutative random variables as a functional was introduced in Sec.~\ref{ch01:sec1.2}). Let $\{S_{j}:j\in J\}$ be a partition of $S$. The meaning of the \textit{asymptotic freeness}\index{asymptotically free}\index{free!asymptotically} of $(\{X(s, n) : s\in S_{j}\})_{j\in J}$ is that on the one hand $(X(s, n))_{s\in S}$ has the limit distribution $\mu$, and on the other hand $(\mathbb{C}\langle X_{s}|s\in S_{j}\rangle)_{j\in J}$ is free in $(\mathbb{C}\langle X_{s}|s\in S\rangle, \mu)$. This notion is concerned with the convergence under the tracial states $\tau_{n}$. However, it is also natural to consider the almost everywhere convergence under the normalized traces $\mathrm{tr}_{n}$. Asymptotic freeness almost everywhere is a stronger property of random matrix models; it makes no sense in a general abstract setting of noncommutative random variables.

Given a family $(X(s, n))_{s\in S}$ of $n\times n$ random matrices for every $n\in \mathbb{N}$, we say that $(\{X(s, n) :s\in S_{j}\})_{j\in J}$ is \textit{asymptotically free almost everywhere}\index{free!asymptotically, almost everywhere}\index{asymptotically free!almost everywhere} as $n\rightarrow\infty$ if $(X(s, n))_{s\in S}$ has the (non-random) limit distribution almost surely, that is, there exists a distribution $\mu$ on $\mathbb{C}\langle X_{s}|s\in S\rangle$ such that for every $s_{1}, \ldots, s_{m}\in S$
\begin{equation*}
\lim_{n\rightarrow\infty}\mathrm{tr}_{n}(X(s_{1}, n)X(s_{2}, n)\cdots X(s_{m}, n))=\mu(X_{s_{1}}X_{s_{2}}\cdots X_{s_{m}}) \quad \mathrm{a.s.}
\end{equation*}
and if $(\mathbb{C}\langle X_{s}|s\in S_{j}\rangle)_{j\in J}$ is free in $(\mathbb{C}\langle X_{s}|s\in S\rangle, \mu)$. It is easy to see that this is equivalent to saying that the following two conditions hold:
\begin{enumerate}
\item[(i)] For each $j\in J, (X(s, n))_{s\in S_{j}}$ has the (non-random) limit distribution almost surely.

\item[(ii)] For every collection $j_{1}, \ldots, j_{l} \in J$ such that $j_{1} \neq j_{2} \neq \cdots \neq j_{l}$, if $P_{r}(X_{s_{r}(1)}, \ldots, X_{s_{r}(m_{r})})\in \mathbb{C}\langle X_{s}|s\in S_{j_{r}}\rangle$ satisfies
\begin{equation*}
\lim_{n\rightarrow\infty}\mathrm{tr}_{n}(P_{r}(X(s_{r}(1), n), \ldots, X(s_{r}(m_{r}), n)))=0\quad \mathrm{a.s.}
\end{equation*}
for $1\leq r\leq l$, then
\begin{equation*}
\lim_{n\rightarrow\infty}\mathrm{tr}_{n}\left(\prod_{r=1}^{l}P_{r}(X(s_{r}(1), n), \ldots, X(s_{r}(m_{r}), n))\right)=0\ \ \, \mathrm{a.s.}
\end{equation*}
\end{enumerate}

Indeed, note that (i) and (ii) together imply that the whole $(X(s, n))_{s\in S}$ has the almost sure limit distribution. (Prove by induction.)

For the random matrices $X(s, n)$ treated below we usually have
\begin{equation*}
\sup_{n}\tau_{n}((X(s, n)^{*}X(s, n))^{k})<+\infty\quad (s\in S,k\in \mathbb{N}).
\end{equation*}
This is the case if $X(s, n)^{*}X(s, n)$ has a limit distribution which is given as a compactly supported probability measure on $\mathbb{R}$. The above boundedness property implies that each sequence $\{\mathrm{tr}_{n}(\cdots)\}$ in (i) and (ii) is uniformly integrable, and the almost everywhere convergence yields the convergence of the expectations. Restricted to uniformly integrable matrix models, the definition of asymptotic freeness almost everywhere is actually a stronger property than plain asymptotic freeness. In the rest of the section we are concerned with asymptotic freeness in the almost everywhere sense.

\begin{theorem}\label{ch04:the4.3.1}
Let $(U(s, n))_{s\in S}$ be an independent family of $n \times n$ standard unitary random matrices. Let $(D(t, n))_{t\in T}$ be a family of $n\times n$ constant $($i.e. non-random$)$ matrices such that $\sup_{n}\Vert D(t, n)\Vert<+\infty$ for each $ t\in T\ (\Vert \cdot \Vert$ denotes the operator norm$)$ and $(D(t, n), D(t, n)^{*})_{t\in T}$ has the limit distribution. Then the family
\begin{equation*}
\left((\{U(s, n), U(s, n)^{*}\})_{s\in S}, \{D(t, n), D(t, n)^{*}:t\in T\}\right)
\end{equation*}
is asymptotically free almost everywhere as $n\rightarrow\infty$.
\end{theorem}\index{asymptotically free!Haar unitaries}

\begin{proof2}
Theorem~\ref{ch04:the4.2.6} says in particular that $(U(s, n), U(s, n)^{*})$ has the almost sure limit distribution. As is readily seen, we may assume without loss of generality that $\{(D(t, n))_{n\in \mathbb{N}}:t\in T\}$ forms a $^{*}$-subalgebra of $\prod_{n\in \mathbb{N}}M_{n}(\mathbb{C})$. (In fact, the $^{*}$-subalgebra of $\prod_{n\in \mathbb{N}}M_{n}(\mathbb{C})$ generated by $(D(t, n))_{n\in \mathbb{N}}\ (t\in T)$ and the identity $(I_{n})_{n\in \mathbb{N}}$ may be considered as $T$ itself.) Then it suffices (cf. the proof of Theorem~\ref{ch04:the4.2.6}) to prove that if $s_{1}, \ldots, s_{l}\in S,\,m_{1}, \ldots, m_{l}\in \mathbb{Z}\,\backslash \,\{0\}$ and $t_{1}, \ldots, t_{l}\in T$ are such that for $1\leq r\leq l$ either
\begin{equation*}
\mathrm{tr}_{n}(D(t_{r}, n))=0\ (n\in \mathbb{N})
\end{equation*}
or
\begin{equation*}
D(t_{r}, n)=I_{n}\ (n\in \mathbb{N})\quad \mathrm{and} \quad s_{r}\neq s_{r+1}\ (\mathrm{with}\ s_{l+1}=s_{1}),
\end{equation*}
then
\begin{align}
&E(|\mathrm{tr}_{n}(U(s_{1}, n)^{m_{1}}D(t_{1}, n)U(s_{2}, n)^{m_{2}}D(t_{2}, n)\cdots U(s_{l}, n)^{m_{l}}D(t_{l}, n))|^{2}) \notag\\
& \qquad =O(n^{-2})\quad \mathrm{as}\quad n\rightarrow\infty.
\label{ch04:eqn4.3.1}
\end{align}
(Here, note that the assumption $\mathrm{tr}_{n}(D(t_{r}, n))=0\ (n \in \mathbb{N})$ can be imposed instead of $\lim_{n}\mathrm{tr}_{n}(D(t_{r}, n))=0$, because we may replace $D(t_{r}, n)$ by $D(t_{r}, n)- \mathrm{tr}_{n}(D(t_{r}, n))I_{n}.)$

In addition to the notations in the proof of Theorem~\ref{ch04:the4.2.6}, set $k(r) :=|m_{1}|+\cdots +|m_{r}|,\,k(l+r) :=k+k(r)$, and $t_{l+r} :=t_{r}$ for $1\leq r\leq l$. Then the left-hand side of (\ref{ch04:eqn4.3.1}) is expressed as
\begin{align}
& \left(\frac{1}{n}\right)^{2} \sum_{i_{1},\ldots,i_{2k}=1}^{n}\, \sum_{j_{k(1),\ldots,}j_{k(l)},j_{k(l+1)+1,\ldots,}j_{k(2l)+1}=1}^{n}\left(\prod_{r=1}^{l}d_{j_{k(r)}i_{k(r)+1}}(t_{r}, n)\right) \notag\\
&\quad \qquad \qquad \quad \ \times\left(\prod_{r=l+1}^{2l}\overline{d}_{i_{k(r)}j_{k(r)+1}}(t_{r}, n)\right)E\left(\prod_{h=1}^{2k}u_{i_{h}j_{h}}(s(h), \varepsilon(h), n)\right),
\label{ch04:eqn4.3.2}
\end{align}
where $i_{k(l)+1}=i_{1},\, j_{k(2l)+1}=j_{k+1}$, and
\begin{equation}
\left\{\begin{array}{ll}
j_{h}=i_{h+1}& \mathrm{for}\ h\in\{1, \ldots, k\}\,\backslash\, \{k(1),\ldots,k(l)\},\\
i_{h}=j_{h+1}& \mathrm{for}\ h\in\{k+1,\ldots,2k\}\,\backslash\, \{k(l+1),\ldots,k(2l)\}.\\
\end{array}\right.
\label{ch04:eqn4.3.3}
\end{equation}

For any nonzero term of (\ref{ch04:eqn4.3.2}), choose two pair partitions $\mathcal{U}, \mathcal{V}$ of $[2k]$ as in the proof of Theorem~\ref{ch04:the4.2.6}. These $\mathcal{U}, \mathcal{V}$ together with (\ref{ch04:eqn4.3.3}) give many equalities among $i_{1}, \ldots, i_{2k}$, so the equivalence relation $\mathcal{R}(\mathcal{U}, \mathcal{V})$ on $[2k]$ is defined as in the proof of Theorem~\ref{ch04:the4.2.6}. However, singleton equivalence classes may appear in this case. If $\{h\}$ is a singleton equivalence class of $\mathcal{R}(\mathcal{U}, \mathcal{V})$, then the following must hold: When $1\leq h\leq k$, either $\{h, h-1\}\in \mathcal{U}, \varepsilon(h)=1,\,\varepsilon(h-1)=-1$ or $\{h, h-1\}\in \mathcal{V},\,\varepsilon(h)=-1,\,\varepsilon(h-1)=1$, and for some $1\leq r\leq l$
\begin{equation}
h=k(r)+1\,,\ s_{r}=s_{r+1}\,,\ j_{k(r)}=i_{k(r)+1}\,,\ \mathrm{tr}_{n}(D(t_{r}, n))=0\,(n\ \in \mathbb{N}),
\label{ch04:eqn4.3.4}
\end{equation}
where $h\in[k]$ and $r\in[l]$ are understood in the cyclic order. On the other hand, when $k+1\leq h\leq 2k$, either $\{h, h+1\}\in \mathcal{U},\, \varepsilon(h)=1,\,\varepsilon(h+1)=-1$ or $\{h,h+1\}\in \mathcal{V},\,\varepsilon(h)=-1,\,\varepsilon(h+1)=1$, and for some $l+1\leq r\leq 2l$
\begin{equation}
h=k(r)\,,\ s_{r}=s_{r+1}\,,\ i_{k(r)}=j_{k(r)+1}\,,\ \mathrm{tr}_{n}(D(t_{r}, n))=0\ (n\in \mathbb{N}),
\label{ch04:eqn4.3.5}
\end{equation}
where $h\in\{k+1, \ldots, 2k\}$ and $r\in\{l+1, \ldots, 2l\}$ are understood in the cyclic order.

Now, fix $\mathcal{U}, \mathcal{V}$ and let $h(1), \ldots, h(k_{0})$ be the representatives from the equivalence classes of $\mathcal{R}(\mathcal{U}, \mathcal{V})$ where $h(1), \ldots, h(l_{0})$ are from the singleton equivalence classes $(0\leq l_{0}\leq k_{0})$. It is obvious that
\begin{equation*}
k_{0}\leq l_{0}+\frac{2k-l_{0}}{2}=k+\frac{l_{0}}{2}.
\end{equation*}
When $(i_{1}, \ldots, i_{2k})$ is subject to $\mathcal{R}(\mathcal{U}, \mathcal{V})$, the terms of (\ref{ch04:eqn4.3.2}) are determined by $(\iota_{1}, \ldots, \iota_{k_{0}}):=(i_{h(1)}, \ldots, i_{h(k_{0})})$, so that we can set
\begin{align*}
&Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}}):=E\left(\prod_{h=1}^{2k}u_{i_{h}j_{h}}(s(h), \varepsilon(h), n)\right),\\
&C_{n}(\iota_{1}, \ldots, \iota_{k_{0}}):=\left(\prod_{r=1}^{l}d_{j_{k(r)}i_{k(r)+1}}(t_{r}, n)\right)\left(\prod_{r=l+1}^{2l}\bar{d}_{i_{k(r)}j_{k(r)+1}}(t_{r}, n)\right),
\end{align*}
where $j_{1}, \ldots, j_{2k}$ are determined subject to $\mathcal{U}, \mathcal{V}$, that is, $j_{h'}=i_{h}$ if $\varepsilon(h')=-1$ and $\{h, h'\}\in \mathcal{U}$, or if $\varepsilon(h')=1$ and $\{h, h'\}\in \mathcal{V}$. Then it remains to prove that for any partition $\mathcal{W}$ of $\{1, \ldots, k_{0}\}$ we have
\begin{equation}
\sum_{(\iota_{1},\ldots,\iota_{k_{0}}):\mathcal{W}}C_{n}(\iota_{1}, \ldots, \iota_{k_{0}})Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}})=O(1) \quad \mathrm{as}\quad n\rightarrow\infty,
\label{ch04:eqn4.3.6}
\end{equation}
where the summation is over $(\iota_{1}, \ldots, \iota_{k_{0}})\in[n]^{k_{0}}$ such that $\iota_{p}=\iota_{q}$ if $p, q$ are in the same block of $\mathcal{W}$ and otherwise $\iota_{p}\neq\iota_{q}$. Indeed, the sum in (\ref{ch04:eqn4.3.2}) can be divided into finite disjoint portions, each of which is written as the sum in (\ref{ch04:eqn4.3.6}) subject to some possible triple $(\mathcal{U}, \mathcal{V}, \mathcal{W})$.

First, assume $l_{0}\leq 1$, and so $k_{0}\leq k$. Since $C_{n}(\iota_{1}, \ldots, \iota_{k_{0}})$ are uniformly bounded and by (\ref{ch04:eqn4.2.13})
\begin{equation*}
\sum_{\iota_{1},\ldots,\iota_{k_{0}}=1}^{n}|Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}})|=n^{k_{0}}O(n^{-k})=O(1),
\end{equation*}
(\ref{ch04:eqn4.3.6}) holds for any $\mathcal{W}$.

Next assume $2\leq l_{0}\leq 3$, and so $k_{0}\leq k+1$. If $\#\mathcal{W}\leq k_{0}-1\ (\#\mathcal{W}$ denotes the number of blocks of $\mathcal{W}$), then
\begin{equation*}
\sum_{(\iota_{1},\ldots,\iota_{k_{0}}):\mathcal{W}}|Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}})|=n^{k_{0}-1}O(n^{-k})=O(1).
\end{equation*}
Assume that $\#\mathcal{W}=k_{0}$, so ``$(\iota_{1}, \ldots, \iota_{k_{0}}): \mathcal{W}$'' means that $\iota_{1}, \ldots, \iota_{k_{0}}$ are all distinct. Choose $\{h\}\in \mathcal{R}(\mathcal{U}, \mathcal{V})$ because $l_{0}\geq 2$; then according to (\ref{ch04:eqn4.3.4}) and (\ref{ch04:eqn4.3.5}) either
\begin{equation*}
d_{j_{k(r)}i_{k(r)+1}}(t_{r}, n)\ (=d_{i_{h}i_{h}}(t_{r}, n)) \quad \mathrm{for\ some}\quad 1\leq r\leq l
\end{equation*}
or
\begin{equation*}
\bar{d}_{i_{k(r)}j_{k(r)+1}}(t_{r}, n)\ (=\bar{d}_{i_{h}i_{h}}(t_{r}, n)) \quad \mathrm{for\ some} \quad l+1\leq r\leq 2l
\end{equation*}
appears only once in the product $C_{n}(\iota_{1}, \ldots, \iota_{k_{0}})$. So we may write
\begin{equation}
C_{n}(\iota_{1}, \ldots, \iota_{k_{0}})=\tilde{d}_{\iota_{1}\iota_{1}}(t_{r_{1}}, n)\tilde{C}_{n}(\iota_{2}, \ldots, \iota_{k_{0}}) \label{ch04:eqn4.3.7}
\end{equation}
for some $1 \leq r_{1}\leq 2l$, where $\tilde{d}_{\iota\iota} (\cdot, \cdot)$ means $d_{\iota\iota}(\cdot, \cdot)$ or $\bar{d}_{\iota\iota}(\cdot, \cdot)$. Note that the permutation invariance (\ref{ch04:eqn4.2.3}) implies that $Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}})$ has a constant value for all distinct $\iota_{1}, \ldots, \iota_{k_{0}}$. Hence we have
\begin{align}
&\left|\sum_{(\iota,\ldots,\iota_{k_{0}}):\mathcal{W}}C_{n}(\iota_{1}, \ldots, \iota_{k_{0}})Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}})\right|\notag\\
& \qquad =\left|\sum_{(\iota_{2},\ldots,\iota_{k_{0}}):\mathcal{W}}\left(\sum_{\iota_{1}\neq\iota_{2},\ldots,\iota_{k_{0}}}\tilde{d}_{\iota_{1}\iota_{1}}(t_{r_{1}},\ n)\right)\tilde{C}_{n}(\iota_{2}, \ldots, \iota_{k_{0}})Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}})\right| \notag\\
& \qquad =n^{k_{0}-1}O(n^{-k})=O(1),
\label{ch04:eqn4.3.8}
\end{align}
because
\begin{equation*}
\sum_{\iota_{1}\neq\iota_{2},\ldots,\iota_{k_{0}}}d_{\iota_{1}\iota_{1}}(t_{r_{1}}, n)=-\sum_{\iota_{1}=\iota_{2},\ldots,\iota_{k_{0}}}d_{\iota_{1}\iota_{1}}(t_{r_{1}}, n)=O(1)
\end{equation*}
due to $\mathrm{tr}_{n}(D(t_{r_{1}}, n))=0\ (n\in \mathbb{N})$.

Now assume $4\leq l_{0}\leq 5$, and so $k_{0}\leq k+2$. As above we get (\ref{ch04:eqn4.3.6}) for any $\mathcal{W}$ with $\#\mathcal{W}\leq k_{0}-2$. When $\#\mathcal{W}=k_{0}-1$, we can choose $\alpha\in\{1, \ldots, l_{0}\}$ such that $\{\alpha\}$ is a singleton block of $\mathcal{W}$. Then either $d_{\iota_{\alpha}\iota_{\alpha}}(t_{r}, n)$ or $\bar{d}_{\iota_{\alpha}\iota_{\alpha}}(t_{r}, n)$ appears only once in $C_{n}(\iota_{1}, \ldots, \iota_{k_{0}})$. Letting $\alpha=1$ and writing $C_{n}(\iota_{1}, \ldots, \iota_{k_{0}})$ as (\ref{ch04:eqn4.3.7}), we get the same expression as (\ref{ch04:eqn4.3.8}), so that (\ref{ch04:eqn4.3.6}) holds because $\#\mathcal{W}=k_{0}-1$ yields $\#\{(\iota_{2}, \ldots, \iota_{k_{0}}):\mathcal{W}\}=O(n^{k_{{0}-2}})$. When $\#\mathcal{W}=k_{0}$, we can write
\begin{equation*}
C_{n}(\iota_{1}, \ldots, \iota_{k_{0}})=\tilde{d}_{\iota_{1}\iota_{1}}(t_{r_{1}},n)\tilde{d}_{\iota_{2}\iota_{2}}(t_{r_{2}}, n)\tilde{C}_{n}(\iota_{3}, \ldots, \iota_{k_{0}})
\end{equation*}
for some $1\leq r_{1}, r_{2}\leq 2l$. Since $Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}})$ is constant for all distinct $\iota_{1}, \ldots, \iota_{k_{0}}$, we have
\begin{align*}
&\left|\sum_{(\iota_{1},\ldots,\iota_{k_{0}}):\mathcal{W}}C_{n}(\iota_{1}, \ldots, \iota_{k_{0}})Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}})\right|\\
& \qquad = \left|\sum_{(\iota_{3},\ldots,\iota_{k_{0}}):\mathcal{W}} \left(\sum_{\iota_{2}\neq \iota_{3},\ldots,\iota_{k_{0}}}\tilde{d}_{\iota_{2}\iota_{2}}(t_{r_{2}},n) \sum_{\iota_{1}\neq \iota_{2},\ldots,\iota_{k_{0}}} \tilde{d}_{\iota_{1}\iota_{1}}(t_{r_{1}},n)\right)\right.\\
& \qquad \qquad \qquad \qquad \qquad \qquad \quad \left. \times\tilde{C}_{n}(\iota_{3}, \ldots, \iota_{k_{0}})Q_{n}(\iota_{1}, \ldots, \iota_{k_{0}})\right|\\
& \qquad =n^{k_{0}-2}O(n^{-k})=O(1).
\end{align*}
We can proceed similarly in the case $6 \leq l_{0}\leq 7$ and so on, and the proof is completed.
\end{proof2}

It is noteworthy that there is no restriction on the type of constant matrices $D(t, n)$ in Theorem~\ref{ch04:the4.3.1}; the assumptions on them are only the boundedness (for separate $t$) and the existence of the limit distribution. Furthermore, the latter assumption was not used in the proof of (\ref{ch04:eqn4.3.1}), and we actually proved that the estimate (\ref{ch04:eqn4.3.1}) is uniform for $D(t, n)$ whenever $\Vert D(t, n)\Vert\leq R$ for some $R>0$. We here record this fact as a lemma for later use in Chap.~\ref{ch06:chap06}.

\begin{lemma}\label{ch04:lem4.3.2}
Let $(U(s, n))_{s\in S}$ be an independent family of $n \times n$ standard unitary random matrices. Let $s_{1}, \ldots, s_{l}\in S, m_{1},\, \ldots, m_{l}\in \mathbb{Z}\,\backslash\, \{0\}$ and $R>0$. Then
\begin{align*}
&E(|\mathrm{tr}_{n}(U(s_{1}, n)^{m_{1}}D_{1}(n)U(s_{2}, n)^{m_{2}}D_{2}(n)\cdots U(s_{l}, n)^{m_{l}}D_{l}(n))|^{2})\\
& \qquad =O(n^{-2}) \quad as \quad n\rightarrow\infty
\end{align*}
uniformly for the choice of any $D_{r}(n)\in M_{n}(\mathbb{C})\ (1\leq r\leq l)$ such that for $l\leq r\leq l$ either
\begin{equation*}
\mathrm{tr}_{n}(D_{r}(n)) =0 \quad and \quad \Vert D_{r}(n)\Vert\leq R\quad (n\in \mathbb{N})
\end{equation*}
or
\begin{equation*}
D_{r}(n)=I_{n}\ \,(n\ \in \mathbb{N})\quad and \quad s_{r}\neq s_{r+1}\quad (with\ s_{l+1}=s_{1}).
\end{equation*}
\end{lemma}

\begin{example}\label{ch04:exa4.3.3}
If the unitary $V(n) \in M_{n}(\mathbb{C})$ permutes the basis vectors cyclically and $v$ is a Haar unitary, then $(V(n), V(n)^{*})$ converges to $(v, v^{*})$ in distribution. In fact, we have $V(n)^{n}=I_{n}$ and $\mathrm{tr}_{n}(V(n)^{k})=0$ for $k=1,2, \ldots, n-1$. This implies that $\lim_{n}\,\mathrm{tr}_{n}(V(n)^{k})=0$ for all $k\in \mathbb{Z}\,\backslash\,\{0\}$. Further, let $U(n)$ be an $n\times n$ standard random unitary. Theorem~\ref{ch04:the4.3.1} implies that
\begin{equation*}
\lim_{n\rightarrow\infty}\mathrm{tr}_{n}(U(n)^{m_{1}} V(n)^{m_{2}}\cdots U(n)^{m_{2l-1}}V(n)^{m_{2l}})=0
\end{equation*}
almost surely for every $m_{1}, m_{2}, \ldots, m_{2l}\in \mathbb{Z}\ \backslash\ \{0\}$. Therefore $V(n)$ and $U(n)$ are asymptotically free almost everywhere. This tells us that if $(U(n))_{n\in \mathbb{N}}$ is chosen from $\prod_{n\in \mathbb{N}}\mathcal{U}(n)$ randomly (according to the Haar probability), then $(U(n), V(n))$ converges with probability $1$ to the free pair of Haar unitaries.
\end{example}

Next we prepare a technical lemma whose proof requires the notions of the weak topology and the L\'{e}vy metric\index{L\'{e}vy metric} on the space of probability measures. Let $X$ be a Polish space (i.e. a complete separable metric space), $C_{b}(X)$ the space of bounded continuous functions on $X$, and $\mathcal{M}(X)$ the space of probability Borel meausres on $X$. The \textit{weak topology}\index{weak topology} on $\mathcal{M}(X)$ is defined from the dual pair $\mu(f) :=\int\, f\, d\mu\ (\mu\in \mathcal{M}(X),\ f\in C_{b}(X))$; that is, $\mu_{n}\rightarrow\mu$ weakly means $\mu_{n}(f)\rightarrow\mu(f)$ for all $f\in C_{b}(X)$. The so-called \textit{L\'{e}vy metric} on $\mathcal{M}(X)$ is defined as
\begin{align}
\rho(\mu, \nu):=\inf\{\delta>0:\mu(F)&\leq \nu(F^{\delta})+\delta\ \mathrm{and}\ \nu(F)\leq\mu(F^{\delta})+\delta \label{ch04:eqn4.3.9}\\
&\ \hbox{for every closed}\ F\subset X\} \notag
\end{align}
$(\mu, \nu\in \mathcal{M}(X))$, where $F^{\delta} :=\{x\in X:d(x, F)<\delta\}$. A basic fact ([\citen{bib56}], Sec.~\ref{ch03:sec3.2}) is that the metric $\rho$ in (\ref{ch04:eqn4.3.9}) is compatible with the weak topology on $\mathcal{M}(X)$ and $(\mathcal{M}(X), \rho)$ is a Polish space.

In particular, let $X=\mathbb{R}$. In this case, several equivalent conditions for weak convergence are known. Indeed, the weak convergence $\mu_{n}\rightarrow\mu$ is equivalent to each of the following:
\begin{enumerate}
\item[(i)] the $\mathrm{w}^{*}$-convergence: $\mu_{n}(f)\rightarrow\mu(f)$ for all $f\in C_{0}(\mathbb{R})$, the space of continuous functions vanishing at infinity;

\item[(ii)] $\int e^{\mathrm{i}\,tx}\,d\mu_{n}(x)\rightarrow\int e^{\mathrm{i}\,tx}\,d\mu(x)$ for all $t\in \mathbb{R}$;

\item[(iii)] $\mu_{n}((-\infty, t])\rightarrow\mu((-\infty, t])$ for all $t\in \mathbb{R}$ at which $\mu((-\infty, t])$ is continuous.
\end{enumerate}
When each $\mu_{n}$ has all moments and $\mu$ is compactly supported, we note ([\citen{bib33}], Theorem 30.2) that if $\mu_{n}$ converges in distribution (i.e. in moments, $m_{k}(\mu_{n})\rightarrow m_{k}(\mu)$ for all $k\in \mathbb{N}$), then $\mu_{n}\rightarrow\mu$ weakly; however, the converse is not necessarily true unless the supports of the $\mu_{n}$ are uniformly bounded.

\begin{lemma}\label{ch04:lem4.3.4}
For every $p\geq 1,\ R>0$ and $\varepsilon >0$, there exist $k_{0}\in \mathbb{N}$  and $\delta>0$ such that, for every $n\in \mathbb{N}$ and every $\lambda, \eta\in \mathbb{R}_{\leq}^{n}$, if $|\eta_{i}|\leq R\ (1\leq i\leq n)$ and $|\sum_{i=1}^{n}\lambda_{i}^{k}-\sum_{i=1}^{n}\eta_{i}^{k}|\leq n\delta$ for all $1\leq k\leq k_{0}$, then $\sum_{i=1}^{n}|\lambda_{i}-\eta_{i}|^{p}\leq n\varepsilon$.
\end{lemma}

\begin{proof2}
Write $\kappa_{\lambda} :=\frac{1}{n}\sum_{i=1}^{n}\delta(\lambda_{i})\ (\in \mathcal{M}(\mathbb{R})),\, \Vert\lambda\Vert_{\infty} :=\max_{i}|\lambda_{i}|$ and $\Vert\lambda\Vert_{p} := (\tfrac{1}{n}\sum_{i}|\lambda_{i}|^{p})^{1/p}$ for $\lambda\in \mathbb{R}^{n}$. We may assume that $R=1$. First we prove the assertion under the assumption that $\Vert\lambda\Vert_{\infty}\leq 1$ as well as $\Vert\eta\Vert_{\infty}\leq 1$. From the above remarked fact on the weak topology, it suffices to show that $\Vert\lambda-\eta\Vert_{p}$ is arbitrarily small (independently of $n$) as $\rho(\kappa_{\lambda}, \kappa_{\eta})\rightarrow 0$. Let $m\in \mathbb{N}$ and divide $[-1, 1]$ into $J(l):=[(l-1)m^{-3}, lm^{-3})\,(-m^{3}+1\leq l\leq m^{3}-1)$ and $J(m^{3}):=[1-m^{-3},1]$. For each $j=-m+1, \ldots, m$, a number $l_{j}$ can be chosen from $\{(j-1)m^{2}+1, \ldots, jm^{2}\}$ so that $\kappa_{\lambda}(J(l_{j}))\leq m^{-2}$. Take closed intervals $F_{-m} :=[-1, (l_{-m+1}-1)m^{-3}], F_{j}:=[l_{j}m^{-3}, (l_{j+1}-1)m^{-3}]\ (-m<j<m)$, and $F_{m}:=[l_{m}m^{-3},1]$. Then the length of $F_{j}$ is less than $2m^{-1}$. If $\rho(\kappa_{\lambda}, \kappa_{\eta})<\delta :=m^{-3}/2$, then for $-m\leq j\leq m$ we get $\kappa_{\lambda}(F_{j})\leq\kappa_{\eta}(F_{j}^{\delta})+\delta$; that is,
\begin{equation*}
\#\{i:\lambda_{i}\in F_{j}\}\leq\#\{i:\eta_{i}\in F_{j}^{\delta}\}+n\delta.
\end{equation*}
Hence we can select $\lambda_{i}\ (i\in I_{j})$ from $\{\lambda_{1}, \ldots, \lambda_{n}\}\cap F_{j}$ so that
\begin{equation*}
\#\{i:\lambda_{i}\in F_{j}\}-n\delta \leq\# I_{j}\leq\#\{i:\eta_{i}\in F_{j}^{\delta}\}.
\end{equation*}
Since the $F_{j}^{\delta}$'s are disjoint, we have a permutation $\pi$ on $[n]$ such that $\eta_{\pi(i)}\in F_{j}^{\delta}$ for all $i\in I_{j}\ (-m\leq j\leq m)$. When the coordinates of $\lambda, \eta\in \mathbb{R}^{n}$ are increasingly arranged, the following is well-known:
\begin{equation*}
\sum_{i=1}^{n}|\lambda_{i}-\eta_{i}|^{p}\leq\sum_{i=1}^{n}|\lambda_{i}-\eta_{\pi(i)}|^{p}
\end{equation*}
for every permutation $\pi$. Since
\begin{align*}
\frac{1}{n}\sum_{j=-m}^{m}\# I_{j} & \ \geq \ \,\frac{1}{n}\sum_{j=-m}^{m}\#\{i:\lambda_{i}\in F_{j}\}-(2m+1)\delta\\
& \ \geq \ \,1-\kappa_{\lambda}\left(\bigcup_{j=-m+1}^{m}J(l_{j})\right)-2m^{-1}\geq 1-4m^{-1}\,,
\end{align*}
we get
\begin{align*}
\frac{1}{n}\sum_{i=1}^{n}|\lambda_{i}-\eta_{i}|^{p} \,& \ \,\leq \ \frac{1}{n}\sum_{j=-m}^{m} \sum_{\,i\in I_{j}}|\lambda_{i}-\eta_{\pi(i)}|^{p}+4m^{-1}2^{p}\\
& \ \,\leq \ \,(2m^{-1}+\delta)^{p}+2^{p+2}m^{-1}\leq(3^{p}+2^{p+2})m^{-1}.
\end{align*}

By the assertion proved above, given $\varepsilon >0$ there exist $k_{1}\in \mathbb{N}$ and $\delta>0$ with $\delta<\varepsilon/2$ such that if $\lambda, \eta\in \mathbb{R}_{\leq}^{n}$ satisfy $\Vert\lambda\Vert_{\infty}, \Vert\eta\Vert_{\infty}\leq 1$ and $|m_{k}(\kappa_{\lambda})-m_{k}(\kappa_{\eta})|\leq 2\delta$ for $1\leq k\leq k_{1}$, then $\Vert\lambda-\eta\Vert_{p}\leq\varepsilon/2$. For any $\lambda\in \mathbb{R}_{\leq}^{n}$ let $\tilde{\lambda}$ be given as
\begin{equation*}
\tilde{\lambda}_{i}:=\left\{\begin{array}{ll}
-1 & \mathrm{if}\ \lambda_{i}<-1,\\
\lambda_{i} & \mathrm{if}\ -1\ \leq\lambda_{i}\leq 1,\\
1 & \mathrm{if}\ \lambda_{i}>1.\\
\end{array}\right.
\end{equation*}
Then
\begin{equation*}
|m_{k}(\kappa_{\lambda})-m_{k}(\kappa_{\tilde{\lambda}})|\leq\frac{1}{n}\sum_{|\lambda_{i}|>1}(|\lambda_{i}|^{k}-1)\leq\left(\sup_{t\geq 1}\frac{t^{k}-1}{t^{r}}\right)m_{r}(\kappa_{\lambda})
\end{equation*}
and
\begin{equation*}
\frac{1}{n}\sum_{i=1}^{n}|\lambda_{i}-\tilde{\lambda}_{i}|^{p}=\frac{1}{n}\sum_{|\lambda_{i}|>1}(|\lambda_{i}|-1)^{p}\leq\left(\sup_{t\geq 1}\frac{t-1}{t^{r/p}}\right)^{p}m_{r}(\kappa_{\lambda})
\end{equation*}
for any $r\in \mathbb{N}$. Since $\sup_{t\geq 1}(t^{k}-1)/t^{r}\rightarrow 0$ as $r\rightarrow\infty$, we can choose $k_{0}\ (\geq k_{1})$ such that
\begin{equation*}
\sup_{t\geq 1}\frac{t^{k}-1}{t^{k_{0}}}\leq\sup_{t\geq 1}\frac{t^{k}-1}{t^{k_{0}/p}}\leq\frac{\delta}{1+\delta}\qquad (1 \leq k\leq k_{1}).
\end{equation*}
If $\lambda, \eta\in \mathbb{R}_{\leq}^{n}$ with $\Vert\eta\Vert_{\infty}\leq 1$ and $|m_{k}(\kappa_{\lambda})-m_{k}(\kappa_{\eta})|\leq\delta$ for $1\leq k\leq k_{0}$, then we have
\begin{align*}
&|m_{k}(\kappa_{\lambda})-m_{k}(\kappa_{\tilde{\lambda}})|\leq\frac{\delta}{1+\delta}(m_{k_{0}}(\kappa_{\eta})+\delta)\leq\delta \qquad (1\leq k\leq k_{1}),\\
&\frac{1}{n}\sum_{i=1}^{n}|\lambda_{i}-\tilde{\lambda}_{i}|^{p}\leq\left(\frac{\delta}{1+\delta}\right)^{p}(m_{k_{0}}(\kappa_{\eta})+\delta)\leq\delta^{p}.
\end{align*}
These estimates imply that $|m_{k}(\kappa_{\tilde{\lambda}})-m_{k}(\kappa_{\eta})|\leq 2\delta\ (1\leq k\leq k_{1})$ and
\begin{equation*}
\Vert\lambda-\eta\Vert_{p}\leq\Vert\lambda-\tilde{\lambda}\Vert_{p}+\Vert\tilde{\lambda}-\eta\Vert_{p}\leq\delta+\varepsilon/2\leq\varepsilon\,.
\end{equation*}
\end{proof2}

We say that an $n \times n$ selfadjoint random matrix $T$ is \textit{unitarily invariant} if the distribution on $M_{n}(\mathbb{C})^{sa}$ of $T$ is equal to that of the unitary transformation $VTV^{*}$ for any $V\in \mathcal{U}(n)$. A function of a standard selfadjoint random Gaussian matrix is unitarily invariant. We are in a position to show the asymptotic freeness of an independent family of unitarily invariant selfadjoint random matrices and non-random matrices.

In the proof we shall use, besides the operator norm $\Vert A\Vert$, the Schatten $p$-norm (with respect to $\mathrm{tr}_{n}$) $\Vert A\Vert_{p}:=\mathrm{tr}_{n}(|A|^{p})^{1/p}$ for $A\in M_{n}(\mathbb{C})$, where $1\leq p<\infty$. Note that $|\mathrm{tr}_{n}(A)|\leq\Vert A\Vert_{1}\leq\Vert A\Vert_{p}\leq\Vert A\Vert$. The H\"{o}lder inequality $\Vert AB\Vert_{r}\leq\Vert A\Vert_{p}\Vert B\Vert_{q}$ when $1/r=1/p+1/q$ is particularly useful.

\begin{theorem}\label{ch04:the4.3.5}
Let $(H(s, n))_{s\in S}$ be an independent family of $n\times n$ unitarily invariant\index{unitarily invariant random matrix}\index{unitarily invariant} selfadjoint random matrices, and let $(D(t, n))_{t\in T}$ be as in Theorem~\emph{\ref{ch04:the4.3.1}}. If $H(s, n)$ converges in distribution $($with respect to $\mathrm{tr}_{n})$ almost surely to a compactly supported $\rho_{s}\in\mathcal{M}(\mathbb{R})$ for each $s\in S$, then the family
\begin{equation*}
\left((\{H(s, n)\})_{s\in S}, \{D(t, n), D(t, n)^{*}:t\in T\}\right)
\end{equation*}
is asymptotically\index{asymptotically free!unitarily invariant selfadjoint matrix} free almost everywhere as $n\rightarrow\infty$.
\end{theorem}

\begin{proof2}
Take the diagonalization
\begin{equation*}
H(s, n)=U(s, n)\Lambda(s, n)U(s, n)^{*},
\end{equation*}
where $U(s, n)$ is a unitary random matrix and
\begin{equation*}
\Lambda(s, n)= \mathbf{Diag} (\lambda_{1}(s, n), \ldots, \lambda_{n}(s, n))
\end{equation*}
is a diagonal random matrix such that $\lambda_{1}(s, n)\leq\lambda_{2}(s, n)\leq\ldots\leq\lambda_{n}(s, n)$. We can make $(\{U(s, n), \Lambda(s, n)\})_{s\in S}$ an independent family. Choose an independent family $(V(s, n))_{s\in S}$ of standard unitary matrices which are also independent of $(\{U(s, n), \Lambda(s, n)\})_{s\in S}$. Then $(V(s, n)U(s, n))_{s\in S}$ becomes an independent family of standard unitary matrices, and $V(s, n)H(s, n)V(s, n)^{*}$ has the same distribution as $H(s, n)$ due to the unitary invariance. In this way, we may assume without loss of generality that $(U(s, n))_{s\in S}$ is an independent family of standard unitary matrices.

By assumption, for each $s\, \in\, S$ the empirical eigenvalue distribution $\frac{1}{n}\sum_{i=1}^{n}\delta(\lambda_{i}(s, n))$ converges in distribution to a compactly supported measure $\rho_{s}$ almost surely as $n\rightarrow\infty$. We can choose (non-random) $\xi_{1}(s, n) \leq\xi_{2}(s, n)\leq\ldots\leq \xi_{n}(s, n)$ in $\mathrm{supp}\,\rho_{s}$ such that $\tfrac{1}{n}\sum_{i=1}^{n}\delta(\xi_{i}(s, n))\rightarrow\rho_{s}$ in distribution as $n\rightarrow\infty$. Now set $\Xi(s, n) := \mathbf{Diag}(\xi_{1}(s, n), \ldots, \xi_{n}(s, n))$. Then for every $s\in S$ Lemma~\ref{ch04:lem4.3.4} implies that
\begin{equation}
\lim_{n\rightarrow\infty}\Vert\Lambda(s, n)-\Xi(s, n)\Vert_{p}=0 \quad \mathrm{a.s.} \qquad (p\geq 1).
\label{ch04:eqn4.3.10}
\end{equation}
For any $m\in \mathbb{N}$ and $p\geq 1$, using the H\"{o}lder inequality we get
\begin{align*}
&\Vert\Lambda(s, n)^{m}-\Xi(s, n)^{m}\Vert_{p}\\
&\qquad \leq\sum_{j=1}^{m}\Vert\Lambda(s,n)\Vert_{mp}^{m-j}\Vert\Lambda(s, n) -\Xi(s, n)\Vert_{mp}\Vert\Xi(s, n)\Vert^{j-1}\rightarrow 0\ \ \mathrm{a.s.}
\end{align*}
due to (\ref{ch04:eqn4.3.10}). Hence for any polynomial $P$ and $p\geq 1$ we have
\begin{equation}
\lim_{n\rightarrow\infty}\Vert P(\Lambda(s, n))-P(\Xi(s, n))\Vert_{p}=0\ \,\mathrm{a.s.}
\label{ch04:eqn4.3.11}
\end{equation}

To prove the result, we may assume as in the proof of Theorem~\ref{ch04:the4.3.1} that $\{(D(t, n))_{n\in \mathbb{N}}:t\in T\}$ forms a $^{*}$-subalgebra of $\prod_{n\in \mathbb{N}}M_{n}(\mathbb{C})$. We have to prove that if $s_{1}, \ldots, s_{l}\in S,\ P_{1}, \ldots, P_{l}\in \mathbb{C}\langle X\rangle$ and $t_{1}, \ldots, t_{l}\in T$ are such that for $1\leq r\leq l$
\begin{equation}
\lim_{n\rightarrow\infty}\mathrm{tr}_{n}(P_{r}(H(s_{r}, n)))=0\ \,\mathrm{a.s.}
\label{ch04:eqn4.3.12}
\end{equation}
and either
\begin{equation*}
\lim_{n\rightarrow\infty}\mathrm{tr}_{n}(D(t_{r}, n))=0
\end{equation*}
or
\begin{equation*}
D(t_{r}, n)=I_{n}\quad (n\in \mathbb{N}) \quad \mathrm{and}\quad s_{r}\neq s_{r+1},
\end{equation*}
then
\begin{equation*}
\lim_{n\rightarrow\infty}\ \mathrm{tr}_{n}\left(\prod_{r=1}^{l}P_{r}(H(s_{r}, n))D(t_{r}, n)\right)=0 \quad \mathrm{a.s.},
\end{equation*}
that is,
\begin{equation}
\lim_{n\rightarrow\infty}\ \mathrm{tr}_{n}\left(\prod_{r=1}^{l}U(s_{r}, n)P_{r}(\Lambda(s_{r}, n))U(s_{r}, n)^{*}D(t_{r}, n)\right)=0\ \,\mathrm{a.s.}
\label{ch04:eqn4.3.13}
\end{equation}
Using the H\"{o}lder inequality again, we get
\begin{align*}
&\left| \mathrm{tr}_{n}\left(\prod_{r=1}^{l}U(s_{r}, n)P_{r}(\Lambda(s_{r}, n))U(s_{r},n)^{*}D(t_{r}, n)\right)\right.\\
& \qquad \qquad \qquad \qquad \left.-\mathrm{tr}_{n}\left(\prod_{r=1}^{l}U(s_{r}, n)P_{r}(\Xi(s_{r}, n))U(s_{r}, n)^{*}D(t_{r}, n)\right)\right|\\
& \quad \quad \leq\sum_{m=1}^{l}\left(\prod_{r=1}^{m-1}\Vert P_{r}(\Xi(s_{r}, n))\Vert\right)\Vert P_{m}(\Lambda(s_{m}, n))-P_{m}(\Xi(s_{m}, n))\Vert_{l}\\
&\qquad \qquad \qquad \qquad \times\left(\prod_{r=m+1}^{l}\Vert P_{r}(\Lambda(s_{r}, n))\Vert_{l}\right)\prod_{r=1}^{l}\Vert D(t_{r}, n)\Vert\\
&\qquad \ \rightarrow 0 \ \,\mathrm{a.s.}
\end{align*}
by (\ref{ch04:eqn4.3.11}). On the other hand, since $\mathrm{tr}_{n}(P_{r}(\Xi(s_{r}, n)))\rightarrow 0$ by (\ref{ch04:eqn4.3.11}) and (\ref{ch04:eqn4.3.12}), Theorem~\ref{ch04:the4.3.1} implies that
\begin{equation*}
\lim_{n\rightarrow\infty}\mathrm{tr}_{n}\left(\prod_{r=1}^{l}U(s_{r}, n)P_{r}(\Xi(s_{r},n))U(s_{r}, n)^{*}D(t_{r}, n)\right)=0\ \,\mathrm{a.s.}
\end{equation*}
Here it should be remarked that each term $P_{r}(\Xi(s_{r}, n))$ is separated from $D(t_{r'}, n)$ by $U(s_{r}, n)$ or $U(s_{r}, n)^{*}$, so we do not need to assume the existence of the limit distribution of $(D(t, n))_{t\in T}$ and $(\Xi(s, n))_{s\in S}$ combined. Thus (\ref{ch04:eqn4.3.13}) follows.
\end{proof2}

By Theorems~\ref{ch04:the4.1.7} and \ref{ch04:the4.3.5} we have

\begin{corollary}\label{ch04:cor4.3.6}
Let $(H(s, n))_{s\in S}$ be an independent family of $n\times n$ standard self-adjoint Gaussian matrices,\index{asymptotically free!selfadjoint Gaussian matrix} and let $(D(t, n))_{t\in T}$ be as in Theorem~\emph{\ref{ch04:the4.3.1}}. Then the family
\begin{equation*}
\left((\{H(s, n)\})_{s\in S}, \{D(t, n), D(t, n)^{*}:t\in T\}\right)
\end{equation*}
is asymptotically free almost everywhere as $n\rightarrow\infty$.
\end{corollary}

The corollary contains the asymptotic freeness of independent standard selfadjoint Gaussian matrices in the strong sense. As we noted at the begining of the section, this implies asymptotic freeness in the mean:
\begin{equation*}
\tau_{n}(H(s(1), n)H(s(2), n)\cdots H(s(m), n))\rightarrow\tau(a_{s(1)}a_{s(2)}\cdots a_{s(m)}),
\end{equation*}
where $(a_{s})_{s\in S}$ is a free family of semicircular elements. Since asymptotic freeness is a description of joint moments of independent selfadjoint Gaussian matrices, it could be regarded as a deep extension of the Wigner theorem. Thorbj{\o}rnsen [\citen{bib189}] generalized the combinatorial formula (\ref{ch04:eqn4.1.19}) to several independent matrices:
\begin{align}
&\tau_{n}(H(s(1), n)H(s(2), n)\cdots H(s(m), n)) \notag\\
& \qquad =\sum_{\gamma\in\Gamma(s(1),s(2),\ldots,s(m))}n^{d(\gamma)-k-1},
\label{ch04:eqn4.3.14}
\end{align}
where $\Gamma(s(1), s(2), \ldots, s(m))$ is the set of permutations $\gamma$ in the \textit{symmetric group}\index{group!symmetric}\index{symmetric group} $\mathbf{S}_{m}$ which satisfy the conditions $\gamma(j)\neq j,\, \gamma(\gamma(j))=j$ and $s(j)=s(\gamma(j))$ for all $1\leq j\leq m$. Since $\tau(a_{s(1)}a_{s(2)}\cdots a_{s(m)})=\#\Gamma(s(1), s(2), \ldots, s(m))$, the asymptotic freeness follows from (\ref{ch04:eqn4.3.14}).

Continuing the last discussion of the previous section, one can conclude that Theorem~\ref{ch04:the4.3.1} holds analogously when $(U(s,n))_{s\in S}$ is replaced by an independent family $(O(s, n))_{s\in S}$ of standard orthogonal random matrices. In this way, one can similarly prove the previous theorem in the case when $(H(s, n))_{s\in S}$ is an independent family of real symmetric random matrices having an orthogonally invariant distribution and an almost sure limit distribution (in particular, the $H(s, n)$ could be independent standard real symmetric Gaussian matrices). Furthermore, one can apply the method in the proof of Theorem~\ref{ch04:the4.3.5} to extend Theorem~\ref{ch04:the4.3.1} itself to the case when $(U(s, n))_{s\in S}$ is an independent family of unitary random matrices such that the distribuion of $U(s, n)$ is equal to that of $VU(s, n)V^{*}$ for any $V\in \mathcal{U}(n)$ and such that $U(s, n)$ converges in distribution almost surely to a measure on $\mathbb{T}$. (Such random unitaries are considered in Sec.~\ref{ch05:sec5.4}.)

\begin{example}\label{ch04:exa4.3.7}
Let $T(n)$ be a sequence of real symmetric random matrices such that the $T_{ij}(n)\ (1\leq i\leq j\leq n)$ are independent Gaussian random variables with mean $m/n$ and variance $(1+\delta_{ij})\sigma^{2}/n$. Then the almost sure limiting eigenvalue density of $T(n)$ is the semicircle law with mean $0$ and variance $\sigma^{2}$.

Indeed, we decompose $T(n)$ as the sum of a non-random matrix $D(n)$ with all entries equal to $m/n$ and a centered Gaussian random matrix $T_{0}(n)$. The asymptotic eigenvalue distribution of the latter is the semicircle law $w$, due to Theorem~\ref{ch04:the4.1.5}. The first summand is $m$ times a rank one projection, so the limiting eigenvalue distribution is $\delta_{0}$. Since $D(n)$ and $T_{0}(n)$ are asymptotically free\index{asymptotically free!orthogonal matrices} almost everywhere, the limiting eigenvalue distribution of $T(n)$ is $\delta_{0} \boxplus w=w$.
\end{example}

A consequence of Corollary~\ref{ch04:cor4.3.6} is that a sequence of standard non-selfadjoint Gaussian matrices forms a matrix model of the \textit{circular distribution}.\index{circular!distribution, random matrix model} An $n\times n$ standard non-selfadjoint Gaussian matrix $X(n)$ is given in the form $X(n)\,=\, (H^{(1)}(n)+\mathrm{i}\,H^{(2)}(n))/\sqrt{2}$, where $H^{(1)}(n)$ and $H^{(2)}(n)$ are independent standard selfadjoint Gaussian matrices. Corollary~\ref{ch04:cor4.3.6} shows that $(H^{(1)}(n), H^{(2)}(n))$ has the limit distribution almost surely, and it is the distribution of the free pair $(a, b)$ of standard semicircular elements. So the distribution of $(X(n), X(n)^{*})$ converges to that of $(c, c^{*})$, where $c=(a+\mathrm{i}\,b)/\sqrt{2}$ is a circular element. A free family of circular elements may be easily represented in terms of creation operators in the full Fock space. Let $\mathcal{H}$ be a Hilbert space with an orthonormal basis $\{f_{s}\}_{s\in S}\cup\{g_{s}\}_{s\in S}$. Set $c_{s} :=\ell(f_{s})^{*}+\ell(g_{s})$ on the Fock space $\mathcal{F}(\mathcal{H})$. When the vacuum state is considered, $(c_{s})_{s\in S}$ constitutes a free family of circular elements, as is seen from the proof of Example~\ref{ch02:exa2.6.2}. The following is the asymptotic freeness of standard non-selfadjoint Gaussian matrices.

\begin{corollary}\label{ch04:cor4.3.8}
Let $(X(s, n))_{s\in S}$ be an independent family of $n\times n$ standard non-selfadjoint Gaussian matrices, and let $(D(t, n))_{t\in T}$ be as in Theorem~\emph{\ref{ch04:the4.3.1}}. Then the family
\begin{equation*}
\left((\{X(s, n), X(s, n)^{*}\})_{s\in S},\, \{D(t, n), D(t, n)^{*}:t\in T\}\right)
\end{equation*}
is asymptotically free almost everywhere as $n\rightarrow\infty$. Moreover, the limit distribution of $(X(s, n), X(s, n)^{*})_{s\in S}$ is the distribution of a free family $(c_{s}, c_{s}^{*})_{s\in S}$ of circular elements given above.
\end{corollary}

\begin{proof2}
The family $(X(s, n))_{s\in S}$ can be written as
\begin{equation*}
X(s, n) =\frac{H^{(1)}(s,n)+\mathrm{i}\,H^{(2)}(s,n)}{\sqrt{2}}\,,
\end{equation*}
where $(H^{(1)}(s, n))_{s\in S}\cup(H^{(2)}(s, n))_{s\in S}$ is an independent family of $n\times n$ standard selfadjoint Gaussians. Corollary~\ref{ch04:cor4.3.6} says that
\begin{equation*}
\left((\{H^{(1)}(s, n)\})_{s\in S},\, (\{H^{(2)}(s, n)\})_{s\in S}, \{D(t, n), D(t, n)^{*}:t\in T\}\right)
\end{equation*}
is asymptotically free almost everywhere. In view of Proposition~\ref{ch02:pro2.2.8} this implies that so is the family
\begin{equation*}
\left((\{H^{(1)} (s, n), H^{(2)}(s, n)\})_{s\in S}, \{D(t, n), D(t, n)^{*}:t\in T\}\right),
\end{equation*}
which means the first assertion. The assertion on the limit distribution is immediate from the discussion just before the corollary.
\end{proof2}

By an obvious alteration in the statement and proof, the corollary easily extends to elliptic Gaussian random matrices.

The free convolution may be used to compute the asymptotic eigenvalue distribution of the sum of independent random matrices. Informally, the next result says that if $\mu$ is the eigenvalue distribution of a huge selfadjoint random matrix\index{random matrix!bi-unitarily invariant} $A$ and similarly $\nu$ is the eigenvalue distribution of $B$, then $\mu \boxplus \nu$ is nearly the eigenvalue distribution of $A+B$ when $A$ and $B$ are in generic position.

\begin{proposition}\label{ch04:pro4.3.9}
For $n\in \mathbb{N}$ let $A(n)$ and $B(n)$ be $n\times n$ selfadjoint random matrices, and let $U(n)$ be an $n\times n$ standard unitary random matrix independent of $A(n), B(n)$. Assume that the empirical eigenvalue distribution of $A(n)$ $($resp. $B(n))$ converges in distribution almost surely to a compactly supported probability measure $\mu$ $($resp. $\nu)$. Then $(A(n), U(n)B(n)U(n)^{*})$ is asymptotically free almost everywhere, and the limiting eigenvalue distribution of $A(n)+U(n)B(n)U(n)^{*}$ is the free convolution $\mu \boxplus \nu$.
\end{proposition}

\begin{proof2}
Write
\begin{align*}
A(n) & \ \,= \ \,V_{1}(n)\mathbf{Diag}(a_{1}(n),\ldots, a_{n}(n))V_{1}(n)^{*},\\
B(n) & \ \,= \ \,V_{2}(n)\mathbf{Diag}(b_{1}(n),\ldots, b_{n}(n))V_{2}(n)^{*},
\end{align*}
where $ a_{1}(n)\leq\ldots \leq a_{n}(n),\,b_{1}(n) \leq\ldots\leq b_{n}(n)$ and $V_{1}(n), V_{2}(n)$ are random unitaries. The assumption guarantees that $U(n)$ is independent of $V_{1}(n), V_{2}(n)$. Hence $V_{1}(n)^{*}U(n)V_{2}(n)$ is still a standard unitary matrix. So we may assume that $A(n),B(n)$ are diagonal random matrices whose diagonals are ordered increasingly (though $U(n)$ is no longer independent of $A(n), B(n)$). As in the proof of Theorem~\ref{ch04:the4.3.5}, choose (non-random) $\varphi_{1}(n)\leq\varphi_{2}(n) \leq\ldots\leq\varphi_{n}(n)$ in $\mathrm{supp}\, \mu$ such that $\tfrac{1}{n}\sum_{i=1}^{n}\delta(\varphi_{i}(n))\rightarrow\mu$ in distribution as $ n\rightarrow\infty$, and set $\Phi(n):=\mathbf{Diag} (\varphi_{1}(n), \ldots, \varphi_{n}(n))$. Similarly set $\Psi(n):= \mathbf{Diag} (\psi_{1}(n), \ldots, \psi_{n}(n))$ for $\nu$. Then Lemma~\ref{ch04:lem4.3.4} implies that $\lim_{n}\Vert A(n)-\Phi(n)\Vert_{p} = 0$ a.s. and $\lim_{n}\Vert B(n)-\Psi(n)\Vert_{p}\,=\,0$ a.s. for all $p\geq 1$. Now the proof using Theorem~\ref{ch04:the4.3.1} goes through similarly to the proof of Theorem~\ref{ch04:the4.3.5}, so we omit the details. The second assertion is immediate from the first.
\end{proof2}

Let $H(n)$ be a sequence of real symmetric or complex selfadjoint Gaussian random matrices which constitutes a standard matrix model of the semicircle law $w_{r}$. Furthermore, let $A(n)$ be a sequence of diagonal random matrices with increasingly ordered diagonals such that the empirical density of $A(n)$ converges to a compactly supported measure $\mu$ almost surely. Since $H(n)$ is diagonalized by a standard orthogonal or unitary matrix, it follows as in Proposition~\ref{ch04:pro4.3.9} that the almost sure limiting eigenvalue distribution of $A(n)+H(n)$ is the free convolution\index{free!convolution, additive}\index{additive free convolution} $\mu \boxplus w_{r}$. The measure $\mu \boxplus w_{r}$ can be computed in principle by the $R$-series. Pastur called this diagonal perturbation of the Gaussian matrix the \textit{deformed Wigner ensemble}\index{deformed!Wigner ensemble}\index{ensemble!deformed Wigner} and its limiting eigenvalue distribution the \textit{deformed semicircle law}.\index{deformed!semicircle law}\index{law!deformed semicircle} Historically, this was much earlier than the breakthrough of free probability.

We say that an $n\times n$ random matrix $T$ is \textit{bi-unitarily invariant}\index{bi-unitarily invariant random matrix} if the distribution on $M_{n}(\mathbb{C})$ of $T$ is equal to that of $V_{1}TV_{2}$ for any $V_{1}, V_{2}\in \mathcal{U}(n)$. Let $M_{n}(\mathbb{C})^{+}$ denote the set of positive semidefinite matrices in $M_{n}(\mathbb{C})$. For a non-singular $X\in M_{n}(\mathbb{C})$ one has a unique polar decomposition $X=UH$ with $U\in \mathcal{U}(n)$ and $H=|X|= (X^{*}X)^{1/2}\in M_{n}(\mathbb{C})^{+}$.

\begin{lemma}\label{ch04:lem4.3.10}
An $n\times n$ random matrix $T$ is bi-unitarily invariant if and only if its distribution on $M_{n}(\mathbb{C})$ is equal to that of a random matrix of the form $UH$ such that
\begin{enumerate}
\item[(1)] $U$ is an $n\times n$ standard unitray random matrix,

\item[(2)] $H$ is an $n\times n$ unitarily invariant positive semidefinite random matrix, and

\item[(3)] $U$ and $H$ are independent.
\end{enumerate}
\end{lemma}

\begin{proof2}
Let $U$ and $H$ be as stated in (1)--(3). For any $V_{1}, V_{2}\in \mathcal{U}(n)$ it follows that the distributions on $M_{n}(\mathbb{C})$ of $V_{1}(UH)V_{2}=(V_{1}UV_{2})(V_{2}^{*}HV_{2})$ and $UH$ are the same. Hence $UH$ is bi-unitarily invariant.

Conversely, assume that $T$ is a bi-unitarily invariant random matrix defined on a probability space $(\Omega, \mathbf{P})$. Here we write the underlying probability space explicitly to make the proof precise. Let $T=U_{0}H$ be the polar decomposition with a unitary random matrix $U_{0}$ and $H=(T^{*}T)^{1/2}$. Note that $H$ is unique while $U_{0}$ is not. The bi-unitary invariance of $T$ implies the unitary invariance of $H$. Now choose a standard unitary matrix $V$ on another probability space $(\Omega', \mathbf{P}')$ and define a unitary random matrix $U(\omega', \omega) :=V(\omega')U_{0}(\omega)$ on $(\Omega'\times\Omega, \mathbf{P}'\otimes \mathbf{P})$. It is immediate to see that the distributions of $T$ and $VT=UH$ are the same. For any Borel sets $\Gamma\subset \mathcal{U}(n)$ and $\Xi\subset M_{n}(\mathbb{C})$ we have
\begin{align*}
&(\mathbf{P}'\otimes \mathbf{P})(U\in\Gamma,H\in\Xi)\\
& \qquad =\int\left(\int\chi_{\Gamma}(V(\omega')U_{0}(\omega))\,d\mathbf{P}'(\omega')\right)\chi_{\Xi}(H(\omega))\,d\mathbf{P}(\omega)\\
& \qquad =\gamma_{n}(\Gamma)\mathbf{P}(H\in\Xi)\,,
\end{align*}
where $\gamma_{n}$ is the Haar measure on $\mathcal{U}(n)$. This shows that $U$ is Haar distributed and $U, H$ are independent. Hence the required properties of $U, H$ are shown.
\end{proof2}

When a bi-unitarily invariant random matrix $T$ is non-singular almost surely, then the polar decomposition $T=UH$ is unique (almost surely), and it is easily verified that $U$ and $H$ themselves satisfy the properties (1)--(3) above.

\begin{theorem}\label{ch04:the4.3.11}
Let $(X(s, n))_{s\in S}$ be an independent family of $n\times n$ bi-unitarily invariant random matrices, and let $(D(t, n))_{t\in T}$ be as in Theorem~\emph{\ref{ch04:the4.3.1}}. If $X(s, n)^{*}X(s, n)$ converges in distribution almost surely to a compactly supported $\rho_{s}\in\mathcal{M} (\mathbb{R}^{+})$ for each $s\in S$, then the family
\begin{equation*}
\left((\{X(s, n), X(s, n)^{*}\})_{s\in S},\, \{D(t, n), D(t, n)^{*}:t\in T\}\right)
\end{equation*}
is asymptotically free almost everywhere as $n\rightarrow\infty$. Moreover, the almost sure limit distribution of $(X(s, n), X(s, n)^{*})_{s\in S}$ is the distribution of a free family $(x_{s}, x_{s}^{*})_{s\in S}$ of $R$-diagonal elements, where $x_{s}^{*}x_{s}$ has the distribution $\rho_{s}$.
\end{theorem}

\begin{proof2}
According to Lemma~\ref{ch04:lem4.3.10} we may write $X(s, n)=U(s, n)H(s, n)$, where $U(s, n)$ and $H(s, n)$ are as stated in (1)--(3) of the lemma. Furthermore, as in the proof of Theorem~\ref{ch04:the4.3.5}, we may write $H(s, n)=V(s, n)\Lambda(s, n)V(s,n)^{*}$, where $V(s, n)$ is a standard unitary matrix and $\Lambda(s, n)$ is a diagonal random matrix with increasingly ordered diagonals. Here we can make $(U(s, n))_{s\in S}\cup(V(s, n))_{s\in S}$ an independent family. Since $H(s, n)^{2}=X(s, n)^{*}X(s, n)$, it follows (use Lemma~\ref{ch04:lem4.3.4}) that $H(s, n)$ converges in distribution almost surely to the image measure of $\rho_{s}$ by $t\mapsto t^{1/2}$ for each $s\in S$. Hence the method in proving Theorem~\ref{ch04:the4.3.5} can be applied to show that
\begin{equation*}
\left((\{U(s, n)\})_{s\in S},\, (\{H(s, n)\})_{s\in S}, \{D(t, n), D(t, n)^{*}:t\in T\}\right)
\end{equation*}
is asymptotically free almost everywhere. Furthermore, this implies that $(X(s, n), X(s, n)^{*})$ converges in distribution to $(u_{s}h_{s}, h_{s}u_{s}^{*})$ almost surely, where $u_{s}$ is a Haar unitary and $h_{s}$ is a positive element such that $h_{s}$ is free from $\{u_{s}, u_{s}^{*}\}$ and $h_{s}^{2}$ has the distribution $\rho_{s}$. Hence we have the conclusion by Corollary~\ref{ch02:cor2.6.15}.
\end{proof2}

Let $x$ be a noncommutative random variable and $T(n)$ an $n\times n$ random matrix for every $n\in \mathbb{N}$. We say that $T(n)$ is an \textit{almost everywhere random matrix model}\index{almost everywhere random matrix model} of $x$ if the distribution of $(T(n), T(n)^{*})$ with respect to $\mathrm{tr}_{n}$ converges almost surely to that of $(x, x^{*})$ as the matrix size $n$ goes to infinity. The asymptotic freeness results in this section tell us how to construct almost everywhere random matrix models for certain free families of noncommutative random variables. In particular, independent standard selfadjoint or non-selfadjoint Gaussian matrices provide models of free semicircular or circular elements. More examples of random matrix models will be discussed in the next section.

\section{Random matrix models of noncommutative random variables}
\label{ch04:sec4.4}

\noindent In this section we study some particular noncommutative random variables and their random matrix models. The polar decomposition of an $R$-diagonal element is discussed, and $R$-diagonal elements are characterized by the existence of an almost sure bi-unitarily invariant random matrix model. In particular, we treat a specified random matrix model of an $R$-diagonal element $x$ when $x^{*}x$ has the free Poisson distribution. A random matrix model\index{random matrix!model, almost everywhere} for the compound free Poisson distribution is also given. The random matrix models we discuss have limits almost everywhere.

First, we show that in the polar decomposition of a circular element the unitary and positive parts are free. A random matrix model is useful in the proof of the next lemma.

\begin{lemma}\label{ch04:lem4.4.1}
Let $x$ be a circular element,\index{circular!element} and let $v$ be a Haar unitary in a $C^{*}$-probability space. If $x$ and $\{v, v^{*}\}$ are in free relation, then $vx$ is circular.
\end{lemma}

\begin{proof2}
Let $X(n)$ be an $n\times n$ standard non-selfadjoint Gaussian matrix, which is a random matrix model of $x$. If $D(n):=\mathbf{Diag} (1, e^{2\pi \mathrm{i}\,/n}, \ldots, e^{2\pi(n-1)\mathrm{i}\,/n})$, then $(D(n), D(n)^{*})$ obviously converges to $(v, v^{*})$ in distribution. According to Corollary~\ref{ch04:cor4.3.8}, $(\{X(n), X(n)^{*}\}, \{D(n), D(n)^{*}\})$ is asymptotically free, and hence $D(n)X(n)$ is a random matrix model of $vx$. Since $D(n)X(n)$ is a standard non-selfadjoint Gaussian again, we obtain the conclusion.
\end{proof2}

Hereafter in this book we shall sometimes work with a noncommutative probability space over a von Neumann algebra. A $W^{*}$-\textit{probability space}\index{$W^{*}$-probability space, tracial} $(\mathcal{M},\varphi)$ is a noncommutative probability space consisting of a von Neumann algebra $\mathcal{M}$ and a faithful normal state $\varphi$ on $\mathcal{M}$. In particular, when $\tau$ is a faithful normal tracial state on $\mathcal{M}$, we call $(M, \tau)$ a \textit{tracial $W^{*}$-probability space}.\index{tracial $W^{*}$-probability space} (See Sec.~\ref{ch07:sec7.1} for a brief survey on von Neumann algebras.)

\begin{proposition}\label{ch04:pro4.4.2}
Let $x$ be a noncommutative random variable in a tracial $W^{*}$-probability space,\index{$W^{*}$-probability space} and let $x=uh$ be the polar decomposition. Then $x$ is a circular element if and only if the following three conditions hold:
\begin{enumerate}
\item[(i)] $u$ is a Haar unitary.

\item[(ii)] $h$ is a quarter-circular element.

\item[(iii)] $h$ is free from $\{u, u^{*}\}$.
\end{enumerate}
\end{proposition}

\begin{proof2}
Assume that $x$ is circular. The ondition (ii) was shown in Example~\ref{ch02:exa2.6.2}. Hence the kernel of $h$ is $\{0\}$, and so $u$ is a unitary. Let $y_{\delta} :=x(\delta \textbf{1}+x^{*}x)^{-1/2}$ for $\delta>0$. We have, for all $p\geq 1$,
\begin{equation*}
\Vert u-y_{\delta}\Vert_{p}=\Vert \mathbf{1}-h(\delta \mathbf{1}+h^{2})^{-1/2}\Vert_{p}\rightarrow 0 \quad \mathrm{as}\quad \delta\rightarrow+0,
\end{equation*}
where $\Vert a\Vert_{p} :=\tau(|a|^{p})^{1/p}$ (the Schatten $p$-norm with respect to $\tau$) for $a\in \mathcal{M}$. Then, appealing to the H\"{o}lder inequality, we can get
\begin{equation*}
\lim_{\delta\rightarrow+0}\Vert P(h, u, u^{*})-P(h, y_{\delta}, y_{\delta}^{*})\Vert_{1}=0
\end{equation*}
for any polynomial $P$ of three noncommuting indeterminates. Furthermore, $h$ and $y_{\delta}$ are approximated in norm by polynomials of $x, x^{*}$. Therefore, the distribution of $(h, u, u^{*})$ is determined from that of $(x, x^{*})$, so it is independent of the choice of a circular element $x$. Since $e^{\mathrm{i}\,\theta}x$ is also circular (consider the representation $x=\ell(f)^{*}+\ell(g)$, cf. Example~\ref{ch02:exa2.6.2}), we must have $\tau((e^{\mathrm{i}\,\theta}u)^{k})=\tau(u^{k})$ for all $k\in \mathbb{Z}$. This yields (i).

To show (iii), let $\mathcal{A}$ and $\mathcal{B}$ be the algebraic $^{*}$-subalgebras generated by $x$ and by $h$, respectively. A $^{*}$-automorphism $\alpha$ on $\mathcal{A}$ is determined by $\alpha(x)=-x$. Since the distribution of $(x, x^{*})$ is invariant under $\alpha,\, \tau=\tau\circ\alpha$ holds on $\mathcal{A}$ and hence $\alpha$ extends to $\mathcal{A}''$ (i.e. the closure of $\mathcal{A}$ in the strong operator topology). Note that $\mathcal{A}''\supset \mathcal{B}\cup\{u\}$. We have $\alpha(y)=y$ for all $y\in \mathcal{B}$, and $\alpha(u)=-u$. So $\tau(uy)=\tau(\alpha(uy))=-\tau(uy)$, and $\tau(uy)$ must vanish for every $y\in \mathcal{B}$. Now choose a Haar unitary $v\in \mathcal{M}$ which is free from $\mathcal{A}''$. (This is always possible if we enlarge $\mathcal{M}$ by taking a free product.) Set $w:=vu$. It is not difficult to see that
\begin{equation}
\tau(y_{0}w^{k_{1}}y_{1}w^{k_{2}}\cdots y_{n-1}w^{k_{n}}y_{n})=0
\label{ch04:eqn4.4.1}
\end{equation}
whenever $k_{j}\in \mathbb{Z}\,\backslash\, \{0\}$ and $y_{j}\in \mathcal{B}$ with $\tau(y_{j})=0$ (but $y_{0}, y_{n}$ may be $\textbf{1}$). Indeed, the array inside the tracial state can be rewritten in the form $z_{0}v_{1}z_{1}v_{2}\cdots v_{m}z_{m}$ with $v_{i}\in\{v, v^{*}\}$ and $z_{i}\in\{y_{0}, y_{n}, u, uy_{j}, y_{j}u^{*}, uy_{j}u^{*}\}$. Hence (\ref{ch04:eqn4.4.1}) is a consequence of the free relation of $v$ and $\mathcal{A}''$, so that $w$ and $\mathcal{B}$ are free. In this way we conclude that $vx$ has the polar decomposition $wh$ with free $w$ and $h$. Since $vx$ is circular due to the previous lemma, $(h, u, u^{*})$ and $(h, w, u^{*})$ have the same distribution by the fact proved in the first paragraph. So (iii) follows.

The converse is obvious, because the distribution of $(x, x^{*})$ is fixed by the conditions (i)--(iii).
\end{proof2}

On the other hand, a semicircular element $a$ has the polar decomposition $a=sh$ such that the positive part $h$ is quarter-circular and the selfadjoint unitary part $s$ has the distribution $\tfrac{1}{2}(\delta(-1)+\delta(1))$. Note that $s$ and $h$ are not free, because they commute.

\begin{example}\label{ch04:exa4.4.3}
Let $a$ be a \textit{semicircular element}\index{semicircular!element} and $v$ a Haar unitary in a tracial $W^{*}$-probability space. If $a$ and $v$ are in free relation, then $va$ is \textit{circular}.\index{circular!element}

Indeed, let $a=sh$ be the polar decomposition mentioned above. When one can show that $vs$ is a Haar unitary and it is free from $h$, the conclusion follows from Proposition~\ref{ch04:pro4.4.2} applied to $va\,=\,(vs)h$. Since $s$ and $v$ are free, it is clear that $\tau((vs)^{k})=0$ for all $k\in \mathbb{Z}\,\backslash\,\{0\}$. For the freeness of $vs$ and $h$, we proceed as in the proof of Proposition~\ref{ch04:pro4.4.2} and show that
\begin{equation}
\tau(y_{0}(vs)^{k_{1}}y_{1}(vs)^{k_{2}}\cdots y_{n-1}(vs)^{k_{n}}y_{n})=0
\label{ch04:eqn4.4.2}
\end{equation}
whenever $k_{j}\in \mathbb{Z}\,\backslash\,\{0\}$ and $y_{j}$ are polynomials of $h$ with $\tau(y_{j})=0$ (but $y_{0}, y_{n}$ may be $\mathbf{1}$). One can rewrite the array inside the trace in (\ref{ch04:eqn4.4.2}) as $z_{0}v_{1}z_{1}v_{2}\cdots v_{m}z_{m}$, where $v_{i}\in\{v, v^{*}\}$ and $z_{i}\in\{s, y_{j}, sy_{j}\}$ thanks to $sy_{j}=y_{j}s$. Here note that $\tau(sh^{n})=0$ for all $n\in \mathbb{N}$; in fact, $\tau(sh^{n})=\tau(sa^{n})=0$ for even $n$ and $\tau(sh^{n})=\tau(a^{n})=0$ for odd $n$. This gives $\tau(sy_{j})=0$ for any $y_{j}$. Hence (\ref{ch04:eqn4.4.2}) follows from the free relation of $a$ and $v$.
\end{example}

It is tempting to view the above example as the free analogue of the following probabilistic statement: If $\xi$ and $\eta$ are independent random variables such that $\xi$ is a real Gaussian and $\eta$ is uniformly distributed on the unit circle $\mathbb{T}$, then $\eta\xi$ has a rotation-invariant normal distribution.

$R$-diagonal elements (with an additional kernel assumption) in a tracial $W^{*}$-probability space can be characterized in terms of their polar decomposition.

\begin{proposition}\label{ch04:pro4.4.4}
Let $x$ be an element in a tracial $W^{*}$-probability space such that $\mathrm{ker}\, x=\{0\}$. Let $x=uh$ be the polar decomposition $(u$ is a unique unitary, because $\mathrm{ker}\, x=\{0\})$. Then $x$ is an $R$-diagonal element\index{$R$-diagonal!element} if and only if $u$ is a Haar unitary and $h$ is free from $\{u, u^{*}\}$.
\end{proposition}

\begin{proof2}
Assume that $x$ is $R$-diagonal, and choose a Haar unitary $u'$ and a positive element $h'$ as in Corollary~\ref{ch02:cor2.6.15}. Here we may assume that $x, u', h'$ are in the same tracial $W^{*}$-probability space $(\mathcal{M}, \tau)$. Let $\mathcal{A}$ and $\mathcal{B}$ be the algebraic $^{*}$-subalgebras generated by $x$ and $u'h'$, respectively. Then the $^{*}$-isomorphism $\alpha : \mathcal{A}\rightarrow \mathcal{B}$ sending $x$ to $u'h'$ preserves $\tau$, because the distributions of $(x, x^{*})$ and $(u'h', h'u^{\prime*})$ are the same. Hence $\alpha$ extends to a $\tau$-preserving isomorphism of $\mathcal{A}''$ onto $\mathcal{B}''$. Since the distribution of $h'$ has no atom at $0, u'$ and $h'$ are in $\mathcal{A}''$, so we can set $u :=\alpha^{-1}(u')$ and $h:=\alpha^{-1}(h')$. Now it is clear that $u$ is a Haar unitary, $h$ is free from $\{u, u^{*}\}$, and $x=uh$ (this is the polar decomposition, from its uniqueness).
\end{proof2}

Note that due to use of Corollary~\ref{ch02:cor2.6.15} the previous proof is not fully contained in the book. Of course, Proposition~\ref{ch04:pro4.4.2} becomes a particular case.

The next proposition says that $R$-diagonal elements can also be characterized in terms of random matrix models.

\begin{theorem}\label{ch04:the4.4.5}
Let $x$ be an element in a $C^{*}$-probability space $(\mathcal{A}, \varphi)$ with a tracial state $\varphi$. Then $x$ is $R$-diagonal if and only if $x$ admits an almost everywhere random matrix model $X(n)$ which is bi-unitarily invariant.
\end{theorem}

\begin{proof2}
Assume that $x$ is $R$-diagonal. By Corollary~\ref{ch02:cor2.6.15} we may write $x=uh$ with a Haar unitary $u$ and a positive element $h$ free from $\{u, u^{*}\}$. Choose a uniformly bounded sequence of constant diagonal positive matrices $D(n)\ (n\in \mathbb{N})$ whose limit distribution is the distribution of $h$. Moreover, choose two independent $n\times n$ standard unitary random matrices $U(n), V(n)$, and define $X(n):= U(n)V(n)D(n)V(n)^{*}$. Then the bi-unitary invariance of $X(n)$ is immediate, and Theorem~\ref{ch04:the4.3.1} says that
\begin{equation*}
(\{U(n), U(n)^{*}\}, \{V(n), V(n)^{*}\}, \{D(n)\})
\end{equation*}
is asymptotically free almost everywhere. Hence $(U(n), U(n)^{*}, V(n)D(n)V(n)^{*})$ converges in distribution to $(u, u^{*}, h)$ almost surely, so $X(n)$ is an almost everywhere random matrix model of $x$.

The converse implication is included in Theorem~\ref{ch04:the4.3.11}.
\end{proof2}

In particular, the above discussions give more light on the polar decomposition of a circular element via the random matrix model. Let $X(n)$ be a standard nonselfadjoint Gaussian matrix. Since $X(n)$ is bi-unitarily invariant, it has the polar decomposition $X(n)=U(n)H(n)$ with independent $U(n)$ and $H(n)$. The limit $n \rightarrow\infty$ in distribution yields the polar decomposition $x=uh$ of a circular element $x$ as stated in Proposition~\ref{ch04:pro4.4.2}. $(X(n)$ is an almost everywhere random matrix model of the circular distribution, as Corollary~\ref{ch04:cor4.3.8} includes this fact.)

Next, we shall discuss the \textit{complexification of Wishart matrices}, which provides random matrix models of $R$-diagonal elements related with the Marchenko-Pastur distribution (or the free Poisson distribution). For $p\geq n$ let $Z(n)$ be a complex $p\times n$ random matrix such that $\mathrm{Re}\,Z_{ij}(n)$ and $\mathrm{Im}\,Z_{ij}(n)\ (1\leq i\leq p,\,1\leq j\leq n)$ are independent with the same distribution $N(0,1/2n)$. The random matrix $Z(n)^{*}Z(n)$ and its square root are unitarily invariant. Further, let $U(n)$ be an $n\times n$ standard unitary random matrix independent of $Z(n)$. Define
\begin{equation}
H(n) :=(Z(n)^{*}Z(n))^{1/2},\quad T(n) :=U(n)H(n)\,.
\label{ch04:eqn4.4.3}
\end{equation}
The complex analogue of Theorem~\ref{ch04:the4.1.9} is true: The mean eigenvalue density of $H(n)=Z(n)^{*}Z(n)$ converges to the free Poisson distribution $\mu_{\lambda}$ as $p/n\rightarrow\lambda>0$. It is known ([\citen{bib211}]) that the convergence is not only in expectation but almost everywhere. (Indeed, this is included in Proposition~\ref{ch04:pro4.4.11} below.) By construction it follows that $T(n)$ is bi-unitarily invariant. Hence, when $n\rightarrow\infty$ and $p/n\rightarrow\lambda$, Theorem~\ref{ch04:the4.3.11} tells us that the complexified Wishart matrix $T(n)$ is an almost everywhere random matrix model of an $R$-diagonal element $x=uh$, where $h^{2}$ has the distribution $\mu_{\lambda}$. (Our term ``complexified Wishart'' is certainly not optimal; complexified\index{Wishart matrix!complexified} is understood in the sense that the eigenvalues are complex since the matrix is not selfadjoint.)

We compute the joint distribution of the eigenvalues of the random matrix $T(n)$, and we record some auxiliary results for later use. From now on we refer to the measure
\begin{equation*}
d\Lambda_{n}(A) :=2^{n(n-1)/2}\prod_{i=1}^{n}dA_{ii}\prod_{i<j}d(\mathrm{Re}\,A_{ij})\,d(\mathrm{Im}\,A_{ij})
\end{equation*}
as the Lebesgue measure on the space $M_{n}(\mathbb{C})^{sa}$ of selfadjoint matrices. (Note that this measure differs slightly from the measure (\ref{ch04:eqn4.1.17}) in a normalizing constant; this normalization arises when we take the isometry between $M_{n}(\mathbb{C})^{sa}$ with the Hilbert-Schmidt norm and $\mathbb{R}^{n^{2}}$ with the Euclidean norm.) On the other hand, the measure (\ref{ch04:eqn4.1.25}) is induced via the isometry between $M_{n}(\mathbb{C})$ and $\mathbb{R}^{2n^{2}}$. We denote this Lebesgue measure on $M_{n}(\mathbb{C})$ by $\hat{\Lambda}_{n}$.

For $A\in M_{n}(\mathbb{C})^{sa}$ let $A=VDV^{*}$ be the diagonalization, where $V\in \mathcal{U}(n)$ and $D= \mathbf{Diag}(t_{1}, t_{2}, \ldots, t_{n})$ with $t_{1}\leq t_{2}\leq\ldots\leq t_{n}$. Except for a negligible set, $V$ is unique up to a diagonal unitary factor. So one can change the coordinates as follows: $A\in M_{n}(\mathbb{C})^{sa}\leftrightarrow(V, D)\in \mathcal{U}(n)/T\times \mathbb{R}_{\leq}^{n}$, where $\mathcal{U}(n)/T$ is the homogeneous space divided by the torus of diagonal unitaries. Let $\dot{\gamma}_{n}$ denote the probability measure on $\mathcal{U}(n)/T$ induced from the Haar probability measure $\gamma_{n}$ on $\mathcal{U}(n)$.

\begin{lemma}\label{ch04:lem4.4.6}
The measure $\Lambda_{n}$ is transformed into the product measure
\begin{equation*}
\dot{\gamma}_{n}\otimes\left(\frac{(2\pi)^{n(n-1)/2}}{\prod_{j=1}^{n-1}j!}\prod_{i<j}(t_{i}-t_{j})^{2}\prod_{i=1}^{n}\,dt_{i}\right)
\end{equation*}
on $\mathcal{U}(n)/T\times \mathbb{R}_{\leq}^{n}$ under the above change of coordinates.
\end{lemma}

\begin{proof2}
The proof starts on the same lines as that of Lemma~\ref{ch04:lem4.1.2}, but an additional argument is needed. With an infinitesimal Hermitian $dM :=-\mathrm{i}\,V^{*}\,dV$ we get
\begin{equation*}
dA=V(dD+\mathrm{i}\,(dM\cdot D-D\cdot dM))V^{*},
\end{equation*}
so that
\begin{equation*}
d\Lambda_{n}(A)=\mathrm{const}\cdot \prod_{i<j}(t_{i}-t_{j})^{2}\prod_{i=1}^{n}dt_{i}\prod_{i<j}\,d(\mathrm{Re}\,M_{ij})\,d(\mathrm{Im}\,M_{ij})\,.
\end{equation*}
Thanks to the unitary invariance of $\Lambda_{n}$, the measure
\begin{equation*}
\prod_{i<j}d(\mathrm{Re}\,M_{ij})\,d(\mathrm{Im}\,M_{ij})\quad \mathrm{on}\quad \mathcal{U}(n)/T
\end{equation*}
is invariant under left unitary multiplication. Since $\dot{\gamma}_{n}$ is a unique probability measure on $\mathcal{U}(n)/T$ invariant in this sense, the above measure must be $\mathrm{const}\cdot \dot{\gamma}_{n}$. Thus the conclusion follows because the normalizing constant is $2^{n(n-1)/2}$ times the constant in Lemma~\ref{ch04:lem4.1.6}.
\end{proof2}

Let $\Lambda_{+,n}$ be the measure on $M_{n}(\mathbb{C})^{+}$ induced from $\hat{\Lambda}_{n}$ on $M_{n}(\mathbb{C})$ via the map $X\mapsto X^{*}X$. (One can also use the measure induced via $X\mapsto|X|$, but the former is more convenient.) Consider the map (or the coordinate change)
\begin{equation*}
X\in M_{n}(\mathbb{C})\mapsto(U, X^{*}X)\in \mathcal{U}(n)\times M_{n}(\mathbb{C})^{+},
\end{equation*}
where $U$ is the unitary part of $X$. (Note that the singular matrices are negligible with respect to $\hat{\Lambda}_{n}$.) The next lemma shows that $\hat{\Lambda}_{n}$ on $M_{n}(\mathbb{C})$ corresponds (up to a constant) to the product of $\gamma_{n}$ on $\mathcal{U}(n)$ and the restriction of $\Lambda_{n}$ on $M_{n}(\mathbb{C})^{+}$ under this map.

\begin{lemma}\label{ch04:lem4.4.7}
The measure $\hat{\Lambda}_{n}$ is transformed to the product measure $\gamma_{n}\otimes\Lambda_{+,n}$ under the above coordinate change $X\mapsto(U, X^{*}X)$. Furthermore, the measure $\Lambda_{+,n}$ is a constant multiple of the restriction of $\Lambda_{n}$ on $M_{n}(\mathbb{C})^{+}$ as follows:
\begin{equation*}
\Lambda_{+,n}=C_{n}\Lambda_{n}|_{M_{n}(\mathbb{C})^{+}} \quad with \quad C_{n}=\frac{\pi^{n(n+1)/2}}{2^{n(n-1)/2}\prod_{j=1}^{n-1}j!}.
\end{equation*}
\end{lemma}

\begin{proof2}
One can further take the coordinate change of $X^{*}X$ by the diagonalization. So write $X^{*}X=VDV^{*}$ and $X=UVD^{1/2}V^{*}$, where $U\in \mathcal{U}(n),\, V\in \mathcal{U}(n)/T$ and $D= \mathbf{Diag} (t_{1}, \ldots, t_{n})$ with $0<t_{1}\leq\ldots\leq t_{n}$ (the case $t_{1}=0$ is negligible). We differentiate $X=UVD^{1/2}V^{*}$ to obtain
\begin{equation*}
dX=UV[\tfrac{1}{2}D^{-1/2}dD+\mathrm{i}\,((V^{*}\cdot dL\cdot V)D^{1/2}+(dM\cdot D^{1/2}-D^{1/2}\cdot dM))]V^{*}
\end{equation*}
with infinitesimal Hermitians $dL :=-\mathrm{i}\,U^{*}dU$ and $dM :=-\mathrm{i}\,V^{*}dV$. Hence we compute
\begin{align*}
d\hat{\Lambda}_{n}(A) & \ = \ \frac{1}{2^{n}}\prod_{i=1}^{n}(V^{*}\cdot dL\cdot V)_{ii}\prod_{i<j}\mathrm{Re}(V^{*}\cdot dL\cdot V)_{ij}\mathrm{Im}(V^{*}\cdot dL\cdot V)_{ij}\\
& \qquad \qquad \times\prod_{i<j}(\mathrm{Re}\,dM_{ij})(\mathrm{Im}\,dM_{ij})\prod_{i<j}(t_{i}-t_{j})^{2}\prod_{i=1}^{n}dt_{i}\\
& \ = \ \frac{1}{2^{n}}\prod_{i=1}^{n}dL_{ii}\prod_{i<j}d(\mathrm{Re}\,L_{ij})\,d(\mathrm{Im}\,L_{ij})\\
& \qquad \qquad \times\prod_{i<j}d(\mathrm{Re}\,M_{ij})\,d(\mathrm{Im}\,M_{ij})\prod_{i<j}(t_{i}-t_{j})^{2}\prod_{i=1}^{n}\,dt_{i}\,,
\end{align*}
and the bi-unitary invariance of $\hat{\Lambda}_{n}$ implies
\begin{align*}
&\prod_{i=1}^{n}dL_{ii}\prod_{i<j}d(\mathrm{Re}\,L_{ij})\,d(\mathrm{Im}\,L_{ij})=\mathrm{const}\cdot d\gamma_{n}(U)\,,\\
& \prod_{i<j}d(\mathrm{Re}\,M_{ij})\,d(\mathrm{Im}\,M_{ij})=\mathrm{const}\cdot d\dot{\gamma}_{n}(V)\,.
\end{align*}
Therefore, we infer that $\hat{\Lambda}_{n}$ is transformed to the measure
\begin{equation*}
\gamma_{n}\otimes\dot{\gamma}_{n}\otimes\left(C_{n}'\prod_{i<j}(t_{i}-t_{j})^{2}\prod_{i=1}^{n}dt_{i}\right)
\end{equation*}
on $\mathcal{U}(n)\times \mathcal{U}(n)/T\times(\mathbb{R}^{+})_{\leq}^{n}$ under the map $X\mapsto(U, V, D)$. To determine the constant $C_{n}'$, use the $n\times n$ standard non-selfadjoint Gaussian matrix having the density (\ref{ch04:eqn4.1.24}), and compute
\begin{align*}
1 & \ =\ \left(\frac{\pi}{n}\right)^{-n^{2}}C_{n}'\int_{0}^{\infty}\cdots\int_{0}^{\infty}\exp\left(-n\sum_{i=1}^{n}t_{i}\right)
\prod_{i<j}(t_{i}-t_{j})^{2}\,dt_{1}\cdots dt_{n}\\
& \ = \ \pi^{-n^{2}}C_{n}'\int_{0}^{\infty}\cdots\int_{0}^{\infty}\exp\left(-\sum_{i=1}^{n}x_{i}\right)\prod_{i<j}(x_{i}-x_{j})^{2}\,dx_{1}\cdots dx_{n}\\
& \ = \ \pi^{-n^{2}}C_{n}'\left(\prod_{j=1}^{n-1}j!\right)^{2}
\end{align*}
by the Selberg integral ([\citen{bib121}], p. 354). Hence, under the coordinate change $M_{n}(\mathbb{C})^{+}\leftrightarrow \mathcal{U}(n)/T\times(\mathbb{R}^{+})_{\leq}^{n}$ the measure $\Lambda_{+,n}$ is written as
\begin{equation}
\dot{\gamma}_{n}\otimes\left(\frac{\pi^{n^{2}}}{(\prod_{j=1}^{n-1}j!)^{2}}\prod_{i<j}(t_{i}-t_{j})^{2}\prod_{i=1}^{n}dt_{i}\right).
\label{ch04:eqn4.4.4}
\end{equation}
Comparing this with the expression of $\Lambda_{n}$ in Lemma~\ref{ch04:lem4.4.6} gives the desired conclusion.
\end{proof2}

It is noteworthy that taking the map $X\mapsto|X|$ (instead of $X\mapsto X^{*}X$) we find the following induced measure on $M_{n}(\mathbb{C})^{+}$:
\begin{equation*}
\dot{\gamma}_{n}\otimes\left(\frac{2^{n}\pi^{n^{2}}}{(\prod_{j=1}^{n-1}j!)^{2}}\prod_{i<j}(t_{i}^{2}-t_{j}^{2})^{2}\prod_{i=1}^{n}t_{i}\prod_{i=1}^{n}dt_{i}\right)
\end{equation*}
under the coordinate change $M_{n}(\mathbb{C})^{+}\leftrightarrow \mathcal{U}(n)/T\times(\mathbb{R}^{+})_{\leq}^{n}$.

\begin{lemma}\label{ch04:lem4.4.8}
Let $p\geq n$, and let $T(n)$ be the random matrix given in \emph{(\ref{ch04:eqn4.4.3})}. Then the joint probability density of the eigenvalues of $T(n)$ with respect to the Lebesgue measure $d\zeta_{1}\cdots d\zeta_{n}$ is
\begin{equation*}
C_{np}\exp\left(-n\sum_{i=1}^{n}|\zeta_{i}|^{2}\right)\prod_{i=1}^{n}|\zeta_{i}|^{2(p-n)}\prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2}.
\end{equation*}
\end{lemma}

\begin{proof2}
Let $R$ be a complex $p\times n$ random matrix with independent standard Gaussian $\mathrm{Re}\,R_{ij}, \mathrm{Im}\,R_{ij}$, and set $W :=R^{*}R$, a complex Wishart matrix. Lemma~\ref{ch04:lem4.1.8} is slightly modified in the complex case, and we have
\begin{equation*}
E(\exp(\mathrm{Tr}\,AW))=\det(I-2A)^{-p}
\end{equation*}
for every $A\in M_{n}(\mathbb{C})^{sa}$ with $A\leq I/2$. The argument in deriving the density (\ref{ch04:eqn4.1.21}) works in the present case as well, and the density on $M_{n}(\mathbb{C})^{+}$ of $W$ with respect to $d\Lambda_{n}$ is computed as
\begin{equation*}
p(H)=C_{np}\exp\left(-\frac{1}{2} \mathrm{Tr}\,H\right)(\det H)^{p-n}.
\end{equation*}
Hence the density of $H(n)^{2}=Z(n)^{*}Z(n)$ in (\ref{ch04:eqn4.4.3}) is
\begin{equation}
C_{np}\exp(-n\mathrm{Tr}\,H)(\det H)^{p-n},
\label{ch04:eqn4.4.5}
\end{equation}
with a different constant $C_{np}$.

Since $U(n)$ and $Z(n)$ are independent, the distribution of $T(n)$ is written as
\begin{equation*}
d\gamma_{n}(U)\otimes(C_{np}\exp(-n\mathrm{Tr}\,H)(\det H)^{p-n}\,d\Lambda_{n}(H))
\end{equation*}
under the change of coordinates $X\in M_{n}(\mathbb{C})\mapsto(U, X^{*}X)\in \mathcal{U}(n)\times M_{n}(\mathbb{C})^{+}$, where $U$ is the unitary part of $X$. We conclude from Lemma~\ref{ch04:lem4.4.7} that the distribution of $T(n)$ has the density
\begin{equation*}
C_{np}'\exp(-n\mathrm{Tr}\,X^{*}X)(\det X^{*}X)^{p-n}
\end{equation*}
with respect to $dX=d\hat{\Lambda}_{n}(X)$.

Now one can proceed as in the proof of Lemma~\ref{ch04:lem4.1.10}. After a triangulation the joint eigenvalue density is obtained as stated. The proof is left to the reader.
\end{proof2}

When $R$ is a $p\times n$ random matrix\index{random matrix!model for compound free Poisson distribution} with independent Gaussian $\mathrm{Re}\,R_{ij}, \mathrm{Im}\,R_{ij}$ of $N(0,1/2n)$ and $B$ is a $p\times p$ selfadjoint random matrix independent of $R$, we have an $n\times n$ selfadjoint random matrix $R^{*}BR$ called a \textit{compound Wishart matrix}.\index{asymptotically free!compound Wishart matrices}\index{compound!Wishart matrix}

In the rest of this section we show that a suitably arranged sequence of compound Wishart matrices is a random matrix model of the \textit{compound free Poisson distribution}\index{compound!free Poisson distribution} introduced in Sec.~\ref{ch03:sec3.3}. First, the asymptotic freeness almost everywhere is shown for an independent family of compound\index{Wishart matrix!compound} Wishart matrices.

Let $S$ be a set. For $s\in S$ and $n \in \mathbb{N}$ let $p(s, n)\in \mathbb{N}$ be such that
\begin{equation*}
p(s, n)/n\rightarrow\lambda_{s}\in(0, \infty)\quad \mathrm{as} \quad n\rightarrow\infty.
\end{equation*}
Let $Z(s, n)$ be a $p(s, n)\times n$ random matrix such that $\mathrm{Re}\,Z_{ij}(s, n)$ and $\mathrm{Im}\,Z_{ij}(s, n)\ (1\ \leq i\leq p(s, n),\ 1\ \leq j\ \leq n)$ are independent with the same distribution $N(0, 1/2n)$. Let $B(s, n)$ be a $p(s, n)\,\times\, p(s, n)$ selfadjoint random matrix. Assume that $(Z(s, n))_{s\in S}\,\cup\,(B(s, n))_{s\in S}$ is an independent family for each $n\in \mathbb{N}$. For each $s\in S$ assume further that $B(s, n)$ converges in distribution almost surely to a compactly supported $\rho_{s}\in \mathcal{M}(\mathbb{R})$.

\begin{proposition}\label{ch04:pro4.4.9}
Let $(Z(s, n))_{s\in S}$ and $(B(s, n))_{s\in S}$ be given as above, and let $(D(t, n))_{t\in T}$ be as in Theorem~\emph{\ref{ch04:the4.3.1}}. Then
\begin{equation*}
\left((\{Z(s,n)^{*}B(s, n)Z(s, n)\})_{s\in S},\, \{D(t, n), D(t, n)^{*}:t\in T\}\right)
\end{equation*}
is asymptotically free almost everywhere as $n\rightarrow\infty$.
\end{proposition}

\begin{proof2}
First we treat the case where $p(s, n)=n$ for all $s\in S$, and so the $Z(s, n)$'s are $n\times n$ standard non-selfadjoint Gaussian matrices denoted by $X(s, n)$. Write
\begin{equation*}
B(s, n)=V(s, n)\mathbf{Diag} (b_{1}(s, n), \ldots, b_{n}(s, n))V(s, n)^{*},
\end{equation*}
where $b_{1}(s, n) \leq\ldots\leq b_{n}(s, n)$ and $V(s, n)$ is a random unitary. By assumption, $(X(s, n))_{s\in S}\,\cup\,(V(s, n))_{s\in S}$ can be an indepedent family, so that $(V(s, n)^{*}X(s, n))_{s\in S}$ is still an independent family of standard non-selfadjoint Gaussians. Hence we may assume that $B(s, n)$ is diagonal with increasingly ordered diagonals (though it is no longer independent of $X(s,n)$). Since $(X(s, n)^{*}B(s, n)X(s,\ n))_{s\in S}$ is an independent family of $n\times n$ unitarily invariant selfadjoint random matrices, Theorem~\ref{ch04:the4.3.5} yields the result once we show that $X(s, n)^{*}B(s, n)X(s, n)$ converges in distribution almost surely to a compactly supported measure for each $s\in S$. To prove this, choose (non-random) $\xi_{1}(s, n)\, \leq\,\ldots\,\leq\, \xi_{n}(s, n)$ in $\mathrm{supp}\,\rho_{s}$ such that $\tfrac{1}{n}\sum_{i=1}^{n}\delta(\xi_{x}(s, n))\, \rightarrow\, \rho_{s}$ as $n \rightarrow\infty$, and set $\Xi(s, n) := \mathbf{Diag} (\xi_{1}(s, n), \ldots, \xi_{n}(s,n))$. Then by Lemma~\ref{ch04:lem4.3.4} we get $\lim_{n\rightarrow\infty}\Vert B(s, n)\,-\,\Xi(s, n)\Vert_{p}=0$ a.s. for all $p\geq 1$. So, as in the proof of Theorem~\ref{ch04:the4.3.5}, it is enough to consider $X(s, n)^{*}\Xi(s, n)X(s, n)$ instead of $X(s,n)^{*}B(s, n)X(s, n)$. Corollary~\ref{ch04:cor4.3.6} shows that $(\{X(s, n), X(s, n)^{*}\}, \{\Xi(s, n)\})$ is asymptotically free almost everywhere (for each $s\in S$). This implies the existence of the almost sure limit distribution of $X(s, n)^{*}B(s, n)X(s, n)$, and we have the conclusion in the case of $p(s, n)=n$.

For the general case, let $X(s, r, n)\ (s\in S, r\in \mathbb{N})$ be independent $n\times n$ standard non-selfadjoint Gaussians, and for each $s\in S$ choose $d_{s}\in \mathbb{N}$ such that $p(s, n)\leq d_{s}n$ for all $n$. As above, we may assume that $B(s, n)= \mathbf{Diag} (b_{1}(s, n), \ldots, b_{p(s,n)}(s, n))$ with $b_{1}(s, n)\leq\ldots\leq b_{p(s,n)}(s, n)$. Set
\begin{equation*}
B(s, r, n) :=\mathbf{Diag}(b_{(r-1)n+1}(s,n), \ldots, b_{rn}(s,n))\qquad (1\leq r\leq d_{s}),
\end{equation*}
where $b_{i}(s, n) :=0$ for $i>p(s, n)$. It is obvious that the distribution of $B(s, r, n)$ converges almost surely to some part (with renormalization) of $\rho_{s}$ for any $s\in S$ and $1\leq r\leq d_{s}$. Furthermore, we may write
\begin{equation*}
Z(s, n)^{*}B(s, n)Z(s, n)=\sum_{r=1}^{d_{s}}X(s, r, n)^{*}B(s, r, n)X(s, r, n)\,,
\end{equation*}
and the above case says that
\begin{equation*}
\left((\{X(s, r, n)^{*}B(s, r, n)X(s, r, n)\})_{s\in S,\,1\leq r\leq d_{s}},\, \{D(t, n), D(t, n)^{*}:t\in T\}\right)
\end{equation*}
is asymptotically free almost everywhere. This yields the conclusion.
\end{proof2}

Below we treat a single sequence of compound Wishart matrices, and write $Z(n),\, B(n),\, \lambda,\, \rho$ without index $s$. Our aim is to show that the almost sure limit distribution of $Z(n)^{*}B(n)Z(n)$ is the compound free Poisson distribution $\pi_{\rho,\lambda}$. (In particular, the distribution of the complex Wishart matrix $Z(n)^{*}Z(n)$ converges almost surely to the free Poisson distribution $\mu_{\lambda}$.) As it is clear from the above proof that the limit distribution is determined only by $\rho$ and $\lambda$, we may assume that the $B(n)$'s are real constant diagonal matrices and their diagonals are ordered increasingly.

Since $p(n)/n\rightarrow\lambda \in(0, \infty)$, one can choose $d(n)\in \mathbb{N}$ such that $n,  p(n)\leq d(n),\ n/d(n)\rightarrow\theta$ and $p(n)/d(n)\rightarrow\lambda\theta$ for some $0<\theta\leq 1$ (also $\lambda\theta\leq 1$). Let $X(n)$ be a $d(n)\times d(n)$ standard non-selfadjoint Gaussian matrix, $P(n) :=\mathbf{Diag} (1, \ldots, 1,0, \ldots, 0)$ with $n$ ones and $d(n)-n$ zeros, and $\tilde{B}(n):=B(n)\oplus 0_{d(n)-p(n)}$. Then we can write
\begin{equation}
Z(n)^{*}B(n)Z(n)\oplus 0_{d(n)-n}=\frac{d(n)}{n}P(n)X(n)^{*}\tilde{B}(n)X(n)P(n)\,,
\label{ch04:eqn4.4.6}
\end{equation}
where the factor $d(n)/n$ arises because the variance of the real and imaginary parts of $X_{ij}(n)$ is $1/2d(n)$ while that of $Z_{ij}(n)$ is $1/2n$. By Corollary~\ref{ch04:cor4.3.8} the limit distribution of $P(n)X(n)^{*}\tilde{B}(n)X(n)P(n)$ is the distribution of $pc^{*}bcp$ in a $C^{*}$-probability space $(\mathcal{A}, \varphi)$, where $c, p, b\in \mathcal{A}$ are such that
\begin{enumerate}
\item[(i)] $c$ is a circular element,

\item[(ii)] $p$ is a projection with $\varphi(p)=\theta$,

\item[(ii)] $b$ is selfadjoint, $bp=pb$, and the distribution of $b$ is $\lambda\theta\rho+(1-\lambda\theta)\delta(0)$, and

\item[(iv)] $\{c, c^{*}\}$ and $\{p, b\}$ are free.
\end{enumerate}
In this way, we conclude from (\ref{ch04:eqn4.4.6}) that the distribution of the $n\times n$ random matrix $Z(n)^{*}B(n)Z(n)$ converges almost surely to that of $\varphi(p)^{-1}pc^{*}bcp$ in $(p\mathcal{A}p, \varphi(p)^{-1}\varphi|_{p\mathcal{A}p})$. Note that the choice of $0<\theta\leq 1$ is not essential in the above construction. In particular, when $p(n)\leq n$ for all $n\in \mathbb{N}$, we may take $d(n)=n$ (so $\theta=1$ and $p=\textbf{1}$); otherwise we need $\theta<1$.

\begin{lemma}\label{ch04:lem4.4.10}
In a $C^{*}$-probability space $(\mathcal{A}, \varphi)$ let $c$ be a circular element, let $p$ be a projection with $\varphi(p)>0$, and let $b_{1}, \ldots, b_{N}$ be selfadjoint elements such that $b_{i}p=pb_{i}$ and $b_{i}b_{j}=0$ for $i\neq j$. If $\{c, c^{*}\}$ and $\{b_{1}, \ldots, b_{N},p\}$ are free, then $\{pcb_{1}c^{*}p, \ldots, pcb_{N}c^{*}p\}$ is a free family in $(p\mathcal{A}p, \varphi(p)^{-1}\varphi|_{p\mathcal{A}p})$. $($A related result was shown in Example~\emph{\ref{ch02:exa2.6.9}}.$)$
\end{lemma}

\begin{proof2}
One can easily see that there are real constant diagonal matrices $P(n), B_{1}(n), \ldots, B_{N}(n)$ of size $n$ such that $P(n)^{2}=P(n),\, B_{i}(n)B_{j}(n)=0$ for $i\neq j$ and the limit distribution of $(P(n), B_{1}(n), \ldots, B_{N}(n))$ is equal to the distribution of $(p, b_{1}, \ldots, b_{N})$ in $(\mathcal{A}, \varphi)$. Let $X(n)$ be an $n\times n$ standard non-selfadjoint Gaussian. Since by Corollary~\ref{ch04:cor4.3.8}
\begin{equation*}
(\{X(n), X(n)^{*}\},\, \{P(n), B_{1}(n), \ldots, B_{N}(n)\})
\end{equation*}
is asymptotically free, the distribution of $(pcb_{i}c^{*}p)_{1\leq i\leq N}$ in $(\mathcal{A}, \varphi)$ is equal to the limit distribution of $(P(n)X(n)^{*}B_{i}(n)X(n)P(n))_{1\leq i\leq N}$. Set $p(n) :=$ rank $P(n)$, so that $p(n)/n\rightarrow\varphi(p)$. From the disjointness of $B_{i}(n)$ we can write, for $1\leq i\leq N$,
\begin{equation}
P(n)X(n)B_{i}(n)X(n)^{*}P(n)=Z_{i}(n)^{*}B_{i}(n)Z_{i}(n)\oplus 0_{n-p(n)}\,,
\label{ch04:eqn4.4.7}
\end{equation}
where $(Z_{i}(n))_{1\leq i\leq N}$ is an independent family of $p(n)\times n$ Gaussian matrices as in Proposition~\ref{ch04:pro4.4.9}. The proposition says that $(Z_{i}(n)^{*}B_{i}(n)Z_{i}(n))_{1\leq i\leq N}$ is asymptotically free. Hence by (\ref{ch04:eqn4.4.7}) we have the freeness of $(pcb_{i}c^{*}p)_{1\leq i\leq N}$ in $(p\mathcal{A}p, \varphi(p)^{-1}\varphi|_{p\mathcal{A}p})$.
\end{proof2}

\begin{proposition}\label{ch04:pro4.4.11}
The almost sure limit distribution of $Z(n)^{*}B(n)Z(n)$ is the compound free Poisson distribution $\pi_{\rho,\lambda}$.
\end{proposition}

\begin{proof2}
Let $c,p, b$ be as in the above (i)--(iv) (where a $W^{*}$-probability space can be taken for $(\mathcal{A}, \varphi))$. According to the above argument it suffices to prove that $\pi_{\rho,\lambda}$ is the distribution of $\varphi(p)^{-1}pc^{*}bcp$ in $(p\mathcal{A}p, \varphi(p)^{-1}\varphi|_{p\mathcal{A}p})$. First, when $q\in \mathcal{A}$ is a projection, the distribution of $\varphi(p)^{-1}pc^{*}qcp\in p\mathcal{A}p$ is the free Poisson distribution $\mu_{\lambda}$ with $\lambda =\varphi(q)/\varphi(p)$ (with $\mu_{0}=\delta(0))$. Indeed, this can be readily seen by looking at (\ref{ch04:eqn4.4.6}), where $B(n)$ is a diagonal projection $Q(n)$ such that rank $Q(n)/d(n)\rightarrow \varphi(q)$ or rank $Q(n)/n\rightarrow\varphi(q)/\varphi(p)$. (This follows from Example~\ref{ch02:exa2.4.7} as well.) Next assume that the spectrum of $b$ is finite, so $b=\sum_{i=1}^{N}\xi_{i}q_{i}$ is the spectral decomposition and the distribution $\nu_{b}$ of $b$ is given by $\nu_{b}=\sum_{i=1}^{N}\varphi(q_{i})\delta(\xi_{i})$. Since $\{pc^{*}q_{1}cp, \ldots, pc^{*}q_{N}cp\}$ is a free family by Lemma~\ref{ch04:lem4.4.10}, we have
\begin{align*}
\mathcal{R}_{\varphi(p)^{-1}pc^{*}bcp}(z) & \ = \ \sum_{i=1}^{N}\xi_{i}\mathcal{R}_{\varphi(p)^{-1}pc^{*}q_{i}cp}(\xi_{i}z)\\
& \ =\ \sum_{i=1}^{N}\xi_{i}\frac{\varphi(q_{i})/\varphi(p)}{1-\xi_{i}z}=\frac{1}{\varphi(p)}\int\frac{x}{1-xz}\,d\nu_{b}(x)\,.
\end{align*}
For a general $b$, choose a sequence $b_{j}$ with finite spectra such that $b_{j}p=pb_{j}, \Vert b_{j}-b\Vert\rightarrow 0$ and $\{p, b_{j}\}$ is free from $\{c, c^{*}\}$. Since $\Vert pc^{*}b_{j}cp-pc^{*}bcp\Vert\rightarrow 0$, it follows that $\mathcal{R}_{\varphi(p)^{-1}pc^{*}b_{j}cp}(z)\rightarrow \mathcal{R}_{\varphi(p)^{-1}pc^{*}bcp}(z)$ uniformly in a neighborhood of $z=0$ (cf. Lemma~\ref{ch03:lem3.3.4}). Hence we have
\begin{align*}
\mathcal{R}_{\varphi(p)^{-1}pc^{*}bcp}(z) & \ = \ \lim_{j\rightarrow\infty}\frac{1}{\varphi(p)}\int\frac{x}{1-xz}\,d\nu_{b_{j}}(x)\\
& \ = \ \frac{1}{\varphi(p)}\int\frac{x}{1-xz}\,d\nu_{b}(x)=\lambda\int\frac{x}{1-xz}\,d\rho(x),
\end{align*}
because $\varphi(p)=\theta$ and $\nu_{b}=\lambda\theta\rho+(1-\lambda\theta)\delta(0)$. By definition (Sec.~\ref{ch03:sec3.3}) this means that the distribution of $\varphi(p)^{-1}pc^{*}bcp$ in $(p\mathcal{A}p, \varphi(p)^{-1}\varphi|_{p\mathcal{A}p})$ is $\pi_{\rho,\lambda}$.
\end{proof2}

In the above argument one can replace a circular element by a semicircular element. Let $c,p, b$ be as above, and let $a$ be a standard semicircular element free from $\{p, b\}$ in $(\mathcal{A}, \varphi)$. Then both distributions of $\varphi(p)^{-1}pc^{*}bcp$ and $\varphi(p)^{-1} pabap$ in $(p\mathcal{A}p, \varphi(p)^{-1}\varphi|_{p\mathcal{A}p})$ are the same $\pi_{\rho,\lambda}$. Indeed, $p$ is free from $c^{*}bc$ and also from $aba$. This can be seen from Proposition~\ref{ch04:pro4.4.9} and a similar result where the $Z(s,n)$'s are replaced by independent standard selfadjoint Gaussians. Hence it is enough to check that the distributions of $c^{*}bc$ and $aba$ are the same. When $c, a, b$ are realized in a tracial $W^{*}$-probability space, these distributions are equal to those of $|c^{*}|\,b\,|c^{*}|$ and $|a|\,b\,|a|$, respectively. Since $|a|$ as well as $|c^{*}|$ is a quarter-circular free from $b$, we have the conclusion.

\textbf{Notes and Remarks.}There are several surveys about the relation of random matrices to physics; the first one was written by Wigner himself [\citen{bib214}]. More recent ones are [\citen{bib191}] and [\citen{bib95}].

The classic papers of Wigner appeared in 1955 and in 1958 on the eigenvalue distribution of random matrices. His result provided the asymptotic mean eigenvalue density, first in [\citen{bib212}] for matrices taking the entries $\pm 1$, each with probability one-half, then in [\citen{bib213}] for entries distributed symmetrically about zero. Later Arnold [\citen{bib6}] proved that the limit holds almost everywhere, and not only in expectation. In fact, Wigner's main interest was not exactly the eigenvalue distribution but the distribution of the difference of consecutive eigenvalues. Let $\left[\begin{array}{ll}
a & b\\
b & c\\
\end{array}\right]$ be a random matrix and let $\lambda_{1}\geq\lambda_{2}$ be its eigenvalues. Then the ``spacing'' $\lambda_{1}-\lambda_{2}$ is $\sqrt{(a-c)^{2}+4b^{2}}$. If $a, b$ and $c$ are independent Gaussian random variables, then under certain conditions on the dispersions $(\lambda_{1}-\lambda_{2})^{2}$ has the $\chi^{2}$-or exponential distribution. So the density of $\lambda_{1}-\lambda_{2}$ is
\begin{equation*}
p(x)=\left\{\begin{array}{ll}
\frac{\pi}{2}x\exp\left(-\frac{\pi}{4}x^{2}\right) & \mathrm{for}\ x>0,\\
0 & \mathrm{otherwise}.\\
\end{array}\right.
\end{equation*}
It was expected by Wigner, the \textit{Wigner Surmise},\index{Wigner surmise} that if the matrix size tends to infinity, then the difference of the consecutive eigenvalues of a Gaussian matrix has this limit distribution. Actually, Wigner's surmise is inaccurate, cf. [\citen{bib121}], Sec. 1.5.

Mehta's book [\citen{bib121}] and Girko's monograph [\citen{bib87}] are the most comprehensive sources on random matrices. Theorem~\ref{ch04:the4.1.5} is a special case of [\citen{bib6}] with a simpler proof, and an analytic proof in the Gaussian case is found in [\citen{bib146}], Appendix B. The computation of the Fourier transform related to the random matrix $H_{0}+H(n)$ (above Theorem~\ref{ch04:the4.1.7}) is from [\citen{bib45}]. The Laplace transform of the mean eigenvalue density of a standard selfadjoint Gaussian matrix is expressed in terms of confluent hypergeometric functions in [\citen{bib98}].

A more detailed discussion on the \textit{Wishart matrix}\index{Wishart matrix} is in the book [\citen{bib4}]. In particular, the Jacobian of the transformation $S=BTB$ used after Lemma~\ref{ch04:lem4.1.8} is also computed there. A combinatorial proof of Theorem~\ref{ch04:the4.1.9} is in [\citen{bib142}], and it seems that the moments of the Marchenko-Pastur distribution were explicitly given there first. A stronger form of the theorem was obtained earlier by Wachter [\citen{bib211}]. It is not easy to give a precise historical account on the Wishart and random covariance matrices. [\citen{bib98}] contains several interesting old references and a recursion for the moments of the complex Wishart matrix.

It was shown in [\citen{bib79}] that all eigenvalues of the $n\times n$ standard symmetric Gaussian matrix are in $[-c, c]$ for $c>2$ with probability $1-o(1)$ as $n\rightarrow\infty$. There are several other papers concerned with the asymptotics of the largest eigenvalue or the norm (also the spectral radius) of symmetric or non-symmetric random matrices; see [\citen{bib83}], [\citen{bib84}], [\citen{bib10}] for instance. A main result of [\citen{bib98}] with applications to theory of exact $C^{*}$-algebras is a very far-reaching generalization of the fact that the largest eigenvalue of the standard selfadjoint Gaussian matrix converges to 2 almost surely.

According to [\citen{bib121}], complex non-selfadjoint Gaussian matrices were first studied by Ginibre [\citen{bib85}] in 1965.

The \textit{asymptotic free} property of random matrices\index{asymptotically free!random matrices} was established by Voiculescu [\citen{bib201}] in the case of Gaussian random matrices together with diagonal constant matrices. Dykema [\citen{bib61}] proved the same result in the case of general (non-Gaussian) random matrices together with block-diagonal constant matrices (with bounded block-size). The inclusion of constant matrices in these results is useful in applications to von Neumann algebra theory (in particular, to problems on free group factors); see [\citen{bib199}], [\citen{bib155}], [\citen{bib157}], [\citen{bib158}], [\citen{bib63}]. In [\citen{bib201}] Voiculescu obtained the asymptotic freeness of standard unitary random matrices as well by considering the unitary part in the polar decomposition of non-selfadjoint Gaussian matrices. Also, Speicher [\citen{bib174}] used a similar method to prove Proposition~\ref{ch04:pro4.3.9} in the case of non-random $A(n)$ and $B(n)$. Voiculescu [\citen{bib208}] strengthened his asymptotic freeness result so that the restriction on the type of constant matrices was removed. Another proof of the asymptotic freeness is in [\citen{bib167}]. A different approach using Feynman diagrams was treated in [\citen{bib216}] to obtain asymptotic freeness for unitary random matrices (plus constant matrices). Our approach here is opposite to Voiculescu's. In Sec.~\ref{ch04:sec4.3} we first treated the asymptotic freeness of the standard unitary random matrices, and then went to the case of unitarily invariant selfadjoint random matrices (in particular, standard selfadjoint Gaussians) via the diagonalization process. In [\citen{bib189}] Thorbj{\o}rnsen deduced from (\ref{ch04:eqn4.3.14}) that
\begin{equation*}
|\tau_{n}(H(s(1), n)H(s(2), n)\ldots H(s(m), n))-\tau(a_{s(1)}a_{s(2)}\ldots a_{s(m)})|=O(n^{-2}),
\end{equation*}
and used this relation to prove almost sure convergence.

Lemma~\ref{ch04:lem4.3.4} was given in [\citen{bib203}], and it will be used in the next chapter too. Proposition~\ref{ch04:pro4.4.2} is from [\citen{bib199}]; its combinatorial proof is found in [\citen{bib12}]. A detailed discussion on the distribution of matrices under the polar decomposition is found in the book [\citen{bib87}]; Lemma~\ref{ch04:lem4.4.7} is from Sec.~\ref{ch01:sec1.1} of that book, with modifications.

Proposition~\ref{ch04:pro4.4.4} is from [\citen{bib137}]. The relation of bi-unitarily invariant matrix models and $R$-diagonal elements is new. In Theorem~\ref{ch04:the4.4.5} the assumption of almost sure convergence is necessary. If $U(n)$ is a Haar distributed random unitary and $\xi$ is a random variable independent of $U(n)$ with the property $\mathbf{Prob} (\xi=\alpha)= \mathbf{Prob}(\xi=\beta)=1/2\ (\alpha\neq\beta)$, then $\xi U(n)$ is bi-unitarily invariant and has a limit distribution in the sense of joint momemts. However, the limit $x$ is not $R$-diagonal since $x^{*}x=xx^{*}$. This example was communicated to the authors by U. Haagerup. Similarly, let $H(n)$ be a standard selfadjoint Gaussian matrix and $\xi$ a real random variable independent of $H(n)$, taken as above. Then $(H(n), \xi I_{n})$ has the limit distribution in the sense of mean convergence, but the free relation does not appear in the limit. The concept of asymptotic freeness almost everywhere should be more appropriate than the plain asymptotic freeness when unitarily invariant random matrices are concerned.

Propositions~\ref{ch04:pro4.4.9} and \ref{ch04:pro4.4.11} on compound Wishart matrices are new, while results similar to Lemma~\ref{ch04:lem4.4.10} were given in [\citen{bib136}]. The authors thank R. Speicher for bringing compound Wishart matrices and compound free Poisson distributions to their attention.

\chapter{Large Deviations for Random Matrices}
\label{ch05:chap05}

\noindent The concept of entropy originated from thermodynamics and became a mathematical notion in the work of Gibbs and Boltzmann. Later it got importance in information theory and in the statistical problem of testing hypothesis. The entropy $-\int f(x)\log f(x)\,dx$ of a probability density $f$ appears mostly in limit theorems.

Even the \textit{central limit theorem}\index{central limit theorem} of probability theory is understandable in terms of entropy. Let $\xi_{1}, \xi_{2}, \ldots$ be a sequence of independent identically distributed random variables of mean $0$. Then the random variables $\eta_{n}=(\xi_{1}+\xi_{2}+\cdots+\xi_{n})/\sqrt{n}$ have the same variance and their entropy is increasing (when $n$ runs over the powers of 2). The limiting Gaussian variable has maximal entropy among distributions of given variance.

The reason for the observation that entropy shows up in so many limit problems is the fact that this quantity often governs the asymptotics of probabilities. This is very clear in the large deviation theory, which concerns limit theorems with exponential convergence. The rate of the convergence is described by an entropy functional.

Voiculescu recognized that the free relation may be modeled asymptotically by independent random matrices, and studied the asymptotics of the Boltzmann-Gibbs entropy of large random matrices. In this way he arrived at the appropriate entropy concept from the point of view of the free relation. In fact, the same quantity has been used in potential theory under the name logarithmic energy. However, Voiculescu's work opened a completely new perspective for the logarithmic energy, and the new terminology ``free entropy'' expresses the new aspects.

The free entropy appears as an important component of the rate function in large deviation theorems for random matrices. Wigner's original result was the convergence of the mean eigenvalue distribution of a certain Gaussian random matrix to the semicircle law. His result was improved by showing that the empirical eigenvalue distribution converges almost everywhere. The large deviation result was hinted at in the work of Voiculescu, but first proven by Ben Arous and Guionnet. According to this result the convergence is exponentially fast. Besides the selfadjoint Gaussian case we treat some non-selfadjoint random matrices and some unitary random matrices. In all cases the limiting eigenvalue distribution is determined by minimization of the rate function. The minus of the rate function is the weighted logarithmic energy familiar in potential theory, and the minimization problem can be solved by means of a general theorem of Mhaskar and Saff.

This chapter explains the background of Voiculescu's free entropy for a single (selfadjoint) noncommutative random variable. The logarithmic double integral is in the rate function and therefore it could be expressed as the limit of volumes of approximating matrices. In fact, this will be the way towards the extension of the concept of free entropy to several noncommutative random vairables, which will be the subject of the next chapter.

\section{Boltzmann entropy and large deviations}
\label{ch05:sec5.1}

\noindent In this section the flavour of large deviation theory is given after a concise discussion of the Boltzmann-Gibbs or differential entropy and relative entropy.\index{entropy!relative}\index{relative entropy} We approach large deviations from the law of large numbers and observe that the rate of the exponential convergence is strongly related to an entropy functional.

Let $\mu$ be a measure on $\mathbb{R}^{n}$ with density $f(x)\equiv f(x_{1}, x_{2}, \ldots, x_{k})$. The \textit{Boltzmann-Gibbs} (or \textit{differential}) \textit{entropy}\index{differential entropy} of $\mu$ is defined as
\begin{equation*}
S(\mu)=S(f) :=-\int f(x)\log f(x)\,dx
\end{equation*}
(whenever this has a meaning). In particular, if $\mu$ has a \textit{multivariate normal distribution}\index{distribution!normal, multivariate} $N(m, \Sigma)$ whose density is
\begin{equation}
\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\tfrac{1}{2}\langle \Sigma^{-1}(x-m), (x-m)\rangle\right)
\label{ch05:eqn5.1.1}
\end{equation}
with the mean vector $m$ and the covariance matrix $\Sigma$, then its entropy is equal to
\begin{equation*}
\frac{n}{2}\log(2\pi e)+\frac{1}{2}\log|\Sigma|\,,
\end{equation*}
where $|\Sigma|$ denotes the determinant of $\Sigma$.

To explain the behavior of the Boltzmann-Gibbs\index{entropy!Boltzmann-Gibbs} entropy\index{Boltzmann-Gibbs entropy} we shall use the apparently more complicated concept of relative entropy. If $\mu_{i}\ (i=1,2)$ are measures on $\mathbb{R}^{n}$ with densities $f_{i}(x)$, then the \textit{relative entropy} of $\mu_{1}$ with respect to $\mu_{2}$ is defined as
\begin{equation}
S(\mu_{1}, \mu_{2})=S(f_{1}, f_{2}) :=\int f_{1}(x)(\log f_{1}(x)-\log f_{2}(x))\,dx.
\label{ch05:eqn5.1.2}
\end{equation}
This quantity is nonnegative when $\mu_{1}$ and $\mu_{2}$ are probability measures. It is worth noting that $S(\mu_{1}, \mu_{2})$ is not symmetric in its two variables; $\mu_{1}$ and $\mu_{2}$ play really different roles.

Assume now that $\mu_{1}$ has the mean vector $m$ and the covariance matrix $C$, and moreover $\mu_{2}$ is normal with the density (\ref{ch05:eqn5.1.1}). Then
\begin{equation*}
S(\mu_{1}, \mu_{2})=-S(\mu_{1})+\frac{n}{2}\log(2\pi)+\frac{1}{2}\log|\Sigma|+\frac{1}{2}\mathrm{Tr}(\Sigma^{-1}C)\,.
\end{equation*}
So we can see that the Boltzmann-Gibbs entropy is essentially the relative entropy with respect to a Gaussian distribution with the same mean and covariance up to a $\mathrm{sign}$ and an additive constant depending on dimension and covariance. Moreover, the nonnegativity of $S(\mu_{1}, \mu_{2})$ yields the inequality
\begin{equation}
S(\mu_{1})\leq\frac{n}{2}\log(2\pi e)+\frac{1}{2}\log|C|
\label{ch05:eqn5.1.3}
\end{equation}
if $\Sigma=C$ is chosen. Thus we have proved the known fact that the normal density (\ref{ch05:eqn5.1.1}) maximizes the Boltzmann-Gibbs entropy if the mean vector $m$ and the covariance matrix $\Sigma$ are fixed.

Another simple observation is that $S(\mu_{1}, \mu_{2})=-S(\mu_{1})$ if $\mu_{2}$ is chosen to be the Lebesgue measure on $\mathbb{R}^{n}$. This fact is less useful, because the Lebesgue measure is infinite on $\mathbb{R}^{n}$. However, if $\mu_{1}$ is compactly supported, then the Lebesgue measure on the support can be normalized into a probability measure.

The relative entropy functional often appears as the rate function in large deviation theorems. Given a complete separable metric space $\mathcal{X}$, assume that a sequence $(\nu_{n})$ of probability measures on $\mathcal{X}$ converges to a point measure $\delta(x_{0})$ at $x_{0}\in \mathcal{X}$. (It will be shown below that the setting of the law of large numbers provides such a situation.) For an open set $G$ whose closure does not contain $x_{0}$, we have $\nu_{n}(G)\rightarrow 0$. In order to speak of large deviation it is necessary that this convergence should be exponential. When
\begin{equation}
\liminf_{n\rightarrow\infty}L_{n}\log \nu_{n}(G)\geq-\inf\{I(x):x\in G\}
\label{ch05:eqn5.1.4}
\end{equation}
for some sequence $0<L_{n}\rightarrow 0$ and for a nonnegative function $I$ on $\mathcal{X}$, there is room for the large deviation theory. Given a nonnegative and lower semicontinuous function $I$ on $\mathcal{X}$, we say that $(\nu_{n})$ satisfies the \textit{large deviation principle}\index{large deviation!principle} with the \textit{rate function}\index{rate function} $I$ (in the scale $L_{n}$) if for every Borel subset $\Gamma$ of $\mathcal{X}$ we have
\begin{align*}
-\inf\{I(x):x\in\Gamma^{\circ}\} & \ \leq \ \liminf_{n\rightarrow\infty}L_{n}\log \nu_{n}(\Gamma)\\
& \ \leq\ \limsup_{n\rightarrow\infty} L_{n}\log\nu_{n}(\Gamma)\leq-\inf\{I(x):x\in\overline{\Gamma}\},
\end{align*}
where $\Gamma^{\circ}$ and $\overline{\Gamma}$ denote the interior and closure of $\Gamma$. This is equivalent to saying that (\ref{ch05:eqn5.1.4}) holds for every open set $G\subset \mathcal{X}$ and
\begin{equation}
\limsup_{n\rightarrow\infty}L_{n}\log \nu_{n}(F)\leq-\inf\{I(x):x\in F\}
\label{ch05:eqn5.1.5}
\end{equation}
holds for every closed set $F\subset \mathcal{X}$. If the latter condition (\ref{ch05:eqn5.1.5}) holds only for compact $F$ (and the former condition is satisfied as well), then the \textit{weak large deviation principle} is said to hold. When the level sets $\{x\in \mathcal{X}:I(x)\leq c\}$ are compact for all $c\geq 0$, it is often said that $I$ is a \textit{good rate function}.\index{rate function!good}\index{good rate function}

Let $\eta_{1}, \eta_{2}, \ldots$ be random variables and $\nu_{n}$ the distribution of $\eta_{n}$. Assume that the limit
\begin{equation}
F(\lambda) :=\lim_{n\rightarrow\infty}\frac{1}{n}\log E(\exp n\lambda\eta_{n})
\label{ch05:eqn5.1.6}
\end{equation}
exists and is finite for every $\lambda\in \mathbb{R}$, and moreover assume that $F(\lambda)$ is a differentiable function of $\lambda$. Under these conditions the \textit{Ellis theorem}\index{Ellis theorem} ([\citen{bib73}], Theorem II.6.1) tells us that the large deviation principle holds for $(\nu_{n})$ with $L_{n}=n^{-1}$ and with the rate function
\begin{equation*}
I(t):=\sup\{t\lambda-F(\lambda):\lambda\in \mathbb{R}\},
\end{equation*}
which is the convex conjugate of (\ref{ch05:eqn5.1.6}). In particular, when $\eta_{n}$ has the normal distribution with mean $0$ and variance $\sigma^{2}/n$, we have $F(\lambda)=\sigma^{2}\lambda^{2}/2$ and $I(t)= t^{2}/2\sigma^{2}$. In this case, the rough meaning of the large deviation result is
\begin{equation*}
\mathbf{Prob}(|\eta_{n}|\geq u)\approx\exp(-nu^{2}/2\sigma^{2})\quad \mathrm{for}\quad u>0.
\end{equation*}

Next, the so-called \textit{level-2 large deviation\index{large deviation!level-2} theorem} will be explained for a sequence $\xi_{1}, \xi_{2}, \ldots$ of independent identically distributed random variables. In brief, this theorem states the exponential convergence of the sequence
\begin{equation}
\frac{\delta(\xi_{1})+\delta(\xi_{2})+\cdots+\delta(\xi_{n})}{n}
\label{ch05:eqn5.1.7}
\end{equation}
of atomic random measures to the point measure $\delta_{\nu}$, where $\nu$ is the common distribution of the random variables $\xi_{n}$. Note that the convergence itself without the additional features is just the law of large numbers. The complete separable metric space in the present case is the space $\mathcal{X}=\mathcal{M}(\mathbb{R})$ of probability Borel measures on $\mathbb{R}$. $\mathcal{M}(\mathbb{R})$ is endowed with the weak topology. $(\mathcal{M}(\mathbb{R})$ is a metrizable space; for instance, the so-called L\'{e}vy metric is compatible with the weak topology, see (\ref{ch04:eqn4.3.9}).) The random measure (\ref{ch05:eqn5.1.7}) is regarded as a random variable with values in $\mathcal{M}(\mathbb{R})$. The \textit{large deviation principle}\index{large deviation!principle, weak}\index{weak large deviation principle}\index{large deviation!principle} consists of two conditions; one refers to an open set $G\subset \mathcal{M}(\mathbb{R})$ and the other one tells about a closed set $F\subset \mathcal{M}(\mathbb{R})$. Namely,
\begin{align*}
&\limsup_{n\rightarrow\infty}\frac{1}{n}\log \mathbf{Prob}\left(\frac{\delta(\xi_{1})+\delta(\xi_{2})+\cdots+\delta(\xi_{n})}{n}\in F\right)\leq-\inf\{I(\mu):\mu\in F\}\,,\\
&\,\liminf_{n\rightarrow\infty}\frac{1}{n}\log\mathbf{Prob}\left(\frac{\delta(\xi_{1})+\delta(\xi_{2})+\cdots+\delta(\xi_{n})}{n}\in G\right)\geq-\inf\{I(\mu):\mu\in G\}\,,
\end{align*}
where $I(\mu) :=S(\mu, \nu)$ is the \textit{relative entropy}\index{entropy!relative}\index{relative entropy} of $\mu$ with respect to $\nu$, defined by
\begin{equation*}
S(\mu, \nu) :=\int\frac{d\mu}{d\nu}\log\frac{d\mu}{d\nu}\,d\nu=\int\log\frac{d\mu}{d\nu}\,d\mu
\end{equation*}
if $\mu$ is absolutely continuous with respect to $\nu$ and $d\mu/d\nu$ is the Radon-Nikod\'{y}m derivative; otherwise $S(\mu,\nu) := +\infty$. This is written as (\ref{ch05:eqn5.1.2}) whenever $\mu, \nu$ have the densities. The relative entropy $S(\cdot\,, \nu)$ is a weakly lower semicontinuous, strictly convex functional, and its level sets are known to be compact. In this way, the large deviation principle in the sense of the above definition holds with the good rate function $S(\cdot\,, \nu)$. This is the fundamental example of a level-2 large deviation theorem, due to Sanov ([\citen{bib56}], Sec.~\ref{ch03:sec3.2}).

Let $\mu$ and $\nu$ be probability measures on $\mathbb{R}$ and assume that $\mu$ is compactly supported. Let $\nu^{n}$ denote the $n$-fold product measure $\nu\otimes \nu\otimes\cdots\otimes \nu$ and $m_{k}(\mu)$ the $k$th moment of $\mu$, i.e. $m_{k}(\mu)=\int x^{k}\,d\mu(x)$ for $k\in \mathbb{N}$. For $x=(x_{1}, \ldots, x_{n})\in \mathbb{R}^{n}$ we denote by $\kappa_{n}(x)$ the discrete measure $\tfrac{1}{n}(\delta(x_{1})+\delta(x_{2})+\cdots+\delta(x_{n}))$. Then we can apply the Sanov theorem to show the following:

\begin{proposition}\label{ch05:pro5.1.1}
Assume that $\mu$ and $\nu$ are probability measures on $\mathbb{R}$ and $\mu$ is supported in a compact subset $K$ of $\mathbb{R}$. Then
\begin{align*}
& \lim_{\begin{subarray}{c}r\rightarrow\infty\\ \varepsilon\rightarrow+0\\ \end{subarray}}\limsup_{n\rightarrow\infty}\frac{1}{n}\log \nu^{n}(\{x\in \mathbb{R}^{n}:|m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\})\\
& \qquad = \lim_{\begin{subarray}{c}r\rightarrow\infty\\ \varepsilon\rightarrow+0\\ \end{subarray}} \liminf_{n\rightarrow\infty}\frac{1}{n}\log\nu^{n} (\{x\in \mathbb{R}^{n}:|m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\})\\
& \qquad = \lim_{\begin{subarray}{c}r\rightarrow\infty\\ \varepsilon\rightarrow+0\\ \end{subarray}}\lim_{n\rightarrow\infty}\frac{1}{n}\log \nu^{n}(\{x\in K^{n}:|m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\})\\
& \qquad =-S(\mu, \nu)\,.
\end{align*}
\end{proposition}

\begin{proof2}
First, we prove the last equality by application of the Sanov theorem\index{Sanov theorem} to the conditional measure $\nu_{K} :=\nu/\nu (K)$ in the space $\mathcal{M}(K)$. For $r\in \mathbb{N}$ and $\varepsilon >0$ the set
\begin{equation*}
F(r, \varepsilon) :=\{\kappa\in\mathcal{M}(K) : |m_{k}(\kappa)-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\}
\end{equation*}
is closed in $\mathcal{M}(K)$, and, replacing $\leq \varepsilon$ by $<\varepsilon$, we get an open set $G(r, \varepsilon)$. The Sanov theorem says that
\begin{align*}
&\limsup_{n\rightarrow\infty}\frac{1}{n}\log \nu_{K}^{n}(\{x\in K^{n}: \kappa_{n}(x)\in F(r, \varepsilon ) \})\\
& \qquad \leq-\inf\{S(\kappa, \nu_{K}) : \kappa\ \in F(r, \varepsilon )\ \}\,,
\end{align*}
and the opposite inequality also holds if $\limsup$ is replaced by $\liminf$ and $F(r, \varepsilon)$ by $G(r, \varepsilon)$. If $\inf\{S(\kappa, \nu_{K}):\kappa \in F(r, \varepsilon)\}=+\infty$ for some $r=r_{1}$ and $\varepsilon =\varepsilon_{1}$, then
\begin{align*}
& \lim_{n\rightarrow\infty}\frac{1}{n}\log\nu_{K}^{n}(\{x\in K^{n} : \kappa_{n}(x)\in F(r, \varepsilon)\})\\
& \qquad =-\inf\{S(\kappa, \nu_{K}) : \kappa \in F(r, \varepsilon)\}=-\infty
\end{align*}
for every $r\geq r_{1}$ and $\varepsilon \leq \varepsilon_{1}$. So assume that $\inf\{S(\kappa, \nu_{K}): \kappa \in F(r, \varepsilon)\}<+\infty$ for all $r$ and $\varepsilon$. Then, by the convexity of relative entropy, it is easy to check that
\begin{equation*}
\inf\{S(\kappa, \nu_{K}): \kappa \in F(r, \varepsilon)\}=\inf\{S(\kappa, \nu_{K}): \kappa \in G(r, \varepsilon)\}.
\end{equation*}
Since $S(\kappa, \nu_{K})=S(\kappa, \nu) +\log \nu(K)$ for $\kappa \in \mathcal{M}(K)$, we have
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{1}{n}\log \nu^{n} (\{x\in K^{n}:\kappa_{n}(x)\in F(r, \varepsilon)\})=-\inf\{S(\kappa, \nu):\kappa \in F(r, \varepsilon)\}.
\end{equation*}
Note that the sets $G(r, \varepsilon)$ constitute a neighborhood base of $\mu$ in $\mathcal{M}(K)$. Hence the infimum in the above tends to $S(\mu,\nu)$ as $\varepsilon \rightarrow\infty$ and $r\rightarrow+0$, so the last equality holds true.

Next, one can apply the Sanov theorem to a measurable subset
\begin{equation*}
\Gamma(r, \varepsilon):=\{\kappa\in \mathcal{M}(\mathbb{R}):|m_{k}(\kappa)-m_{k}(\mu)|\leq\varepsilon ,\ k\leq r\}
\end{equation*}
of $\mathcal{M}(\mathbb{R})$, to get
\begin{equation*}
\limsup_{n\rightarrow\infty}\frac{1}{n}\log \nu^{n} (\{x\in \mathbb{R}^{n} : \kappa_{n}(x)\in\Gamma(r, \varepsilon)\})\leq-\inf\{S(\kappa, \nu) : \kappa \in\overline{\Gamma(r,\varepsilon)}\}.
\end{equation*}
So it remains to show that
\begin{equation*}
\lim_{\begin{subarray}{c}r\rightarrow\infty\\ \varepsilon\rightarrow+0\\ \end{subarray}}\inf\{S(\kappa, \nu) :\kappa \in\overline{\Gamma(r,\varepsilon)}\}=S(\mu, \nu)\,.
\end{equation*}
But, by the weak lower semicontinuity of $S(\cdot,\nu)$, it suffices to see that $\kappa_{j}\rightarrow\mu$ weakly for any $\kappa_{j}\in\Gamma(r_{j}, \varepsilon_{j})$ with $r_{j}\rightarrow\infty$ and $\varepsilon_{j}\rightarrow+0$. This is a consequence of the known fact on convergence of probability measures mentioned just before Lemma~\ref{ch04:lem4.3.4}.
\end{proof2}

For the above $\nu$ we can choose a Gaussian measure with the variance $\sigma^{2}$ of $\mu$. In this case the double limit in the previous proposition is $S(\mu)-\tfrac{1}{2}\log(2\pi e\sigma^{2})$. The measure $\mu$ being compactly supported, we may choose $\nu$ as the normalized Lebesgue measure of a large compact interval. The normalizing constant $c$ contributes to both sides a term $\log c$, so they are eliminated. Therefore, we have
\begin{align}
&\lim_{\begin{subarray}{c}r\rightarrow\infty\\ \varepsilon\rightarrow+0\\ \end{subarray}}\lim_{n\rightarrow\infty}\frac{1}{n}\log\lambda^{n}(\{x\in[-R, R]^{n}:|m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon,\, k\leq r\}) \notag\\
& \qquad =S(\mu)\,,
\label{ch05:eqn5.1.8}
\end{align}
where $R$ is large enough and $\lambda^{n}$ is the $n$-dimensional Lebesgue measure. This formula is understood as follows. For a fixed measure $\mu$ on $\mathbb{R}$ we consider the set $G_{n}(r, \varepsilon)$ of all $x\in \mathbb{R}^{n}$ such that $\kappa_{n}(x)$ approximates $\mu$ in the moments up to $r$ and to the extent $\varepsilon$. Then the entropy of $\mu$ governs the asymptotics of $\lambda^{n}(G_{n}(r, \varepsilon))$ when $\varepsilon$ is very small and $r$ is very large.

Let us emphasize that in Proposition~\ref{ch05:pro5.1.1} the reference measure $\nu^{n}$ is of product type. One can regard the content of the next section in such a way that instead of a product measure we use the joint distribution of the eigenvalues of a certain random matrix such as a standard symmetric Gaussian matrix. The eigenvalues of such a Gaussian matrix are not independent; physicists would say they repel each other.

\section{Entropy and random matrices}
\label{ch05:sec5.2}

\noindent In this section we deal with the analogue of Proposition~\ref{ch05:pro5.1.1} when the measure $\nu^{n}$ is different. To motivate the choice of the measure we first solve an entropy maximization problem. When the mean and covariance are fixed, the Gaussian random variable has maximal Boltzmann-Gibbs entropy. The analogue of this statement for random matrices will be formulated in terms of the tracial functional $\tau_{n}$, given as
\begin{equation*}
\tau_{n}(X):=\frac{1}{n}\sum_{i=1}^{n}E(X_{ii})
\end{equation*}
for an $n\times n$ random matrix $X$ (whenever the expectations exist). The entropy of an $n\times n$ symmetric random matrix $[\xi_{ij}]_{i,j=1}^{n}$ is understood as the Boltzmann-Gibbs entropy of the joint distribution of the random variables $\{\xi_{ij}:1\leq i\leq j\leq n\}$, that is, the upper diagonal part of the matrix. Then the symmetric Gaussian matrix maximizes the entropy under a constraint on $\tau_{n}(X^{2})$. This is the content of the first proposition of this section. Later on, the joint distribution of the eigenvalues of the symmetric Gaussian matrix will be chosen to be the reference measure on $\mathbb{R}^{n}$. In this way we can arrive at Voiculescu's entropy along the lines of Proposition~\ref{ch05:pro5.1.1}.

\begin{proposition}\label{ch05:pro5.2.1}
The entropy on the set of $n\times n$ symmetric random matrices $T$ satisfying the conditions $\tau_{n}(T)=m$ and $\tau_{n}((T-mI)^{2})=\sigma^{2}$ is maximized by the matrix $[\xi_{ij}]$ which has Gaussian independent entries and
\begin{equation*}
E(\xi_{ij})=\delta_{ij}m,\quad E((\xi_{ii}-m)^{2})=\frac{2\sigma^{2}}{n+1}=2E(\xi_{ij}^{2})\quad for \quad i\neq j.
\end{equation*}
For $m=0$ and $\sigma=1$ the maximizer is the standard symmetric\index{Gaussian matrix!standard symmetric} Gaussian matrix.\index{entropy maximum!symmetric matrices}
\end{proposition}

\begin{proof2}
We have to maximize the Boltzmann-Gibbs entropy of the random variables $\{\xi_{ij}:1\leq i\leq j\leq n\}$ under the constraints
\begin{equation*}
\sum_{i=1}^{n}E(\xi_{ii})=nm,\ \sum_{i=1}^{n}E((\xi_{ii}-m)^{2})+2\sum_{i<j}E(\xi_{ij}^{2})=n\sigma^{2}\,.
\end{equation*}
We may assume that $m=0$, and we know from the additivity and subadditivity of the entropy that the maximum entropy is attained when the variables $\xi_{ij}$ are independent. Let $m_{ij}$ be the mean and $v_{ij}$ the variance of $\xi_{ij}$. By (\ref{ch05:eqn5.1.3}) the entropy of the joint distribution is estimated from above by that of a normal distribution. Up to an additive and a multiplicative constant the entropy is $\sum_{i\leq j}\log v_{ij}$, and we trivially have
\begin{equation*}
\sum_{i\leq j}\log v_{ij}\leq\sum_{i\leq j}\log E(\xi_{ij}^{2}).
\end{equation*}
The arithmetic-geometric mean inequality gives
\begin{align*}
& 2^{n(n-1)/2}\prod_{i\leq j}E(\xi_{ij}^{2})=\prod_{i=1}^{n}E(\xi_{ii}^{2})\prod_{i<j}2E(\xi_{ij}^{2})\\
& \qquad \leq\left(\frac{\sum_{i=1}^{n}E(\xi_{ii}^{2})+\sum_{i<j}2E(\xi_{ij}^{2})}{n(n+1)/2}\right)^{n(n+1)/2}=\left(\frac{n\sigma^{2}}{n(n+1)/2}\right)^{n(n+1)/2}
\end{align*}
Analysis of all these inequalities shows that they are saturated when (and only when) $\xi_{ij}$ has the normal distribution with mean zero and $E(\xi_{ii}^{2})=2E(\xi_{ij}^{2})$ for $i\neq j$.
\end{proof2}

The distribution of the maximizer in the previous proposition will be denoted by $N^{(n)}(m, \sigma^{2})$. In particular, $N^{(n)}(0,1)$ is the distribution of the $n\times n$ standard symmetric Gaussian matrix familiar from Example~\ref{ch04:exa4.1.1}. We return to the subject of maximal entropy matrix ensembles after the proof of Voiculescu's theorem.

It was Voiculescu's idea to modify the formula of Proposition~\ref{ch05:pro5.1.1} and to use it as a definition of another kind of entropy of a measure $\mu$. The modified formula is written as
\begin{align}
&\lim_{\begin{subarray}{c}r\rightarrow\infty\\ \varepsilon\rightarrow+0\\ \end{subarray}}\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log \nu_{n}(\{A\in M_{n}(\mathbb{R})^{sa}: \notag\\
& \qquad \qquad \qquad \qquad \qquad |\mathrm{tr}_{n}(A^{k})-m_{k}(\mu)|\leq \varepsilon,\ k\leq r\}),
\label{ch05:eqn5.2.1}
\end{align}
where $\nu_{n}$ must be a measure on $M_{n}(\mathbb{R})^{sa}$. Observe that the original formula of Proposition~\ref{ch05:pro5.1.1} is recovered if we require $A$ to be diagonal and replace the normalization $1/n^{2}$ by $1/n$. The Gaussian measure $N^{(n)}(0, \sigma^{2})$ is a natural candidate for $\nu_{n}$. The advantage of the Gaussian measure is the invariance under orthogonal transformations, which allows the eigenvalues of $A$ to enter. If $(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})$ are the eigenvalues of $A$, then $\tau(A^{k})=\tfrac{1}{n}\sum_{i=1}^{n}\lambda_{i}^{k}=m_{k}(\kappa_{n}(\lambda))$ and
\begin{align*}
& \nu_{n}(\{A\in M_{n}(\mathbb{R})^{sa} : |\mathrm{tr}_{n}(A^{k})-m_{k}(\mu)|\leq \varepsilon,\ k\leq r\})\\
& \quad \ \ =\bar{\nu}_{n}(\{\lambda\in \mathbb{R}^{n}:|m_{k}(\kappa_{n}(\lambda))-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\}),
\end{align*}
where $\bar{\nu}_{n}$ is the measure on $\mathbb{R}^{n}$ induced by $\nu_{n}$.

According to the density (\ref{ch04:eqn4.1.9}) the measure $N^{(n)}(0, \sigma^{2})$ on $M_{n}(\mathbb{R})^{sa}$ (the real symmetric matrices) induces the joint probability density
\begin{equation*}
\frac{1}{Z_{n}}\exp\left(-\frac{n+1}{4\sigma^{2}}\sum_{i=1}^{n}\lambda_{i}^{2}\right)\prod_{i<j}|\lambda_{i}-\lambda_{j}|
\end{equation*}
on $\mathbb{R}^{n}$ (the space of the eigenvalues), where $Z_{n}$ is the normalization constant. Slightly more generally, we consider the probability measure on $\mathbb{R}^{n}$ having the probability density
\begin{equation}
\frac{1}{Z_{\beta,\sigma}^{(n)}}\exp\left(-\frac{n+1}{4\sigma^{2}}\sum_{i=1}^{n}\lambda_{i}^{2}\right)\prod_{i<j}|\lambda_{i}-\lambda_{j}|^{2\beta},
\label{ch05:eqn5.2.2}
\end{equation}
where $\beta$ is a fixed positive number.

\begin{theorem}\label{ch05:the5.2.2}
Let $\bar{\nu}_{n}$ denote the probability measure with density \emph{(\ref{ch05:eqn5.2.2})} on $\mathbb{R}^{n}$. If $\mu$ is a compactly supported probability measure on $\mathbb{R}$, then
\begin{align*}
&\lim_{\begin{subarray}{c}r\rightarrow\infty\\ \varepsilon\rightarrow+0\\ \end{subarray}}\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log\bar{\nu}_{n}(\{\lambda\in \mathbb{R}^{n}:|m_{k}(\kappa_{n}(\lambda))-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\})\\
& \qquad =\beta\iint\log|x-y|\,d\mu(x)\,d\mu(y)-\frac{1}{4\sigma^{2}}\int x^{2}\,d\mu(x)-\frac{\beta}{2}\log(2\beta\sigma^{2})+\frac{3\beta}{4}.
\end{align*}
The same holds true when $\limsup$ is replaced by $\liminf$.
\end{theorem}

\begin{proof2}
First, we deal with the asymptotics of the normalization constant $Z_{\beta,\sigma}^{(n)}$. Rewriting (\ref{ch04:eqn4.1.7}), we get
\begin{align}
Z_{\beta,\sigma}^{(n)} & \ =  \ \int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}\exp\left(-\frac{n+1}{4\sigma^{2}}\sum_{i=1}^{n}x_{i}^{2}\right)\prod_{i<j}|x_{i}-x_{j}|^{2\beta}\,dx_{1}\cdots dx_{n} \notag\\
& \ = \ (2\pi)^{n/2}\left(\frac{n+1}{2\sigma^{2}}\right)^{-n(\beta(n-1)+1)/2}\prod_{j=1}^{n}\frac{\Gamma(1+j\beta)}{\Gamma(1+\beta)}\,.
\label{ch05:eqn5.2.3}
\end{align}
The asymptotics of $\Gamma(1+j\beta)$ is provided by the \textit{Stirling formula}\index{Stirling formula}
\begin{equation*}
\log\Gamma(1+x)=x\log x-x+\frac{1}{2}\log x+\frac{1}{2}\log(2\pi)+o(1).
\end{equation*}
Hence
\begin{align*}
& \lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{\beta,\sigma}^{(n)}\\
& \qquad =\lim_{n\rightarrow\infty}\left[\frac{\beta}{2}\log(2\sigma^{2})-\frac{\beta}{2}\log(n+1)+\frac{1}{n^{2}}\sum_{j=1}^{n}\beta j\log\beta j-\frac{\beta}{2}\right]\\
& \qquad =\frac{\beta}{2}(\log(2\sigma^{2})-1)+\lim_{n\rightarrow\infty}\frac{\beta}{n}\sum_{j=1}^{n}\frac{j}{n}\log\frac{\beta j}{n}\\
& \qquad =\frac{\beta}{2}(\log(2\sigma^{2})-1)+\beta\int_{0}^{1}x\log\beta x\,dx\\
& \qquad =\frac{\beta}{2}\log(2\beta\sigma^{2})-\frac{3\beta}{4},
\end{align*}
and we write $B$ for this number.

Let $\Gamma_{n}(\mu;r, \varepsilon)$ denote the set whose measure $\bar{\nu}_{n}$ is taken in the statement of the theorem. Then it suffices to show the following two estimates:
\begin{align}
& \liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log\bar{\nu}_{n}(\Gamma_{n}(\mu;r, \varepsilon)) \notag\\
& \qquad \geq\beta\iint\log|x-y|\,d\mu(x)\,d\mu(y)-\frac{1}{4\sigma^{2}}\int x^{2}\,d\mu(x)-B
\label{ch05:eqn5.2.4}
\end{align}
for every $r\in \mathbb{N}$ and $\varepsilon >0$, and
\begin{align}
&\lim_{\begin{subarray}{c}r\rightarrow\infty\\ \varepsilon \rightarrow +0 \\ \end{subarray}}\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log\bar{\nu}_{n}(\Gamma_{n}(\mu;r, \varepsilon)) \notag\\
& \qquad \leq\beta\iint\log|x-y|\,d\mu(x)\,d\mu(y)-\frac{1}{4\sigma^{2}}\int x^{2}\,d\mu(x)-B\,.
\label{ch05:eqn5.2.5}
\end{align}

To show the lower estimate (\ref{ch05:eqn5.2.4}), a suitable regularization argument enables us to assume that the support of $\mu$ is an interval $[a, b]$ and $\mu$ has a continuous density $f>0$ on $[a, b]$, so that $\delta\leq f(x)\leq\delta^{-1}\ (a\leq x\leq b)$ for some $\delta>0$. The details of this argument are omitted here; we shall give them in the proof of a large deviation theorem in Sec.~\ref{ch05:sec5.4} (see the proof of Lemma~\ref{ch05:lem5.4.6}). For each $n\in \mathbb{N}$ let $a<a_{1}^{(n)}<b_{1}^{(n)}<a_{2}^{(n)}<\ldots<a_{n}^{(n)}<b_{n}^{(n)}=b$ be such that
\begin{equation*}
\int_{a}^{a_{i}^{(n)}}f(x)\,dx=\frac{i-\frac{1}{2}}{n},\quad \int_{a}^{b_{i}^{(n)}}f(x)\,dx=\frac{i}{n} \qquad (1\leq i\leq n).
\end{equation*}
Then it is immediate that
\begin{equation}
\frac{\delta}{2n}\leq b_{i}^{(n)}-a_{i}^{(n)}\leq\frac{1}{2n\delta}\qquad (1\leq i\leq n).
\label{ch05:eqn5.2.6}
\end{equation}
Furthermore, if we define
\begin{equation*}
\Delta_{n}:=\{(\lambda_{1}, \lambda_{2}, \ldots,\ \lambda_{n})\in \mathbb{R}^{n}:a_{i}^{(n)}\leq\lambda_{i}\leq b_{i}^{(n)},\ 1\leq i\leq n\}\,,
\end{equation*}
then it is obvious that
\begin{equation*}
\Delta_{n}\subset\Gamma_{n}(\mu;r, \varepsilon)
\end{equation*}
for all $n$ large enough. We want to estimate the measure $\bar{\nu}_{n}$ of $\Delta_{n}$. Recall that $\bar{\nu}_{n}(\Delta_{n})$ is the integral of the function (\ref{ch05:eqn5.2.2}) over $\Delta_{n}$. On $\Delta_{n}$ we have
\begin{equation*}
\lambda_{j}-\lambda_{i}\geq a_{j}^{(n)}-b_{i}^{(n)}\qquad  (i<j)
\end{equation*}
and
\begin{equation*}
\exp\left(-\frac{n+1}{4\sigma^{2}}\lambda_{i}^{2}\right)\geq\exp\left(-\frac{n+1}{4\sigma^{2}}(b_{i}^{(n)})^{2}\right).
\end{equation*}
Hence
\begin{equation*}
\bar{\nu}_{n}(\Delta_{n})\geq\frac{1}{Z_{\beta,\sigma}^{(n)}}\exp\left(-\frac{n+1}{4\sigma^{2}}\sum_{i=1}^{n}(b_{i}^{(n)})^{2}\right)
\prod_{i<j}(a_{j}^{(n)}-b_{i}^{(n)})^{2\beta}\prod_{i=1}^{n}(b_{i}^{(n)}-a_{i}^{(n)})\,.
\end{equation*}
Now we can establish the asymptotics of each factor. The normalization constant was treated already. Furthermore, let $g: [0,1]\rightarrow[a, b]$ be the inverse function of $t\mapsto\int_{a}^{t}f(x)\,dx$. Since $a_{j}^{(n)}=g((j-\tfrac{1}{2})/n)$ and $b_{j}^{(n)}=g(j/n)$, we have
\begin{align*}
\lim_{n\rightarrow\infty}\frac{1}{n^{2}}\left(-\frac{n+1}{4\sigma^{2}}\sum_{i}(b_{i}^{(n)})^{2}\right)&=-\frac{1}{4\sigma^{2}}\int_{0}^{1}g(t)^{2}\,dt
=-\frac{1}{4\sigma^{2}}\int x^{2}f(x)\,dx\,,\\
\lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log\prod_{i<j}(a_{j}^{(n)}-b_{i}^{(n)})^{2\beta} & \ = \ 2\beta\iint_{0\leq s\leq t\leq 1}\log(g(t)-g(s))\,ds\,dt\\
& \ = \ \beta\iint f(x)f(y)\log|x-y|\,dx\,dy\,,
\end{align*}
and by (\ref{ch05:eqn5.2.6})
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log\prod_{i=1}^{n}(b_{i}^{(n)}-a_{i}^{(n)})=0.
\end{equation*}
Therefore (\ref{ch05:eqn5.2.4}) is obtained.

To deal with the upper estimate (\ref{ch05:eqn5.2.5}) we apply Lemma~\ref{ch04:lem4.3.4}. Assume that $\mathrm{supp}\,\mu\subset[-R, R]$, and fix $\varepsilon >0$ and $0<\alpha<1/2$. Set $\theta :=(\alpha+2\alpha^{2})/(\alpha+2)$ and take $k_{0}, \delta$ according to Lemma~\ref{ch04:lem4.3.4} for $p=2,\,\varepsilon ^{2}\theta$ in place of $\varepsilon$, and the above $R$. For any large $n$ we can choose $\eta^{(n)}\in \Gamma_{n}(\mu;r, \delta/2)$ such that $|\eta_{i}^{(n)}|\leq R$. Put
\begin{equation*}
\Omega_{n}:=\left\{\lambda\in \mathbb{R}^{n}:\min_{\pi}\sum_{i=1}^{n}(\lambda_{i}-\eta_{\pi(i)}^{(n)})^{2}\leq n \varepsilon^{2}\theta\right\},
\end{equation*}
where the minimum is taken for all permutations $\pi$ on $[n]$. Then
\begin{equation*}
\Gamma_{n}(\mu;r, \delta/2)\subset\Omega_{n}
\end{equation*}
whenever $r\geq k_{0}$. This inclusion allows us to estimate $\bar{\nu}_{n}(\Omega_{n})$ to find an upper bound for $\bar{\nu}_{n}(\Gamma_{n}(\mu;r, \delta/2))$. The Lebesgue measure of $\Omega_{n}$ is majorized by $n!$ times the $n$-dimensional volume of a ball of radius $\varepsilon \sqrt{n\theta}$, that is,
\begin{equation*}
\int\cdots\int_{\Omega_{n}}dx_{1}\cdots dx_{n}\leq n!\,\frac{\pi^{n/2}}{\Gamma(n/2+1)}(n\varepsilon^{2}\theta)^{n/2}.
\end{equation*}
The values of the density (\ref{ch05:eqn5.2.2}) are to be majorized on $\Omega_{n}$. When $M_{n}$ is a bound for $\prod_{i<j}|\lambda_{i}-\lambda_{j}|^{2\beta}$ and $M_{n}'$ is a bound for $\exp(-(n+1)/4\sigma^{2}\sum_{i=1}^{n}\lambda_{i}^{2})$ on $\Omega_{n}$, we obviously have
\begin{equation*}
\bar{\nu}_{n}(\Omega_{n})\leq\frac{n!\,(\pi n\varepsilon^{2}\theta)^{n/2}}{\Gamma(n/2+1)} \cdot \frac{1}{Z_{\beta,\sigma}^{(n)}}M_{n}M_{n}'
\end{equation*}
and
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log\frac{n!\,(\pi n\theta\varepsilon^{2})^{n/2}}{\Gamma(n/2+1)}=0\,.
\end{equation*}

For $\lambda, \eta\in \mathbb{R}^{n}$ satisfying $\sum_{i}(\lambda_{i}-\eta_{i})^{2}\leq n\varepsilon^{2}\theta$, let $v_{i} :=|\lambda_{i}-\eta_{i}|$ and $\delta_{i}:=\theta^{-1}v_{i}^{2}$. Then
\begin{equation*}
\sum_{i}\delta_{i}\leq n\varepsilon^{2}, \qquad (1+2\alpha^{-1})v_{i}^{2}=(1+2\alpha)\delta_{i},
\end{equation*}
and we have
\begin{align*}
|\lambda_{i}-\lambda_{j}|^{2} & \ \leq \ \ (|\eta_{i}-\eta_{j}|+v_{i}+v_{j})^{2} \\
& \ = \ \ (\eta_{i}-\eta_{j})^{2}+v_{i}^{2}+v_{j}^{2}+2(|\eta_{i}-\eta_{j}|v_{i}+|\eta_{i}-\eta_{j}|v_{j}+v_{i}v_{j}) \\
& \ \leq\ \ (\eta_{i}-\eta_{j})^{2}+v_{i}^{2}+v_{j}^{2}+\alpha(\eta_{i}-\eta_{j})^{2}+\alpha^{-1}v_{i}^{2} \\
& \qquad \ \ \,+\alpha(\eta_{i}-\eta_{j})^{2}+\alpha^{-1}v_{j}^{2}+v_{i}^{2}+v_{j}^{2} \\
& \ \leq\ \ (1+2\alpha)(\eta_{i}-\eta_{j})^{2}+(1+2\alpha^{-1})(v_{i}^{2}+v_{j}^{2})\\
& \ =\ \ (1+2\alpha)((\eta_{i}-\eta_{j})^{2}+\delta_{i}+\delta_{j}) \\
& \ \leq\ \ (1+2\alpha)((\eta_{i}-\eta_{j})^{2}+\varepsilon)(1+(\delta_{i}+\delta_{j})/\varepsilon)\\
& \ \leq\ \ (1+2\alpha)((\eta_{i}-\eta_{j})^{2}+\varepsilon)e^{(\delta_{i}+\delta_{j})/\varepsilon}\,,
\end{align*}
and taking the product of these inequalities yields
\begin{equation*}
\prod_{i<j}|\lambda_{i}-\lambda_{j}|^{2\beta}\leq(1+2\alpha)^{\beta n(n+2)/2}e^{2\beta n^{2}\varepsilon}\prod_{i<j}((\eta_{i}-\eta_{j})^{2}+\varepsilon)^{\beta}.
\end{equation*}
Therefore,
\begin{align*}
&\underset{{n\rightarrow \infty}}{\lim \sup}\frac{1}{n^{2}}\log M_{n} \\
& \qquad \leq\frac{\beta}{2}\log(1+2\alpha)+2\beta\varepsilon+\beta \,\underset{n\rightarrow \infty}{\lim \sup}\,\frac{1}{n^{2}}\log\prod_{i<j}\,((\eta_{i}^{(n)}-\eta_{j}^{(n)})^{2}+\varepsilon)\,.
\end{align*}
Here the first two terms are arbitrarily small as $\alpha, \varepsilon$ are small. The third term approximates
\begin{equation*}
\frac{\beta}{2}\iint \log\,((x-y)^{2}+\varepsilon)\,d\mu(x)d\mu(y)
\end{equation*}
as the discrete measure $\frac{1}{n}\,(\delta(\eta_{1}^{(n)})+\cdots+\delta(\eta_{n}^{(n)}))$ approximates the measure $\mu$ (when $r$ is large and $\varepsilon$ small).

On the other hand, for $\lambda, \eta\in \mathbb{R}^{n}$ as above, we have
\begin{equation*}
\eta_{i}^{2}\leq(|\lambda_{i}|+v_{i})^{2}\leq(1+\alpha)\lambda_{i}^{2}+(1+\alpha^{-1})v_{i}^{2}\leq(1+\alpha)\lambda_{i}^{2}+(1+2\alpha)\delta_{i},
\end{equation*}
and so
\begin{equation*}
-\sum_{i}\lambda_{i}^{2}\leq\frac{2n\varepsilon^{2}}{1+\alpha}-\frac{1}{1+\alpha}\sum_{i}\eta_{i}^{2}\,.
\end{equation*}
This implies that
\begin{equation*}
\limsup_{n\rightarrow \infty}\,\frac{1}{n^{2}}\log M_{n}'\leq\frac{2\varepsilon^{2}}{(1+\alpha)4\sigma^{2}}-\frac{1}{(1+\alpha)4\sigma^{2}}\,\limsup_{n\rightarrow \infty}\frac{1}{n}\sum_{i}(\eta_{i}^{(n)})^{2}.
\end{equation*}
In the limit as $ r\rightarrow\infty$ and $\varepsilon \rightarrow+0$ the term
\begin{equation*}
-\frac{1}{4\sigma^{2}}\int x^{2}\,d\mu(x)
\end{equation*}
appears. In this way we obtain (\ref{ch05:eqn5.2.5}), completing the proof of the theorem.
\end{proof2}

In the above discussions one can replace real symmetric matrices by complex selfadjoint ones. The difference between the real and complex cases is not essential; use of the complex field might be closer to a functional analytic viewpoint. Let $X$ be a selfadjoint random matrix. The entropy $S(X)$ is defined as the Boltzmann-Gibbs entropy of the joint probability density of the real random variables $\{X_{ii} : 1\leq i\leq n\} \cup\{\mathrm{Re} \, X_{ij}:1\leq i<j\leq n\}\cup\{\mathrm{Im} \, X_{ij}:1\leq i<j\leq n\}$. The lines of the proof of Proposition~\ref{ch05:pro5.2.1} are easily adapted to the complex case, and the entropy of the $n\times n$ standard selfadjoint Gaussian matrix (introduced in Sec.~\ref{ch04:sec4.1}) is maximal among $n\times n$ selfadjoint random matrices $X$ satisfying $\tau(X)=0$ and $\tau(X^{2})=1$. Furthermore, the joint eigenvalue distribution of the $n\times n$ standard selfadjoint Gaussian matrix has the probability density (\ref{ch04:eqn4.1.18}). When we choose this as the reference measure $\bar{\nu}_{n}$, Theorem~\ref{ch05:the5.2.2} holds with $\beta=1$ and $\sigma^{2}=1/2$, while $\beta=1/2$ and $\sigma^{2}=1$ when $\bar{\nu}_{n}$ arises from the $n\times n$ standard symmetric real Gaussian matrix.

The previous theorem motivated Voiculescu to regard the double integral
\begin{equation}
\Sigma(\mu):=\iint\log|x-y|\,d\mu(x)\,d\mu(y)
\label{ch05:eqn5.2.7}
\end{equation}
as a kind of entropy of a probability measure $\mu$ on $\mathbb{R}$ (or $\mathbb{C}$). In fact, he used the term \textit{free entropy}. The use of the adjective ``free'' is not explained by the theorem. Later on, this entropy will be extended to the case of several noncommutative random variables, and it will turn out that the new entropy is additive for variables in free relation, while the Boltzmann-Gibbs entropy is known to be additive for independent variables. It is already time to warn that this free entropy is not related to the free entropy of thermodynamics. Note that when $\mu$ is compactly supported, the integral (\ref{ch05:eqn5.2.7}) always exists, although it can be $-\infty$, for example if $\mu$ has an atom.

Now return to \textit{maximum entropy matrix ensembles}.\index{maximum entropy!matrix ensembles} As is seen from the previous section, the Boltzmann-Gibbs entropy\index{entropy!free}\index{free entropy} $S(X)$ of a selfadjoint random matrix $X$ is written as the relative entropy
\begin{equation*}
S(X)=-S(\mu, \Lambda)\,,
\end{equation*}
where $\mu$ is the distribution measure on $M_{n}(\mathbb{C})^{sa}$ of $X$ and $\Lambda=dA$ is the product Lebesgue measure (\ref{ch04:eqn4.1.17}) on $M_{n}(\mathbb{C})^{sa}\cong \mathbb{R}^{n^{2}}$. Let $V:\mathbb{R}\rightarrow \mathbb{R}$ be a continuous function. We aim to maximize $S(X)$ under the constraint $\tau(V(X))=c$, where $V(X)$ is understood in the sense of functional calculus for selfadjoint matrices. In terms of eigenvalues the constraint is $\sum_{i}V(\lambda_{i})=nc$.

First we prove that the maximizer is among the unitary conjugation-invariant measures on $M_{n}(\mathbb{C})^{sa}$. Indeed, if
\begin{equation*}
X_{0} :=\int UXU^{*}\,dU, \quad \mu_{0} :=\int\mu(U\cdot U^{*})\,dU
\end{equation*}
(integrations are with respect to the normalized Haar measure on $\mathcal{U}(n))$, then by the lower semicontinuity and convexity of relative entropy we have
\begin{equation*}
S(\mu_{0}, \Lambda)\leq\int S(\mu(U\cdot U^{*}), \Lambda)\,dU=S(\mu, \Lambda),
\end{equation*}
and hence $S(X_{0})\geq S(X)$. Now let $X$ be a unitarily invariant selfadjoint random matrix with a joint eigenvalue density $f(\lambda)\Delta(\lambda)^{2}$ (see (\ref{ch04:eqn4.1.5}) for $\Delta(\lambda))$. Then
\begin{equation*}
\tau(V(X))=\frac{1}{n}\int\left(\sum_{i}V(\lambda_{i})\right)f(\lambda)\Delta(\lambda)^{2}d\lambda\,.
\end{equation*}
So the maximization of $S(X)$ when $\tau(V(X))$ is fixed is equivalent to the minimization of
\begin{equation*}
\int f(\lambda)\Delta(\lambda)^{2}(\log f(\lambda)\Delta(\lambda)^{2}-\log\Delta(\lambda)^{2})\, d\lambda =\int(f(\lambda)\log f(\lambda))\Delta(\lambda)^{2}d\lambda
\end{equation*}
when $\int(\sum_{i}V(\lambda_{i}))f(\lambda)\Delta(\lambda)^{2}\,d\lambda$ is fixed. It is known that the maximizer has the form
\begin{equation*}
f(\lambda)=\frac{1}{Z}\exp\left(-t\sum_{i}V(\lambda_{i})\right),
\end{equation*}
where $Z$ is only for normalization and the real parameter $t$ should be chosen so that the constraint is satisfied. Going back from the eigenvalue space to the matrix space (cf. Lemma~\ref{ch04:lem4.1.6}), we find that the maximizer is of the form
\begin{equation*}
Z^{-1}\exp(-t\mathrm{Tr}\,V(A))\,dA.
\end{equation*}
The measure
\begin{equation}
\frac{1}{Z_{n}}\exp(-n\mathrm{Tr}\, V(A))\,dA
\label{ch05:eqn5.2.8}
\end{equation}
on the space $M_{n}(\mathbb{C})^{sa}$ is also called the \textit{orthogonal polynomial ensemble}.\index{ensemble!orthogonal polynomial}\index{orthogonal polynomial ensemble} The reason for this is the fact that the correlation functions of the eigenvalues are conveniently expressed in terms of the orthogonal polynomials with respect to the weight function $w_{V}(x):=\exp(-V(x))$ defined on $\mathbb{R}$.

\section{Logarithmic energy and free entropy}
\label{ch05:sec5.3}

Voiculescu's entropy $\Sigma(\mu)=\iint\log|x-y|\,d\mu(x)\,d\mu(y)$ was introduced only recently, but with a different sign it is a classical quantity in two-dimensional potential theory. This section is devoted to properties of the free entropy, which is nothing else but the negative logarithmic energy. The main subject is the maximization of free entropy under different constraints.

We start with an example from \textit{electrostatics}.\index{electrostatics} Let $[-a, a]$ be a compact interval in $\mathbb{R}$. If $\mu$ is a probability measure on $[-a, a]$, then the double integral
\begin{equation}
\iint\log\frac{1}{|x-y|}\,d\mu(x)\,d\mu(y)
\label{ch05:eqn5.3.1}
\end{equation}
is called the \textit{logarithmic energy}\index{logarithmic energy} of $\mu$. The integral always exists, but it can be $+\infty$ if $\mu$ has an atom, for example. In a physical picture the measure $\mu$ may be thought of as the distribution of electric charges along the interval
\begin{equation*}
[-a, a]=\{(x, y)\in \mathbb{R}^{2}\, :\, -a\leq x\leq a,\, y=0\}
\end{equation*}
in a two-dimensional universe. The integral (\ref{ch05:eqn5.3.1}) yields the \textit{Coulomb energy} of the two-dimensional electrostatic field due to electrostatic repulsion if the repulson force between charges is proportional to the inverse of the distance. This follows from the two-dimensional Coulomb law. To have a connection with the ``real'' 3- dimensional electrostatics, we regard the interval as the cross section of the infinite strip
\begin{equation*}
\{(x, y, z)\in \mathbb{R}^{3}:-a\leq x\leq a,\ y=0, \ z\in \mathbb{R}\}
\end{equation*}
on which the distribution density of charges does not depend on the $z$ coordinate. To get the true physical energy of a real charge density, one should replace $-\log |x-y|$ by $1/|x-y|$ in the integral.

When the interval is an ideal conductor, the equilibrium charge distribution is the minimizer of the energy functional. The advantage of the 2-dimensional \textit{x-y} model is the mathematical convenience of the complex variable $x+\mathrm{i}\,y$. The \textit{ complex potential}\index{complex potential}\index{Coulomb energy} of a (two-dimensional) electrostatic field is an analytic function
\begin{equation*}
F(x+\mathrm{i}\,y)=u(x, y)+\mathrm{i}\,v(x, y)
\end{equation*}
(in the domain free of charges), for which the potential function $v(x, y)$ of the given electrostatic field is the imaginary part. The level lines $v(x, y)=C$ are the equipotential lines of the given field, and $u(x, y)=C$ gives the force lines. (These two curves are orthogonal to each other, thanks to the Cauchy-Riemann relations.)

The equilibrium distribution on $[-a, a]$ is the so-called \textit{arcsine distribution}\index{distribution!arcsine}\index{arcsine distribution}
\begin{equation*}
h(x):=\left\{\begin{array}{ll}
\frac{1}{\pi\sqrt{a^{2}-x^{2}}} & \mathrm{if}-a<x<a,\\
\\
0 & \mathrm{otherwise}.
\end{array}\right.
\end{equation*}
The corresponding complex potential
\begin{equation*}
F(z):=-\frac{\mathrm{i}}{\pi\sqrt{z^{2}-a^{2}}} \qquad \qquad (z=x+\mathrm{i}\,y)
\end{equation*}
is analytic in the complement of $\{x+\mathrm{i}\,y:-a\leq x\leq a,\ y=0\}$. We have
\begin{equation*}
\lim_{y\rightarrow 0}\mathrm{Im}\, F(x+\mathrm{i}\,y)=\left\{\begin{array}{ll}
0 & \mathrm{if}\ |x|<a,\\
\\
-\dfrac{1}{\pi\sqrt{x^{2}-a^{2}}} & \mathrm{if}\ |x|>a,
\end{array}\right.
\end{equation*}
and observe that the interval $[-a, a]$ is equipotential, indeed. Therefore the limit
\begin{equation*}
\lim\limits_{y\rightarrow 0}\mathrm{Re} \,F(x+\mathrm{i}\,y)=h(x)
\end{equation*}
gives the equilibrium distribution. Note that $F(z)$ is $\mathrm{i}/\pi$ times the Cauchy transform of the measure $\mu=h(x)\,dx$ (see Sec.~\ref{ch03:sec3.1}).

It is instructive to approach the above problem by discretization. Let $x_{0}, x_{1}, \ldots, x_{n}, x_{n+1}\in[-1,1]$, and minimize the function
\begin{equation*}
T_{0}(x_{0}, x_{1}, \ldots, x_{n}, x_{n+1}):=-\sum_{i<j}\log|x_{i}-x_{j}|\,.
\end{equation*}
We think that there are $n$ equal charges at the positions $x_{i}$, and $T_{0}$ takes its minimum at the positions $x_{i}=t_{i}$ of equilibrium. Our physical intuition says that in the equilibrium there are charges at $\pm 1$ due to the repulsion. Hence equivalently we maximize the function
\begin{equation*}
T(x_{1}, x_{2}, \ldots, x_{n}):=-\sum_{i<j}\log|x_{i}-x_{j}|-\sum_{i}\log(1-x_{i})-\sum_{i}\log(1+x_{i}).
\end{equation*}
It is no more difficult to deal with the more general function
\begin{align}
& T^{\alpha,\beta}(x_{1}, x_{2}, \ldots, x_{n}) \notag\\
& =-\sum_{i<j}\log|x_{i}-x_{j}|-\sum_{i}\log(1-x_{i})^{(\alpha+1)/2}-\sum_{i}\log(1+x_{i})^{(\beta+1)/2}.
\label{ch05:eqn5.3.2}
\end{align}
From the conditions $\partial T^{\alpha,\beta}/\partial x_{i}=0$ one can deduce a differential equation for the function $f_n(x)=(x-t_{1})(x-t_{2})\cdots(x-t_{n})$, where $t_{1}, t_{2}, \ldots, t_{n}$ are the minimum positions of the variables. It turns out ([\citen{bib187}], Sec. 6.7) that $f_{n}$ is the \textit{Jacobi polynomial} $P_{n}^{\alpha,\beta}(x)$ if $\alpha, \beta>-1$. The Jacobi polynomials\index{Jacobi polynomial} $P_{n}^{\alpha,\beta}(x)$ are orthogonal with respect to the measure $(1-x)^{\alpha}(1+x)^{\beta}\,dx$ on $[-1, 1]$. If $x_{1,n}, x_{2,n}, \ldots, x_{n,n}$ stands for the zeros $t_{i}$ of $P_{n}^{\alpha,\beta}(x)$, then it is plausible on the basis of the electrostatic model that the sequence of atomic measures $\frac{1}{n}(\delta(x_{1,n})+\delta(x_{2,n})+\cdots+\delta(x_{n,n}))$ tends to the arcsine law weakly, independently of $\alpha$ and $\beta$. This is due to the fact that the arcsine law is the equilibrium measure on $[-1, 1]$ and the constants $\alpha$ and $\beta$ affect only $2n$ terms in (\ref{ch05:eqn5.3.2}) while the total nunber of terms is about $n^{2}/2$. Actually, the convergence to the semicircle law is known in theory of orthogonal polynomials.

It is convenient to extend the logarithmic energy (\ref{ch05:eqn5.3.1}) to signed measures. The so-called \textit{logarithmic energy}\index{logarithmic energy} $E(\nu)$ of a signed measure $\nu$ on $\mathbb{C}$ is given as
\begin{equation*}
E(\nu):=\iint\log\frac{1}{|x-y|}\,d\nu(x)\,d\nu(y)
\end{equation*}
whenever
\begin{equation*}
\iint\left|\log\frac{1}{|x-y|}\right|d|\nu|(x)\,d|\nu|(y)<+\infty
\end{equation*}
($|\nu|$ denotes the total variation of $\nu$); otherwise put $E(\nu) :=+\infty$. The logarithmic energy plays a role in potential theory. For a probability measure $\mu$ the free entropy given in (\ref{ch05:eqn5.2.7}) differs in sign from $E(\mu)$. The following lemma from potential theory (cf. [\citen{bib114}], Theorem 1.16 or [\citen{bib165}], Lemma I.1.8) will be useful in this section. The proof is provided for convenience.


\begin{lemma}
\label{ch05:lem5.3.1}
Let $\nu$ is a compactly supported signed measure on $\mathbb{C}$ such that $\nu(1)= 0$. Then $E(\nu)\geq 0$, and $E(\nu)=0$ if and only if $\nu=0$.
\end{lemma}

\begin{proof2}
Recall that a real symmetric kernel $L(x, y)$ is called \textit{negative definite} if
\begin{equation}
\sum_{i}\sum_{j}c_{i}c_{j}L(x_{i}, x_{j})\leq 0
\label{ch05:eqn5.3.3}
\end{equation}
whenever real numbers $c_{1}, \ldots, c_{n}$ satisfy $\sum_{i=1}^{n}c_{i}=0$. It follows by approximation that for a continuous negative definite kernel\index{negative definite kernel}\index{logarithmic kernel} $L(x, y)$ one gets
\begin{equation*}
\iint L(x, y)\,d\nu(x)\,d\nu(y)\leq 0
\end{equation*}
if $\nu$ is a compactly supported signed measure such that $\nu(1)=0$. Indeed, approximating $\nu$ by atomic measures, one can have a double integral which reduces to a double sum of the form (\ref{ch05:eqn5.3.3}).

The \textit{logarithmic kernel} $K(x, y) :=\log|x-y|$ has a singularity at $x=y$, and to avoid this we set, for positive $\varepsilon$,
\begin{equation*}
K_{\varepsilon}(x,y):=\log(\varepsilon+|x-y|)=\int_{0}^{\infty}\left(\frac{1}{1+t}-\frac{1}{t+\varepsilon+|x-y|}\right)dt\,.
\end{equation*}
This kernel $K_{\varepsilon}(x, y)$ is the integral of negative definite kernels ([\citen{bib20}], Chap.\ref{ch03:chap03}), and it is negative definite by itself. So we have
\begin{equation*}
\iint K_{\varepsilon}(x, y)\,d_{\nu}(x)\,d\nu(y)\leq 0\,,
\end{equation*}
and we take $\varepsilon\rightarrow+0$ to conclude that $E(\nu)\geq 0$ whenever $K(x, y)$ is integrable with respect to $|\nu|\otimes|\nu|$ (otherwise, $E(\nu)=+\infty$ by definition).

Now assume $E(\nu)=0$. For $ 0<\varepsilon<R<+\infty$ we get
\begin{align*}
& -\int_{\varepsilon}^{R}\left(\iint\frac{1}{t+|x-y|}\,d\nu(x)\,d\nu(y)\right)dt \\
& \qquad =\int_{\varepsilon}^{R}\left(\iint\left(\frac{1}{1+t}-\frac{1}{t+|x-y|}\right)d\nu(x)\,d{\nu}(y)\right)dt \\
& \qquad =\iint \left(\log(\varepsilon+|x-y|)+\log\frac{1+R}{(1+\varepsilon)(R+|x-y|)}\right)\, d\nu(x)\,d\nu(y)
\end{align*}
by the Fubini theorem. Here note ([\citen{bib20}]) that $(t+|x-y|)^{-1}$ is a \textit{ positive definite kernel}\index{positive definite kernel} for any $t>0$, and hence
\begin{equation*}
\iint\frac{1}{t+|x-y|}\,d\nu(x)\,d\nu(y)\geq 0 \qquad (t>0).
\end{equation*}
So we can take the limit of the above as $\varepsilon \rightarrow+0$ and $ R\rightarrow+\infty$ to obtain
\begin{equation*}
\int_{0}^{\infty}\left(\iint\frac{1}{t+|x-y|}\, d\nu(x)\,d\nu(y)\right)dt=0,
\end{equation*}
which implies that
\begin{equation*}
\iint\frac{1}{t+|x-y|}\,d\nu(x)\,d\nu(y)=0 \qquad (t>0).
\end{equation*}
Taking the expansion
\begin{equation*}
\frac{1}{t+|x-y|}=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{t^{n+1}}|x-y|^{n}
\end{equation*}
in a neighborhood of $ t=\infty$, we have
\begin{equation*}
\iint|x-y|^{2n}\,d\nu(x)\,d\nu(y)=0 \qquad (n=0,1,2,\ldots).
\end{equation*}
This means that
\begin{equation*}
\sum_{i,j=0}^{n}(-1)^{i+j}\left(\begin{array}{c}
n\\
i
\end{array}\right)\left(\begin{array}{c}
n\\
j
\end{array}\right)\int x^{i}\overline{x}^{j}\,d\nu(x)\int x^{n-i}\bar{x}^{n-j}\,d\nu(x)=0
\end{equation*}
for all $n$. Now we can easily show by induction that $\int x^{i}\bar{x}^{j}\,d\nu(x)=0$ for all $i, j=0, 1, 2, \ldots$, which is enough to conclude that $\nu=0$.
\end{proof2}

\begin{proposition}
\label{ch05:pro5.3.2}
The free entropy functional\index{free entropy!functional} $\Sigma(\mu)$ is weakly upper semicontinuous and concave on the set of probability measures restricted on any compact subset of $\mathbb{C}$. Moreover, it is strictly concave in the sense that $\Sigma(\lambda\mu_{1}+(1-\lambda)\mu_{2})> \lambda\Sigma(\mu_{1})+(1-\lambda)\Sigma(\mu_{2})$ if $0<\lambda<1$ and $\mu_{1}, \mu_{2}$ are compactly supported probability measures such that $\mu_{1}\neq\mu_{2}, \, \Sigma(\mu_{1})>-\infty$ and $\Sigma(\mu_{2})>-\infty$.
\end{proposition}

\begin{proof2}
Let $K_{\varepsilon}(x, y)$ be the kernel given in the previous proof. The weak upper semicontinuity follows because $\Sigma(\mu)$ is written as
\begin{equation*}
\Sigma(\mu)=\inf_{\varepsilon >0}\iint K_{\varepsilon}(x, y)\,d\mu(x)\,d\mu(y)
\end{equation*}
and the above double integral is continuous in the weak topology when the support of $\mu$ is restricted on a compact subset.

    To prove the strictly concavity, let $\mu_{1}\neq\mu_{2}$ be compactly supported measures such that $\Sigma(\mu_{1})>-\infty$ and $\Sigma(\mu_{2})>-\infty$. First we show that
\begin{equation*}
E(\mu_{1}, \mu_{2}):=\iint\log\frac{1}{|x-y|}\,d\mu_{1}(x)\,d\mu_{2}(y)
\end{equation*}
is finite. Since the kernel $K_{\varepsilon}(x, y)$ is negative definite, we get
\begin{align*}
0 & \ \geq \ \iint K_{\varepsilon}(x, y)\, d(\mu_{1}-\mu_{2})(x)\, d(\mu_{1}-\mu_{2})(y) \\
& \ \geq \ \Sigma(\mu_{1})+\Sigma(\mu_{2})-2\iint K_{\varepsilon}(x, y)\,d\mu_{1}(x)\,d\mu_{2}(y)\,.
\end{align*}
Letting $\varepsilon\rightarrow+0$ yields $\Sigma(\mu_{1})+\Sigma(\mu_{2})+2E(\mu_{1}, \mu_{2})\leq 0$, so $ E(\mu_{1}, \mu_{2})<+\infty$. Now we are in the situation where $E(\mu_{1}), \, E(\mu_{2})$ and $E(\mu_{1}, \mu_{2})$ are all finite. Then we have for, $0<\lambda<1$,
\begin{equation*}
E(\lambda\mu_{1}+(1-\lambda)\mu_{2})=E(\mu_{2})+2\lambda E(\mu_{2}, \mu_{1}-\mu_{2})+\lambda^{2}E(\mu_{1}-\mu_{2}),
\end{equation*}
and by Lemma~\ref{ch05:lem5.3.1}
\begin{equation*}
\frac{d^{2}}{d\lambda^{2}}E(\lambda\mu_{1}+(1-\lambda)\mu_{2})=E(\mu_{1}-\mu_{2})>0\,.
\end{equation*}
This implies that $\Sigma(\mu)$ is strictly concave (hence also concave).
\end{proof2}

Let $S$ be a closed subset in $\mathbb{R}$ (or $\mathbb{C}$). Let $\mathcal{M}(S)$ denote the set of all probability measures whose support $\mathrm{supp}\,\mu$ is included in $S$. Moreover, let $w:S\rightarrow[0,\, \infty)$ be a \textit{weight function}, which is assumed for simplicity to satisfy the following conditions:
\begin{enumerate}
\item[(a)] $w$ is continuous on $S$.

\item[(b)] $S_{0} :=\{x\in S:w(x)>0\}$ has positive (inner logarithmic) \textit{ capacity},\index{capacity} that is, $ E(\mu)<+\infty$ for some probability measure $\mu$ such that $\mathrm{supp}\,\mu\subset S_{0}$.

\item[(c)] $|x|w(x)\rightarrow 0$ as $x\in S, \, |x|\rightarrow\infty$, when $S$ is unbounded.
\end{enumerate}

Let $Q(x) :=-\log w(x)$ and define the \textit{weighted energy integral}\index{weighted!energy integral} (or \textit{weighted potential})\index{weighted!potential}\index{weight function}
\begin{equation}
E_{Q}(\mu):=\iint\left(\log\frac{1}{|x-y|}+Q(x)+Q(y)\right)d\mu(x)\,d\mu(y).
\label{ch05:eqn5.3.4}
\end{equation}
The terminology was explained at the beginning of this section where the relation to electrostatics was discussed. One observes that $E_{Q}(\mu)>-\infty$ is well-defined, thanks to the above assumptions. Then the next theorem, due to Mhaskar and Saff ([\citen{bib165}], Theorem I.1.3), is fundamental in the theory of weighted potentials, and it is proved by the adaptation of the classical Frostman method.

\begin{theorem}
\label{ch05:the5.3.3}
With the above assumptions, there exists a unique $\mu_{0}\in \mathcal{M}(S)$ such that
\begin{equation*}
E_{Q}(\mu_{0})=\inf\{E_{Q}(\mu)\, :\, \mu\in\mathcal{M}(S)\}.
\end{equation*}
Then $E_{Q}(\mu_{0})$ is finite, $\mu_{0}$ has finite logarithmic energy, and $\mathrm{supp}\,\mu_{0}$ is compact. Furthermore, the minimizer $\mu_{0}$ is characterized as $\mu_{0}\in\mathcal{M}(S)$ with compact support such that for some real number $B$ the following holds:
\begin{equation*}
\int\log|x-y|\,d\mu_{0}(y)\left\{\begin{array}{l}
=Q(x)-B  \quad \mathit{if} \ x\in \mathrm{supp}\,\mu_{0},\\
\\
\leq Q(x)-B  \quad \mathit{if} \ x\in S\, \backslash \, \mathrm{supp}\,\mu_{0}.
\end{array}\right.
\end{equation*}
In this case, $B=E_{Q}(\mu_{0})-\int Qd\mu_{0}$.
\end{theorem}

When $S$ is a compact set (having positive capacity), a unique minimizer $\mu_{S}$ for $E(\mu)$ (or maximizer of $\Sigma(\mu)$) on $\mathcal{M}(S)$ is sometimes called the \textit{equilibrium measure} on $S$. For instance, as we mentioned at the beginning of this section, the arcsine law
\begin{equation*}
h(x):=\frac{1}{\pi\sqrt{a-x^{2}}}\chi_{(-a,a)}(x)
\end{equation*}
is the equilibrium measure\index{equilibrium measure} on $[-a, a]$, and $\Sigma(h)=\log\frac{a}{2}$. In fact,
\begin{equation*}
\int_{-a}^{a}h(y)\log|x-y|\,dy=\log\frac{a}{2} \qquad (-a\leq x\leq a).
\end{equation*}

Next we solve the maximization problems for free entropy under constraints on the $p$th moment. Since the maximization of free entropy is equivalent to the minimization of a weighted energy integral, we can benefit from the previous theorem.

For $p, r>0$ the probability density
\begin{equation*}
v_{r}^{(p)}(x):=\left\{\begin{array}{ll}
\displaystyle \frac{p}{\pi r^{p}}\int_{|x|}^{r}\frac{t^{p-1}}{\sqrt{t^{2}-x^{2}}}\,dt & \mathrm{if}-r\leq x\leq r,\\
\\
0 & \mathrm{otherwise},
\end{array}\right.
\end{equation*}
is called the \textit{Ullman distribution},\index{Ullman distribution}\index{distribution!Ullman} and will play a role below. For instance,
\begin{align*}
v_{r}^{(1)}(x) & \ = \ \chi_{[-r,r]}(x)\frac{1}{\pi r}\log\frac{r+\sqrt{r^{2}-x^{2}}}{|x|}\,, \\
v_{r}^{(2)}(x) & \ = \ w_{r}(x)\,, \\
v_{r}^{(3)}(x) & \ = \ \chi_{[-r,r]}(x)\frac{3}{2\pi r^{3}}\left(x^{2}\log\frac{r+\sqrt{r^{2}-x^{2}}}{|x|}+r\sqrt{r^{2}-x^{2}}\right),\\
v_{r}^{(4)}(x) & \ = \ \chi_{[-r,r]}(x)\frac{4(2x^{2}+r^{2})}{3\pi r^{4}}\sqrt{r^{2}-x^{2}}\,.
\end{align*}
Note that $\mathrm{supp}\,v_{r}^{(p)}=[-r, r], \, v_{r}^{(p)}(\pm r)=0$ and
\begin{align}
\int_{-r}^{r}|x|^{p}v_{r}^{(p)}(x)\,dx & \ = \ \frac{2p}{\pi r^{p}}\int_{0}^{r}t^{p-1}\left(\int_{0}^{t}\frac{x^{p}}{\sqrt{t^{2}-x^{2}}}\,dx\right)dt\notag \\
&  \  = \ \frac{2p}{\pi r^{p}}\int_{0}^{r}t^{2p-1}\,dt\int_{0}^{1}\frac{x^{p}}{\sqrt{1-x^{2}}}\,dx=\alpha_{p}r^{p},
\label{ch05:eqn5.3.5}
\end{align}
where
\begin{equation*}
\alpha_{p}:=\frac{1}{\pi}\int_{0}^{1}\frac{x^{p}}{\sqrt{1-x^{2}}}\,dx=\frac{\Gamma(\tfrac{p+1}{2})}{2\sqrt{\pi}\Gamma(\frac{p}{2}+1)}.
\end{equation*}
For instance, $\alpha_{1}=1/\pi, \, \alpha_{2}=1/4, \, \alpha_{3}=2/3\pi$ and $\alpha_{4}=3/16$. Moreover, when $0<p\leq 1, v_{r}^{(p)}(x)$ has a singularity at $x=0$.

Let $\xi$ and $\eta$ be independent random variables such that $\xi$ has the standard \textit{ arcsine distribution} and $\eta$ has the \textit{beta}\index{distribution!beta}\index{beta distribution} $(p, 1)$ \textit{distribution}, i.e. {\bf Prob}$(\eta<t)=t^{p} (t\geq 0)$. Then the distribution of $\xi\eta$ is $v_{1}^{(p)}$. This might be useful when one computes moments of the Ullman distribution, although above we computed them directly. Note that when $ p\rightarrow\infty$ the beta $(p, 1)$ distribution converges to the point measure $\delta(1)$, so the Ullman distribution\index{maximum entropy!Ullman distribution}\index{distribution!arcsine} $v_{1}^{(p)}$ coverges to the standard arcsine\index{arcsine distribution} law.

Now we are in a position to maximize the free entropy of $\mu\in \mathcal{M}(\mathbb{R})$ when the $p$th moment is fixed.

\begin{proposition}
\label{ch05:pro5.3.4}
Let $p, r>0$ \textit{and} $\alpha_{p}$ be as above. Among the probability measures $\mu$ on $\mathbb{R}$ with $\int|x|^{p}d\mu(x)\leq\alpha_{p}r^{p}\ (or=\alpha_{p}r^{p})$, the Ullman distribution $v_{r}^{(p)}$ has maximal free entropy and $\Sigma(v_{r}^{(p)})=\log(r/2)-1/2p$. Furthermore, $v_{r}^{(p)}$ is the unique maximizer of the functinal\index{distribution!Ullman}
\begin{equation*}
\Sigma(\mu)-\frac{1}{p\alpha_{p}r^{p}}\int|x|^{p}d\mu(x) \quad on \quad \mathcal{M}(\mathbb{R}).
\end{equation*}
\end{proposition}

\begin{proof2}
First note that $\Sigma(\mu)<+\infty$ is well-defined whenever the $p$th moment of $\mu$ is finite, because $\log|x-y|\leq C(|x|^{p}+|y|^{p})$ for some constant $C>0$. Also note that the constraint $\int|x|^{p}d\mu(x)\leq\alpha_{p}r^{p}$ is equivalent to $\int|x|^{p}d\mu(x)=\alpha_{p}r^{p}$. Indeed, if we take the dilation $\tilde{\mu} :=D_{\lambda}\mu$, i.e. $\tilde{\mu}(B)=\mu(\lambda^{-1}B)$ for $B\subset \mathbb{R}$, where $\lambda>0$, then $\int|x|^{p}d\tilde{\mu}(x)=\lambda^{p}\int|x|^{p}d\mu(x)$ and $\Sigma(\tilde{\mu})=\Sigma(\mu)+\log\lambda$.

It is known ([\citen{bib165}], Sec. IV.5) that
\begin{equation}
 \int_{-r}^{r}v_{r}^{(p)}(y)\log|x-y|\,dy\left\{\begin{array}{l}
=\dfrac{|x|^{p}}{2p\alpha_{p}r^{p}}+\log\dfrac{r}{2}-\dfrac{1}{p} \quad \mathrm{if} \ |x|\leq r,\\
<\dfrac{|x|^{p}}{2p\alpha_{p}r^{p}}+\log\dfrac{r}{2}-\dfrac{1}{p} \quad \mathrm{if}\ |x|>r.
\end{array}\right.
\label{ch05:eqn5.3.6}
\end{equation}
We set $Q(x) :=(2p\alpha_{p}r^{p})^{-1}|x|^{p}$ on $\mathbb{R}$ and apply Theorem~\ref{ch05:the5.3.3}. Then (\ref{ch05:eqn5.3.4}) may be written as
\begin{equation*}
E_{Q}(\mu)=-\Sigma(\mu)+\frac{1}{p\alpha_{p}r^{p}}\int|x|^{p}d\mu(x)\,,
\end{equation*}
so in minimizing $E_{Q}(\mu)$ we may restrict $\mu\in \mathcal{M}(\mathbb{R})$ to those with finite $p$th moment. For such $\mu$ set $\lambda :=((1/\alpha_{p}r^{p})\int|x|^{p}d\mu(x))^{1/p}$ and $\tilde{\mu};=D_{1/\lambda}\mu$. Then we have $\int|x|^{p}d\tilde{\mu}(x)=\alpha_{p}r^{p}$ and
\begin{equation*}
E_{Q}(\mu)=-\Sigma(\tilde{\mu})+\frac{\lambda^{p}}{p}-\log\lambda\,.
\end{equation*}
Since $\lambda^{p}/p-\log\,\lambda \ (\lambda\,>\,0)$ takes the minimum $1/p$ at $\lambda=1$, it follows that the minimization problem for $E_{Q}(\mu)$ on $\mathcal{M}(\mathbb{R})$ is equivalent to the maximization problem for $\Sigma(\mu)$ on $\{\mu\in\mathcal{M}(\mathbb{R}) : \int|x|^{p}d\mu(x)=\alpha_{p}r^{p}\}$. Hence, according to (\ref{ch05:eqn5.3.6}), Theorem~\ref{ch05:the5.3.3} implies that $v_{r}^{(p)}$ is the unique maximizer for $\Sigma(\mu)$ with $\int|x|^{p}d\mu(x)=\alpha_{p}r^{p}$. Furthermore, by (\ref{ch05:eqn5.3.5}) and (\ref{ch05:eqn5.3.6}) we get
\begin{equation*}
\Sigma(v_{r}^{(p)})=\log\frac{r}{2}-\frac{1}{2p}\,.
\end{equation*}
\end{proof2}

In particular, when $p=2$, the \textit{semicircle law $w_{m,r}$} has maximal free entropy among the probability measures on $\mathbb{R}$ with variance $\leq r^{2}/4$, and
\begin{equation*}
\Sigma(w_{m,r})=\log\frac{r}{2}-\frac{1}{4}.
\end{equation*}
This fact is one of the reasons why the semicircle law\index{semicircle law} is regarded as the free analogue of the normal distribution; recall (see Sec.~\ref{ch05:sec5.1}) that the latter maximizes the differential (or Boltzmann-Gibbs) entropy when the variance is fixed.

The maximizer $v_{r}^{(p)}$ in Proposition~\ref{ch05:pro5.3.4} is symmetric. Let $\mathcal{M}_{s}(\mathbb{R})$ denote the set of all symmetric probability measures $\mu$ on $\mathbb{R}$. Define a bijective correspondence between $\mathcal{M}_{s}(\mathbb{R})$ and $\mathcal{M}(\mathbb{R}^{+})$ by
\begin{equation*}
T:\mathcal{M}_{s}(\mathbb{R})\rightarrow \mathcal{M}(\mathbb{R}^{+})\,,  \quad (T\mu)(B):=\mu(\sigma^{-1}B)\,,
\end{equation*}
where $\sigma : \mathbb{R}\rightarrow \mathbb{R}^{+}, \, \sigma(x) :=x^{2}$. Note that if $\mu\in\mathcal{M}_{s}(\mathbb{R})$ has density $f$ satisfying $f(x)=f(-x)$ , then $T\mu$ has the density $Tf$ given by
\begin{equation*}
(Tf)(x):=\frac{f(\sqrt{x})}{\sqrt{x}}\,.
\end{equation*}
We have the following properties:
\begin{enumerate}
\item[(1)] $\int|x|^{2p}d\mu(x)=\int x^{p}d(T\mu)(x)$ for $p>0$.

\item[(2)] $2\Sigma(\mu)=\Sigma(T\mu)$ whenever $\Sigma(\mu)$ or $\Sigma(T\mu)$ is well-defined.
\end{enumerate}
Indeed, (1) is immediate and (2) comes from an elementary computation.

Now we are going to maximize the free entropy on the set of probability measures on $\mathbb{R}^{+}$ with a given $p$th moment. By means of the transformation $T$ this extremum problem is reduced to the one previously treated in Proposition~\ref{ch05:pro5.3.4}. For $p, r>0$ define the probability density $u_{r}^{(p)}$ by $u_{r}^{(p)}(x):=v_{\sqrt{2r}}^{(2p)}(\sqrt{x})/\sqrt{x}$; more concretely,
\begin{equation*}
u_{r}^{(p)}(x)=\left\{\begin{array}{ll}
\displaystyle\frac{p}{\pi r^{p}}\int_{x/2}^{r}\frac{t^{p-1}}{\sqrt{2tx-x^{2}}}\,dt & \mathrm{if} \ 0\leq x\leq 2r,\\
0 & \mathrm{otherwise}.
\end{array}\right.
\end{equation*}
For instance,
\begin{align}
u_{r}^{(1)}(x) & \ = \ \chi_{[0,2r]}(x)\frac{\sqrt{2rx-x^{2}}}{\pi rx}, \label{ch05:eqn5.3.7} \\
u_{r}^{(2)}(x) & \ = \ \chi_{[0,2r]}(x)\frac{2(x+r)}{3\pi r^{2}x}\sqrt{2rx-x^{2}}.\notag
\end{align}

\begin{proposition}
\label{ch05:pro5.3.5}
Let $p, r>0$ and $\tilde{\alpha}_{p}:=2^{p-1}\Gamma(p+\frac{1}{2})/\sqrt{\pi}\Gamma(p+1)$. Among the probability measures $\mu$ supported in $\mathbb{R}^{+}$ with $\int x^{p}d\mu(x)\leq\tilde{\alpha}_{p}r^{p}\ (or=\tilde{\alpha}_{p}r^{p})$, the distribution $u_{r}^{(p)}$ has maximal free entropy and $\Sigma(u_{r}^{(p)})=\log(r/2)-1/2p$. Furthermore, $u_{r}^{(p)}$ is a unique maximizer of the functional
\begin{equation*}
\Sigma(\mu)-\frac{1}{p\tilde{\alpha}_{p}r^{p}}\int x^{p}d\mu(x) \quad on \quad \mathcal{M}(\mathbb{R}^{+}).
\end{equation*}
\end{proposition}

In particular, since $\tilde{\alpha}_{1}=1/2$, the case $p=1$ says that for each $m>0$ the distribution $u_{2m}^{(1)}$ has maximal free entropy among the probability measures on $\mathbb{R}^{+}$ with mean $\leq m$. In the case of the classical differential entropy, the exponential distribution is the maximizer when the support is $\mathbb{R}^{+}$ and the mean is given. So the family of distributions (\ref{ch05:eqn5.3.7}) is considered as the free analogue of the exponential distributions.\index{distribution!exponential}\index{exponential distribution} Note that (\ref{ch05:eqn5.3.7}) for $r=2$ is the Marchenko-Pastur distribution $\mu_{1}$ which appeared in connection with random matrices (see Sec.~\ref{ch04:sec4.1}).

As to probability measures on $\mathbb{C}$, the maximization of free entropy under constraints of the $p$th moment is solved as follows. For $p, R>0$ define the distribution $\lambda_{R}^{(p)}$ supported on the disk $\{\zeta\in \mathbb{C}\, :\, |\zeta|\leq R\}$ by
\begin{equation*}
\lambda_{R}^{(p)} :=\frac{p}{2\pi R^{p}}d\theta\cdot r^{p-1}\chi_{[0,R]}(r)\, dr \qquad (\zeta=re^{\mathrm{i}\,\theta}).
\end{equation*}
In particular, when $p=2, \, \lambda_{R}^{(2)}$ is the uniform distribution on $\{\zeta\in \mathbb{C}\, :\, |\zeta|\leq R\}$ that is called the \textit{circular law}.\index{circular!law}\index{law!circular}

\begin{proposition}
\label{ch05:pro5.3.6}
Let $p, R>0$. Among $\mu\in\mathcal{M}(\mathbb{C})$ with $\int|\zeta|^{p}d\mu(\zeta)\leq R^{p}/2\ \,(or =R^{p}/2)$, the distribution $\lambda_{R}^{(p)}$ has maximal free entropy and $\Sigma(\lambda_{R}^{(p)})=\log R-1/2p$. Furthermore, $\lambda_{R}^{(p)}$ is the unique maximizer of the functional
\begin{equation*}
\Sigma(\mu)-\frac{2}{pR^{p}}\int|\zeta|^{p}\,d\mu(\zeta) \quad on \quad \mathcal{M}(\mathbb{C}).
\end{equation*}
\end{proposition}

\begin{proof2}
Since
\begin{equation}
\frac{1}{2\pi}\int_{0}^{2\pi}\log|\zeta-re^{\mathrm{i}\,\theta}|\,d\theta=\left\{\begin{array}{ll}
\log r & \mathrm{if} \ |\zeta|\leq r, \\
\\
\log |\zeta | &  \mathrm{if} \ |\zeta|>r,
\end{array}\right.
\label{ch05:eqn5.3.8}
\end{equation}
it is easy to compute
\begin{equation*}
\int\log|\zeta-\eta|\,d\lambda_{R}^{(p)}(\eta)=\left\{\begin{array}{ll}
\displaystyle\frac{|\zeta|^{p}}{pR^{p}}+\log R-\frac{1}{p} & \mathrm{if}\ |\zeta|\leq R,\\
\\
\displaystyle\log|\zeta|<\frac{|\zeta|^{p}}{pR^{p}}+\log R-\frac{1}{p} & \mathrm{if}\ |\zeta|>R.
\end{array}\right.
\end{equation*}
For any $\mu\in \mathcal{M}(\mathbb{C})$ with $\int|\zeta|^{p}\,d\mu(\zeta)<+\infty$, set $\lambda :=((2/R^{p})\int|\zeta|^{p}\,d\mu(\zeta))^{1/p}$ and $\tilde{\mu} :=D_{1/\lambda}\mu$. Then $\int|\zeta|^{p}\,d\tilde{\mu}(\zeta)=R^{p}/2$ and
\begin{equation*}
-\Sigma(\mu)+\frac{2}{pR^{p}}\int|\zeta|^{p}d\mu(\zeta)=-\Sigma(\tilde{\mu})+\frac{\lambda^{p}}{p}-\log\lambda.
\end{equation*}
Hence, as in the proof of Proposition~\ref{ch05:pro5.3.4}, we can apply Theorem~\ref{ch05:the5.3.3} to get the result.
\end{proof2}

The Marchenko-Pastur distribution\index{maximum entropy!Marchenko-Pastur distribution}\index{distribution!Marchenko-Pastur} (alias the free Poisson distribution) $\mu_{\lambda}\, (\lambda> 0)$ was given in Example~\ref{ch03:exa3.3.5}. Next we prove that $\mu_{\lambda}$ with $\lambda\geq 1$ is a maximizer of a certain free entropy functional.

\begin{proposition}
\label{ch05:pro5.3.7}
When $\lambda\geq 1$, the Marchenko-Pastur distribution $\mu_{\lambda}$ is a unique maximizer of the functional
\begin{equation}
\Sigma(\mu)-\int(x-(\lambda-1)\log x)\, d\mu(x) \quad on \quad \mathcal{M}(\mathbb{R}^{+}).
\label{ch05:eqn5.3.9}
\end{equation}
Furthermore,
\begin{equation}
\Sigma(\mu_{\lambda})=\frac{1}{2}\int(x-(\lambda-1)\log x)\,d\mu_{\lambda}(x)+\frac{1}{2}(\lambda\log\lambda-1-\lambda).
\label{ch05:eqn5.3.10}
\end{equation}
\end{proposition}

\begin{proof2}
From the proof of Example~\ref{ch03:exa3.3.5} we know that the Cauchy transform of $\mu_{\lambda}$ is
\begin{equation*}
G_{\mu_{\lambda}}(z)=\frac{z+(1-\lambda)-\sqrt{(z-1-\lambda)^{2}-4\lambda}}{2z}.
\end{equation*}
Hence the Hilbert transform of
\begin{equation*}
f_{\lambda}(x):=\frac{\sqrt{4\lambda-(x-1-\lambda)^{2}}}{2\pi x}\chi_{[(1-\sqrt{\lambda})^{2}, (1+\sqrt{\lambda})^{2}]}(x)
\end{equation*}
is
\begin{equation*}
Hf_{\lambda}(x)=\frac{1}{\pi}\lim_{y\rightarrow+0}\, \mathrm{Re} \, G_{\mu_{\lambda}}(x+\mathrm{i}\,y)=\frac{1}{2\pi}\left(1-\frac{\lambda-1}{x}\right)
\end{equation*}
if $(1-\sqrt{\lambda})^{2}<x<(1+\sqrt{\lambda})^{2}$. Put
\begin{equation*}
F(x):=\int f_{\lambda}(y)\log|x-y|\,dy.
\end{equation*}
Since $F'(x)=\pi Hf_{\lambda}(x)$ in the sense of distributions in $((1-\sqrt{\lambda})^{2}, (1+\sqrt{\lambda})^{2})$, we have
\begin{equation*}
F(x)=\frac{1}{2}(x-(\lambda-1)\log x)+C
\end{equation*}
for $x\in[(1-\sqrt{\lambda})^{2}, (1+\sqrt{\lambda})^{2}]$, where $C$ is a constant. On the other hand, $F(x)$ is differentiable in the usual sense in $\mathbb{R}^{+}\, \backslash \, [(1-\sqrt{\lambda})^{2}, (1+\sqrt{\lambda})^{2}]$, and
\begin{align*}
F'(x) & \ = \ \int\frac{f_{\lambda}(y)}{x-y}\,dy=G_{\mu_{\lambda}}(x) \\
& \ =\ \frac{x+(1-\lambda)-\sqrt{(x-1-\lambda)^{2}-4\lambda}}{2x}<\frac{1}{2}\left(1-\frac{\lambda-1}{x}\right)
\end{align*}
for $x\in \mathbb{R}^{+}\, \backslash \, [(1-\sqrt{\lambda})^{2}, (1+\sqrt{\lambda})^{2}]$. Since $F(x)$ is continuous at $x=(1\pm\sqrt{\lambda})^{2}$, we have
\begin{equation}
\int\log|x-y|\,d\mu_{\lambda}(y)\left\{\begin{array}{ll}
=\frac{1}{2}(x-(\lambda-1)\log x)+C \quad \mathrm{if}\ x\in \mathrm{supp}\,\mu_{\lambda},\\
\\
<\frac{1}{2}(x-(\lambda-1)\log x)+C \quad \mathrm{if} \ x\in \mathbb{R}^{+}\, \backslash \, \mathrm{supp}\,\mu_{\lambda}.
\end{array}\right.
\label{ch05:eqn5.3.11}
\end{equation}
We can apply Theorem~\ref{ch05:the5.3.3} to get the first assertion, because the weight function $w(x):=e^{-x/2}x^{(\lambda-1)/2}$ on $\mathbb{R}^{+}$ satisfies (i)--(iii) stated above Theorem~\ref{ch05:the5.3.3}.

Taking $ x=1+\lambda$ in (\ref{ch05:eqn5.3.11}), we get
\begin{align*}
C& =\int_{(1-\sqrt{\lambda})^{2}}^{(1+\sqrt{\lambda})^{2}}\frac{\sqrt{4\lambda-(y-1-\lambda)^{2}}}{2\pi y}\log|y-1-\lambda|\,dy \\
& \qquad \ \ -\frac{1}{2}(1+\lambda-(\lambda-1)\log(1+\lambda)).
\end{align*}
We compute
\begin{align*}
& \int_{(1-\sqrt{\lambda})^{2}}^{(1+\sqrt{\lambda})^{2}}\frac{\sqrt{4\lambda-(y-1-\lambda)^{2}}}{2\pi y}\log|y-1-\lambda|\, dy \\
& \qquad  =\log 2\sqrt{\lambda}+\frac{\sqrt{\lambda}}{\pi}\int_{-1}^{1}\frac{\sqrt{1-x^{2}}}{\frac{1+\lambda}{2\sqrt{\lambda}}+x}\log|x|\,dx \\
& \qquad  =\log 2\sqrt{\lambda}+\frac{1+\lambda}{\pi}\int_{0}^{1}\frac{\sqrt{1-x^{2}}}{\frac{(1+\lambda)^{2}}{4\lambda}-x^{2}}\log x\, dx \\
& \qquad  =\log 2\sqrt{\lambda}+\frac{1+\lambda}{\pi}\left(\frac{\pi(b-1)}{2b}\log\frac{2b-1}{2b}-\frac{\pi}{2b}\log 2\right),
\end{align*}
where $b:=(1+\lambda)/2$. The last equality in the above is due to the integral formula ([\citen{bib115}], Theorem 4.3)
\begin{equation*}
\int_{0}^{1}\frac{\sqrt{1-x^{2}}}{\alpha-x^{2}}\log x\, dx=\frac{\pi(b-1)}{2b}\log\frac{2b-1}{2b}-\frac{\pi}{2b}\log 2,
\end{equation*}
where $\alpha\in \mathbb{R}\, \backslash \, [0,1)$ and $b=\alpha+\mathrm{sign}\,\alpha\sqrt{\alpha^{2}-\alpha}$. Therefore,
\begin{equation*}
C=\frac{1}{2}(\lambda\log\lambda-1-\lambda),
\end{equation*}
so that (\ref{ch05:eqn5.3.10}) is shown.
\end{proof2}

For $0\leq \rho<R$ let $m_{\rho,R}$ denote the uniform distribution on the annulus $\{\zeta\in \mathbb{C} : \rho\leq|\zeta|\leq R\}$, that is,
\begin{equation*}
m_{\rho,R} :=\frac{1}{\pi(R^{2}-\rho^{2})}\,d\theta\cdot r\chi_{[\rho,R]}(r)\,dr  \qquad (\zeta=re^{\mathrm{i}\,\theta}).
\end{equation*}
We may call this the \textit{annular law}.\index{maximum entropy!annular law} The next proposition says that the measure $m_{\sqrt{\lambda-1},\sqrt{\lambda}}$ with $\lambda\geq 1$ is a maximizer of a functional which can be regarded as the complexification of (\ref{ch05:eqn5.3.9}).

\begin{proposition}
\label{ch05:pro5.3.8}
When $\lambda\geq 1$, the annular law\index{law!annular}\index{annular law} $m_{\sqrt{\lambda-1},\sqrt{\lambda}}$ is the unique maximizer of the functional
\begin{equation*}
\Sigma(\mu)-\int(|\zeta|^{2}-2(\lambda-1)\log|\zeta|)\,d\mu(\zeta) \quad on \quad \mathcal{M}(\mathbb{C}).
\end{equation*}
Moreover,
\begin{equation}
\Sigma(m_{\sqrt{\lambda-1},\sqrt{\lambda}})=-\frac{3}{4}+\frac{1}{2}(\lambda+\log\lambda+(\lambda-1)^{2}\log(1-\lambda^{-1})).
\label{ch05:eqn5.3.12}
\end{equation}
\end{proposition}

\begin{proof2}
From (\ref{ch05:eqn5.3.8}) it is easy to compute
\begin{align}
& \int\log|\zeta-\eta|\,dm_{\sqrt{\lambda-1},\sqrt{\lambda}}(\eta) \notag \\
& =\left\{\begin{array}{ll}
\frac{1}{2}(|\zeta|^{2}-2(\lambda-1)\log|\zeta|+\lambda\log\lambda-\lambda) & \ \ \mathrm{if}\ \sqrt{\lambda-1}\leq|\zeta|\leq\sqrt{\lambda},\\
\\
\frac{1}{2}(\lambda\log\lambda-(\lambda-1)\log(\lambda-1)-1) & \ \ \mathrm{if}\ |\zeta|<\sqrt{\lambda-1}, \\
\\
\log|\zeta| & \ \ \mathrm{if}\ |\zeta|>\sqrt{\lambda}.\\
\end{array}\right.
 \label{ch05:eqn5.3.13}
\end{align}
Also, the following are readily checked:
\begin{align*}
& \lambda\log\lambda -(\lambda-1)\log(\lambda-1)-1 \\
& \qquad \quad \ <|\zeta|^{2}-2(\lambda-1)\log|\zeta|+\lambda\log\lambda-\lambda \quad \mathrm{for} \quad |\zeta|<\sqrt{\lambda-1}, \\
& 2\log|\zeta| <|\zeta|^{2}-2(\lambda-1)\log|\zeta|+\lambda\log\lambda-\lambda \quad \mathrm{for} \quad |\zeta|>\sqrt{\lambda}.
\end{align*}
Hence the first assertion is obtained by Theorem~\ref{ch05:the5.3.3}. A direct computation from (\ref{ch05:eqn5.3.13}) gives (\ref{ch05:eqn5.3.12}).
\end{proof2}

In the rest of this section let us treat three more results on maximization of the free entropy. The Poisson kernel plays an essential role in analytic function theory in the unit disk. For $\alpha\in \mathbb{C}, |\alpha|<1$, we have the \textit{Poisson kernel measure}\index{Poisson!kernel measure} $p_{\alpha}$ supported on the unit circle $\mathbb{T}$, defined by
\begin{equation}
p_{\alpha} :=\frac{1-|\alpha|^{2}}{|\zeta-\alpha|^{2}}\,d\zeta \qquad (\zeta=e^{\mathrm{i}\,\theta},\ d\zeta=d\theta/2\pi).
\label{ch05:eqn5.3.14}
\end{equation}
The next maximization result is related to this.

\begin{proposition}
\label{ch05:pro5.3.9}
For any $\alpha\in \mathbb{C}, \, |\alpha|<1$, the Poisson kernel measure\index{maximum entropy!Poisson kernel measure} $p_{\alpha}$ is the unique maximizer of the functional
\begin{equation*}
\Sigma(\mu)-\int\log|\zeta\bar{\alpha}-1|^{2}\,d\mu(\zeta) \quad on \quad \mathcal{M}(\overline{\mathbb{D}}),
\end{equation*}
where $\overline{\mathbb{D}} :=\{\zeta\in \mathbb{C} : |\zeta|\leq 1\}$. Furthermore, $p_{\alpha}$ maximizes $\Sigma(\mu)$ among $\mu\in\mathcal{M}(\overline{\mathbb{D}})$ with $\int\log|\zeta\bar{\alpha}-1|\,d\mu(\zeta)=\log(1-|\alpha|^{2})$, and $\Sigma(p_{\alpha})=\log(1-|\alpha|^{2})$.
\end{proposition}

\begin{proof2}
By noting that $\log|\zeta|$ is harmonic in $\mathbb{C}\, \backslash \, \{0\}$, it is easy to check that
\begin{equation*}
\int\log|\zeta-\eta|\, dp_{\alpha}(\eta)=\log|\zeta\bar{\alpha}-1| \qquad (\zeta\in\overline{\mathbb{D}}).
\end{equation*}
Hence Theorem~\ref{ch05:the5.3.3} implies the first assertion. Furthermore,
\begin{equation*}
\Sigma(p_{\alpha})=\int\log|\zeta\bar{\alpha}-1|\,dp_{\alpha}(\zeta)=\log(1-|\alpha|^{2}).
\end{equation*}
Next, assume that $\mu\in \mathcal{M}(\overline{\mathbb{D}})$ and $\int\log|\zeta\bar{\alpha}-1|d\mu(\zeta)=\log(1-|\alpha|^{2})$. Then, by Lemma~\ref{ch05:lem5.3.1},
\begin{align*}
0 & \ \leq \ E(\mu-p_{\alpha}) \\
& \ = \ -\ \Sigma(\mu)-\Sigma(p_{\alpha})+2\iint\log|\zeta-\eta|dp_{\alpha}(\eta)d\mu(\zeta) \\
& \ = \ - \ \Sigma(\mu)+\log(1-|\alpha|^{2}),
\end{align*}
so $\Sigma(\mu)\leq\log(1-|\alpha|^{2})$ , and equality occurs if and only if $\mu=p_{\alpha}$.
\end{proof2}

In particular, when $\alpha=0$, the above proposition gives a known fact: the Lebesgue probability measure on $\mathbb{T}$ is the equilibrium measure on $\overline{\mathbb{D}}$ (or $\mathbb{T}$).

Define a one-parameter family of probability measures $\rho_{\lambda}\, (0<\lambda\leq\infty)$ on $\mathbb{T}$ by
\begin{equation}
\rho_{\lambda}:=\left\{\begin{array}{ll}
\dfrac{1}{2\pi}\left(1+\dfrac{2}{\lambda}\cos\theta\right)d\theta & \mathrm{if}\ 2\leq\lambda\leq\infty,\\
\\
\dfrac{2}{\pi\lambda}\cos\dfrac{\theta}{2}\sqrt{\dfrac{\lambda}{2}-\sin^{2}\dfrac{\theta}{2}}\chi_{[-a,a]}(\theta)\,d\theta & \mathrm{if} \ 0<\lambda<2,
\end{array}\right.
\label{ch05:eqn5.3.15}
\end{equation}
where $a :=2$ arcsin $\sqrt{\lambda}/2$. In the following we show that these distributions are maximizers of the free entropy of $\mu\in \mathcal{M}(\mathbb{T})$ when the mean $\int\zeta d\mu(\zeta)$ is fixed.

\begin{proposition}
\label{ch05:pro5.3.10}
For any $ 0<\lambda\leq\infty$ the distribution $\rho_{\lambda}$ is a unique maximizer of the functinal
\begin{equation*}
\Sigma(\mu)+\frac{2}{\lambda}\int\mathrm{Re}\, \zeta \, d\mu(\zeta) \quad on \quad \mathcal{M}(\mathbb{T}).
\end{equation*}
Furthermore, when $2 \leq\lambda\leq\infty, \ \rho_{\lambda}$ maximizes $\Sigma(\mu)$ among $\mu\in \mathcal{M}(\mathbb{T})$ with $\int\mathrm{Re}\,\zeta\, d\mu(\zeta)$ $($or $\int\,\zeta \,d\mu(\zeta))=1/\lambda$, and $\Sigma(\rho_{\lambda})=-1/\lambda^{2}$. When $0<\lambda<2, \ \rho_{\lambda}$ maximizes $\Sigma(\mu)$ among $\mu\in \mathcal{M}(\mathbb{T})$ with $\int\mathrm{Re}\,\zeta \, d\mu(\zeta)$ $($or $\int\zeta \, d\mu(\zeta))=1-\lambda/4$, and $\Sigma(\rho_{\lambda})=\frac{1}{2}\log(\lambda/2)-1/4$.
\end{proposition}

\begin{proof2}
When $ 2\leq\lambda\leq\infty$ the computation is straightforward. For $\zeta=e^{\mathrm{i}\,t}$,
\begin{align*}
\int\log|\zeta-\eta|\,d\rho_{\lambda}(\eta) & \ = \ \frac{1}{2\pi}\int_{0}^{2\pi}\left(1+\frac{2}{\lambda}\cos(\theta+t)\right)\log|1-e^{\mathrm{i}\,\theta}|\,d\theta \\
& \ = \ \frac{1}{2\pi\lambda}\int_{0}^{2\pi}\cos(\theta+t)\log 2(1-\cos\theta)\,d\theta \\
& \ =\ \frac{\cos t}{2\pi\lambda}\int_{0}^{2\pi}\cos\theta\log(1-\cos\theta)\,d\theta \\
& \ = \ \frac{\cos t}{\pi\lambda}\int_{-1}^{1}\frac{t}{\sqrt{1-t^{2}}}\log(1-t)\,dt \\
& \ = \ -\frac{\cos t}{\pi\lambda}\int_{-1}^{1}\sqrt{\frac{1+t}{1-t}}\,dt=-\frac{1}{\lambda}\cos t.
\end{align*}
This implies the first assertion by Theorem~\ref{ch05:the5.3.3}. Moreover,
\begin{equation*}
\Sigma(\rho_{\lambda})=-\frac{1}{2\pi\lambda}\int_{0}^{2\pi}\cos t\left(1+\frac{2}{\lambda}\cos t\right)dt=-\frac{1}{\lambda^{2}}.
\end{equation*}
If $\mu\in\mathcal{M}(\mathbb{T})$ satisfies $\int\mathrm{Re}\,\zeta \, d\mu(\zeta)=1/\lambda$, then by Lemma~\ref{ch05:lem5.3.1}
\begin{align*}
0\leq E(\mu-\rho_{\lambda}) & \ = \ -\Sigma(\mu)-\Sigma(\rho_{\lambda})+2\iint\log|\zeta-\eta|\,d\rho_{\lambda}(\eta)\,d\mu(\zeta) \\
& \ = \ -\Sigma(\mu)+\frac{1}{\lambda^{2}}-\frac{2}{\lambda}\int\mathrm{Re}\,\zeta \,d\mu(\zeta)=-\Sigma(\mu)-\frac{1}{\lambda^{2}},
\end{align*}
so $\Sigma(\mu)\leq-1/\lambda^{2}$, and equality occurs if and only if $\mu=\rho_{\lambda}$.

When $0<\lambda<2$ the computation is quite involved. We use the technique of the ``Hilbert transform'' on the circle. Put $\alpha :=\sqrt{\lambda/2}$ and $\beta :=2$ arcsin $\alpha$. Define, for $\zeta=e^{\mathrm{i}\,t}$,
\begin{align*}
F(t) & \ := \ \int\log|\zeta-\eta|\,d\rho_{\lambda}(\eta) \\
& \ =\ \frac{2}{\pi\lambda}\int_{-\beta}^{\beta}\cos\frac{\theta}{2}\sqrt{\frac{\lambda}{2}-\sin^{2}\frac{\theta}{2}}\log|1-e^{\mathrm{i}\,(\theta-t)}|\,d\theta \\
& \ =\ \frac{2}{\pi\lambda}\int_{-\beta}^{\beta}\cos\frac{\theta}{2}\sqrt{\frac{\lambda}{2}-\sin^{2}\frac{\theta}{2}}\log\left(2\left|\sin\frac{\theta-t}{2}\right|\right)\,d\theta.
\end{align*}
When $|t|<\alpha$, the differential of $F(t)$ in the sense of distributions in $(-\alpha, \alpha)$ is given as
\begin{equation*}
F'(t)=\frac{1}{\pi\lambda}\int_{-\beta}^{\beta}\cos\frac{\theta}{2}\sqrt{\frac{\lambda}{2}-\sin^{2}\frac{\theta}{2}}\cot\frac{\theta-t}{2}\,d\theta.
\end{equation*}
(This, as well as the integrals below, is understood as a principal value integral.) We proceed to compute
\begin{align*}
F'(t) & \ = \ \frac{2}{\pi\lambda}\int_{-\alpha}^{\alpha}\sqrt{\alpha^{2}-x^{2}}\frac{\sqrt{1-x^{2}}\cos\frac{t}{2}+x\sin\frac{t}{2}}{x\cos\frac{t}{2}-\sqrt{1-x^{2}}\sin\frac{t}{2}}\,dx \\
& \ = \ \frac{1}{\pi\lambda}\int_{-\alpha}^{\alpha}\sqrt{\alpha^{2}-x^{2}}\,\frac{\sin t+4x\sqrt{1-x^{2}}}{x^{2}-\sin^{2}\frac{t}{2}}\,dx \\
& \ = \ \frac{\sin t}{\pi\lambda}\int_{-\alpha}^{\alpha}\frac{\sqrt{\alpha^{2}-x^{2}}}{x^{2}-\sin^{2}\frac{t}{2}}\,dx.
\end{align*}
Since the above principal value integral is equal to $-\pi$ (cf. [\citen{bib121}], p. 74), we have $F'(t)=-\lambda^{-1}\sin t$, and hence
\begin{equation*}
F(t)=-\frac{1}{\lambda}\cos t+\frac{1}{2}\log\frac{\lambda}{2}+\frac{1}{\lambda}-\frac{1}{2} \qquad (|t|<\alpha),
\end{equation*}
because
\begin{align*}
F(0) & \ =  \ \frac{2}{\pi\lambda}\int_{-\beta}^{\beta}\cos\frac{\theta}{2}\sqrt{\frac{\lambda}{2}-\sin^{2}\frac{\theta}{2}}\log\left(2\left|\sin\frac{\theta}{2}\right|\right)d\theta \\
&  \ = \ \frac{8}{\pi\lambda}\int_{0}^{\alpha}\sqrt{\alpha^{2}-x^{2}}\log(2x)\,dx \\
& \ = \ \frac{4}{\pi}\int_{0}^{1}\sqrt{1-x^{2}}\log(2\alpha x)\,dx=\frac{1}{2}\log\frac{\lambda}{2}-\frac{1}{2}.
\end{align*}
On the other hand, when $|t|>\alpha, \, F(t)$ is differentiable in the usual sense and
\begin{align*}
F'(t) & \ = \ \frac{\sin t}{2\pi\alpha^{2}}\int_{-\alpha}^{\alpha}\frac{\sqrt{\alpha^{2}-x^{2}}}{x^{2}-\sin^{2}\frac{t}{2}}\,dx \\
& \ < \ \frac{\sin t}{2\pi\alpha^{2}}\int_{-\alpha}^{\alpha}\frac{\sqrt{\alpha^{2}-x^{2}}}{x^{2}-\alpha^{2}}\,dx=-\frac{1}{\lambda}\sin t.
\end{align*}
Therefore, since $F(t)$ is continuous at $ t=\pm\alpha$, we obtain
\begin{equation*}
\int\log|\zeta-\eta|\,d\rho_{\lambda}(\eta)\left\{\begin{array}{ll}
=-\frac{1}{\lambda}\mathrm{Re}\,\zeta+\frac{1}{2}\log\frac{\lambda}{2}+\frac{1}{\lambda}-\frac{1}{2} \quad \mathrm{if} \ \zeta\in \mathrm{supp}\,\rho_{\lambda},\\
\\
<-\frac{1}{\lambda}\mathrm{Re}\,\zeta+\frac{1}{2}\log\frac{\lambda}{2}+\frac{1}{\lambda}-\frac{1}{2}\quad \mathrm{if} \ \zeta\in \mathbb{T}\, \backslash \, \mathrm{supp}\,\rho_{\lambda}.
\end{array}\right.
\end{equation*}
This implies the first assertion by Theorem~\ref{ch05:the5.3.3}. Moreover,
\begin{align*}
\Sigma(\rho_{\lambda}) & \ = \ -\frac{2}{\pi\lambda^{2}}\int_{-\beta}^{\beta}\cos t\cos\frac{t}{2}\sqrt{\frac{\lambda}{2}-\sin^{2}\frac{t}{2}}\,dt+\frac{1}{2}\log\frac{\lambda}{2}+\frac{1}{\lambda}-\frac{1}{2} \\
& \ =\ -\frac{8}{\pi\lambda^{2}}\int_{0}^{\alpha}(1-2x^{2})\sqrt{\alpha^{2}-x^{2}}\,dx+\frac{1}{2}\log\frac{\lambda}{2}+\frac{1}{\lambda}-\frac{1}{2} \\
& \ = \ \frac{1}{2}\log\frac{\lambda}{2}-\frac{1}{4}.
\end{align*}
If $\mu\in \mathcal{M}(\mathbb{T})$ satisfies $\int\mathrm{Re}\, \zeta \, d\mu(\zeta)=1-\lambda/4$, then by Lemma~\ref{ch05:lem5.3.1}
\begin{align*}
0 & \ \leq \ E(\mu-\rho_{\lambda}) \\
& \ = \ -\Sigma(\mu)-\Sigma(\rho_{\lambda})+2\iint\log|\zeta-\eta|\,d\rho_{\lambda}(\eta)\,d\mu(\zeta) \\
& \ \leq \ -\Sigma(\mu)-\frac{1}{2}\log\frac{\lambda}{2}+\frac{1}{4}+2\left(-\frac{1}{\lambda}\left(1-\frac{\lambda}{4}\right)+\frac{1}{2}\log\frac{\lambda}{2}+\frac{1}{\lambda}-\frac{1}{2}\right) \\
& \ = \ -\Sigma(\mu)+\frac{1}{2}\log\frac{\lambda}{2}-\frac{1}{4},
\end{align*}
so $\Sigma(\mu)\leq\frac{1}{2}\log(\lambda/2)-1/4$, and equality occurs if and only if $\mu=\rho_{\lambda}$.
\end{proof2}

For $-1<\tau<1$ and $R>0$ let $m_{R}^{(\tau)}$ denote the uniform distribution on the ellipse
\begin{equation}
E_{R}^{(\tau)} :=\{x+\mathrm{i}\,y : \frac{x^{2}}{(1+\tau)^{2}}+\frac{y^{2}}{(1-\tau)^{2}}\leq R^{2}\},
\label{ch05:eqn5.3.16}
\end{equation}
which is sometimes called the \textit{elliptic law}.\index{maximum entropy!elliptic law}\index{law!elliptic} The next proposition says that the elliptic law appears as the solution of some maximization problem for a functional containing a free entropy term.

\begin{proposition}
\label{ch05:pro5.3.11}
Let $-1 <\tau<1$ and $R>0$. The elliptic law\index{elliptic!law} $m_{R}^{(\tau)}$ is a unique maximizer of the functional
\begin{equation*}
\Sigma(\mu)-\frac{1}{R^{2}}\int\left(\frac{x^{2}}{1+\tau}+\frac{y^{2}}{1-\tau}\right)d\mu(\zeta) \quad on \quad \mathcal{M}(\mathbb{C}) \qquad (\zeta=x+\mathrm{i}\,y).
\end{equation*}
Furthermore, among $\mu\in \mathcal{M}(\mathbb{C})$ with $\int(x^{2}/(1+\tau)+y^{2}/(1-\tau))\,d\mu(\zeta)\leq R^{2}/2$ $($or $=R^{2}/2), \, m_{R}^{(\tau)}$ maximizes $\Sigma(\mu)$, and $\Sigma(m_{R}^{(\tau)})=\log R-1/4$ independently of the parameter $\tau$.
\end{proposition}

To prove the proposition we need

\begin{lemma}
\label{ch05:lem5.3.12}
For $\zeta=x+\mathrm{i}\,y$,
\begin{equation*}
\int\log|\zeta-\eta|\,dm_{R}^{(\tau)}(\eta)\left\{\begin{array}{ll}
\displaystyle =\frac{1}{2R^{2}}\left(\frac{x^{2}}{1+\tau}+\frac{y^{2}}{1-\tau}\right)+\log R-\frac{1}{2} &   \mathit{if} \ \zeta\in E_{R}^{(\tau)}, \\
\displaystyle <\frac{1}{2R^{2}}\left(\frac{x^{2}}{1+\tau}+\frac{y^{2}}{1-\tau}\right)+\log R \quad \,\frac{1}{2} &  \mathit{if} \ \zeta\in \mathbb{C}\, \backslash \, E_{R}^{(\tau)}.
\end{array}\right.
\end{equation*}
\end{lemma}

\begin{proof2}
Since $m_{R}^{(\tau)}=D_{R}m_{1}^{(\tau)}$, we immediately get
\begin{equation*}
\int\log|\zeta-\eta|\,dm_{R}^{(\tau)}(\eta)=\int\log|R^{-1}\zeta-\eta|\,dm_{1}^{(\tau)}+\log R.
\end{equation*}
So it is enough to show the case $R=1$. Moreover we may assume $0\leq\tau<1$ by symmetry. Set
\begin{equation*}
\left\{\begin{array}{ll}
f_{1}(x, y):=\dfrac{x}{1+\tau}-\mathrm{i}\dfrac{y}{1-\tau} & \quad \mathrm{if}\ \zeta\in E_{1}^{(\tau)},\\
f_{2}(x, y) :=\dfrac{\zeta}{2\tau}\left(1-\sqrt{1-\dfrac{4\tau}{\zeta^{2}}}\right) & \quad \mathrm{if} \ \zeta\in \mathbb{C}\, \backslash \, E_{1}^{(\tau)}.
\end{array}\right.
\end{equation*}
Then $f_{2}(\zeta)$ is an analytic function (a branch such that $f_{2}(\infty)=0$) in $\mathbb{C}\, \backslash \, E_{1}^{(\tau)}$ and $f_{2}(x, y)=f_{1}(x, y)$ on the boundary of $E_{1}^{(\tau)}$. Since
\begin{equation*}
\left\{\begin{array}{ll}\displaystyle
\frac{1}{2\pi}\left(\frac{\partial}{\partial x}f_{1}(x, y)+\mathrm{i}\frac{\partial}{\partial y}f_{1}(x, y)\right)=\frac{1}{\pi(1-\tau^{2})} & \quad \mathrm{if}\ \zeta\in E_{1}^{(\tau)},\\
\\
\displaystyle \frac{1}{2\pi}\left(\frac{\partial}{\partial x}f_{2}(x, y)+\mathrm{i}\frac{\partial}{\partial y}f_{2}(x, y)\right)=0 & \quad \mathrm{if} \ \zeta\in \mathbb{C}\,\backslash \, E_{1}^{(\tau)},
\end{array}\right.
\end{equation*}
one can use the Gauss integral formula ([\citen{bib125}], Appendix IV) to obtain
\begin{equation*}
\int\frac{dm_{1}^{(\tau)}(\eta)}{\zeta-\eta}=\left\{\begin{array}{l}
f_{1}(x, y) \quad \mathrm{if}\ \zeta\in E_{1}^{(\tau)},\\
f_{2}(x, y) \quad \mathrm{if}\ \zeta\in \mathbb{C}\, \backslash \, E_{1}^{(\tau)}.
\end{array}\right.
\end{equation*}

Next we show that if $x+\mathrm{i}\,y\in \mathbb{C}\, \backslash \, E_{1}^{(\tau)}$ and $x, y\geq 0$, then
\begin{equation}
\left\{\begin{array}{ll}
\displaystyle \mathrm{Re} \, f_{2}(x, y)\leq\frac{x}{1+\tau} & \ (\mathrm{strictly} \ \mathrm{if}\ x>0),\\
\\
\displaystyle -\mathrm{Im} \, f_{2}(x, y)\leq\frac{y}{1-\tau} & \  (\mathrm{strictly}\ \mathrm{if}\ y>0).
\end{array}\right.
\label{ch05:eqn5.3.17}
\end{equation}
We may assume $x, y>0$, because the case $x=0$ or $y=0$ is straightforward. Let $\zeta_{0}=x_{0}+\mathrm{i}\,y_{0}(x_{0}, y_{0}>0)$ be on the boundary of $E_{1}^{(\tau)}$. For $t\geq 1$, if we write $x(t)-\mathrm{i}\,y(t)=f_{2}(t\zeta_{0})/t$, then $(x(t), y(t))$ is an intersection of two hyperbolic curves:
\begin{align}
& \left(x-\frac{x_{0}}{2\tau}\right)^{2}-\left(y+\frac{y_{0}}{2\tau}\right)^{2}=\frac{x_{0}^{2}-y_{0}^{2}-4\tau/t^{2}}{4\tau^{2}},\label{ch05:eqn5.3.18} \\
& \left(x-\frac{x_{0}}{2\tau}\right)\left(y+\frac{y_{0}}{2\tau}\right)=-\frac{x_{0}y_{0}}{4\tau^{2}}.
\label{ch05:eqn5.3.19}
\end{align}
Note that
\begin{equation*}
x_{0}^{2}-y_{0}^{2}-4\tau=\frac{4\tau}{(1+\tau)^{2}}x_{0}^{2}-(1+\tau)^{2}\leq 4\tau-(1+\tau)^{2}<0.
\end{equation*}
Looking at the graphs of (\ref{ch05:eqn5.3.18}) and (\ref{ch05:eqn5.3.19}), one can easily see that when $t$ increases from 1 the point $(x(t), y(t))$ moves along the left-upper half of the curve (\ref{ch05:eqn5.3.19}) in the left-lower direction from $(x(1), y(1))=(x_{0}/(1+\tau), y_{0}/(1-\tau))$.  Hence $x(t)<x_{0}/(1+\tau)$ and $y(t)<y_{0}/(1-\tau)$ for all $t>1$, implying (\ref{ch05:eqn5.3.17}).

Let $\zeta_{0}=x_{0}+\mathrm{i}\,y_{0}$ be on the boundary of $E_{1}^{(\tau)}$, and for $t\geq 0$ set
\begin{align*}
\phi(t) & \ := \int\log|t\zeta_{0}-\eta|\, dm_{1}^{(\tau)}(\eta) \\
& \ = \ \frac{1}{2\pi(1-\tau^{2})}\iint_{E_{1}^{(\tau)}}\log((tx_{0}-x)^{2}+(ty_{0}-y)^{2})\,dx\,dy.
\end{align*}
Here we may assume $x_{0}, y_{0}\geq 0$ by symmetry. Since
\begin{align*}
\phi'(t) & \ = \ \frac{1}{\pi(1-\tau^{2})}\iint_{E_{\tau, 1}}\frac{(tx_{0}-x)x_{0}+(ty_{0}-y)y_{0}}{(tx_{0}-x)^{2}+(ty_{0}-y)^{2}}dx\,dy \\
& \ =\ x_{0}\, \mathrm{Re}\int\frac{dm_{1}^{(\tau)}(\eta)}{t\zeta_{0}-\eta}-y_{0}\,\mathrm{Im}\int\frac{dm_{1}^{(\tau)}(\eta)}{t\zeta_{0}-\eta},
\end{align*}
we have
\begin{equation*}
\phi'(t)\left\{\begin{array}{ll}
\displaystyle =\frac{tx_{0}^{2}}{1+\tau}+\frac{ty_{0}^{2}}{1-\tau} & \ \mathrm{if}\ 0<t<1,\\
\displaystyle <\frac{tx_{0}^{2}}{1+\tau}+\frac{ty_{0}^{2}}{1-\tau}& \ \mathrm{if}\ t>1,
\end{array}\right.
\end{equation*}
using (\ref{ch05:eqn5.3.17}) for the inequality. The above estimate yields
\begin{equation*}
\phi(t)\left\{\begin{array}{ll}
\displaystyle =\frac{1}{2}\left(\frac{t^{2}x_{0}^{2}}{1+\tau}+\frac{t^{2}y_{0}^{2}}{1-\tau}\right)+\phi(0) & \ \mathrm{if}\ 0<t<1,\\
\displaystyle <\frac{1}{2}\left(\frac{t^{2}x_{0}^{2}}{1+\tau}+\frac{t^{2}y_{0}^{2}}{1-\tau}\right)+\phi(0) & \ \mathrm{if}\ t>1.
\end{array}\right.
\end{equation*}
Finally, the computation of $\phi(0)=-1/2$ is as follows:
\begin{align*}
\phi(0) & \ = \ \frac{1}{2\pi(1-\tau^{2})}\iint_{E_{\tau, 1}}\log(x^{2}+y^{2})\,dx\, dy \\
& \ =\ \frac{1}{2\pi}\int_{0}^{1}r\, dr\int_{0}^{2\pi}\log r^{2}((1+\tau)^{2}\cos^{2}\theta+(1-\tau)^{2}\sin^{2}\theta)\,d\theta \\
& \ = \ \int_{0}^{1}2r\log r\, dr+\frac{1}{2\pi}\int_{0}^{1}r\, dr\int_{0}^{2\pi}\log(1+\tau^{2}+2\tau\,\cos 2\theta)\, d\theta \\
& \ = \ - \frac{1}{2}.
\end{align*}
\end{proof2}

\begin{proof2}[Proof of Proposition~\ref{ch05:pro5.3.11}] The first assertion follows from Theorem~\ref{ch05:the5.3.3} and Lemma~\ref{ch05:lem5.3.12}. Let $Q(\zeta) :=x^{2}/(1+\tau)+y^{2}/(1-\tau)(\zeta=x+\mathrm{i}\,y)$. For any $\mu\in \mathcal{M}(\mathbb{C})$ with $\int Q(\zeta)d\mu(\zeta)<+\infty$, set $\lambda :=((2/R^{2})\int Q(\zeta)\,d\mu(\zeta))^{1/2}$ and $\tilde{\mu} :=D_{1/\lambda}\mu$. Then we have $\int Q(\zeta)d\tilde{\mu}(\zeta)=R^{2}/2$ and
\begin{equation*}
-\Sigma(\mu)+\frac{1}{R^{2}}\int Q(\zeta)\,d\mu(\zeta)=-\Sigma(\tilde{\mu})+\frac{\lambda^{2}}{2}-\log\lambda.
\end{equation*}
Hence the second assertion can be shown as in the proof of Proposition~\ref{ch05:pro5.3.4}. Furthermore, by Lemma~\ref{ch05:lem5.3.12} we compute
\begin{align*}
\Sigma(m_{R}^{(\tau)}) & \ = \ \Sigma(m_{1}^{(\tau)})+\log R \\
& \ = \ \frac{1}{2\pi(1-\tau^{2})}\iint_{E_{1}^{(\tau)}}\left(\frac{x^{2}}{1+\tau}+\frac{y^{2}}{1-\tau}\right)dx\,dy+\log R-\frac{1}{2} \\
& \ = \ \frac{1}{2\pi}\int_{0}^{1}r^{3}\, dr\int_{0}^{2\pi}((1+\tau)\cos^{2}\theta+(1-\tau)\sin^{2}\theta)\,d\theta \\
& \qquad \quad +\log R-\frac{1}{2} \\
& \ = \ \log R-\frac{1}{4}.
\end{align*}
\end{proof2}

\section{Gaussian and unitary random matrices}
\label{ch05:sec5.4}

It is useful to give a short summary of the contents of Sections~\ref{ch05:sec5.1} and \ref{ch05:sec5.2}. We started from the level-2 large deviation theorem for the mean of independent identically distributed random variables. This result is something like the strengthening of the law of large numbers, because, beyond stating the limit distribution of the means, the rate of convergence is exactly established. Then we deduced that the differential entropy, or more generally the relative entropy, is related to the volume of atomic measures approximating the given probability measure. That was Proposition~\ref{ch05:pro5.1.1}, giving the limit of
\begin{equation}
\frac{1}{n}\log\nu^{n} (\{x\in \mathbb{R}^{n} : |m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon, \ k\leq r\}),
\label{ch05:eqn5.4.1}
\end{equation}
and also the formula (\ref{ch05:eqn5.1.8}) was observed. Roughly speaking, when $n$ is large, the Lebesgue measure of the set of those points $x\in \mathbb{R}^{n}$ such that the atomic measure $\frac{1}{n}(\delta(x_{1})+\delta(x_{2})+\cdots+\delta(x_{n}))$ is very close to the given measure $\mu$ is about $\exp(n S(\mu))$. Then we saw that Voiculescu's free entropy looks like the logarithm of the volume of real symmetric (or complex selfadjoint) matrices approximating the given measure in the sense of moments. This is the content of the formula (\ref{ch05:eqn5.2.1}), and it was proved in a slightly different form as Theorem~\ref{ch05:the5.2.2}. Since moments of a selfadjoint matrix depend only on the eigenvalues, we passed from the space of matrices to the space of eigenvalues and studied the limit of
\begin{equation}
\frac{1}{n^{2}}\log\bar{\nu}_{n}(\{\lambda\in \mathbb{R}^{n}\, :\, |m_{k}(\kappa_{n}(\lambda))-m_{k}(\mu)|\leq\varepsilon , \ k\leq r\}).
\label{ch05:eqn5.4.2}
\end{equation}
Beyond the similarity of (\ref{ch05:eqn5.4.1}) and (\ref{ch05:eqn5.4.2}), the essential difference is not the normalization but rather the reference measure. A product measure $\nu^{n}$ is taken in (\ref{ch05:eqn5.4.1}), while $\overline{\nu}_{n}$ has the more complicated density (\ref{ch05:eqn5.2.2}).

Our aim now is to show that Voiculescu's Theorem~\ref{ch05:the5.2.2} carries the essence of a large deviation result in which the free entropy (\ref{ch05:eqn5.2.7}) is the interesting part of the rate function.

Before stating the theorem let us explain more explicitly the general framework of large deviations related to random matrices. For each $n\in \mathbb{N}$ let an $n\times n$ random matrix $X(n)$ be given, and let $\nu_{n}$ be its distribution on $M_{n}(\mathbb{C})$. For instance, if $X(n)$ is real symmetric (or complex selfadjoint), then $\nu_{n}$ is supported on $M_{n}(\mathbb{R})^{sa} ($or $M_{n}(\mathbb{C})^{sa})$. Assume further that $\nu_{n}$ is invariant under orthogonal (or unitary) conjugation. Then $\nu_{n}$ is in fact determined by the probability measure $\overline{\nu}_{n}$ induced on $\mathbb{R}^{n}$, the space of eigenvalues. Define a Borel measurable mapping $\mathbf{K}_{n}$ from $M_{n}(\mathbb{R})^{sa} ($or $M_{n}(\mathbb{C})^{sa})$ into $\mathcal{M}(\mathbb{R})$ by
\begin{equation}
\mathbf{K}_{n}(A) :=\frac{1}{n}\sum_{i=1}^{n}\delta(\lambda_{i}(A)),
\label{ch05:eqn5.4.3}
\end{equation}
where $\lambda_{1}(A), \ldots, \lambda_{n}(A)$ are the eigenvalues of $A$ and $\mathcal{M}(\mathbb{R})$ is endowed with the weak topology. The random measure $R_{n}:=\mathbf{K}_{n}(X(n))$ is the empirical eigenvalue distribution\index{empirical!eigenvalue distribution} of $X(n)$. Let $P_{n}$ denote the distribution of the random measure $R_{n}$, which is a probability measure on $\mathcal{M}(\mathbb{R})$. We call $P_{n}$ as well as $R_{n}$ the \textit{empirical eigenvalue distribution} of $X(n)$. For every Borel set $\Gamma$ of $\mathcal{M}(\mathbb{R})$ we have
\begin{equation*}
P_{n}(\Gamma)= \mathbf{Prob} (R_{n}\in\Gamma)=\nu_{n}(\mathbf{K}_{n}^{-1}\Gamma)=\overline{\nu}_{n}(\{x\in \mathbb{R}^{n} : \kappa_{n}(x)\in\Gamma\}),
\end{equation*}
where $\kappa_{n}(x) :=\frac{1}{n}\sum_{i=1}^{n}\delta(x_{i})$.

In the previous section many interesting distributions are realized as the maximizer of a free entropy functional of the form $\Sigma(\mu)-\int Q(x)\,d\mu(x)$. As we discussed in Sec.~\ref{ch04:sec4.1}, some of them arise as the limit distribution $\mu_{0}$ of a certain random matrix model $X(n)$ . In this section we are concerned with the large deviation\index{large deviation!principle, weak} (in the scale $n^{-2}$) for $(R_{n})$ or $(P_{n})$ defined as above from $X(n)$. It is expected that a rate function $I$ is minus a constant multiple of the free entropy functional above (up to an additive constant) and $\mu_{0}$ is the unique minimizer of $I$ with $I(\mu_{0})=0$. If this is the case, the large deviation principle for $(P_{n})$ means that the eigenvalue distribution of $X(n)$ converges exponentially fast to the limit distribution $\mu_{0}$.

In the theory of large deviations, there is a standard method which is based on the concept of exponential tightness. When we are concerned with the large deviation in the scale $L_{n}$ for a sequence $(P_{n})$ of measures on a Polish space $\mathcal{X}$ ($=\mathcal{M}(\mathbb{R})$ for instance), the sequence $P_{n}$ is said to be \textit{exponentially tight}\index{exponentially tight} if for any $\varepsilon>0$ there exists a compact $K_{\varepsilon}\subset \mathcal{X}$ such that
\begin{equation*}
\limsup_{n\rightarrow\infty}\, L_{n}\log P_{n}(\mathcal{X}\, \backslash \, K_{\varepsilon})\leq-\frac{1}{\varepsilon}.
\end{equation*}
This condition becomes trivial if the space $\mathcal{X}$ itself is compact. Let $\mathcal{A}$ be a base for the topology of $\mathcal{X}$. If for every $x\in \mathcal{X}$ we have
\begin{align}
I(x) & \ = \ \sup_{G\in \mathcal{A} \, ,x\in G}\left[-\limsup_{{n\rightarrow\infty}}\, L_{n}\log P_{n}(G)\right] \notag \\
& \ = \ \sup_{G\in \mathcal{A}, \, x\in G}\left[-\liminf_{{n\rightarrow\infty}}L_{n}\log P_{n}(G)\right],
\label{ch05:eqn5.4.4}
\end{align}
then $(P_{n})$ satisfies the \textit{weak large deviation principle} with a rate function $I$. A standard fact ([\citen{bib55}], Lemma 1.2.18, Sec. 4.1.2) is that when $(P_{n})$ is exponentially tight and (\ref{ch05:eqn5.4.4}) holds, $(P_{n})$ satisfies the large deviation principle and the rate function $I$ is automatically good.

Here we note another important fact. Let $(R_{n})$ be a sequence of random probability measures on a Polish space $X$, and let $P_{n}$ be the distribution on $\mathcal{M}(X)$ of $R_{n}$. If $(P_{n})$ satisfies the large deviation principle in the scale $n^{-2}$ with a good rate function $I$ having the unique minimizer $\mu_{0}$, then $R_{n}$ converges to $\mu_{0}$ in the weak topology almost surely. The proof is easy, using the L\'{e}vy metric. Indeed, let $\rho$ be the L\'{e}vy metric for the weak topology on $\mathcal{M}(X)$ (see (\ref{ch04:eqn4.3.9})). For every $\varepsilon>0$, if we take a closed set $F:=\{\mu\in\mathcal{M} (X):\rho(\mu, \mu_{0})\geq\varepsilon\}$, then
\begin{equation*}
\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(F)\leq-\inf\{I(\mu):\mu\in F\}<0,
\end{equation*}
because $I$ is good and $\mu_{0}$ is the unique minimizer. So we get $\sum_{n=1}^{\infty}P_{n}(F)<+\infty$. Since $P_{n}(F)= \mathbf{Prob} (\{\omega : \rho(R_{n}(\omega), \mu_{0})\geq\varepsilon\})$, the Borel-Cantelli lemma yields
\begin{equation*}
\mathbf{Prob} \left(\limsup_{n}\{\omega : \rho(R_{n}(\omega), \mu_{0})\geq\varepsilon\}\right)=0,
\end{equation*}
and this gives us the desired conclusion.

Next we state the large deviation result corresponding to Theorem~\ref{ch05:the5.2.2}. This result is due to Ben Arous and Guionnet.

\begin{theorem}
\label{ch05:the5.4.1}
Assume that the joint distribution of the random vector $\lambda^{(n)}\in \mathbb{R}^{n}$ is the measure $\bar{\nu}_{n}$ given by the density \emph{(\ref{ch05:eqn5.2.2})}. Then the sequence of random atomic measures
\begin{equation*}
R_{n}:=\frac{\delta(\lambda_{1}^{(n)})+\delta(\lambda_{2}^{(n)})+\cdots+\delta(\lambda_{n}^{(n)})}{n}
\end{equation*}
satisfies the large deviation principle in the scale $n^{-2}$ with the good rate function
\begin{equation*}
I(\mu):=-\beta\Sigma(\mu)+\frac{1}{4\sigma^{2}}\int x^{2}\,d\mu(x)+\frac{\beta}{2}\log(2\beta\sigma^{2})-\frac{3\beta}{4}
\end{equation*}
for $\mu\in \mathcal{M}(\mathbb{R})$. Furthermore, the semicircular distribution $w_{2\sqrt{2\beta\sigma^{2}}}$ is the unique minimizer of I with $I(w_{2\sqrt{2\beta\sigma^{2}}})=0$.
\end{theorem}

First, note that the assertion about the minimizer immediately follows from Proposition~\ref{ch05:pro5.3.4}. The following lemma is about the rate function, though it will be shown in a more general setting in the course of proving Theorem~\ref{ch05:the5.4.3}.

\begin{lemma}
\label{ch05:lem5.4.2}
The functional $I$ given in the theorem is convex and lower semi-continuous in the weak topology on $\mathcal{M}(\mathbb{R})$. Moreover, $I(\mu)\geq 0$ for all $\mu\in \mathcal{M}(\mathbb{R})$, and $\{\mu\in \mathcal{M}(\mathbb{R})\, :\, I(\mu)\leq c\}$ is compact for every $c\geq 0$. Therefore, $I$ satisfies all requirements as a good rate function.
\end{lemma}

\begin{proof2}
Note that $I$ is well-defined on $\mathcal{M}(\mathbb{R})$, and Proposition~\ref{ch05:pro5.3.4} says that it takes the minimum $0$ at $w_{2\sqrt{2\beta\sigma^{2}}}$. We may omit the constant term from $I$ and consider the kernel
\begin{align*}
F(x, y) & \ := \ \frac{1}{8\sigma^{2}}(x^{2}+y^{2})-\beta\log|x-y| \\
& \ \ = \ \frac{1}{8\sigma^{2}}((x^{2}+y^{2})-4\beta\sigma^{2}\log(x^{2}+y^{2}))+\frac{\beta}{2}\log\frac{x^{2}+y^{2}}{(x-y)^{2}}.
\end{align*}
This is bounded below because the first term is and $(x^{2}+y^{2})/(x-y)^{2}\geq 1/2$ for the second term. Hence the functional
\begin{equation*}
\iint F(x, y)\, d\mu(x)\, d\mu(y)=-\beta\Sigma(\mu)+\frac{1}{4\sigma^{2}}\int x^{2}\,d\mu(x)
\end{equation*}
is lower semicontinuous. The convexity of this functional can be seen immediately by Proposition~\ref{ch05:pro5.3.2}. Choose $C>0$ such that $F(x, y)+C\geq 0$ for all $x, y$. For any $\alpha>0$, since $ F(x, y)\rightarrow+\infty$ as $x^{2}+y^{2}\rightarrow+\infty$, let $R$ be large enough so that $ F(x, y)\geq\alpha$ if $x^{2}+y^{2}\geq 2R^{2}$. Then for every $\mu\in\mathcal{M}(\mathbb{R})$ we have
\begin{align*}
\mu([-R, R]^{c})^{2} & \ = \ (\mu\otimes\mu)(\{(x, y) \, : \, |x|>R, \, |y|>R\}) \\
& \ \leq \ (\mu\otimes\mu)(\{(x, y):x^{2}+y^{2}\geq 2R^{2}\}) \\
& \ \leq\ \frac{1}{\alpha}\iint(F(x, y)+C)\,d\mu(x)\,d\mu(y).
\end{align*}
This implies that for any $c\geq 0$ the set $\{\mu\in\mathcal{M}(\mathbb{R}) : \iint F(x, y)\, d\mu(x)\, d\mu(y)\leq c\}$ is tight and hence compact.
\end{proof2}

From now on we shall treat a more general probability measure than (\ref{ch05:eqn5.2.2}). Let $Q(x)$ be a real continuous function on $\mathbb{R}$ such that for any $\varepsilon>0$
\begin{equation}
\lim_{|x|\rightarrow\infty} |x|\exp(-\varepsilon Q(x))=0.
\label{ch05:eqn5.4.5}
\end{equation}
Assume that $\bar{\nu}_{n}$ on $\mathbb{R}^{n}$ has the joint probability density
\begin{equation*}
\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{n}Q(t_{i})\right)\prod_{i<j}|t_{i}-t_{j}|^{2\beta},
\end{equation*}
where $\beta>0$ is fixed (independent of $n$) and $Z_{n}$ is the normalization constant, i.e.
\begin{equation*}
Z_{n}=\int\cdots\int\exp\left(-n\sum_{i=1}^{n}Q(t_{i})\right)\prod_{i<j}|t_{i}-t_{j}|^{2\beta}dt_{1}\cdots dt_{n}.
\end{equation*}
(The assumption (\ref{ch05:eqn5.4.5}) implies that this integral is finite.)

The large deviation theorem we are going to prove tells about the random measure $R_{n}:=\frac{1}{n}\sum_{i=1}^{n}\delta(\lambda_{i})$ when $\lambda\in \mathbb{R}^{n}$ is distributed according to $\bar{\nu}_{n}$, or about the probability measure
\begin{equation*}
P_{n}(\Gamma):=\bar{\nu}_{n}(\{x\in \mathbb{R}^{n}:\kappa_{n}(x)\in\Gamma\})
\end{equation*}
for Borel sets $\Gamma \subset \mathcal{M}(\mathbb{R})$.

\begin{theorem}
\label{ch05:the5.4.3}
The finite limit $B:=\lim_{n\rightarrow\infty}n^{-2}\log Z_{n}$ exists, and $(P_{n})$ satisfies the large deviation principle in the scale $n^{-2}$ with the good rate function
\begin{equation*}
I(\mu):=-\beta\Sigma(\mu)+\int Q(x)\,d\mu(x)+B
\end{equation*}
for $\mu\in \mathcal{M}(\mathbb{R})$. Furthermore, there exists a unique $\mu_{0}\in\mathcal{M}(\mathbb{R})$ such that $I(\mu_{0})=0$.
\end{theorem}

To prove the theorem, fix a kernel
\begin{equation*}
F(x, y):=-\beta\log|x-y|+\frac{1}{2}(Q(x)+Q(y))
\end{equation*}
and its cutoff
\begin{equation*}
F_{\alpha}(x, y) :=\min\{\phi(x, y), \alpha\} \quad \mathrm{for} \quad \alpha>0.
\end{equation*}
Since
\begin{equation*}
F(x, y)\geq-\beta[\log(|x|\exp(-Q(x)/2\beta))+\log(|y|\exp(-Q(y)/2\beta))]
\end{equation*}
whenever $|x|,  |y|\geq 2$, it follows that $F_{\alpha}(x, y)$ is bounded and continuous. Therefore,
\begin{equation*}
\mu\in \mathcal{M}(\mathbb{R})\mapsto\iint F_{\alpha}(x, y)\, d\mu(x)\, d\mu(y)
\end{equation*}
is continuous and
\begin{equation*}
-\beta\Sigma(\mu)+\int Q(x)\,d\mu(x)=\iint F(x, y)\,d\mu(x)\,d\mu(y)
\end{equation*}
is lower semicontinuous in the weak topology on $\mathcal{M}(\mathbb{R})$.

\begin{lemma}
\label{ch05:lem5.4.4}
\begin{equation*}
\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}\leq-\inf_{\mu\in \mathcal{M}(\mathbb{R})}\iint F(x, y)\,d\mu(x)\,d\mu(y).
\end{equation*}
\end{lemma}
\begin{proof2}
We estimate as follows:
\begin{align*}
Z_{n} & \ = \ \int\cdots\int\exp \left(-\sum_{i=1}^{n}Q(t_{i})\right) \\
& \qquad \qquad \qquad \times\exp\left(-\sum_{i<j}(Q(t_{i})+Q(t_{j}))\right)\prod_{i<j}|t_{i}-t_{j}|^{2\beta}dt_{1}\cdots dt_{n} \\
& \ = \ \int\cdots\int\exp\left(-\sum_{i=1}^{n}Q(t_{i})\right)\exp\left(-2\sum_{i<j}F(t_{i}, t_{j})\right)\,dt_{1}\cdots dt_{n} \\
& \ \leq \ \int\cdots\int\exp\left(-\sum_{i=1}^{n}Q(t_{i})\right) \\
& \qquad \qquad \qquad \ \times\exp\left(-n^{2}\iint_{\{x\neq y\}}F(x, y)\,d\mu_{t}(x)\,d\mu_{t}(y)\right)\,dt_{1}\cdots dt_{n} \\
& \ \leq \ \exp\left(-n^{2}\inf_{\mu}\iint_{\{x\neq y\}}F(x, y)\,d\mu(x)\,d\mu(y)\right) \\
& \qquad \qquad \qquad \times\int\cdots\int\exp\left(-\sum_{i=1}^{n}Q(t_{i})\right)\,dt_{l}\cdots \, dt_{n} \\
& \ = \ \left(\int e^{-Q(x)}\, dx\right)^{n}\exp\left(-n^{2}\inf_{\mu}\iint F(x, y)\,d\mu(x)\,d\mu(y)\right),
\end{align*}
implying the lemma.
\end{proof2}

\begin{lemma}
\label{ch05:lem5.4.5} For every $\mu\in\mathcal{M}(\mathbb{R})$,
\begin{equation*}
\inf_{G}\left[\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\right]\leq-\iint F(x, y)\, d\mu(x)\, d\mu(y)-\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n},
\end{equation*}
where $G$ runs over a neighborhood base of $\mu$.
\end{lemma}

\begin{proof2}
For any neighborhood $G$ of $\mu\in\mathcal{M}(\mathbb{R})$, put
\begin{equation*}
\tilde{G}:=\{t\in \mathbb{R}^{n}:\kappa_{n}(t)\in G\}.
\end{equation*}
As in the proof of Lemma~\ref{ch05:lem5.4.4}, we get
\begin{align*}
& P_{n}(G) =\bar{\nu}_{n}(\tilde{G}) \\
& =\frac{1}{Z_{n}}\int\cdots\int_{\tilde{G}}\exp\left(-\sum_{i=1}^{n}Q(t_{i})\right)\exp\left(-2\sum_{i<j}F(t_{i}, t_{j})\right)\,dt_{1}\cdots \, dt_{n} \\
& \ \leq\frac{1}{Z_{n}}\int\cdots\int_{\tilde{G}}\exp\left(-\sum_{i=1}^{n}Q(t_{i})\right) \\
& \qquad \qquad \qquad \ \times\exp\left(-n^{2}\iint F_{\alpha}(x, y)\,d\mu_{t}(x)\,d\mu_{t}(y)+n\alpha\right)\,dt_{1}\cdots \,dt_{n} \\
& =\frac{1}{Z_{n}}\left(\int e^{-Q(x)}\,dx\right)^{n}\exp\left(-n^{2}\inf_{\mu'\in G}\iint F_{\alpha}(x, y)d\mu'(x)\,d\mu'(y)+n\alpha\right).
\end{align*}
Therefore,
\begin{equation*}
\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\leq-\inf_{\mu'\in G}\iint  F_{\alpha}(x, y)\,d\mu'(x)\,d\mu'(y)-\liminf_{n\rightarrow \infty}\frac{1}{n^{2}}\log Z_{n}.
\end{equation*}
Thanks to the weak continuity of $\mu'\mapsto\iint F_{\alpha}(x, y)\,d\mu'(x)\,d\mu'(y)$ we get
\begin{equation*}
\inf_{G}\left[\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\right]\leq-\iint F_{\alpha}(x, y)\,d\mu(x)\,d\mu(y)-\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}.
\end{equation*}
Letting $\alpha\rightarrow+\infty$ completes the proof of the inequality.
\end{proof2}

\begin{lemma}
\label{ch05:lem5.4.6}
For every $\mu\in \mathcal{M}(\mathbb{R})$,
\begin{equation*}
\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}\geq-\iint F(x, y)\,d\mu(x)\,d\mu(y)
\end{equation*}
and
\begin{equation*}
\inf_{G}\left[\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\right]\geq-\iint F(x, y)\,d\mu(x)\, d\mu(y)-\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n},
\end{equation*}
where $G$ runs over a neighborhood base of $\mu$.
\end{lemma}

\begin{proof2}
It is obvious that
\begin{equation*}
\mu\in \mathcal{M}(\mathbb{R})\mapsto\inf\left\{\liminf_{n\rightarrow\infty}\,\frac{1}{n^{2}}\log P_{n}(G) : G \ \mathrm{a \ neighborhood \ of} \ \mu\right\}
\end{equation*}
is upper semicontinuous. Since $F(x, y)$ is bounded below, we get
\begin{equation*}
\iint F(x, y)\,d\mu(x)\,d\mu(y)=\lim_{k\rightarrow\infty}\iint F(x, y)\,d\mu_{k}(x)\,d\mu_{k}(y)
\end{equation*}
with $\mu_{k} :=\mu([-k, k])^{-1}\chi_{[-k,k]}\mu$. So it suffices to assume that $\mu$ has a compact support. For $\varepsilon >0$ let $\phi_{\varepsilon}$ be a nonnegative $C^{\infty}$-function supported in $[-\varepsilon, \varepsilon]$ such that $\int\phi_{\varepsilon}(x)\,dx=1$, and let $\phi_{\varepsilon}*\mu$ be the convolution of $\mu$ with $\phi_{\varepsilon}$. Thanks to the properties of $\Sigma(\mu)$ given in Proposition~\ref{ch05:pro5.3.2}, it is easy to see that
\begin{equation*}
\Sigma(\phi_{\varepsilon}*\mu)\geq\Sigma(\mu).
\end{equation*}
Also
\begin{equation*}
\lim_{\varepsilon\rightarrow+0}\int Q(x)\,d(\phi_{\varepsilon}*\mu)(x)=\int Q(x)\,d\mu(x)\,.
\end{equation*}
Hence we may assume that $\mu$ has a continuous density with compact support. Moreover, let $m$ be the uniform distribution on an interval $[a, b]$ including $\mathrm{supp}\, \mu$. Then it suffices to show the required inequalities for each $(1-\delta)\mu+\delta m(0<\delta<1)$. After all, we may assume that $\mu$ has a continuous density $f>0$ on $\mathrm{supp}\,\mu=[a, b]$ so that $\delta\leq f(x)\leq\delta^{-1}\, (a\leq x\leq b)$ for some $\delta>0$.

For each $n\in \mathbb{N}$ let $a<a_{1}^{(n)}<b_{1}^{(n)}<a_{2}^{(n)}<\ldots<a_{n}^{(n)}<b_{n}^{(n)}=b$ be such that
\begin{equation*}
\int_{a}^{a_{i}^{(n)}}f(x)\,dx=\frac{i-\frac{1}{2}}{n}, \quad \int_{a}^{b_{i}^{(n)}}f(x)\,dx=\frac{i}{n} \qquad (1 \leq i\leq n).
\end{equation*}
Then it immediately follows that
\begin{equation*}
\frac{\delta}{2n}\leq b_{i}^{(n)}-a_{i}^{(n)}\leq\frac{1}{2n\delta} \qquad (1\ \leq i\leq n).
\end{equation*}
Define
\begin{equation*}
\Delta_{n}:=\{(t_{1}, \ldots, t_{n})\in \mathbb{R}^{n}:a_{i}^{(n)}\leq t_{i}\leq b_{i}^{(n)}, \ 1\leq i\leq n\}.
\end{equation*}
For any neighborhood $G$ of $\mu$, it is clear that
\begin{equation*}
\Delta_{n}\subset\tilde{G}:=\{t\in \mathbb{R}^{n}:\kappa_{n}(t)\in G\}
\end{equation*}
for all $n$ large enough. Therefore, for large $n$ we have
\begin{align*}
& P_{n}(G)=\bar{\nu}_{n}(\tilde{G})\geq\bar{\nu}_{n}(\Delta_{n}) \\
& \qquad =\frac{1}{Z_{n}}\int\cdots\int_{\Delta_{n}}\exp\left(-n\sum_{i=1}^{n}Q(t_{i})\right)\prod_{i<j}|t_{i}-t_{j}|^{2\beta}\,dt_{1}\cdots dt_{n} \\
& \qquad \geq\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{n}\xi_{i}^{(n)}\right)\prod_{i<j}(a_{j}^{(n)}-b_{i}^{(n)})^{2\beta}\int\cdots\int_{\Delta_{n}}dt_{1}\cdots dt_{n} \\
& \qquad \geq\frac{1}{Z_{n}}\left(\frac{\delta}{2n}\right)^{n}\exp\left(-n\sum_{i=1}^{n}\xi_{i}^{(n)}\right)\prod_{i<j}(a_{j}^{(n)}-b_{i}^{(n)})^{2\beta},
\end{align*}
where $\xi_{i}^{(n)} :=\max\{Q(x) : a_{i}^{(n)}\leq x\leq b_{i}^{(n)}\}$. Now let $g:[0,1]\rightarrow[a, b]$ be the inverse function of $t\mapsto\int_{a}^{t}f(x)\,dx$. Since $a_{i}^{(n)}=g((i-\frac{1}{2})/n)$ and $b_{i}^{(n)}=g(i/n)$ , we get
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{n}\xi_{i}^{(n)}=\int_{0}^{1}Q(g(t))\,dt=\int_{a}^{b}Q(x)f(x)\,dt=\int Q(x)\,d\mu(x)
\end{equation*}
and
\begin{align*}
& \lim_{n\rightarrow\infty}\frac{2}{n^{2}}\sum_{i<j}\log(a_{j}^{(n)}-b_{i}^{(n)})=2\iint_{0\leq s<t\leq 1}\log(g(t)-g(s))\,ds\,dt \\
& \qquad =\int_{0}^{1}\int_{0}^{1}\log|g(s)-g(t)|\,ds\,dt=\iint f(x)f(y)\log|x-y|\, dx\, dy=\Sigma(\mu).
\end{align*}
Therefore,
\begin{equation*}
0\geq\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\geq-\iint\ F(x, y)\,d\mu(x)\,d\mu(y)-\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}
\end{equation*}
and
\begin{equation*}
\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\geq-\iint F(x, y)\,d\mu(x)\,d\mu(y)-\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n},
\end{equation*}
as desired.
\end{proof2}

\begin{lemma}
\label{ch05:lem5.4.7}
The finite limit $B=\lim_{n\rightarrow\infty}n^{-2}\log Z_{n}$ exists.
\end{lemma}

\begin{proof2}
By Lemmas~\ref{ch05:lem5.4.4} and \ref{ch05:lem5.4.6} we have
\begin{equation*}
\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}\leq-\inf_{\mu}\iint F(x, y)\,d\mu(x)\,d\mu(y)\leq \liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}.
\end{equation*}
This gives the result, because Theorem~\ref{ch05:the5.3.3} says that $\mu\mapsto\iint F(x, y)\,d\mu(x)\,d\mu(y)$ attains the minimum.
\end{proof2}

\begin{lemma}
\label{ch05:lem5.4.8} $(P_{n})$ is exponentially tight.
\end{lemma}

\begin{proof2}
For any $\alpha>0$ set
\begin{equation*}
K_{\alpha}:=\left\{\mu\in \mathcal{M}(\mathbb{R}):\int Q(x)\, d\mu(x)\leq\alpha\right\}.
\end{equation*}
Since $Q(x)\rightarrow+\infty$ as $|x|\rightarrow+\infty$ by the assumption (\ref{ch05:eqn5.4.5}), it is easy to see that
\begin{equation*}
\sup_{\mu\in K_{\alpha}}\mu([-R,\ R]^{c})\rightarrow 0 \quad \mathrm{as} \quad R\rightarrow+\infty,
\end{equation*}
and hence $K_{\alpha}$ is compact in the weak topology. We get
\begin{align*}
& P_{n}(K_{\alpha}^{c})=\bar{\nu}_n\left(\left\{t\in \mathbb{R}^{n}:\frac{1}{n}\sum_{i=1}^{n}Q(t_{i})>\alpha\right\}\right) \\
& =\frac{1}{Z_{n}}\int\cdots\int_{\{\frac{1}{n}\Sigma_{i=1}^{n}Q(t_{i})>\alpha\}}\exp\left(-n\sum_{i=1}^{n}Q(t_{i})\right)\prod_{i<j}|t_{i}-t_{j}|^{2\beta}\,dt_{1}\cdots dt_{n} \\
& \leq\frac{1}{Z_{n}}\exp\left(-\frac{n^{2}\alpha}{2}\right)\int\cdots\int\exp\left(-\frac{n}{2}\sum_{i=1}^{n}Q(t_{i})\right)\prod_{i<j}|t_{i}-t_{j}|^{2\beta}\,dt_{1}\cdots dt_{n}.
\end{align*}
When $Q(x)$ is replaced by $Q(x)/2$, the finite limit
\begin{equation*}
B_{2}=\lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log\int\cdots\int\exp\left(-\frac{n}{2}\sum_{i=1}^{n}Q(t_{i})\right)\prod_{i<j}|t_{i}-t_{j}|^{2\beta}dt_{l}\cdots dt_{n}
\end{equation*}
exists, and so does $B$ by Lemma~\ref{ch05:lem5.4.7}. Hence the above estimate gives
\begin{equation*}
\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(K_{\alpha}^{c})\leq-B+B_{2}-\frac{\alpha}{2}.
\end{equation*}
Since $\alpha>0$ is arbitrary, we have the conclusion.
\end{proof2}

Now we are in a position to complete the proof of Theorem~\ref{ch05:the5.4.3} and its particular case, Theorem~\ref{ch05:the5.4.1}.

\begin{proof}[\textit{End of proof of Theorem}~\ref{ch05:the5.4.3}:] The proof is in the previous lammas: The weak large deviation principle holds, and the exponential tightness of $(P_{n})$ gives the large deviation principle with the good rate function $I(\mu)=\iint F(x, y)\,d\mu(x)\,d\mu(y)+B$. The existence of the unique minimizer is due to Theorem~\ref{ch05:the5.3.3}.
\end{proof}

Assume that the minimizer $\mu_{0}$ of $I$ has support $[a, b]$ and density $\psi$. Differentiating the equality condition in Theorem~\ref{ch05:the5.3.3} leads to the following singular integral equation:
\begin{equation}
\int_{a}^{b}\frac{\psi(y)}{x-y}\,dy=\frac{Q'(x)}{2\beta}  \quad \mathrm{for} \quad a<x<b,
\label{ch05:eqn5.4.6}
\end{equation}
where the integral is taken in the sense of principal value. This integral equation can be justified when $Q(x)$ satisfies a certain regularity condition (cf. [\citen{bib165}], Sec. IV.3). For instance, it is known ([\citen{bib145}], based on [\citen{bib124}], Chap. 11) that if $Q(x)$ is a convex polynomial of even degree, then the solution of (\ref{ch05:eqn5.4.6}) is
\begin{align}
\psi(x) & \ = \ \frac{\sqrt{(x-a)(b-x)}}{2\beta\pi^{2}}\int_{a}^{b}\frac{Q'(y)}{(y-x)\sqrt{(y-a)(b-y)}}\,dy \notag\\
& \ = \ \frac{\sqrt{(x-a)(b-x)}}{2\beta\pi^{2}}\int_{a}^{b}\frac{Q'(x)-Q'(y)}{x-y}\frac{dy}{\sqrt{(y-a)(b-y)}},
\label{ch05:eqn5.4.7}
\end{align}
where $a, b$ are determined by the equations
\begin{equation*}
\int_{a}^{b}\frac{Q'(x)}{\sqrt{(x-a)(b-x)}}\,dx=0, \quad \int_{a}^{b}\frac{xQ'(x)}{\sqrt{(x-a)(b-x)}}\,dx=2\beta\pi.
\end{equation*}
For the second equality of (\ref{ch05:eqn5.4.7}), note that one has
\begin{equation*}
\frac{1}{\pi}\int_{a}^{b}\frac{\log|x-y|}{\sqrt{(y-a)(b-y)}}\,dy=\log\frac{b-a}{4} \qquad (a\leq x\leq b),
\end{equation*}
and hence
\begin{equation*}
\frac{1}{\pi}\int_{a}^{b}\frac{dy}{(x-y)\sqrt{(y-a)(b-y)}}=0 \qquad (a<x<b).
\end{equation*}
Also, note that if the degree of $Q(x)$ is $2p$, then $\psi(x)$ is $\sqrt{(x-a)(b-x)}$ times a polynomial of degree $2p-2$, and the convexity of $Q(x)$ is sufficient for the positivity of $\psi(x)$ on $(a, b)$.

Next, for each $n\in \mathbb{N}$ we take a probability measure $\bar{\nu}_{n}$ on $\mathbb{C}^{n}$ having the density
\begin{equation*}
\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{n}Q(\zeta_{i})\right)\prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2\beta},
\end{equation*}
where $\beta>0$ is fixed and $Q(\zeta)$ is a real continuous function on $\mathbb{C}$ such that
\begin{equation*}
\lim_{|\zeta|\rightarrow\infty}|\zeta|\exp(-\varepsilon Q(\zeta))=0
\end{equation*}
for any $\varepsilon>0$. Then the large deviation principle is obtained for the probability measure
\begin{equation*}
P_{n}(\Gamma):=\bar{\nu}_{n}(\{\zeta\in \mathbb{C}^{n}:\kappa_{n}(\zeta)\in\Gamma\})
\end{equation*}
for Borel sets $\Gamma\subset\mathcal{M}(\mathbb{C})$, where $\kappa_{n}(\zeta) :=\frac{1}{n}\sum_{i=1}^{n}\delta(\zeta_{i})$ for $\zeta=(\zeta_{1}, \ldots, \zeta_{n})\in \mathbb{C}^{n}$.

\begin{theorem}
\label{ch05:the5.4.9}
In the above setting, the finite limit $B:=\lim_{n\rightarrow\infty}n^{-2}\log Z_{n}$ exists and $(P_{n})$ satisfies the large deviation principle in the scale $n^{-2}$ with the good rate function
\begin{equation*}
I(\mu):=-\beta\Sigma(\mu)+\int Q(\zeta)\, d\mu(\zeta)+B \quad on \quad \mathcal{M}(\mathbb{C}).
\end{equation*}
Furthermore, there exists a unique $\mu_{0}\in \mathcal{M}(\mathbb{C})$ such that $I(\mu_{0})=0$.
\end{theorem}

\begin{proof2}
Since the theorem can be proved on the same lines as Theorem~\ref{ch05:the5.4.3}, we only give a sketch. Take a kernel
\begin{equation*}
F(\zeta, \eta):=-\beta\log|\zeta-\eta|+\frac{1}{2}(Q(\zeta)+Q(\eta)) \qquad (\zeta, \eta\in \mathbb{C}).
\end{equation*}
Then $F(\zeta, \eta)$ is bounded below, and
\begin{equation*}
-\beta\Sigma(\mu)+\int Q(\zeta)\,d\mu(\zeta)=\iint F(\zeta, \eta)\,d\mu(\zeta)\,d\mu(\eta)
\end{equation*}
is lower semicontinuous in the weak topology on $\mathcal{M}(\mathbb{C})$. One can obtain three lemmas similar to Lemmas~\ref{ch05:lem5.4.4}--\ref{ch05:lem5.4.6}. The proofs of the first two are the same. The third lemma is proved with a modification as follows.

A suitable regularization process can be performed as in the proof of Lemma~\ref{ch05:lem5.4.6}, so we may assume that $\mathrm{supp}\, \mu=[a, b]\times[c, d]$ and $\mu$ has a continuous density $f$ on $[a, b]\times[c, d]$ satisfying $\delta\leq f\leq\delta^{-1}$ for some $\delta>0$. For each $n\in\mathbb{N}$ let $m:=[\sqrt{n}]$. Let $a=x_{0}<x_{1}<\ldots<x_{m}=b$ be such that
\begin{equation*}
\mu([x_{i-1}, x_{i}]\times[c, d])=\frac{1}{m} \qquad (1 \leq i\leq m).
\end{equation*}
Noting that $m^{2}\leq n\leq m(m+2)$, we can choose $c=y_{i,0}<y_{i,1}<\ldots<y_{i,l_{i}}=d$ for $1\leq i\leq m$ such that $m\leq l_{i}\leq m+2, \, \sum_{i=1}^{m}l_{i}=n$ and
\begin{equation*}
\mu([x_{i-1}, x_{i}]\times[y_{i,j-1}, y_{i,j}])=\frac{1}{ml_{i}} \qquad (1\leq i\leq m, \, 1\leq j\leq l_{i}).
\end{equation*}
Arrange $n$ pieces of rectangles $[x_{i-1}, x_{i}]\times[y_{i,j-1}, y_{i,j}]$ as
\begin{equation*}
R_{i}^{(n)}=[a_{i}^{(n)}, b_{i}^{(n)}]\times[c_{i}^{(n)}, d_{i}^{(n)}] \qquad (1\leq i\leq n),
\end{equation*}
and in each rectangle $R_{i}^{(n)}$ take a small one $S_{i}^{(n)}$ by dividing $R_{i}^{(n)}$ into 9 congruent rectangles and selecting the one in the middle. Then we get
\begin{align}
& \lim_{n\rightarrow\infty} \left(\max_{1 \leq i\leq n} \, \mathrm{diam} (R_{i}^{(n)})\right)\rightarrow 0\, ,
\label{ch05:eqn5.4.8} \\
& \int_{S_{i}^{(n)}}d\zeta\geq\frac{\delta}{9}\int_{R_{i}^{(n)}}f(\zeta)\,d\zeta\geq\frac{\delta}{9m(m+2)}\geq\frac{\delta}{27n} \qquad (1\leq i\leq n). \notag
\end{align}
Define
\begin{equation*}
\Delta_{n}:=\{(\zeta_{1}, \ldots, \zeta_{n})\in \mathbb{C}^{n} : \zeta_{i}\in S_{i}^{(n)}, \ 1\leq i\leq n\}.
\end{equation*}
For any neighborhood $G$ of $\mu$, if $n$ is large enough, then we have
\begin{equation*}
\Delta_{n}\subset\{\zeta\in \mathbb{C}^{n}: \kappa_{n}(\zeta)\in G\},
\end{equation*}
and so
\begin{align*}
& P_{n}(G)\geq\bar{\nu}_{n}(\Delta_{n}) \\
& \quad \geq\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{n} \max_{\zeta \in S_{i}^{(n)}}Q(\zeta_{i})\right) \\
& \qquad \qquad \qquad \ \times\prod_{i<j}\left(\min_{\zeta\in S_{i}^{(n)},\, \eta\in S_{j}^{(n)}}|\zeta-\eta|\right)^{2\beta}\int\cdots\int_{\Delta_{n}}d\zeta_{1}\cdots d\zeta_{n}\\
& \quad \geq\frac{1}{Z_{n}}\left(\frac{\delta}{27n}\right)^{n}\exp\left(-n\sum_{i=1}^{n}\max_{\zeta \in S_{i}^{(n)}}Q(\zeta_{i})\right)\prod_{i<j}\left(\min_{\zeta\in S_{i}^{(n)}\, ,\eta\in S_{j}^{(n)}}|\zeta-\eta|\right)^{2\beta}.
\end{align*}
Therefore, to obtain the required inequalities, it suffices to show that
\begin{equation}
\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{n}\max_{\zeta\in S_{i}^{(n)}}Q(\zeta_{i})=\int Q(\zeta)\,d\mu(\zeta)
\label{ch05:eqn5.4.9}
\end{equation}
and
\begin{equation}
\liminf_{n\rightarrow\infty}\frac{2}{n^{2}}\sum_{i<j}\log\left(\min_{\zeta\in S_{i}^{(n)},\eta\in S_{j}^{(n)}}|\zeta-\eta|\right)\geq\Sigma(\mu). \label{ch05:eqn5.4.10}
\end{equation}
But (\ref{ch05:eqn5.4.9}) is obvious from (\ref{ch05:eqn5.4.8}). We get
\begin{equation*}
\max_{\zeta\in R_{i}^{(n)},\eta\in R_{j}^{(n)}} |\zeta-\eta|\leq \mathrm{const} \cdot  \quad \min_{\zeta\in S_{i}^{(n)},\eta\in S_{j}^{(n)}}|\zeta-\eta|,
\end{equation*}
and for any $\varepsilon >0$
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{2}{n^{2}}\neq\left\{(i,j):i<j, \frac{\max_{\zeta\in R_{i}^{(n)},\eta\in R_{j}^{(n)}}|\zeta-\eta|}{\min_{\zeta\in S_{i}^{(n)},\eta\in S_{j}^{(n)}}|\zeta-\eta|}\leq 1+\varepsilon\right\}=1.
\end{equation*}
Since
\begin{equation*}
\Sigma(\mu)\leq 2\sum_{i<j}\log\left(\max_{\zeta \in R_{i}^{(n)}, \eta\in R_{j}^{(n)}}|\zeta-\eta| \right) \int_{R_{j}^{(n)}}f(\zeta)\,d\zeta, \int_{R_{j}^{(n)}} f(\eta)\, d\eta,
\end{equation*}
we have
\begin{align*}
& \Sigma(\mu)-\liminf_{n\rightarrow\infty} \frac{2}{n^{2}}\sum_{i<j}\log\left(\min_{\zeta\in S_{i}^{(n)},\eta\in S_{j}^{(n)}}|\zeta-\eta|\right) \\
& \qquad \leq\limsup_{n\rightarrow\infty}\frac{2}{n^{2}}\sum_{i<j}\log\left(\frac{\max_{\zeta\in R_{i}^{(n)},\eta\in R_{j}^{(n)}}|\zeta-\eta|}{\min_{\zeta\in S_{i}^{(n)},\eta\in S_{j}^{(n)}}|\zeta-\eta|}\right)=0,
\end{align*}
which implies (\ref{ch05:eqn5.4.10}).

Finally, Lemmas~\ref{ch05:lem5.4.7} and \ref{ch05:lem5.4.8} are shown in the same way, and the proof is completed as before.
\end{proof2}

Theorem~\ref{ch05:the5.4.9} includes the large deviation principle for the empirical eigenvalue distribution of an elliptic Gaussian matrix\index{large deviation!elliptic Gaussian matrix} $Y(n)$ in (\ref{ch04:eqn4.1.23}) whose joint eigenvalue density was given in Lemma~\ref{ch04:lem4.1.10}. In particular, when $Y(n)=X(n)$ is standard Gaussian, the rate function is
\begin{equation*}
I(\mu):=-\Sigma(\mu)+\int|\zeta|^{2}\,d\mu(\zeta)-\frac{3}{4}
\end{equation*}
because $\lim_{n\rightarrow\infty}n^{-2}\log Z_{n}=-\frac{3}{4}$ from (\ref{ch04:eqn4.1.31}), and Proposition~\ref{ch05:pro5.3.6} says that the unique minimizer of $I$ is the uniform distribution on the unit disk. The projection of the uniform distribution on the unit disk to the real or imaginary axis is the semicircle law $w_{1}$. On the other hand, the limit distribution of $X(n)$ is the distribution of a circular element $c$, so that the distributions of $(c+c^{*})/2$ and $(c-c^{*})/2\mathrm{i}$ are $w_{\sqrt{2}}$. So the limit distribution through the eigenvalue distribution is the $1/\sqrt{2}$-compression of the ``real'' limit. This is not strange, because $X(n)$ is non-normal and the spectral radius should be smaller than the operator norm.

For an elliptic Gaussian matrix $Y(n)=uX(n)+vX(n)^{*}(u^{2}+v^{2}=1)$ the rate function is
\begin{equation*}
I(\mu):=-\Sigma(\mu)+\int\left(\frac{x^{2}}{1+\tau}+\frac{y^{2}}{1-\tau}\right)\,d\mu(\zeta)-\frac{3}{4} \qquad (\zeta=x+\mathrm{i}\,y),
\end{equation*}
and the limit distribution of the empirical eigenvalue density is the elliptic law $m_{1}^{(\tau)}$, i.e. the uniform measure on $E_{1}^{(\tau)}$ in (\ref{ch05:eqn5.3.16}), where $\tau=2uv$. Note that the constant term $-\frac{3}{4}$ of the rate function is determined from the requirement $I(m_{1}^{(\tau)})=0$ and Proposition~\ref{ch05:pro5.3.11}.

By virtue of the fact noted above Theorem~\ref{ch05:the5.4.1}, the large deviations\index{large deviation!for unitary matrices} in Theorems~\ref{ch05:the5.4.3} and \ref{ch05:the5.4.9} imply that the empirical eigenvalue density of the corresponding random matrix converges in the weak topology to the measure $\mu_{0}$ almost surely. But the almost sure convergence in moments (slightly better than that in the weak topology) for several random matrices is already known from Theorems~\ref{ch04:the4.1.5}, \ref{ch04:the4.1.7} and the asymptotic freeness results in Sec.~\ref{ch04:sec4.3}.

The large deviation theorem related to unitary random matrices is obtained similarly. Let $\gamma_{n}$ be the Haar probability measure on the unitary group $\mathcal{U}(n)$. Let $Q(\zeta)$ be a real continuous function on the unit circle $\mathbb{T}$ and for each $n\in \mathbb{N}$ take a probability measure $\nu_{n}$ on $\mathcal{U}(n)$ as
\begin{equation*}
\nu_{n} :=\frac{1}{Z_{n}}\exp(-n\mathrm{Tr}\, Q(U))\,d\gamma_{n}(U),
\end{equation*}
where $Z_{n}$ is for normalization. The density of the measure on $\mathbb{T}^{n}$ induced from $\gamma_{n}$ with respect to $d\zeta_{1}\cdots d\zeta_{n}$ (the Haar probability measure on $\mathbb{T}^{n}$) was given in Lemma~\ref{ch04:lem4.2.1}, so the above $\nu_{n}$ induces the measure $\bar{\nu}_{n}$ on $\mathbb{T}^{n}$ having the probability density
\begin{equation*}
\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{n}Q(\zeta_{i})\right)\prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2}.
\end{equation*}
\begin{theorem}
\label{ch05:the5.4.10}
The finite limit $B:=\lim_{n\rightarrow\infty}n^{-2}\log Z_{n}$ exists, and the empirical eigenvalue distribution $P_{n}$ on $\mathcal{M}(\mathbb{T})$ of the above $\nu_{n}$ satisfies the large deviation principle in the scale $n^{-2}$ with the good rate function
\begin{equation*}
I(\mu):=-\Sigma(\mu)+\int_{\mathbb{T}}Q(\zeta)\,d\mu(\zeta)+B \quad on \quad \mathcal{M}(\mathbb{T}).
\end{equation*}
Furthermore, there exists a unique $\mu_{0}\in \mathcal{M}(\mathbb{T})$ such that $I(\mu_{0})=0$.
\end{theorem}

This can be proved more or less similarly to Theorems~\ref{ch05:the5.4.3} and \ref{ch05:the5.4.9}, so we omit the details. Below we just mention a few points.
\begin{enumerate}
\item[(1)] Since $\mathbb{T}$ is compact, the weak topology on $\mathcal{M}(\mathbb{T})$ is the $\mathrm{w}^{*}$-topology, and hence the exponential tightness of $(P_{n})$ is automatic.

\item[(2)] We need a regularization procedure for $\mu\in \mathcal{M}(\mathbb{T})$ as was done in the proof of Lemma~\ref{ch05:lem5.4.6} for $\mu\in\mathcal{M}(\mathbb{R})$. The \textit{Poisson integral}\index{Poisson!integral} is available for this purpose. In fact, for any $\mu\in\mathcal{M}(\mathbb{T})$ and $0<r<1$ define
\end{enumerate}
\begin{equation*}
f_{r}(e^{\mathrm{i}\,\theta}) :=\frac{1}{2\pi}\int_{0}^{2\pi}P_{r}(\theta-t)\,d\mu(t) , \qquad \mu_{r}:=\frac{1}{2\pi}f_{r}(e^{\mathrm{i}\,\theta})\,d\theta,
\end{equation*}
where $P_{r}(\theta) :=(1-r^{2})/(1-2r\cos\theta+r^{2})$, the \textit{Poisson kernel}\index{Poisson!kernel} (used as the density in (\ref{ch05:eqn5.3.14})). Then it is well-known that $\mu_{r}\rightarrow\mu$ in the $\mathrm{w}^{*}$-topology as $r\rightarrow 1$. Furthermore, a basic fact on harmonic extension and the Poisson integral ([\citen{bib110}], Chap. I) is used to compute
\begin{equation*}
\int_{\mathbb{T}}\log|\zeta-\eta|\,d\mu_{r}(\zeta)=\frac{1}{2\pi}\int_{0}^{2\pi}\log|re^{\mathrm{i}\,s}-\eta|\,d\mu(s) \qquad (\eta\in \mathbb{T})
\end{equation*}
and
\begin{equation*}
\iint_{\mathbb{T}^{2}}\log|\zeta-\eta|\,d\mu_{r}(\zeta)\,d\mu_{r}(\eta)=\frac{1}{(2\pi)^{2}}\int_{0}^{2\pi}\int_{0}^{2\pi}\log
|r^{2}e^{\mathrm{i}\,s}-e^{\mathrm{i}\,t}|\,d\mu(s)\,d\mu(t)\,.
\end{equation*}
Hence $\Sigma(\mu_{r})\rightarrow\Sigma(\mu)$ as $r\rightarrow 1$.

(3) The large deviation in Theorem~\ref{ch05:the5.4.10} has a consequence that the empirical eigenvalue density of the $n\times n$ unitary random matrix distributed according to $\nu_{n}$ converges to $\mu_{0}$ almost surely. In particular, its mean eigenvalue distribution converges to $\mu_{0}$, that is, $\int_{\mathcal{U}(n)}\mathrm{tr}_{n}(U^{k})\,d\nu_{n}(U)\rightarrow m_{k}(\mu_{0})$ for all $k\in \mathbb{Z}$. In this way, we obtain a rather general Wigner type theorem for unitary random matrices.

The following are two examples of Theorem~\ref{ch05:the5.4.10} corresponding to Propositions~\ref{ch05:pro5.3.9} and \ref{ch05:pro5.3.10}. First, for $\alpha\in \mathbb{C}, \, |\alpha|<1$, let $Q(\zeta) :=\log|\zeta-\alpha|^{2}\,(\zeta \in \mathbb{T})$ . Then the probability measure $\nu_{n}$ on $\mathcal{U}(n)$ is given as
\begin{equation*}
\nu_n=\frac{1}{Z_{n}}\, \frac{d\gamma_{n}(U)}{\det|U-\alpha I|^{2n}}.
\end{equation*}
Hence $\bar{\nu}_{n}$ on $\mathbb{T}^{n}$ is
\begin{equation*}
\bar{\nu}_{n}=\frac{1}{Z_{n}}\,\frac{\prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2}}{\prod_{i=1}^{n}|\zeta_{i}-\alpha|^{2n}}\,d\zeta_{1}\cdots d\zeta_{n}.
\end{equation*}
Then the associated empirical eigenvalue distribution satisfies the large deviation principle with the rate function
\begin{equation*}
I(\mu):=-\Sigma(\mu)+\int\log|\zeta-\alpha|^{2}\,d\mu(\zeta)-\log(1-|\alpha|^{2}) \quad \mathrm{on} \quad \mathcal{M}(\mathbb{T}),
\end{equation*}
and the Poisson kernel measure $p_{\alpha}$ in (\ref{ch05:eqn5.3.14}) is a unique minimizer of $I$. Also we have
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log\int_{\mathcal{U}(n)}\frac{d\gamma_{n}(U)}{\det|U-\alpha I|^{2n}}=-\log(1-|\alpha|^{2})\,.
\end{equation*}
It does not seem easy to directly compute the asymptotic limit of the above integral.

Second, let $\lambda>0$ and $Q(\zeta):=-(2/\lambda)\mathrm{Re}\,\zeta\,(\zeta\in \mathbb{T})$. Then $\nu_{n}$ on $\mathcal{U}(n)$ is
\begin{equation*}
\nu_{n}=\frac{1}{Z_{n}}\exp\left(\frac{n}{\lambda} \mathrm{Tr}(U+U^{*})\right)d\gamma_{n}(U),
\end{equation*}
and $\bar{\nu}_{n}$ on $\mathbb{T}^{n}$ is
\begin{equation*}
\bar{\nu}_{n}=\frac{1}{Z_{n}}\exp\left(\frac{2n}{\lambda}\sum_{i=1}^{n}\cos\theta_{i}\right)
\prod_{i<j}|e^{\mathrm{i}\,\theta_{i}}-e^{\mathrm{i}\,\theta_{j}}|^{2}\,d\theta_{1}\cdots d\theta_{n}.
\end{equation*}
Then the empirical eigenvalue distribution satisfies the large deviation principle with the rate function
\begin{equation*}
I(\mu):=-\Sigma(\mu)-\frac{2}{\lambda}\int\mathrm{Re}\,\zeta\,d\mu(\zeta)+B \quad \mathrm{on} \quad \mathcal{M}(\mathbb{T}),
\end{equation*}
where
\begin{equation}
B:=\left\{\begin{array}{ll}
\displaystyle \frac{1}{\lambda^{2}} & \mathrm{if} \ \lambda\geq 2,\\
\\
\displaystyle \frac{1}{2}\log\frac{\lambda}{2}+\frac{2}{\lambda}-\frac{3}{4} & \mathrm{if} \ 0<\lambda<2,
\end{array}\right.
\end{equation}
and $\rho_{\lambda}$ in (\ref{ch05:eqn5.3.15}) is the unique minimizer of $I$. Incidentally, the quantity
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log\int_{\mathcal{U}(n)}\exp\left(\frac{n}{\lambda} \mathrm{Tr} (U+U^{*})\right)\,d\gamma_{n}(U)
\end{equation*}
is equal to the above $B$.

\section{The Wishart matrix}
\label{ch05:sec5.5}\index{Wishart matrix}

In this section we continue to study large deviation theorems related to random matrices. Our main interest here is focused on a large deviation for the empirical eigenvalue distribution of the regular and singular \textit{Wishart matrices}. Moreover, some other random matrices are also considered, for example the complexified Wishart matrix and antisymmetric real matrices.

For each $n\in \mathbb{N}$ let a positive integer $p(n)$ be given with $p(n)\geq n$. Let $T(n)$ be a $p(n)\times n$ real random matrix all of whose entries are independent and have the identical distribution $N(0,1)$. Then the $n\times n$ random matrix $n^{-1}T(n)^{t}T(n)$ is the Wishart matrix with a normalization. Its distribution on the space $M_{n}(\mathbb{R})^{+}$ of $n\times n$ positive semidefinite symmetric matrices induces the eigenvalue distribution on $(\mathbb{R}^{+})^{n}$, and according to (\ref{ch04:eqn4.1.22}) the latter distribution has the joint probability density
\begin{equation}
\frac{1}{Z_{n}}\exp\left(-\frac{n}{2}\sum_{i=1}^{n}\lambda_{i}\right)\prod_{i=1}^{n}\lambda_{i}^{(p(n)-n-1)/2}\prod_{i<j}|\lambda_{i}-\lambda_{j}| \label{ch05:eqn5.5.1}
\end{equation}
(cf. also [\citen{bib4}], Chap.~\ref{ch07:chap07}).

Also, let $\hat{T}(n)$ be a $p(n)\times n$ complex random matrix such that $\mathrm{Re}\,\hat{T}_{ij}(n)$ and $\mathrm{Im}\,\hat{T}_{ij}(n)\,(1\leq i\leq p(n),\, 1\leq j\leq n)$ are independent with the identical distribution $N(0, 1)$. Then the $n\times n$ positive semidefinite random matrix $(2n)^{-1}\hat{T}(n)^{*}\hat{T}(n)$ induces the probability distribution on $M_{n}(\mathbb{C})^{+}$, and it follows from (\ref{ch04:eqn4.4.5}) that its joint eigenvalue density on $(\mathbb{R}^{+})^{n}$ is
\begin{equation}
\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{n}\lambda_{i}\right)\prod_{i=1}^{n}\lambda_{i}^{p(n)-n}\prod_{i<j}(\lambda_{i}-\lambda_{j})^{2}.
\label{ch05:eqn5.5.2}
\end{equation}

To obtain the large deviations related to $n^{-1}T(n)^{t}T(n)$ or $(2n)^{-1}\hat{T}(n)^{*}\hat{T}(n)$, it is convenient to take a joint density more general than (\ref{ch05:eqn5.5.1}) and (\ref{ch05:eqn5.5.2}). Let $Q(x)$ be a real continuous function on $\mathbb{R}^{+}$ such that for any $\varepsilon>0$
\begin{equation}
\lim_{x\rightarrow\infty}x\exp(-\varepsilon Q(x))=0.
\label{ch05:eqn5.5.3}
\end{equation}
For each $n\in \mathbb{N}$ let $m(n)\in \mathbb{N}$ and a probability measure $\nu_{n}$ on $M_{m(n)}(\mathbb{R})^{+}$ or $M_{m(n)}(\mathbb{C})^{+}$ be given. Assume that $\nu_{n}$ induces the distribution $\bar{\nu}_{n}$ on $(\mathbb{R}^{+})^{m(n)}$ having the density
\begin{equation}
\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{m(n)}Q(t_{i})\right)\prod_{i=1}^{m(n)}t_{i}^{\gamma(n)}\prod_{1\leq i<j\leq m(n)}|t_{i}-t_{j}|^{2\beta},
\label{ch05:eqn5.5.4}
\end{equation}
where $\beta>0$ is fixed but $\gamma(n)\geq 0$ depends on $n$. Define the empirical eigenvalue distribution $P_{n}$ on $\mathcal{M}(\mathbb{R}^{+})$ by
\begin{equation*}
P_{n}(\Gamma)=\bar{\nu}_{n}(\{t\in(\mathbb{R}^{+})^{m(n)}:\kappa_{m(n)}(t)\in\Gamma\})
\end{equation*}
for Borel sets $\Gamma\subset \mathcal{M}(\mathbb{R}^{+})$, where $\kappa_{m(n)}(t) :=\frac{1}{m(n)}\sum_{i=1}^{m(n)}\delta(t_{i})$ and $\mathcal{M}(\mathbb{R}^{+})$ is endowed with the weak topology as before.

\begin{theorem}
\label{ch05:the5.5.1}
Assume that $m(n)/n\rightarrow\alpha\in(0, \infty)$ and $\gamma(n)/n\rightarrow\gamma\in \mathbb{R}^{+}$ as $n\rightarrow\infty$. Then the finite limit $B :=\lim_{n\rightarrow\infty}n^{-2}\log Z_{n}$ exists, and $(P_{n})$ satisfies the large deviation principle in the scale $n^{-2}$ with the good rate function
\begin{equation*}
I(\mu) :=-\alpha^{2}\beta\Sigma(\mu)+\alpha\int(Q(x)-\gamma\log x)\, d\mu(x)+B
\end{equation*}
for $\mu\in \mathcal{M}(\mathbb{R}^{+})$. Moreover, there exists a unique $\mu_{0}\in \mathcal{M}(\mathbb{R}^{+})$ such that $I(\mu_{0})=0$.
\end{theorem}

To prove the theorem, let us introduce the kernel functions on $(\mathbb{R}^{+})^{2}$ as follows:
\begin{align*}
F(x, y) \ & := \ -\alpha^{2}\beta\log|x-y|+\frac{\alpha}{2}(Q(x)+Q(y))-\frac{\alpha\gamma}{2}(\log x+\log y), \\
\tilde{F}_{n}(x, y) & := -\frac{m(n)^{2}}{n^{2}}\beta\log|x-y|+\frac{m(n)}{2n}(Q(x)+Q(y)) \\
& \qquad \qquad -\frac{m(n)\gamma(n)}{2n^{2}}(\log x+\log y),
\end{align*}
and for $R>0$
\begin{equation*}
F_{R}(x, y) :=\min\{F(x, y),R\}, \quad \tilde{F}_{n,R}(x, y) :=\min\{\tilde{F}_{n}(x, y), R\}.
\end{equation*}
Since
\begin{equation}
F(x, y)\geq-\frac{\alpha(2\alpha\beta+\gamma)}{2}\left[\log\left(x\exp\left(-\frac{Q(x)}{2\alpha\beta+\gamma}\right)\right)+\log\left(y\exp\left(-\frac{Q(y)}{2\alpha\beta+\gamma}\right)\right)\right] \label{ch05:eqn5.5.5}
\end{equation}
(and similarly for $\tilde{F}_{n}(x, y))$ whenever $x, y\geq 2$, it follows from (\ref{ch05:eqn5.5.3}) that $F_{R}(x, y)$ is bounded and continuous. Hence
\begin{equation*}
-\alpha^{2}\beta\Sigma(\mu)+\alpha\int(Q(x)-\gamma\log x)\,d\mu(x)=\iint F(x, y)\,d\mu(x)\,d\mu(y)
\end{equation*}
is a well-defined and lower semicontinuous functional on $\mathcal{M}(\mathbb{R}^{+})$.

\begin{lemma}
\label{ch05:lem5.5.2} For any $R>0,\, \tilde{F}_{n,R}(x, y)\rightarrow F_{R}(x, y)$ uniformly as $n\rightarrow\infty$.
\end{lemma}

\begin{proof2}
Using (\ref{ch05:eqn5.5.5}) for $F$ as well as $\tilde{F}_{n}$ and the assumptions on $m(n), \, \gamma(n)$, one can see that for any $R>0$ there exists $\delta>0$ such that if $(x, y)\not\in[\delta, \delta^{-1}]\times[\delta, \delta^{-1}]$ then $F(x, y)\geq R$ and $\tilde{F}_{n}(x, y)\geq R$ for all $n$. Furthermore, since $\log x$ and $Q(x)$ are bounded on $[\delta, \delta^{-1}], \, \delta_{1}>0$ can be chosen so that if $(x, y)$ does not belong to
\begin{equation*}
\Delta:=\{(x, y):\delta\leq x\leq\delta^{-1}, \ \delta\leq y\leq\delta^{-1}, \ |x-y|\geq\delta_{1}\},
\end{equation*}
then $F(x, y)\geq R$ and $\tilde{F}_{n}(x, y)\geq R$ for all $n$. Obviously $\tilde{F}_{n}(x, y)\rightarrow F(x, y)$ as $ n\rightarrow\infty$ uniformly on $\Delta$, so the conclusion follows.
\end{proof2}

According to Theorem~\ref{ch05:the5.3.3} there exist unique $\mu_0,\tilde{\mu}_{n}\in \mathcal{M}(\mathbb{R}^{+})$ such that
\begin{align*}
\iint F(x, y)\,d\mu_{0}(x) \, d\mu_{0}(y) & \ =\ \inf_{\mu\in \mathcal{M}(\mathbb{R}^{+})}\iint F(x, y)\,d\mu(x)\,d\mu(y), \\
\iint\tilde{F}_{n}(x, y)\,d\tilde{\mu}_{n}(x)\,d\tilde{\mu}_{n}(y) & \ = \ \inf_{\mu\in \mathcal{M}(\mathbb{R}^{+})}\iint \tilde{F}_{n}(x, y)\,d\mu(x)\,d\mu(y).
\end{align*}

\begin{lemma}
\label{ch05:lem5.5.3} $(\tilde{\mu}_{n})$ is tight, and
\begin{equation}
\iint F(x, y)\, d\mu_{0}(x)\,d\mu_{0}(y)\leq\underset{n\rightarrow\infty}{\liminf}\iint\tilde{F}_{n}(x, y)\,d\tilde{\mu}_{n}(x)\,d\tilde{\mu}_{n}(y). \label{ch05:eqn5.5.6}
\end{equation}
\end{lemma}

\begin{proof2}
It is clear that $\iint\tilde{F}_{n}(x, y)\,d\tilde{\mu}_{n}(x)\,d\tilde{\mu}_{n}(y)\leq c\, (n\in \mathbb{N})$ for some $ c<+\infty$. Also, by the estimate (\ref{ch05:eqn5.5.5}) for $\tilde{F}_{n}$, there is a $ d<+\infty$ such that $\tilde{F}_{n}(x, y)\geq-d$ for all $x, y\in \mathbb{R}^{+}$ and $n\in \mathbb{N}$. For $\alpha>0$ let
\begin{equation*}
M_{\alpha} :=\inf\{\tilde{F}_{n}(x, y):n\in \mathbb{N}, \ x, y\geq\alpha\}.
\end{equation*}
Then, by (\ref{ch05:eqn5.5.5}) for $\tilde{F}_{n}$ again, $M_{\alpha}$ can be arbitrarily large when $\alpha\rightarrow+\infty$. Since
\begin{equation*}
c\geq M_{\alpha}\tilde{\mu}_{n}([\alpha, \infty))^{2}-d \qquad (n\in \mathbb{N}),
\end{equation*}
we have $\sup_{n}\tilde{\mu}_{n}([\alpha, \infty))\rightarrow 0$ as $\alpha\rightarrow+\infty$, which means the tightness of $(\tilde{\mu}_{n})$.

Thanks to the tightness (or relative weak compactness) of $(\tilde{\mu}_{n})$, one can choose a subsequence $(\tilde{\mu}_{n(m)})$ such that $\tilde{\mu}_{n(m)}\rightarrow\tilde{\mu}$ weakly for some $\tilde{\mu}\in\mathcal{M}(\mathbb{R}^{+})$ and
\begin{equation*}
\lim_{m\rightarrow\infty}\iint\tilde{F}_{n(m)}(x, y)\, d\tilde{\mu}_{n(m)}\,d\tilde{\mu}_{n(m)}(y)=\liminf_{n\rightarrow\infty}\iint\tilde{F}_{n}(x, y)\,d\tilde{\mu}_{n}(x)\,d\tilde{\mu}_{n}(y).
\end{equation*}
Then
\begin{align*}
& \iint F(x, y)\, d\mu_{0}(x)\, d\mu_{0}(y) \\
 & \quad \ \leq\iint F(x, y)\,d\tilde{\mu}(x)\,d\tilde{\mu}(y) \\
& \quad \ =\sup_{R>0}\iint F_{R}(x, y)\,d\tilde{\mu}(x)\,d\tilde{\mu}(y) \\
& \quad \ =\sup_{R>0}\lim_{m\rightarrow\infty}\iint\tilde{F}_{n(m),R}(x, y)\,d\tilde{\mu}_{n(m)}(x)\,d\tilde{\mu}_{n(m)}(y) \quad (\mathrm{by  \ Lemma~\ref{ch05:lem5.5.2}}) \\
& \quad \leq\lim_{m\rightarrow\infty}\iint\tilde{F}_{n(m)}(x, y)\,d\tilde{\mu}_{n(m)}(x)\,d\tilde{\mu}_{n(m)}(y),
\end{align*}
showing (\ref{ch05:eqn5.5.6}).
\end{proof2}

\begin{lemma}
\label{ch05:lem5.5.4}
\begin{equation*}
\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}\leq-\iint F(x, y)\,d\mu_{0}(x)\,d\mu_{0}(y)\,.
\end{equation*}
\end{lemma}

\begin{proof2}
We estimate
\begin{align*}
Z_{n} & \ = \  \int\cdots\int\exp\left(\frac{n}{m(n)}\sum_{i=1}^{m(n)}\left(-Q(t_{i})+\frac{\gamma(n)}{n}\log t_{i}\right)\right) \\
& \qquad \qquad \qquad \times\exp\left(-\frac{2n^{2}}{m(n)^{2}}\sum_{1\leq i<j\leq m(n)}\tilde{F}_{n}(t_{i}, t_{j})\right)dt_{1}\cdots dt_{m(n)} \\
& \ \leq \ \left [\int\exp\left(\frac{n}{m(n)}\left(-Q(x)+\frac{\gamma(n)}{n}\log x\right)\right)dx\right]^{m(n)} \\
& \qquad \qquad \qquad \quad \ \times\exp\left(-n^{2}\iint\tilde{F}_{n}(x, y)\,d\tilde{\mu}_{n}(x)\,d\tilde{\mu}_{n}(y)\right).
\end{align*}
Since by the assumption (\ref{ch05:eqn5.5.3})
\begin{equation*}
\sup_{n\geq 1}\int\exp\left(\frac{n}{m(n)}\left(-Q(x)+\frac{\gamma(n)}{n}\log x\right)\right)dx<+\infty,
\end{equation*}
the above estimate implies that
\begin{align*}
\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n} & \ \leq \ - \liminf_{n\rightarrow\infty} \iint \tilde{F}_{n}(x, y)\,d\tilde{\mu}_{n}(x)\,d\tilde{\mu}_{n}(y) \\
& \ \leq\ -\iint F(x, y)\,d\mu_{0}(x)\,d\mu_{0}(y)
\end{align*}
thanks to (\ref{ch05:eqn5.5.6}).
\end{proof2}

\begin{lemma}
\label{ch05:lem5.5.5} For every $\mu\in\mathcal{M}(\mathbb{R}^{+})$,
\begin{equation*}
\inf_{G}\left[\underset{n\rightarrow\infty}{\limsup}\frac{1}{n^{2}}\log P_{n}(G)\right]\leq-\iint F(x, y)\,d\mu(x)\,d\mu(y)-\underset{n\rightarrow\infty}{\liminf}\frac{1}{n^{2}}\log Z_{n},
\end{equation*}
where $G$ runs over a neighborhood base of $\mu$.
\end{lemma}

\begin{proof2}
For any neighborhood $G$ of $\mu\ \in\ \mathcal{M}(\mathbb{R}^{+})$ set $\tilde{G}:=\,\{t\ \in\ (\mathbb{R}^{+})^{m(n)}: \kappa_{m(n)}(t)\in G\}$. Then we get
\begin{align*}
P_{n}(G) & \ = \ \frac{1}{Z_{n}}\int\cdots\int_{\tilde{G}}\exp\left(\frac{n}{m(n)}\sum_{i=1}^{m(n)}\left(-Q(t_{i})+\frac{\gamma(n)}{n}\log t_{i}\right)\right) \\
& \qquad \qquad \quad \times\exp\left(-\frac{2n^{2}}{m(n)^{2}}\sum_{1\leq i<j\leq m(n)}\tilde{F}_{n}(t_{i}, t_{j})\right)\,dt_{1}\cdots dt_{m(n)} \\
& \ \leq \ \frac{1}{Z_{n}}\left[\int\exp\left(\frac{n}{m(n)}\left(-Q(x)+\frac{\gamma(n)}{n}\log x\right)\right)dx\right]^{m(n)} \\
& \qquad \qquad \quad \times\exp\left(-n^{2}\inf_{\mu'\in G}\iint\tilde{F}_{n,R}(x, y)\,d\mu'(x)\,d\mu'(y)+nR\right)
\end{align*}
for any $R>0$. Furthermore, by Lemma~\ref{ch05:lem5.5.2}
\begin{equation*}
\lim_{n\rightarrow\infty}\left(\inf_{\mu'\in G}\iint\tilde{F}_{n,R}(x, y)\,d\mu'(x)\,d\mu'(y)\right)=\inf_{\mu'\in G}\iint F_{R}(x, y)\,d\mu'(x)\,d\mu'(y),
\end{equation*}
so that
\begin{equation*}
\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\leq-\inf_{\mu'\in G}\iint F_{R}(x, y)d\mu'(x)d\mu'(y)-\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}\,.
\end{equation*}
Thanks to the continuity of $\mu'\mapsto\iint F_{R}(x, y)\,d\mu'(x)\,d\mu'(y)$, we obtain
\begin{equation*}
\inf_{G}\left[\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\right]\leq-\iint F_{R}(x, y)\,d\mu(x)\,d\mu(y)-\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}\,,
\end{equation*}
which yields the statement as $ R\rightarrow+\infty$.
\end{proof2}

\begin{lemma}
\label{ch05:lem5.5.6}
\begin{equation*}
\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}\geq-\iint F(x, y)\,d\mu_{0}(x)\,d\mu_{0}(y),
\end{equation*}
and for every $\mu\in \mathcal{M}(\mathbb{R}^{+})$
\begin{equation*}
\inf_{G}\left[\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\right]\geq-\iint F(x, y)\,d\mu(x)\,d\mu(y)-\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}\,,
\end{equation*}
where $G$ runs over a neighborhood base of $\mu$.
\end{lemma}

\begin{proof2}
By a regularization argument as in the previous section, we may assume that the support of $\mu$ is a finite interval $[0, R]$ and $\mu$ has a continuous density $f>0$ on $[0, R]$. Hence $\delta\leq f(x)\leq\delta^{-1}\,(0\leq x\leq R)$ for some $\delta>0$. For each $n\in \mathbb{N}$ let $0<a_{1}^{(n)}<b_{1}^{(n)}<a_{2}^{(n)}<b_{2}^{(n)}<\ldots<a_{m(n)}^{(n)}<b_{m(n)}^{(n)}=R$ be such that
\begin{equation*}
\int_{0}^{a_{i}^{(n)}}f(x)\, dx=\frac{i-\frac{1}{2}}{m(n)}, \quad \int_{0}^{b_{i}^{(n)}}f(x)\,dx=\frac{i}{m(n)} \quad (1\leq i\leq m(n)).
\end{equation*}
We get
\begin{equation*}
\frac{\delta}{2m(n)}\leq b_{i}^{(n)}-a_{i}^{(n)}\leq\frac{1}{2m(n)\delta} \qquad (1\leq i\leq m(n)).
\end{equation*}
Define
\begin{equation*}
\Delta_{n}\,:=\{(t_{1}, \ldots, t_{m(n)})\in(\mathbb{R}^{+})^{m(n)}:a_{i}^{(n)}\leq t_{i}\leq b_{i}^{(n)},\ 1\leq i\leq m(n)\}.
\end{equation*}
For any neighborhood $G$ of $\mu$, if $n$ is large enough, then we have $\Delta_{n}\subset\{t\, \in \, (\mathbb{R}^{+})^{m(n)}$ : $\kappa_{m(n)}(t)\in G\}$, and so
\begin{align*}
 P_{n}(G) & \ \geq \ \frac{1}{Z_{n}}\int\cdots\int_{\Delta_{n}}\exp\left(-n\sum_{i=1}^{m(n)}Q(t_{i})\right)\prod_{i=1}^{m(n)}t_{i}^{\gamma(n)} \\
& \qquad \qquad \quad \times\prod_{1\leq i<j\leq m(n)}(a_{j}^{(n)}-b_{i}^{(n)})^{2\beta}\int\cdots\int_{\Delta_{n}}dt_{1}\cdots dt_{m(n)} \\
& \ \geq \ \frac{1}{Z_{n}}\left(\frac{\delta}{2m(n)}\right)^{m(n)}\exp\left(-n\sum_{i=1}^{m(n)}\xi_{i}^{(n)}\right)\prod_{i=1}^{m(n)}(a_{i}^{(n)})^{\gamma(n)} \\
& \qquad \qquad \qquad \quad \ \times\prod_{1\leq i<j\leq m(n)}(a_{j}^{(n)}-b_{i}^{(n)})^{2\beta},
\end{align*}
where $\xi_{i}^{(n)} :=\max\{Q(x) : a_{i}^{(n)}\leq x\leq b_{i}^{(n)}\}$. The following are easy to check:
\begin{align*}
& \lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{m(n)}\xi_{i}^{(n)}=\alpha\int Q(x)\,d\mu(x), \\
& \lim_{n\rightarrow\infty}\frac{\gamma(n)}{n^{2}}\sum_{i=1}^{m(n)}\log a_{i}^{(n)}=\alpha\gamma\int\log x\,d\mu(x), \\
& \lim_{n\rightarrow\infty}\frac{2\beta}{n^{2}}\sum_{1\leq i<j\leq m(n)}\log(a_{j}^{(n)}-b_{i}^{(n)})=\alpha^{2}\beta\Sigma(\mu)\,.
\end{align*}
Therefore,
\begin{equation*}
0\geq\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log P_{n}(G)\geq-\iint F(x, y)\,d\mu(x)\,d\mu(y)-\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}\,,
\end{equation*}
and we take the infimum for $\mu$. Also, we obtain
\begin{equation*}
\underset{n\rightarrow\infty}{\liminf}\frac{1}{n^{2}}\log P_{n}(G)\geq-\iint F(x, y)\,d\mu(x)\,d\mu(y)-\underset{n\rightarrow\infty}{\limsup}\frac{1}{n^{2}}\log Z_{n}\,.
\end{equation*}
\end{proof2}

\begin{proof1}[End of proof of Theorem~\ref{ch05:the5.5.1}]
It is immediate from Lemmas~\ref{ch05:lem5.5.4} and \ref{ch05:lem5.5.6} that the finite limit $B:=\lim_{n\rightarrow\infty}n^{-2}\log Z_{n}$ exists. Moreover, the exponential tightness of $(P_{n})$ can be shown similarly to the proof of Lemma~\ref{ch05:lem5.4.8}. Therefore, Lemmas~\ref{ch05:lem5.5.5} and \ref{ch05:lem5.5.6} imply the conclusion.
\end{proof1}

The joint eigenvalue distribution of the real Wishart matrix $n^{-1}T(n)^{t}T(n)$ with $p(n)\geq n$ has the density (\ref{ch05:eqn5.5.1}). This is a special case of (\ref{ch05:eqn5.5.4}), where $Q(x)=x/2,\,\beta=1/2,\,m(n)=n$ and $\gamma(n)=(p(n)-n-1)/2$. So, when $p(n)/n\rightarrow\lambda\in[1, \infty)$ as $n\rightarrow\infty$, the empirical eigenvalue distribution of $n^{-1}T(n)^{t}T(n)$ satisfies the large deviation principle with the good rate function
\begin{equation}
I(\mu) :=-\frac{1}{2}\Sigma(\mu)+\frac{1}{2}\int(x-(\lambda-1)\log x)\, d\mu(x)+B\,.
\label{ch05:eqn5.5.7}
\end{equation}
Also, since (\ref{ch05:eqn5.5.4}) becomes (\ref{ch05:eqn5.5.2}) when $Q(x)=x, \ \beta=1, \ m(n)=n$ and $\gamma(n)=p(n)-n$, the empirical eigenvalue distribution of the complex Wishart matrix $(2n)^{-1}\hat{T}(n)^{*}\hat{T}(n)$ satisfies the large deviation principle as well, and the rate function is the above (\ref{ch05:eqn5.5.7}) multiplied by 2. According to Proposition~\ref{ch05:pro5.3.7} the Marchenko-Pastur distribution $\mu_{\lambda}$ is the minimizer of (\ref{ch05:eqn5.5.7}).

According to the Selberg integral formula of Laguerre type ([\citen{bib121}], p. 354), the normalization constant $Z_{n}$ in (\ref{ch05:eqn5.5.2}) is given as
\begin{align*}
Z_{n} & \ = \ n^{-np(n)}\int_{0}^{\infty}\cdots\int_{0}^{\infty}\exp\left(-\sum_{i=1}^{n}x_{i}\right)\prod_{i=1}^{n}x_{i}^{p(n)-n} \\
& \qquad \qquad \qquad \qquad \qquad \times\prod_{i<j}(x_{i}-x_{j})^{2}\,dx_{1}\cdots dx_{n}\\
& \ = \ n^{-np(n)}\prod_{j=1}^{n}j!(p(n)-j) !\,.
\end{align*}
Hence, by using the Stirling formula, $B$ in (\ref{ch05:eqn5.5.7}) is computed as follows:
\begin{align*}
2B & \ = \ \lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n} \\
& \ = \ \lim_{n\rightarrow\infty}\left[\left(\frac{1}{n^{2}}\log\prod_{j=1}^{n}j!-\frac{1}{2}\log p(n)\right)\right. \\
& \qquad \qquad +\frac{p(n)^{2}}{n^{2}}\left(\frac{1}{p(n)^{2}}\log\prod_{j=0}^{p(n)-1}j!-\frac{1}{2}\log p(n)\right) \\
& \qquad \qquad -\frac{(p(n)-n)^{2}}{n^{2}}\left(\frac{1}{(p(n)-n)^{2}}\log\prod_{j=0}^{p(n)-n-1}j!-\frac{1}{2}\log(p(n)-n)\right) \\
& \qquad \qquad \left.+\frac{p(n)^{2}}{2n^{2}}\log\frac{p(n)}{n}-\frac{(p(n)-n)^{2}}{2n^{2}}\log\frac{p(n)-n}{n}\right] \\
& = \ -\frac{3}{4}(1+\lambda^{2}-(\lambda-1)^{2})+\frac{\lambda^{2}}{2}\log\lambda-\frac{(\lambda-1)^{2}}{2}\log(\lambda-1) \\
& =\ -\frac{1}{2}(3\lambda-\lambda^{2}\log\lambda+(\lambda-1)^{2}\log(\lambda-1)).
\end{align*}
Therefore, since $I(\mu_{\lambda})=0$ for (\ref{ch05:eqn5.5.7}), we have
\begin{align}
\Sigma(\mu_{\lambda}) & \ = \ \int(x-(\lambda-1)\log x)\,d\mu_{\lambda}(x)\label{ch05:eqn5.5.8} \\
& \qquad \quad -\frac{1}{2}(3\lambda-\lambda^{2}\log\lambda+(\lambda-1)^{2}\log(\lambda-1)). \notag
\end{align}
Combining (\ref{ch05:eqn5.3.9}) and (\ref{ch05:eqn5.5.8}) gives
\begin{equation}
\Sigma(\mu_{\lambda})=-1+\frac{1}{2}(\lambda+\log\lambda+(\lambda-1)^{2}\log(1-\lambda^{-1})).
\label{ch05:eqn5.5.9}
\end{equation}

Summarizing the above arguments, we have

\begin{theorem}
\label{ch05:the5.5.7}
When $p(n)/n\rightarrow\lambda\in[1, \infty)$ as $n\rightarrow\infty$, the empirical eigenvalue distribution of the Wishart matrix $n^{-1}T(n)^{t}T(n)$ satisfies the large deviation\index{large deviation!for Wishart matrix} principle in the scale $n^{-2}$ with the good rate function
\begin{align*}
I(\mu) & \ := \ -\frac{1}{2}\Sigma(\mu)+\frac{1}{2}\int(x-(\lambda-1)\log x)\,d\mu(x) \\
& \qquad \quad -\frac{1}{4}(3\lambda-\lambda^{2}\log\lambda+(\lambda-1)^{2}\log(\lambda-1))
\end{align*}
for $\mu\in \mathcal{M}(\mathbb{R}^{+})$. Moreover, the Marchenko-Pastur distribution $\mu_{\lambda}$ is the unique minimizer of $I$, and its free entropy is given by \emph{(\ref{ch05:eqn5.5.9}).}
\end{theorem}

Next, assume $p(n)<n$ and take the random atomic measure
\begin{equation*}
R_{n} :=\mathbf{K}_{n}(n^{-1}T(n)^{t}T(n)),
\end{equation*}
where $\mathbf{K}_{n}$ was introduced in (\ref{ch05:eqn5.4.3}). Note that the eigenvalues of $n^{-1}T(n)^{t}T(n)$ are those of $n^{-1}T(n)T(n)^{t}$ plus $n-p(n)$ zeros. Furthermore, it is obvious that the eigenvalue distribution of $p(n)^{-1}T(n)T(n)^{t}$ is given by (\ref{ch05:eqn5.5.1}) with $n$ and $p(n)$ interchanged. Hence the eigenvalue distribution $\tilde{\nu}_{n}$ of $n^{-1}T(n)T(n)^{t}$ has the joint density
\begin{equation*}
\frac{1}{Z_{n}}\exp\left(-\frac{n}{2}\sum_{i=1}^{p(n)}\lambda_{i}\right)\prod_{i=1}^{p(n)}\lambda_{i}^{(n-p(n)-1)/2}\prod_{1\leq i<j\leq p(n)}|\lambda_{i}-\lambda_{j}|.
\end{equation*}
In this way we have the following:

\begin{lemma}
\label{ch05:lem5.5.8}
Let $R_{n}$ and $\tilde{\nu}_{n}$ be as above. Then $R_{n}$ is written as
\begin{equation*}
R_{n}=\frac{n-p(n)}{n}\delta(0)+\frac{p(n)}{n}\tilde{R}_{n},
\end{equation*}
where the distribution of $\tilde{R}_{n}$ is given by
\begin{equation*}
\tilde{P}_{n}(\Gamma):=\tilde{\nu}_{n}(\{t\in(\mathbb{R}^{+})^{p(n)}:\kappa_{p(n)}(t)\in\Gamma\})
\end{equation*}
for Borel sets $\Gamma\subset\mathcal{M}(\mathbb{R}^{+})$.
\end{lemma}

For $0<\lambda< 1$ let
\begin{equation*}
\tilde{\mu}_{\lambda}:=\frac{\sqrt{4\lambda-(x-1-\lambda)^{2}}}{2\pi\lambda x}\chi(x)\,dx,
\end{equation*}
where $\chi$ denotes the characteristic function of the interval $[(1-\sqrt{\lambda})^{2}, (1+\sqrt{\lambda})^{2}]$. This probability distribution is known as a variant of the Marchenko-Pastur distribution $\mu_{\lambda}$. They are simply related by $\tilde{\mu}_{\lambda}=D_{\lambda}\mu_{\lambda^{-1}}$.

\begin{lemma}
\label{ch05:lem5.5.9}
Assume that $p(n)/n\rightarrow\lambda\in(0,1)$ as $ n\rightarrow\infty$. Then the sequence $\tilde{P}_{n}$ in the above lemma is exponentially tight and satisfies the large deviation principle in the scale $n^{-2}$ with the rate function
\begin{align*}
\tilde{I}(\mu) & \ := \ -\frac{\lambda^{2}}{2}\Sigma(\mu)+\frac{\lambda}{2}\int(x-(1-\lambda)\log x)\,d\mu(x) \\
& \qquad \quad \ -\frac{1}{4}(3\lambda-\lambda^{2}\log\lambda+(1-\lambda)^{2}\log(1-\lambda))
\end{align*}
for $\mu\in\mathcal{M}(\mathbb{R}^{+})$. The distribution $\tilde{\mu}_{\lambda}$ is the unique minimizer of $\tilde{I}$.
\end{lemma}

\begin{proof2}
By Theorem~\ref{ch05:the5.5.1} for the case $m(n)=p(n)$ and $\gamma(n)=(n-p(n)-1)/2$, we know that $(\tilde{P}_{n})$ satisfies the large deviation principle with the good rate function
\begin{equation*}
\tilde{I}(\mu)=-\frac{\lambda^{2}}{2}\Sigma(\mu)+\frac{\lambda}{2}\int(x-(1-\lambda)\log x)\,d\mu(x)+B\,.
\end{equation*}
Let $I_{\lambda-1}$ be the rate function given in Theorem~\ref{ch05:the5.5.7} for $\lambda^{-1}$ in place of $\lambda$. We get
\begin{align*}
\tilde{I}(D_{\lambda}\mu) & \  = \ -\frac{\lambda^{2}}{2}\Sigma(\mu)+\frac{\lambda^{2}}{2}\int(x-(\lambda^{-1}-1)\log x)\,d\mu(x)+B-\frac{\lambda}{2}\log\lambda, \\
\lambda^{2}I_{\lambda^{-1}}(\mu) & \ = \ -\frac{\lambda^{2}}{2}\Sigma(\mu)+\frac{\lambda^{2}}{2}\int(x-(\lambda^{-1}-1)\log x)\,d\mu(x) \\
& \qquad \qquad -\frac{1}{4}(3\lambda+\log\lambda+(1-\lambda)^{2}\log(\lambda^{-1}-1)),
\end{align*}
which are equal up to a constant. Hence $\tilde{I}(D_{\lambda}\mu)=\lambda^{2}I_{\lambda^{-1}}(\mu)$ indeed holds for all $\mu\in\mathcal{M}(\mathbb{R}^{+})$. This implies that $\tilde{I}$ has the unique minimizer $D_{\lambda}\mu_{\lambda^{-1}}=\tilde{\mu}_{\lambda}$, and
\begin{equation*}
B=-\frac{1}{4}(3\lambda-\lambda^{2}\log\lambda+(1-\lambda)^{2}\log(1-\lambda)).
\end{equation*}
The exponential tightness of $(\tilde{P}_{n})$ was also shown at the end of proof of Theorem~\ref{ch05:the5.5.1}.
\end{proof2}

The next theorem complements Theorem~\ref{ch05:the5.5.7} about the large deviation related to the Wishart matrix. It is remarkable that the limit distribution (or the minimizer of the rate function) has an atom.

\begin{theorem}
\label{ch05:the5.5.10}
Assume that $p(n)<n$ and $p(n)/n\rightarrow\lambda\in(0,\, 1)$ as $n\rightarrow\infty$. Then the empirical eigenvalue distribution of $n^{-1}T(n)^{t}T(n)$ satisfies the large deviation principle in the scale $n^{-2}$ with the good rate function
\begin{equation*}
I(\mu):=\left\{\begin{array}{ll}
\tilde{I}(\tilde{\mu}) & \ \textit{if}\ \mu=(1-\lambda)\delta(0)+\lambda\tilde{\mu},\, \tilde{\mu}\in \mathcal{M}(\mathbb{R}^{+})\ ,\\
+\infty & \ otherwise,
\end{array}\right.
\end{equation*}
where $\tilde{I}$ is given by Lemma ~\ref{ch05:lem5.5.9}. The minimizer of $I$ is $\mu_{\lambda}\,(=(1-\lambda)\delta(0)+\lambda\tilde{\mu}_{\lambda})$.
\end{theorem}

By Lemmas~\ref{ch05:lem5.5.8} and \ref{ch05:lem5.5.9} the following lemma is enough to prove the theorem.

\begin{lemma}
\label{ch05:lem5.5.11}
For $n\in\mathbb{N}$ let $\tilde{R}_{n}$ be a random probability measure on a Polish space $X$, and $\tilde{P}_{n}$ the distribution of $\tilde{R}_{n}$. Let $\mu_{0}$ be a fixed probability measure on $X$, and let $0<\lambda_{n}<1$ be such that $\lambda_{n}\rightarrow\lambda\in(0,1)$. If $(\tilde{P}_{n})$ is exponentially tight and satisfies the large deviation principle in the scale $L_{n}$ with a rate function $\tilde{I}$ on $\mathcal{M}(X)$, then the sequence of random measures $(1-\lambda_{n})\mu_{0}+\lambda_{n}\tilde{R}_{n}$ satisfies the same with the good rate function
\begin{equation*}
I(\mu):=\left\{\begin{array}{ll}
\tilde{I}(\tilde{\mu}) & \ \textit{if} \ \mu=(1-\lambda)\mu_{0}+\lambda\tilde{\mu},\,\tilde{\mu}\in \mathcal{M}(X)\textit{,}\\
+\infty & \ otherwise.
\end{array}\right.
\end{equation*}
\end{lemma}

\begin{proof2}
The distribution $P_{n}$ of $(1-\lambda_{n})\mu_{0}+\lambda_{n}\tilde{R}_{n}$ is given by
\begin{equation*}
P_{n}(\Gamma)=\tilde{P}_{n}(\{\tilde{\mu}\in\mathcal{M}(X):(1-\lambda_{n})\mu_{0}+\lambda_{n}\tilde{\mu}\in\Gamma\})
\end{equation*}
for Borel sets $\Gamma\subset\mathcal{M}(X)$. First we show that $(P_{n})$ is exponentially tight. For any $\varepsilon>0$ there exists a compact $\tilde{K}_{\varepsilon}\subset \mathcal{M}(X)$ such that
\begin{equation*}
\limsup_{n\rightarrow\infty} \, L_{n}\log\tilde{P}_{n}(\tilde{K}_{\varepsilon}^{c})\leq-\frac{1}{\varepsilon}.
\end{equation*}
By noting that the weak topology on $\mathcal{M}(X)$ is metrizable, it is easy to see that the closure $K_{\varepsilon}$ of $\bigcup_{n=1}^{\infty}((1-\lambda_{n})\mu_{0}+\lambda_{n}\tilde{K}_{\varepsilon})$ is compact. Since $P_{n}(K_{\varepsilon}^{c})\leq\tilde{P}_{n}(\tilde{K}_{\varepsilon}^{c})$, we get the conclusion.

Now it suffices to show that for every $\mu\in \mathcal{M}(X)$
\begin{align}
&\inf_{G}\left[\liminf_{n\rightarrow\infty}L_{n}\log P_{n}(G)\right]\, \geq\, -I(\mu), \label{ch05:eqn5.5.10} \\
&\inf_{G}\left[\limsup_{n\rightarrow\infty}L_{n}\log P_{n}(G)\right]\, \leq\, -I(\mu), \label{ch05:eqn5.5.11}
\end{align}
where $G$ runs over neighborhoods of $\mu$. Let $\mathcal{D}$ denote the set $\{(1-\lambda)\mu_{0}+\lambda\tilde{\mu}: \tilde{\mu}\in \mathcal{M}(X)\}$. If $\mu\not\in \mathcal{D}$, then $\mu(C)<(1-\lambda)\mu_{0}(C)$ for some closed $C\subset X$. Then, since $P_{n}(G)=0$ for a neighborhood $G :=\{\mu'\in \mathcal{M}(\mathrm{X}) : \mu'(C)<(1-\lambda)\mu_{0}(C)\}$ of $\mu$, (\ref{ch05:eqn5.5.10}) and (\ref{ch05:eqn5.5.11}) hold in this case. Next assume that $\mu\in \mathcal{D}$ and $\mu= (1-\lambda)\mu_{0}+\lambda\tilde{\mu}$. For any neighborhood $G$ of $\mu$ there exists a neighborhood $\tilde{G}$ of $\tilde{\mu}$ such that $(1-\lambda_{n})\mu_{0}+\lambda_{n}\tilde{G}\subset G$ for large $n$, and hence
\begin{equation*}
\liminf_{n\rightarrow\infty} L_{n}\log P_{n}(G)\, \geq\, \liminf_{n\rightarrow\infty} L_{n}\log\tilde{P}_{n}(\tilde{G})\geq-\tilde{I}(\tilde{\mu})\,.
\end{equation*}
This implies (\ref{ch05:eqn5.5.10}). On the other hand, for any neighborhood $\tilde{G}$ of $\tilde{\mu}$ there exists a neighborhood $G$ of $\mu$ such that $\lambda_{n}^{-1}G-(\lambda_{n}^{-1}-1)\mu_{0}\subset\tilde{G}$ or
\begin{equation*}
\{\tilde{\mu}\in \mathcal{M}(X):(1-\lambda_{n})\mu_{0}+\lambda_{n}\tilde{\mu}\in G\}\subset\tilde{G}
\end{equation*}
for large $n$. Hence
\begin{equation*}
\inf_{G}\left[\limsup_{n\rightarrow\infty}\, L_{n}\log P_{n}(G)\right]\, \leq\, \inf_{\tilde{G}}\left[\limsup_{n\rightarrow\infty} L_{n}\log\tilde{P}_{n}(\tilde{G})\right]\leq-\tilde{I}(\tilde{\mu}),
\end{equation*}
implying (\ref{ch05:eqn5.5.11}).
\end{proof2}

It is a consequence of Theorems~\ref{ch05:the5.5.1} and \ref{ch05:the5.5.10} that the empirical eigenvalue density of the Wishart matrix $n^{-1}T(n)^{t}T(n) (\mathrm{or}\ (2n)^{-1}\hat{T}(n)^{*}\hat{T}(n))$ converges in the weak topology to $\mu_{\lambda}$ almost surely as $n\rightarrow\infty,  \, p(n)/n\rightarrow\lambda$. Indeed, a better result is included in Proposition~\ref{ch04:pro4.4.11}.

In the rest of this section we present three more large deviation results for random matrices.


\begin{proposition}
\label{ch05:pro5.5.12}
Let $T(n)$ be an $n\times n$ standard symmetric Gaussian matrix $($distributed  according  to  $N^{(n)}(0, 1))$, and let $\lambda_{n,1}, \lambda_{n,2}, \ldots, \lambda_{n,n}$ be the eigenvalues of $T(n)$. Then the sequence of random atomic measures
\begin{equation*}
R_{n}:=\frac{\delta(\lambda_{n,1}^{2})+\delta(\lambda_{n,2}^{2})+\cdots+\delta(\lambda_{n,n}^{2})}{n}
\end{equation*}
satisfies the large deviation\index{large deviation!contraction priciple} principle in the scale $n^{-2}$ with the good rate function
\begin{equation*}
I(\mu):=-\frac{1}{2}\Sigma(\mu)+\frac{1}{4}\int x\,d\mu(x)+\frac{1}{2}\log 2-\frac{3}{4} \quad on \quad \mathcal{M}(\mathbb{R}^{+}).
\end{equation*}
The minimizer of $I$ is the distribution $u_{4}^{(1)}$ given in \emph{(\ref{ch05:eqn5.3.7})}.
\end{proposition}

\begin{proof2}
Let $P_{n}$ be the empirical eigenvalue distribution of $T(n)$ on $\mathcal{M}(\mathbb{R})$ and $Q_{n}$ the distribution of $R_{n}$ on $\mathcal{M}(\mathbb{R}^{+})$. The relation between $P_{n}$ and $Q_{n}$ is simply $P_{n} \circ T^{-1}=Q_{n}$ via the transformation $T : \mathcal{M}(\mathbb{R})\rightarrow\mathcal{M}(\mathbb{R}^{+})$ defined by $T\nu := \nu\circ\sigma^{-1}$, where $\sigma(x) :=x^{2}$. According to Theorem~\ref{ch05:the5.4.1} the sequence $P_{n}$ satisfies the large deviation principle in the scale $n^{-2}$, and the good rate function is
\begin{equation*}
I_{0}(\nu) :=-\Sigma(\mu)+\frac{1}{4}\int x^{2}\, d\nu(x)+\frac{1}{2}\log 2-\frac{3}{4}.
\end{equation*}
The \textit{contraction principle}\index{contraction principle} ([\citen{bib55}], Sec. 4.2.1) from the theory of large deviations tells us that the large deviation principle is satisfied by the sequence $Q_{n}$ as well, and the rate function $I$ on $\mathcal{M}(\mathbb{R}^{+})$ is
\begin{equation}
I(\mu)=\inf\{I_{0}(\nu):T{\nu}=\mu\}\,. \label{ch05:eqn5.5.12}
\end{equation}
For a measure $\nu\in \mathcal{M}(\mathbb{R})$, define $\tilde{\nu}\in \mathcal{M}(\mathbb{R})$ by $\tilde{\nu}(E) :=\nu(-E)$. By the convexity of $I_{0}$ (Lemma~\ref{ch05:lem5.4.2}) we have
\begin{equation*}
I_{0}\left(\frac{\nu + \tilde{\nu}}{2}\right)\leq\frac{I_{0}(\nu)+I_{0}(\bar{\nu})}{2}=I_{0}(\nu).
\end{equation*}
Hence the infimum of $I_{0}$ in (\ref{ch05:eqn5.5.12}) is reached at the unique symmetric measure $\nu_{0}$ satisfying $ T\nu_{0}=\mu$. Now
\begin{align*}
I(\mu) & \ = \ I_{0}(\nu_0) \\
& \ =\ -\Sigma(\nu_{0})+\frac{1}{4}\int x^{2}\,d\nu_{0}(x)+\frac{1}{2}\log 2-\frac{3}{4} \\
& \ = \ -\frac{1}{2}\Sigma(\mu)+\frac{1}{4}\int x\, d\mu(x)+\frac{1}{2}\log 2-\frac{3}{4}
\end{align*}
due to the properties (1) and (2) of the transformation $T$ stated above (\ref{ch05:eqn5.3.7}). Finally, Proposition~\ref{ch05:pro5.3.5} gives the assertion about the minimizer.
\end{proof2}

Let $X(n)$ be an $n\times n$ standard non-selfadjoint Gaussian random matrix. The joint eigenvalue density on $(\mathbb{R}^{+})^{n}$ of $X(n)^{*}X(n)$ is (\ref{ch05:eqn5.5.2}) with $p(n)=n$, and that of $|X(n)|=(X(n)^{*}X(n))^{1/2}$ is
\begin{equation*}
\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{n}\lambda_{i}^{2}\right)\prod_{i<j}(\lambda_{i}^{2}-\lambda_{j}^{2})^{2}\prod_{i=1}^{n}\lambda_{i}\,.
\end{equation*}
We already obtained the large deviation of the empirical eigenvalue distribution\index{quarter-circular!istribution} of $X(n)^{*}X(n)$; the good rate function is
\begin{equation*}
I(\mu):=-\Sigma(\mu)+\int x\, d\mu(x)-\frac{3}{2} \quad \mathrm{on} \quad \mathcal{M}(\mathbb{R}^{+})
\end{equation*}
(i.e. the rate function in (\ref{ch05:eqn5.5.7}) multiplied by 2 with $\lambda=1$) and the minimizer is $\mu_{1}$. According to Corollary~\ref{ch04:cor4.3.8} the limit distribution of $|X(n)|$ is the \textit{quarter-circular distribution}.\index{large deviation!quarter-circular distribution} The next proposition gives a large deviation behind this fact. The proof is a simple transformation of the above large deviation via $T : \mathcal{M}(\mathbb{R}^{+})\rightarrow \mathcal{M}(\mathbb{R}^{+})$ defined by $T\nu :=\nu\circ\sigma$ with $\sigma(x) :=x^{2}(x\in \mathbb{R}^{+})$.

\begin{proposition}
\label{ch05:pro5.5.13} Let $X(n)$ be as above. Then the empirical eigenvalue distribution of $|X(n)|$ satisfies the large deviation principle in the scale $n^{-2}$ with the good rate function
\begin{equation*}
I(\mu) :=-\iint\log|x^{2}-y^{2}|\, d\mu(x)\, d\mu(y)+\int x^{2}\, d\mu(x)-\frac{3}{2}
\end{equation*}
for $\mu\in \mathcal{M}(\mathbb{R}^{+})$. Moreover, the quarter-circular\index{Gaussian matrix!standard non-selfadjoint}\index{distribution!quarter-circular}\index{random matrix!model for quarter-circular} distribution $\frac{1}{\pi}\sqrt{4-x^{2}}\chi_{[0,2]}(x)dx$ is the unique minimizer of $I$.
\end{proposition}

There is a more directly defined \textit{random matrix model of the quarter-circular element}. For $n\in \mathbb{N}$ let $T(2n)$ be an \textit{antisymmetric}\index{Gaussian matrix!antisymmetric} $2n\times 2n$ real random matrix such that $T_{ii}(n)=0$ for $1\leq i\leq 2n,\,\{T_{ij}(n) : 1  \leq i<j\leq 2n\}$ is an independent family of Gaussian random variables with the identical distribution $N(0, 1/2n)$, and $T_{ji}(n)=-T_{ij}(n)$ for $1\leq i<j\leq 2n$.

Then the random eigenvalues of $\mathrm{i}\,T(2n)$ are given as $\pm\lambda_{n,1}, \pm\lambda_{n,2}, \ldots, \pm\lambda_{n,n}$, where $\lambda_{n,1}, \ldots, \lambda_{n,n}\geq 0$. It is known ([\citen{bib88}], Corollary 3.2.1, or [\citen{bib121}], Sec. 3.4) that the joint probability density of $\lambda_{n,1}, \lambda_{n,2}, \ldots, \lambda_{n,n}$ is
\begin{equation*}
\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{n}\lambda_{i}^{2}\right)\prod_{i<j}(\lambda_{i}^{2}-\lambda_{j}^{2})^{2}\,.
\end{equation*}
Then the large deviation of the empirical eigenvalue distribution of $(\mathrm{i}\,T(2n))_{+}$ can be proved similarly to Theorem~\ref{ch05:the5.4.3} or \ref{ch05:the5.5.1}, and the rate function is the same as in Proposition~\ref{ch05:pro5.5.13}. In this way, $(\mathrm{i}\,T(2n))_{+}$ becomes a random matrix model for the quarter-circular distribution.

Finally, let $Z(n)$ be a $p(n)\times n$ complex random matrix with $p(n)\geq n$ such that $\mathrm{Re} \, Z_{ij}(n)$ and $\mathrm{Im} \, Z_{ij}\,(n)(1\leq i\leq p(n),\, 1\leq j\leq n)$ are independent with the same distribution $N(0,1/2n)$. Let $U(n)$ be a standard $n\times n$ unitary random matrix such that $U(n)$ and $Z(n)$ are independent. According to Lemma~\ref{ch04:lem4.4.8} the non-selfadjoint random matrix $T(n) :=U(n)(Z(n)^{*}Z(n))^{1/2}$ has the joint eigenvalue density
\begin{equation*}
\frac{1}{Z_{n}}\exp\left(-n\sum_{i=1}^{n}|\zeta_{i}|^{2}\right)\prod_{i=1}^{n}|\zeta_{i}|^{2(p(n)-n)}\prod_{i<j}|\zeta_{i}-\zeta_{j}|^{2} \quad \mathrm{on} \quad  \mathbb{C}^{n}.
\end{equation*}
When $p(n)/n\rightarrow\lambda\in[1, \, \infty)$ as $n\rightarrow\infty,\,T(n)$ constitutes a random matrix model of an $R$-diagonal element $x$ such that $x^{*}x$ has the distribution $\mu_{\lambda}$. The corresponding large deviation result is the following:

\begin{proposition}
\label{ch05:pro5.5.14}
Let $T(n)$ be as above. Then the empirical eigenvalue distribution of $T(n)$ satisfies the large deviation principle in the scale $n^{-2}$ with the good rate function
\begin{align*}
I(\mu) & \ := \ -\Sigma(\mu)+\int(|\zeta|^{2}-2(\lambda-1)\log|\zeta|)\,d\mu(\zeta) \\
& \qquad \quad + \frac{3}{4}- \frac{1}{2}(3\lambda-\lambda^{2}\log\lambda+(\lambda-1)^{2}\log(\lambda-1))
\end{align*}
for $\mu\in\mathcal{M}(\mathbb{C})$. Moreover, the annular law\index{large deviation!annular law} $m_{\sqrt{\lambda-1}, {\sqrt{\lambda}}}$, i.e. the uniform distribution on the annulus $\sqrt{\lambda-1}\leq|\zeta|\leq\sqrt{\lambda}$, is the unique minimizer of $I$.
\end{proposition}

The scheme of the proof is similar to the previous (cf. the proofs of Theorems~\ref{ch05:the5.4.9} and \ref{ch05:the5.5.1}), so we omit the details. The minimizer of $I$ is a consequence of Proposition~\ref{ch05:pro5.3.8}, and the constant term of $I$ is determined from $I(m_{\sqrt{\lambda-1},\sqrt{\lambda}})=0$ and (\ref{ch05:eqn5.3.12}). Incidentally, it is noteworthy that the expressions of $\Sigma(\mu_{\lambda})$ in (\ref{ch05:eqn5.5.9}) and of $\Sigma(m_{\sqrt{\lambda-1},\sqrt{\lambda}})$ in (\ref{ch05:eqn5.3.12}) are the same up to an additive constant.

\section{Entropy and large deviations revisited}
\label{ch05:sec5.6}

This section contains some supplementary material after the many large deviation results of the previous ones. We present an example in which the rate function is the free entropy itself (up to an additive constant). This large deviation formulation yields a new characterization of free entropy.

Let $\bar{\Lambda}_{n}$ be the measure on $\mathbb{R}^{n}$ (instead of $\mathbb{R}_{\leq}^{n}$) induced from the Lebesgue measure $\Lambda_{n}$ on $M_{n}(\mathbb{C})^{sa}$. By Lemma~\ref{ch04:lem4.4.6} we know that $\bar{\Lambda}_{n}$ has the joint density
\begin{equation}
C_{n}\prod_{i<j}(t_{i}-t_{j})^{2} \quad \mathrm{with}  \quad C_{n}=\frac{(2\pi)^{n(n-1)/2}}{\prod_{j=1}^{n}j!}\,.
\label{ch05:eqn5.6.1}
\end{equation}
The Stirling formula gives
\begin{equation}
\lim_{n\rightarrow\infty}\left(\frac{1}{n^{2}}\log C_{n}+\frac{1}{2}\log n\right)=\frac{1}{2}\log(2\pi)+\frac{3}{4}.
\label{ch05:eqn5.6.2}
\end{equation}

For any $R>0$, normalizing the restriction of $\Lambda_{n}$ on $\{A\in M_{n}(\mathbb{C})^{sa}:\Vert A\Vert\leq R\}$, we have the probability measure $\lambda_{n,R}$. Then it induces the probability measure $\overline{\lambda}_{n,R}$ on $[-R, R]^{n}$ having the density
\begin{equation*}
\frac{1}{Z_{n}}\prod_{i<j}(t_{i}-t_{j})^{2},
\end{equation*}
where
\begin{align*}
Z_{n} & \ = \ \int_{-R}^{R}\cdots\int_{-R}^{R}\prod_{i<j}(t_{i}-t_{j})^{2}\,dt_{1}\cdots dt_{n} \\
& \ = \ R^{n^{2}}\int_{-1}^{1}\cdots\int_{-1}^{1}\prod_{i<j}(t_{i}-t_{j})^{2}dt_{1}\cdots dt_{n} \\
& \ = \ (2R)^{n^{2}}\prod_{j=0}^{n-1}\frac{(j+1)!(j!)^{2}}{(n+j)!}\,.
\end{align*}
By the Stirling formula we get
\begin{equation}
\lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log Z_{n}=\log\frac{R}{2}\,.
\label{ch05:eqn5.6.3}
\end{equation}
The empirical distribution on $\mathcal{M}([-R,R])$ of $\lambda_{n,R}$ is given by
\begin{equation*}
P_{n}(\Gamma):=\overline{\lambda}_{n,R}(\{x\in[-R, R]^{n}:\kappa_{n}(x)\in\Gamma\})
\end{equation*}
for Borel sets $\Gamma\subset\mathcal{M}([-R, R])$.

Now, the next proposition is a version of Theorem~\ref{ch05:the5.4.3} where probability measures are restricted to those supported on $[-R, R]$ with $\beta=1$ and $Q(x)=0$. In fact, Theorem~\ref{ch05:the5.4.3} holds true when we take
\begin{equation*}
Q(x)=\left\{\begin{array}{ll}
0 & \mathrm{if}\ |x|\leq R,\\
+\infty & \mathrm{if}\ |x|>R,
\end{array}\right.
\end{equation*}
though $Q(x)$ is not continuous.

\begin{proposition}
\label{ch05:pro5.6.1}
The sequence $P_{n}$ given above satisfies the large deviation principle in the scale $n^{-2}$ with the rate function
\begin{equation*}
I(\mu):=-\Sigma(\mu)+\log\frac{R}{2} \quad on \quad  \mathcal{M}([-R, R]).
\end{equation*}
\end{proposition}

The following theorem of Voiculescu expresses the double logarithmic integral $\Sigma(\mu)$ as a triple limit of volumes. Note that if one chooses the normalization $C_{n}=1$ in (\ref{ch05:eqn5.6.1}), then the formulas become simpler: The terms $\frac{1}{2}\log n$ in (\ref{ch05:eqn5.6.4}) and $\frac{1}{2} \log (2 \pi) + \frac{3}{4}$ in (\ref{ch05:eqn5.6.5}) disappear.

\begin{theorem}
\label{ch05:the5.6.2}
If $\mu$  is a  probability measure supported in $[-R, R]$, then the limit
\begin{align}
\chi_{R}(\mu;r,\varepsilon) & :=\lim_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\bar{\Lambda}_{n}(\{x\in[-R, R]^{n}:\right. \notag\\
& \qquad \quad \left.|m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\})+\frac{1}{2}\log n\right]
\label{ch05:eqn5.6.4}
\end{align}
exists for every $r\in \mathbb{N}$ and $\varepsilon >0$, and
\begin{equation}
\lim_{\begin{subarray}{l}{r\rightarrow\infty} \\ \varepsilon\rightarrow+0 \\ \end{subarray}} \chi_{R}(\mu;r, \varepsilon)=\Sigma(\mu)+\frac{1}{2}\log(2\pi)+\frac{3}{4}\,.
\label{ch05:eqn5.6.5}
\end{equation}
\end{theorem}

\begin{proof2}
Given $\mu$ with $\mathrm{supp}\, \mu\subset[-R, R],\,r\in \mathbb{N}$ and $\varepsilon>0$, take the closed set $F(r, \varepsilon)$ and the open set $G(r, \varepsilon)$ in $\mathcal{M}([-R, R])$ as in the proof of Proposition~\ref{ch05:pro5.1.1}. Then the large deviation of Proposition~\ref{ch05:pro5.6.1} says that
\begin{align*}
& \limsup_{n\rightarrow\infty} \frac{1}{n^{2}}\log\bar{\lambda}_{n,R}(\{x\in[-R,R]^{n} : \kappa_{n}(x)\in F(r, \varepsilon)\}) \\
& \qquad \leq \sup\{\Sigma(\nu):\nu \in F(r,\, \varepsilon)\}-\log\frac{R}{2}, \\
& \liminf_{n\rightarrow\infty} \frac{1}{n^{2}}\log\bar{\lambda}_{n,R}(\{x\in[-R,R]^{n} : \kappa_{n}(x)\in G(r, \varepsilon)\}) \\
& \qquad \geq\sup\{\Sigma(\nu): \nu \in G(r, \varepsilon)\}-\log\frac{R}{2}\,.
\end{align*}
It is easy to check that
\begin{equation*}
\sup\{\Sigma(\nu) :\nu \in F(r, \varepsilon)\}=\sup\{\Sigma(\nu): \nu \in G(r, \varepsilon)\}\,.
\end{equation*}
Hence we have
\begin{align*}
& \lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log\bar{\lambda}_{n,R}(\{x\in[-R, R]^{n} : \kappa_{n}(x)\in F(r, \varepsilon)\}) \\
& \qquad =\sup\{\Sigma(\nu): \nu \in F(r, \varepsilon)\}-\log\frac{R}{2}\,.
\end{align*}
Note that the restriction of $\bar{\Lambda}_{n}$ on $[-R, R]^{n}$ is equal to $C_{n}Z_{n}\bar{\lambda}_{n,R}$. So, thanks to (\ref{ch05:eqn5.6.2}) and (\ref{ch05:eqn5.6.3}), we have
\begin{align*}
& \lim_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\bar{\Lambda}_{n} (\{x\in[-R,R]^{n} : \kappa_{n}(x)\in F(r, \varepsilon)\})+\frac{1}{2}\log n\right] \\
& \qquad =\sup\{\Sigma(\nu):\nu \in F(r, \varepsilon)\}+\frac{1}{2}\log(2\pi)+\frac{3}{4}\,.
\end{align*}
Letting $ r\rightarrow\infty$ and $\varepsilon \rightarrow+0$ yields the conclusion.
\end{proof2}

The left-hand side of (\ref{ch05:eqn5.6.5}) is Voiculescu's second definiton of free entropy for a probability measure or a selfadjoint noncommutative random variable. (He used the notation $\chi(\mu).)$ The extension to $N$-tuples of variables will be discussed in the next chapter.

When $\bar{\nu}_{n}$ has the density (\ref{ch05:eqn5.2.2}) and $\mu\in \mathcal{M}([-R, R])$, one can apply Theorem~\ref{ch05:the5.4.1} (also Lemma~\ref{ch05:lem5.4.2}) to prove that
\begin{align}
& \lim_{\begin{subarray}{l}{r\rightarrow\infty} \\ \varepsilon\rightarrow+0 \\ \end{subarray}} \lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log\bar{\nu}_{n}(\{\lambda\in[-R, R]^{n}:|m_{k}(\kappa_{n}(\lambda))-m_{k}(\mu)|\leq \varepsilon, \, k\leq r\}) \notag \\
& \qquad \quad =\beta\Sigma(\mu)-\frac{1}{4\sigma^{2}}\int x^{2}\, d\mu(x)-\frac{\beta}{2}\log(2\beta\sigma^{2})+\frac{3\beta}{4}
\label{ch05:eqn5.6.6}
\end{align}
(as well as the formulas in Theorem~\ref{ch05:the5.2.2}), along the same lines as the proofs of Proposition~\ref{ch05:pro5.1.1} and the above theorem. In this way, Theorem~\ref{ch05:the5.2.2} and (\ref{ch05:eqn5.6.6}) have a strong resemblance to Proposition~\ref{ch05:pro5.1.1}, while Theorem~\ref{ch05:the5.6.2} is similar to (\ref{ch05:eqn5.1.8}).

We have a variant of Theorem~\ref{ch05:the5.6.2} where $\Lambda_{n}$ is replaced by the measure $\Lambda_{+,n}$ on $M_{n}(\mathbb{C})^{+}$ in Lemma~\ref{ch04:lem4.4.7}. Let $\bar{\Lambda}_{+,n}$ be the measure on $(\mathbb{R}^{+})^{n}$ induced from $\Lambda_{+,n}$. According to (\ref{ch04:eqn4.4.4}) the joint desnity of $\bar{\Lambda}_{+,n}$ is
\begin{equation*}
C_{n}\prod_{i<j}(t_{i}-t_{j})^{2} \quad \mathrm{with}  \quad C_{n}=\frac{\pi^{n^{2}}n!}{(\prod_{j=1}^{n}j!)^{2}}\,,
\end{equation*}
and we have
\begin{equation}
\lim_{n\rightarrow\infty}\left(\frac{1}{n^{2}}\log C_{n}+\log n\right)=\log\pi+\frac{3}{2}\,.
\label{ch05:eqn5.6.7}
\end{equation}
By Lemma~\ref{ch04:lem4.4.7} we notice that $\bar{\Lambda}_{+,n}$ on $[0, R]^{n}$ is nothing but the translation of $\bar{\Lambda}_{n}$ restricted on $[-R/2, R/2]^{n}$ to $[0, R]^{n}$ with a different normalizing constant. So, together with (\ref{ch05:eqn5.6.7}), Theorem~\ref{ch05:the5.6.2} yields the following: If $\mu$ is a probability meausre supported in $[0, R]$, then the limit
\begin{align}
\chi_{+,R}(\mu;r, \varepsilon) & :=\lim_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\bar{\Lambda}_{+,n}(\left\{x\in[0, R]^{n}\right.\right.: \notag \\
& \qquad \left. |m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon,\, k\leq r\})+\log n\right]
\label{ch05:eqn5.6.8}
\end{align}
exists for every $r\in \mathbb{N}$ and $\varepsilon >0$, and
\begin{equation}
\lim_{\begin{subarray}{c}{r\rightarrow\infty} \\ \varepsilon\rightarrow+0 \\ \end{subarray}} \chi_{+,R}(\mu;r, \varepsilon)=\Sigma(\mu)+\log\pi+\frac{3}{2}.
\label{ch05:eqn5.6.9}
\end{equation}
Finally, for later use, we note that
\begin{align}
& \bar{\Lambda}_{n}(\{x\in[-R, R]^{n} : |m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon, \ k\leq r\}) \notag \\
& =\Lambda_{n}(\{A\in M_{n}(\mathbb{C})^{sa} :  \Vert A\Vert\leq R,\ |\mathrm{tr}_{n}(A^{k})-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\}),
\label{ch05:eqn5.6.10}\\
& \bar{\Lambda}_{+,n}(\{x\in[0,\, R]^{n} : |m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon,\  k\leq r\}) \notag \\
& =\Lambda_{+,n}(\{A\in M_{n}(\mathbb{C})^{+}:\Vert A\Vert\leq R,\ |\mathrm{tr}_{n}(A^{k})-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\}).
\label{ch05:eqn5.6.11}
\end{align}

\subsection*{Notes and Remarks} The history of the Boltzmann-Gibbs entropy started at the end of the previous century. The summary on this subject in Sec.~\ref{ch05:sec5.1} is not at all historical. The relative entropy was introduced much later, in the 1950's by Kullback and Leibler. Although the Boltzmann-Gibbs entropy is discussed in most books on statistical mechanics, the relative entropy is not.

The entropic central limit theorem mentioned in the introduction to this chapter is found in [\citen{bib14}].

The general abstract framework of \textit{large deviation}\index{large deviation} was proposed by S.R.S. Varadhan in 1966, although the topic may be traced back much earlier. Cram\'{e}r's theorem for independent identically distributed variables was published in 1938, and the level-2 extension of Sanov was in 1957. One of the first systematic introductions to the theory is the book [\citen{bib73}]. The terminology of three levels was used there. We suggest also the monographs [\citen{bib55}] and [\citen{bib56}] on large deviations. [\citen{bib53}] is a standard reference in information theory; it explains basic properties of the Boltzmann-Gibbs entropy and treats its maximization under constraints. The \textit{maximum entropy} approach to random matrices\index{maximum entropy!random matrices} goes back to Balian [\citen{bib11}]. Classical orthogonal polynomial matrix ensembles are discussed in [\citen{bib121}], Sec. 19.3. If we take the choice
\begin{equation*}
V(x)=\frac{\Gamma(p/2)\Gamma(1/2)}{\Gamma(p+1/2)}|x|^{p}
\end{equation*}
in (\ref{ch05:eqn5.2.8}), then the limiting eigenvalue density is the \textit{Ullman distribution}\index{distribution!Ullman}\index{Ullman distribution} $v_{1}^{(p)}$ discussed in Sec.~\ref{ch05:sec5.3}.

Voiculescu's free entropy (\ref{ch05:eqn5.2.7}) was introduced in [\citen{bib202}], based on heuristic arguments (called Voiculescu's heuristics) about the asymptotics of the Boltzmann-Gibbs entropy of random matrices. (A more rigorous derivation of Voiculescu's heuristics can be found in [\citen{bib15}].) Theorem~\ref{ch05:the5.2.2} is essentially Proposition 4.5 from [\citen{bib203}] together with its original proof, while Theorem~\ref{ch05:the5.6.2} is Voiculescu's original form. Our formulation is slightly different because we use here the joint distribution of the eigenvalues of a standard symmetric (or selfadjoint) Gaussian matrix (or more generally, (\ref{ch05:eqn5.2.2})) as a reference measure. On the other hand, $\mu$ is assumed as in [\citen{bib203}] to have a compact support.

The proof of Theorem~\ref{ch05:the5.6.2} (also Theorem~\ref{ch05:the5.2.2}) via the large deviation principle has some advantages. On the one hand, it contains the existence of the limit as $ n\rightarrow\infty$ in (\ref{ch05:eqn5.6.4}) (also in (\ref{ch05:eqn5.6.6})). On the other hand, the slightly complicated Lemma~\ref{ch04:lem4.3.4} is not needed.

The logarithmic energy plays an important role in potential theory [\citen{bib114}]. The generalized Frostman method for weighted potentials in Theorem~\ref{ch05:the5.3.3} gives a quite useful device for solving the maximizer problem for a free entropy functional. The book [\citen{bib165}] was published during the preparation of our manuscript and contains all that we need here about logarithmic potentials. Some of the maximization problems for free entropy treated in Sec.~\ref{ch05:sec5.3} were studied in [\citen{bib100}]. Proposition~\ref{ch05:pro5.3.10} is taken from [\citen{bib94}].

The remark about the Ullman distribution stated before Proposition~\ref{ch05:pro5.3.4} is from [\citen{bib195}], p. 106. That reference contains more information about the Ullman measure and a section about Gaussian random matrices. We point out that the density of the roots of the Jacobi polynomials converges to the arcsine law.\index{arcsine law}\index{law!arcsine} More generally, if the interval $[-1,1]$ is the support of a measure, then the density of the roots of the corresponding orthogonal polynomials tends to the \textit{arcsine law}. This follows from the Erd\H{o}s-Tur\'{a}n theorem; see the above-mentioned [\citen{bib195}].

The method in proving Lemmas~\ref{ch05:lem5.4.2}, \ref{ch05:lem5.4.4} and \ref{ch05:lem5.4.5} is essentially the same as Ben Arous and Guionnet's in [\citen{bib15}]. They proved the first large deviation result (Theorem 5.4.1) for random matrices, although Voiculescu's paper already had many indications that large deviation should hold. (One observes this by comparison of Theorem~\ref{ch05:the5.2.2} with the conditions in (\ref{ch05:eqn5.4.4}).) The method of proving Lemma~\ref{ch05:lem5.4.6} (as well as Theorem~\ref{ch05:the5.2.2}) is similar to Voiculescu's in [\citen{bib203}]. Theorem~\ref{ch05:the5.4.9} is from [\citen{bib149}]; a special case is the large deviation for the elliptic Gaussian matrix $Y(n)$ in (\ref{ch04:eqn4.1.23}), and its limit distribution is Girko's elliptic law [\citen{bib86}], [\citen{bib90}]. The paper [\citen{bib16}] is about the large deviation related to the \textit{real non-symmetric Gaussian matrix},\index{large deviation!for real non-symmetric Gaussian matrix}\index{Gaussian matrix!real non-symmetric} another random matrix model of the \textit{circular law}.\index{law!circular}\index{circular!law} Theorem~\ref{ch05:the5.4.10} was proved in [\citen{bib101}] in detail. Theorem~\ref{ch05:the5.5.10} has the interesting feature that the minimizer of the rate function has an atom. The approach to large deviations obtained in Sections~\ref{ch05:sec5.4} and \ref{ch05:sec5.5} is based on the explicit form of the joint distribution of the eigenvalues, and does not extend to more general examples of random matrices.


\chapter{Free Entropy of Noncommutative Random Variables}
\label{ch06:chap06}

\noindent Although the previous chapter focused on large deviations for random matrices, Voiculescu's free entropy emerged from the discussion. The scheme of the passage from the classical Boltzmann-Gibbs entropy to the new concept is the following. If $\mu$ is a measure supported in a finite interval $[-R, R]$, then its entropy $S(\mu)$ is a limit of volumes:
\begin{equation*}
\lim_{\begin{subarray}{c}{r\rightarrow\infty} \\ \varepsilon\rightarrow+0 \\ \end{subarray}} \lim_{n\rightarrow\infty}\frac{1}{n}\log\lambda^{n}(\{x\in[-R, R]^{n}:|m_{k}(\kappa_{n}(x))-m_{k}(\mu)|\leq\varepsilon,\ k\leq r\}),
\end{equation*}
where $\lambda^{n}$ is the $n$-dimensional Lebesgue measure, $m_{k}$ denotes the $k$th moment and $\kappa_{n}(x)$ stands for the atomic measure $(\delta(x_{1})+\delta(x_{2})+\cdots+\delta(x_{n}))/n$. When $a$ is a selfadjoint random variable in a noncommutative probability space $(\mathcal{A}, \varphi)$ , it was Voiculescu's idea to modify the classical entropy formula into
\begin{equation*}
\lim_{\begin{subarray}{c}{r\rightarrow\infty} \\ \varepsilon\rightarrow+0 \\ \end{subarray}} \limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log \nu_{n}(\{A\in M_{n}(\mathbb{C})^{sa}:|\mathrm{tr}_{n}(A^{k})-\varphi(a^{k})|\leq\varepsilon,\ k\leq r\})\, ,
\end{equation*}
where now $\nu_{n}$ is an appropriate measure on the space of $n\times n$ selfadjoint matrices.

The emphasis in this chapter is on the multivariate case. The first formula admits an obvious extension to measures on $\mathbb{R}^{N}$ if one uses joint moments, and the classical entropy
\begin{equation*}
-\int_{\mathbb{R}^{N}}f(x)\log f(x)\, dx
\end{equation*}
of a measure $\mu$ with density $f(x)$ is achieved. In this chapter the similar generalization of the second formula is performed: An $N$-tuple $(a_{1}, \ldots, a_{N})$ of noncommutative random variables is approximated in distribution by $N$-tuples $(A_{1}, \ldots, A_{N})$ of selfadjoint matrices, and the multivariate free entropy $\chi(a_{1}, \ldots, a_{N})$ is introduced.

One of the fundamental observations is the fact that the multivariate free entropy $\chi(a_{1}, \ldots, a_{N})$ is additive when the variables $a_{1}, \ldots, a_{N}$ are in free relation: $\chi(a_{1}, \ldots, a_{N})=\chi(a_{1})+\cdots+\chi(a_{N})$. The converse is also true: $a_{1}, \ldots, a_{N}$ are in free relation when this additivity holds with finite $\chi(a_{i})$. On the one hand, this property justifies the terminology, and on the other hand it ensures that the generalization from the Boltzmann-Gibbs entropy is on the right track. The additivity of free entropy under freeness is to be compared with the additivity of Boltzmann-Gibbs entropy under stochastic independence.

Voiculescu's definition of $\chi(a_{1}, \ldots, a_{N})$ is easily modified to introduce free entropies for other types of random variables; for instance, the free entropies $\hat{\chi}(a_{1}, \ldots, a_{N})$ of non-selfadjoint $a_{1}, \ldots, a_{N}$ and $\chi_{u}(u_{1}, \ldots, u_{N})$ of unitary $u_{1}, \ldots, u_{N}$. The study of $\hat{\chi}$ and $\chi_{u}$ is the subject of the second half of the chapter. An important point is that the three types of free entropies $\chi,\hat{\chi}, \chi_{u}$ are interrelated, in particular, when their polar decompositions are considered.

\section{Definition and basic properties}
\label{ch06:sec6.1}

\noindent In this section the free entropy of an $N$-tuple of noncommutative selfadjoint random variables is defined through approximation in distribution by matrices. The subadditivity and upper semicontinuity of free entropy are proven.

The set $M_{n}(\mathbb{C})$ of all matrices is a noncommutative probability space with respect to the normalized trace $\mathrm{tr}_{n}$. Consequenly, we can speak of joint moments, or more generally the joint distribution of an $N$-tuple of matrices. In this section we restrict ourselves to the selfadjoint ones. There is a natural linear bijection between the set $M_{n}(\mathbb{C})^{sa}$ of all selfadjoint matrices and $\mathbb{R}^{n^{2}}$ which is an isometry for the Hilbert-Schmidt and Euclidean norms. The Lebesgue measure of $\mathbb{R}^{n^{2}}$ induces the measure $\Lambda_{n}$ on $M_{n}(\mathbb{C})^{sa}$ under the above bijection. In this chapter, for $n, N\in \mathbb{N}$ the $N$-fold product measure $\Lambda_{n}^{\otimes N}$ on the product space $(M_{n}(\mathbb{C})^{sa})^{N}$ will be denoted simply by $\Lambda$.

Let $(\mathcal{M}, \tau)$ be a tracial $W^{*}$-probability space; that is, $\mathcal{M}$ is a von Neumann algebra with a faithful normal tracial state $\tau$. Throughout the chapter we consider such a noncommutative probability space. Let $a_{1}, \ldots, a_{N}\in\mathcal{M}^{sa}$, where $\mathcal{M}^{sa}$ denotes the space of selfadjoint elements of $\mathcal{M}$. For $n, r\in \mathbb{N},\, \varepsilon >0$ and $R>0$, define the following quantity:
\begin{align*}
&\Gamma_{R}(a_{1}, \ldots, a_{N};n, r, \varepsilon) :=\{(A_{1}, \ldots, A_{N})\in(M_{n}(\mathbb{C})^{sa})^{N} : \Vert A_{i}\Vert\leq R, \\
&\quad |\mathrm{tr}_{n}(A_{i_{1}}\cdots A_{i_{k}})-\tau(a_{i_{1}}\cdots a_{i_{k}})|\leq\varepsilon  \ \mathrm{for \ all} \
 1\leq i_{1}, \ldots, i_{k}\leq N, \, 1\leq k\leq r\}.
\end{align*}
Moreover, define
\begin{align*}
& \chi_{R}(a_{1}, \ldots, a_{N};r, \varepsilon) :=\underset{n\rightarrow\infty}{\limsup} \left[\frac{1}{n^{2}}\log\Lambda(\Gamma_{R}(a_{1}, \ldots, a_{N};n, r, \varepsilon))+\frac{N}{2}\log n\right],\\
& \chi_{R}(a_{1}, \ldots, a_{N}):= \lim_{\begin{subarray}{c}{r\rightarrow\infty} \\ \varepsilon\rightarrow+0 \\ \end{subarray}} \chi_{R}(a_{1}, \ldots, a_{N};r, \varepsilon)=\inf_{\begin{subarray}{l}{r\in \mathrm{N}} \\ \varepsilon >0  \\ \end{subarray}} \chi_{R}(a_{1}, \ldots, a_{N};r, \varepsilon), \\
& \chi(a_{1},\ldots, a_{N}):=\sup_{R>0}\chi_{R}(a_{1}, \ldots, a_{N}).
\end{align*}
Then $\chi(a_{1}, \ldots, a_{N})$ is called the \textit{free entropy} of the $N$-tuple $(a_{1}, \ldots, a_{N})$.

In the case of a single $a\in\mathcal{M}^{sa}$, if $\mu$ is the distribution measure of $a$ (with respect to $\tau$), then we have
\begin{align*}
& \Gamma_{R}(a;n, r, \varepsilon) \\
& \qquad =\{A\in M_{n}(\mathbb{C})^{sa} : \Vert A\Vert\leq R, \ |\mathrm{tr}_{n}(A^{k})-m_{k}(\mu)|\leq\varepsilon,\ 1\leq k\leq r\}.
\end{align*}
Hence Theorem~\ref{ch05:the5.6.2} together with (\ref{ch05:eqn5.6.10}) says that for any $ R\geq\Vert a\Vert$
\begin{equation}
\chi_{R}(a;r, \varepsilon)=\lim_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\Lambda(\Gamma_{R}(a;n, r, \varepsilon))+\frac{1}{2}\log n\right] \label{ch06:eqn6.1.1}
\end{equation}
($\lim\sup$ becomes $\lim$ in this case) and
\begin{equation}
\chi(a)=\chi_{R}(a)=\Sigma(\mu)+\frac{1}{2}\log(2\pi)+\frac{3}{4}.
\label{ch06:eqn6.1.2}
\end{equation}
In this way, the free entropy\index{entropy!free, multivariate}\index{free entropy!multivariate} $\chi(a)$ defined above coincides with $\Sigma(\mu)$ treated in the previous chapter, up to an additive constant. (Note that this constant could be easily removed by an appropriate renormalization of the measure $\Lambda_{n}$, as was stated just before Theorem~\ref{ch05:the5.6.2}.) We also write $\Sigma(a)$ for $\Sigma(\mu)$.

The first result is the \textit{subadditivity} of free entropy.\index{subadditivity of free entropy}\index{free entropy!subadditivity}

\begin{proposition}
\label{ch06:pro6.1.1}
If $C:=\tau(a_{1}^{2}+\cdots+a_{N}^{2})$, then
\begin{equation}
\chi(a_{1}, \ldots, a_{N})\leq\chi(a_{1})+\cdots+\chi(a_{N})\leq\frac{N}{2}\log\frac{2\pi eC}{N}.
\label{ch06:eqn6.1.3}
\end{equation}
\end{proposition}

\begin{proof2}
It is obvious that
\begin{equation*}
\Gamma_{R}(a_{1}, \ldots, a_{N};n, r, \varepsilon)\subset\prod_{i=1}^{N}\Gamma_{R}(a_{i};n, r, \varepsilon),
\end{equation*}
and hence
\begin{equation*}
\Lambda(\Gamma_{R}(a_{1}, \ldots , a_{N};n, r, \varepsilon))\leq\prod_{i=1}^{N}\Lambda(\Gamma_{R}(a_{i};n, r, \varepsilon)).
\end{equation*}
Therefore we have
\begin{equation*}
\chi_{R}(a_{1}, \ldots, a_{N};r, \varepsilon)\leq\sum_{i=1}^{N}\chi_{R}(a_{i};r, \varepsilon),
\end{equation*}
and $\chi_{R}(a_{1}, \ldots, a_{N})\leq\sum_{i=1}^{N}\chi_{R}(a_{i})$ follows. This gives the first inequality in (\ref{ch06:eqn6.1.3}). According to (\ref{ch06:eqn6.1.2}) and Proposition~\ref{ch05:pro5.3.4}, it is known that $\chi(a)\leq\frac{1}{2}\log(2\pi eC_{0})$ for every $a\in \mathcal{M}^{sa}$ with $\tau(a^{2})\leq C_{0}$. Therefore, putting $C_{i}\,:=\tau(a_{i}^{2})$, we have
\begin{equation*}
\sum_{i=1}^{N}\chi(a_{i})\leq\frac{1}{2}\sum_{i=1}^{N}\log(2\pi eC_{i})\leq\frac{N}{2}\log\left(\frac{2\pi e}{N}\sum_{i=1}^{N}C_{i}\right)=\frac{N}{2}\log\frac{2\pi eC}{N}.
\end{equation*}
The second inequality above is due to the concavity of the logarithmic function.
\end{proof2}

\begin{corollary}
\label{ch06:cor6.1.2}
If $\chi(a_{1}, \ldots, a_{N}) > -\infty$, then the distribution of each $a_{i}$ is nonatomic.
\end{corollary}

\begin{proof2}
By the above proposition we get $\chi(a_{i})>-\infty$ for all $i$. Hence, by (\ref{ch06:eqn6.1.2}), $\Sigma(\mu_{i})>-\infty$ for the distribution $\mu_{i}$ of $X_{i}$, and the result follows.
\end{proof2}

\begin{proposition}
\label{ch06:pro6.1.3}
For every $1\leq L<N$,
\begin{equation*}
\chi(a_{1}, \ldots, a_{N})\leq\chi(a_{1}, \ldots, a_{L})+\chi(a_{L+1}, \ldots, a_{N}).
\end{equation*}
\end{proposition}

\begin{proof2}
The result follows immediately from the obvious inclusion
\begin{equation*}
\Gamma_{R}(a_{1}, \ldots, a_{N};n, r, \varepsilon)\subset\Gamma_{R}(a_{1}, \ldots , a_{L};n, r, \varepsilon)\times\Gamma_{R}(a_{L+1}, \ldots, a_{N};n, r, \varepsilon).
\end{equation*}
(Note that the sum of free entropies is always well-defined, due to the estimate in Proposition~\ref{ch06:pro6.1.1}.)
\end{proof2}

For the single variable case we observed in (\ref{ch06:eqn6.1.2}) that $\chi(a)=\chi_{R}(a)$ whenever $ R\geq\Vert a\Vert$. The multivariable case is quite similar.

\begin{proposition}
\label{ch06:pro6.1.4}
$\chi(a_{1}, \ldots, a_{N})=\chi_{R}(a_{1}, \ldots , a_{N})$ whenever $R>\Vert a_{i}\Vert$ for every $1\leq i\leq N$.
\end{proposition}

\begin{proof2}
Let $\rho :=\max_{i}\Vert a_{i}\Vert$. It suffices to show that
\begin{equation*}
\chi_{R}(a_{1}, \ldots, a_{N})\geq\chi_{R_{1}}(a_{1}, \ldots, a_{N}) \quad \mathrm{if}\quad \rho<R<R_{1}.
\end{equation*}
Let $\rho<R_{0}<R<R_{1}<R_{2}$, and define $f : [-R_{2}, R_{2}]\rightarrow[-R, R]$ by
\begin{equation*}
f(t):=\left\{\begin{array}{ll}
\dfrac{R-R_{0}}{R_{2}-R_{0}}(t+R_{0})-R_{0} & \mathrm{if}-R_{2}\leq t\leq-R_{0},\\
\quad t & \mathrm{if}-R_{0}\leq t\leq R_{0},\\
\dfrac{R-R_{0}}{R_{2}-R_{0}}(t-R_{0})+R_{0} & \mathrm{if} \quad  R_{0}\leq t\leq R_{2},
\end{array}\right.
\end{equation*}
Write $F(A_{1}, \ldots, A_{N}) :=(f(A_{1}), \ldots, f(A_{N}))$ for $(A_{1}, \ldots, A_{N})\in(M_{n}(\mathbb{C})^{sa})^{N}$ with $\Vert A_{i}\Vert\leq R_{2}$. Let us show that for every $r\in \mathbb{N}$ and $\varepsilon >0$ there exist an integer $r_{1}\geq r$ and an $\varepsilon_{1}>0$ such that
\begin{equation}
F(\Gamma_{R_{1}}(a_{1}, \ldots, a_{N};n, r_{1},\varepsilon_{1}))\subset\Gamma_{R}(a_{1}, \ldots, a_{N};n, r, \varepsilon) \qquad (n\in \mathbb{N}). \label{ch06:eqn6.1.4}
\end{equation}
Let $\varepsilon_1 :=\varepsilon /2$ and $0<\delta<\varepsilon/2r(R_{2}+1)^{r-1}$, and choose an even integer $r_{1}\geq r$ such that
\begin{equation*}
\frac{\rho^{r_{1}}+\varepsilon_{1}}{R_{0}^{r_{1}}}(R_{2}-R)<\delta.
\end{equation*}
For any $n\in \mathbb{N}$ and $(A_{1}, \ldots, A_{N})\in\Gamma_{R_{1}}(a_{1}, \ldots, a_{N};n, r_{1}, \varepsilon_{1})$, since
\begin{align*}
\rho^{r_{1}}+\varepsilon_{1}  & \ \geq \ \tau(a_{i}^{r_{1}})+\varepsilon_{1}\geq \mathrm{tr}_{n}(A_{i}^{r_{1}}) \\
& \ \geq\ \int_{R_{0}\leq|t|\leq R_{2}}t^{r_{1}}\, d\,\mathrm{tr}_{n}(E_{A_{i}}(t)) \\
& \ \geq \ R_{0}^{r_{1}}\mathrm{tr}_{n}(E_{A_{i}}([-R_{2}, -R_{0}]\cup[R_{0}, R_{2}])) ,
\end{align*}
where $E_{A_{i}}$ is the spectral measure of $A_{i}\ (1\leq i\leq N)$, we have
\begin{align}
& \mathrm{tr}_{n}(|f(A_{i})-A_{i}|) =\int|f(t)-t|\, d\,\mathrm{tr}_{n}(E_{A_{i}}(t)) \notag \\
& \quad \leq(R_{2}-R)\mathrm{tr}_{n}(E_{A_{i}}([-R_{2},\, -R_{0}]\cup[R_{0}, R_{2}]))<\delta.
\label{ch06:eqn6.1.5}
\end{align}
Hence, for every $1\leq k\leq r$ and $1\leq i_{1}, \ldots, i_{k}\leq N$, it is easy to check that
\begin{equation*}
|\mathrm{tr}_{n}(f(A_{i_{1}})\cdots f(A_{i_{k}}))-\mathrm{tr}_{n}(A_{i_{1}}\cdots A_{i_{k}})|\leq kR_{2}^{k-1}\delta<\frac{\varepsilon}{2}.
\end{equation*}
This implies that
\begin{equation*}
|\mathrm{tr}_{n}(f(A_{i_{1}})\cdots f(A_{i_{k}}))-\tau(a_{i_{1}}\cdots a_{i_{k}})|<\varepsilon,
\end{equation*}
and (\ref{ch06:eqn6.1.4}) follows.

Next we compute the Radon-Nikod\'{y}m derivative of the measure $\Lambda\circ f$ with respect to $\Lambda$ restricted on $\{A\in M_{n}(\mathbb{C})^{sa} : \Vert A\Vert\leq R_{1}\}$. We proceed as in the proof of Lemma~\ref{ch04:lem4.1.2} (also Lemma~\ref{ch04:lem4.4.6}), and make the coordinate change $ A\in M_{n}(\mathbb{C})^{sa}\leftrightarrow (U, D)\in \mathcal{U}(n)/T\times \mathbb{R}_{\leq}^{n}$ by the diagonalization $A=UDU^{*},\,D= \mathbf{Diag} (t_{1}, t_{2}, \ldots, t_{n})$ with $t_{1}<t_{2}<\ldots<t_{n}$ (the other degenerate case is negligible). Since
\begin{equation*}
dA=U(dD+D\cdot dM-dM\cdot D)U^{*},
\end{equation*}
where $dM :=-\mathrm{i}U^{*}\cdot dU$ is an infinitesimal Hermitian, we have
\begin{equation*}
\prod_{i\leq j}d(\mathrm{Re} \, A_{ij})\prod_{i<j}d(\mathrm{Im} \, A_{ij})=\prod_{i<j}(t_{i}-t_{j})^{2}\prod_{i=1}^{n}\, dt_{i}\prod_{i<j}d(\mathrm{Re} \, M_{ij})d(\mathrm{Im} \, M_{ij}).
\end{equation*}
Let $\Vert A\Vert\leq R_{1}$. Since $f(A)=Uf(D)U^{*}$, we have
\begin{equation*}
df(A)=U(f'(D)dD+f(D)\cdot dM-dM \cdot f(D))U^{*},
\end{equation*}
so that
\begin{align*}
& \prod_{i\leq j}d(\mathrm{Re} f(A)_{ij})\prod_{i<j}d(\mathrm{Im} \, f(A)_{ij}) \\
& \qquad =\prod_{i<j}(f(t_{i})-f(t_{j}))^{2}\prod_{i=1}^{n}f'(t_{i})\prod_{i=1}^{n}dt_{i}\prod_{i<j}d(\mathrm{Re} \, M_{ij})\,d(\mathrm{Im} \, M_{ij}).
\end{align*}
Taking the ratio of the above two densities yields
\begin{equation}
\frac{d(\Lambda\circ f)}{d\Lambda}(A)=\prod_{i<j}\left(\frac{f(t_{i})-f(t_{j})}{t_{i}-t_{j}}\right)^{2}\prod_{i=1}^{k}f'(t_{i})
\label{ch06:eqn6.1.6}
\end{equation}
almost everywhere on $\{A\in M_{n}(\mathbb{C})^{sa}:\Vert A\Vert\leq R_{1}\}$.

Note that
\begin{equation*}
\left|\frac{f(t_{i})-f(t_{j})}{t_{i}-t_{j}}\right|\left\{\begin{array}{ll}
 =1 & \mathrm{if}\ t_{i}, t_{j}\in(-R_{0},R_{0}),\\
 \geq\dfrac{R-R_{0}}{R_{2}-R_{0}} & \mathrm{otherwise}.
\end{array}\right.
\end{equation*}
If $A\in M_{n}(\mathbb{C})^{sa}$ satisfies $\mathrm{tr}_{n}(E_{A}([-R_{2}, -R_{0}]\cup[R_{0}, R_{2}]))<\delta'$, then the number of eigenvalues of $A$ contained in $[-R_{2}, -R_{0}]\cup[R_{0}, R_{2}]$ is less than $n\delta'$. Hence for every $(A_{1}, \ldots, A_{N})\in\Gamma_{R_{1}}(a_{1}, \ldots, a_{N};n, r_{1}, \varepsilon_1)$, by (\ref{ch06:eqn6.1.5}) we have
\begin{equation*}
\frac{d(\Lambda\circ f)}{d\Lambda}(A_{i})\geq\left(\frac{R-R_{0}}{R_{2}-R_{0}}\right)^{2n^{2}\delta/(R_{2}-R)} \qquad (1\leq i\leq N).
\end{equation*}
Therefore, by (\ref{ch06:eqn6.1.4}) we infer that
\begin{align*}
& \Lambda(\Gamma_{R}(a_{1}, \ldots, a_{N};n, r, \varepsilon)) \\
& \quad \ \geq\left(\frac{R-R_{0}}{R_{2}-R_{0}}\right)^{2Nn^{2}\delta/(R_{2}-R)}\Lambda(\Gamma_{R_{1}}(a_{1}, \ldots, a_{N};n, r_{1}, \varepsilon_{1})) ,
\end{align*}
which implies that
\begin{equation*}
\chi_{R}(a_{1}, \ldots, a_{N};r, \varepsilon)\geq\chi_{R_{1}}(a_{1}, \ldots, a_{N})+\frac{2N\delta}{R_{2}-R}\log\frac{R-R_{0}}{R_{2}-R_{0}}.
\end{equation*}
Since $\delta>0$ is arbitrary, the desired inequality is obtained.
\end{proof2}

The Radon-Nikod\'{y}m derivative (\ref{ch06:eqn6.1.6}) can be also obtained from the Fr\'{e}chet derivative of the mapping $A\mapsto f(A)$. It is known ([\citen{bib22}], Sec. V.3) that
\begin{equation*}
Df(A)(H)=f^{[1]}(A)\circ H,
\end{equation*}
where \ $\circ$\ denotes the Schur product in a basis in which $A$ is diagonal, $\mathbf{Diag}(\lambda_{1}, \ldots, \lambda_{n})$, i.e. $A=\sum_{i}\lambda_{i}e_{ii}$ in some matrix units $(e_{ij})_{1\leq i,j\leq n}$, and moreover $f^{[1]}(A)$ is defined in these matrix units by
\begin{equation*}
f^{[1]}(A)_{ij}:=\left\{\begin{array}{ll}
\dfrac{f(\lambda_{i})-f(\lambda_{j})}{\lambda_{i}-\lambda_{j}} & \mathrm{if}\ \lambda_{i}\neq\lambda_{j},\\
\\
f'(\lambda_{i}) & \mathrm{if}\ \lambda_{i}=\lambda_{j}.
\end{array}\right.
\end{equation*}
Since the Fr\'{e}chet derivative is diagonal in the basis $(e_{ij})_{1\leq i,j\leq n}$ of $M_{n}(\mathbb{C})$, the determinant of $Df(A)$ is easily computed, and it equals (\ref{ch06:eqn6.1.6}).

The \textit{upper semicontinuity}\index{upper semicontinuity of free entropy}\index{free entropy!upper semicontinuity} of $\chi(a_{1}, \ldots, a_{N})$ is shown as follows.

\begin{proposition}
\label{ch06:pro6.1.5}
Let $(a_{1}, \ldots, a_{N})$ and $(a_{m,1}, \ldots, a_{m,N})$ be in $(\mathcal{M}^{sa})^{N}$ for $ m\in \mathbb{N}$. If $(a_{m,1}, \ldots, a_{m,N})\rightarrow(a_{1}, \ldots, a_{N})$ in the distribution sense and $\sup_{m}\Vert a_{m,i}\Vert< +\infty$ for $1\leq i\leq N$, then
\begin{equation*}
\chi(a_{1}, \ldots, a_{N})\geq\underset{m\rightarrow\infty}{\limsup}\, \chi(a_{m,1}, \ldots, a_{m,N}).
\end{equation*}
In particular, this is the case when $a_{m,i}\rightarrow a_{i}$ strongly $($i.e. in the strong operator topology$)$ for $1\leq i\leq N$.
\end{proposition}

\begin{proof2}
Let $ R>\sup_{m,i}\Vert a_{m,i}\Vert$. For any $r\in \mathbb{N}$ and $\varepsilon >0$, the assumption implies that
\begin{equation*}
|\tau(a_{m,i_{1}}\cdots a_{m,i_{k}})-\tau(a_{i_{1}}\cdots a_{i_{k}})|<\varepsilon/2
\end{equation*}
for all $1\leq i_{1}, \ldots, i_{k}\leq N, \, 1\leq k\leq r$, whenever $m$ is large enough. In this case, we have
\begin{equation*}
\Gamma_{R}(a_{m,1}, \ldots, a_{m,N};n, r, \varepsilon/2)\subset\Gamma_{R}(a_{1}, \ldots, a_{N};n, r, \varepsilon),
\end{equation*}
so that
\begin{equation*}
\chi_{R}(a_{m,1}, \ldots, a_{m,N})\leq\chi_{R}(a_{m,1}, \ldots, a_{m,N};r, \varepsilon/2)\leq\chi_{R}(a_{1}, \ldots, a_{N};r, \varepsilon).
\end{equation*}
Hence
\begin{equation*}
\underset{m\rightarrow\infty}{\limsup}\, \chi_{R}(a_{m,1}, \ldots, a_{m,N})\leq\chi_{R}(a_{1}, \ldots, a_{N}),
\end{equation*}
which gives the result by Proposition~\ref{ch06:pro6.1.4}.
\end{proof2}

We give one more basic property of free entropy which will also play a role in the next chapter. Roughly speaking, $\chi(a_{1}, a_{2})$ is larger when $a_{1}$ and $a_{2}$ are ``freer'', and $\chi(a_{1}, a_{2})$ does not change if the ``degree of freeness'' of the variables is left unchanged. More concretely, a particular case of the next proposition is $\chi(a_{1}, a_{2})=\chi(a_{1}, a_{2}+g(a_{1}))$ for any function $g$. The invariance under perturbation by polynomials of the other variable(s) is a new phenomenon compared with the Boltzmann-Gibbs entropy.

In the proof below we take polynomials of noncommuting indeterminates. The polynomial
\begin{equation*}
P(X_{1}, \ldots, X_{N})=\sum \mathrm{coef} [i(1), i(2), \ldots, i(k)]X_{i(1)}X_{i(2)}\cdots X_{i(k)}
\end{equation*}
is called selfadjoint if $\overline{\mathrm{coef}[i(1),i(2),\ldots,i(k)]}= \mathrm{coef} [i(k), \ldots, i(2), i(1)]$.

\begin{proposition}
\label{ch06:pro6.1.6}
Let $a_{1}, \ldots, a_{N}, b_{1}, \ldots, b_{N}\in \mathcal{M}^{sa}$. If $b_{1}=a_{1}$ and $ b_{i}-a_{i}\in \{a_{1}, \ldots, a_{i-1}\}''$ for $2\leq i\leq N$, then
\begin{equation*}
\chi(a_{1}, \ldots, a_{N})=\chi(b_{1}, \ldots, b_{N}).
\end{equation*}
\end{proposition}

\begin{proof2}
By assumption we can choose selfadjoint noncommutative polynomials $P_{m,i}(X_{1}, \ldots, X_{i-1})$ for $2\leq i\leq N, \, m\in \mathbb{N}$, such that $P_{m,i}(a_{1}, \ldots, a_{i-1})\rightarrow b_{i}-a_{i}$ strongly as $ m\rightarrow\infty$. Set $b_{m,1}:=b_{1}=a_{1}$ and
\begin{equation*}
b_{m,i} :=a_{i}+P_{m,i}(a_{1}, \ldots, a_{i-1}) \qquad (2\leq i\leq N).
\end{equation*}
Then $b_{m,i}\rightarrow b_{i}$ strongly as $m\rightarrow\infty$. Now define a map $\Psi : (M_{n}(\mathbb{C})^{sa})^{N}\rightarrow (M_{n}(\mathbb{C})^{sa})^{N}, \, \Psi(A_{1}, \ldots, A_{N})=(B_{1}, \ldots, B_{N})$ , by
\begin{equation*}
B_{1} :=A_{1}, \quad B_{i} :=A_{i}+P_{m,i}(A_{1}, \ldots, A_{i-1}) \qquad (2\leq i\leq N).
\end{equation*}
Then the map $\Psi$ preserves the Lebesgue measure $\Lambda$, i.e. $\Lambda \circ \Psi=\Lambda$. For any $m, r\in \mathbb{N}, \, \varepsilon >0$ and $R>0$ there exist $r_{1}\in \mathbb{N}$ and $\varepsilon_{1}>0$ such that
\begin{equation*}
\Psi(\Gamma_{R}(a_{1}, \ldots, a_{N};n, r_{1}, \varepsilon_{1}))\subset\Gamma_{R}(b_{m,1}, \ldots, b_{m,N};n, r, \varepsilon) \qquad (n\in \mathbb{N}),
\end{equation*}
and hence
\begin{equation*}
\chi_{R}(a_{1}, \ldots, a_{N};r_{1}, \varepsilon_1)\leq\chi_{R}(b_{m,1}, \ldots, b_{m,N};r, \varepsilon).
\end{equation*}
This implies $\chi(a_{1}, \ldots, a_{N}) \ \leq \ \chi(b_{m,1}, \ldots, b_{m,N})$, so that $\chi(a_{1}, \ldots,\ a_{N}) \leq \chi(b_{1}, \ldots, b_{N})$ by upper semicontinuity (Proposition~\ref{ch06:pro6.1.5}). The reverse inequality follows by symmetry (we have  $a_{i}-b_{i}\in\{b_{1}, \ldots, b_{i-1}\}'', 2\leq i\leq N$, too).
\end{proof2}

\section{Calculus for power series of noncommutative variables}
\label{ch06:sec6.2}

In this section a functional calculus of operators for power series of noncommuting indeterminates is developed, and the total differential is defined. Moreover, in finite von Neumann algebras a noncommutative (free) Jacobian is introduced on the basis of the Fuglede-Kadison determinant theory.

Let $(\mathcal{M}, \tau)$ be a tracial $W^{*}$-probability space. $\mathcal{M}$ is conveniently represented on the Hilbert space $L^{2}(\mathcal{M})=L^{2}(\mathcal{M}, \tau)$. This is the completion of $\mathcal{M}$ with respect to the inner product $\langle a, b\rangle :=\tau(b^{*}a)$. The left multiplication $L_{a}b:=ab\,(b\in \mathcal{M}\subset L^{2}(\mathcal{M}))$ is called the \textit{standard representation}\index{standard!representation} of $\mathcal{M}$. The \textit{opposite von Neumann algebra}\index{opposite von Neumann algebra}\index{von Neumann algebra!opposite} $\mathcal{M}^{op}=\{a^{o} : a \in \mathcal{M}\}$ is the same as $\mathcal{M}$ except the multiplication is reversed: $a^{o}b^{o}=(ba)^{o}$. The right multiplication $R_{a}b:=ba$ is a faithful representation of the opposite von Neumann algebra $\mathcal{M}^{op}$. The cyclic vector $\xi := \mathbf{1}\in L^{2}(\mathcal{M})$ induces the functional $\tau$ on both $\mathcal{M}$ and $\mathcal{M}^{op}$. The von Neumann algebra tensor product $\mathcal{M}\otimes \mathcal{M}^{op}$ acts on the tensor product Hilbert space $L^{2}(\mathcal{M})\otimes L^{2}(\mathcal{M})$. The vector $\xi\otimes\xi$ generates a functional on $\mathcal{M}\otimes \mathcal{M}^{op}$ which is a faithful normal trace on $\mathcal{M}\otimes\mathcal{M}^{op}$, denoted by $\tau\otimes\tau$. (A more detailed discussion on tensor products is found in books on operator algebras, for example [\citen{bib108}] or [\citen{bib188}]; however, this chapter does not require much knowledge.)

The Fuglede-Kadison determinant\index{Fuglede-Kadison determinant} ([\citen{bib79}]) is nothing else but the extension of the concept of determinant of a matrix to an arbitrary finite von Neumann algebra. For an invertible element $a\in \mathcal{M}$ the (positive-valued) \textit{Fuglede-Kadison determinant} $\Delta(a)=\Delta(|a|)$ is defined by means of the spectral theorem. When $|a|=\int_{0}^{\infty}\lambda \, de(\lambda)$ is the spectral decomposition of $|a|,\,\Delta(a)$ is given as
\begin{equation}
\Delta(a) :=\exp\int_{0}^{\infty}\log\lambda \, d\tau(e(\lambda)).
\label{ch06:eqn6.2.1}
\end{equation}
The properties $\Delta(a^{-1})=\Delta(a)^{-1}$ and $\Delta(u_{1}au_{2})=\Delta(a)$ for unitaries $u_{1}, \,u_{2}\in \mathcal{M}$ are straightforward consequences of the definition. The multiplicativity
\begin{equation*}
\Delta(a_{1}a_{2})=\Delta(a_{1})\Delta(a_{2})
\end{equation*}
is a deeper result for invertible $a_{1}, a_{2}\in \mathcal{M}$. In fact, the definition (\ref{ch06:eqn6.2.1}) is available for a general normal semifinite trace $\tau$ and a general element $a\in \mathcal{M}$; then $ 0\leq \Delta(a)\leq\infty$. When $\mathcal{M}$ is the matrix algebra $M_{n}(\mathbb{C})$ and $\tau$ is the trace $\mathrm{Tr}_{n}$, it is straightforward to see that $\Delta(a)=|\det a|$, where $\det a$ is the common determinant of $a$.

The von Neumann algebra on which we use the Fuglede-Kadison determinant is $\mathcal{N} :=M_{N}(\mathbb{C})\otimes(\mathcal{M}\otimes\mathcal{M}^{op})$. This is a finite von Neumann algebra. In our approach $\mathcal{M}\otimes\mathcal{M}^{op}$ acts on the Hilbert space $L^{2}(\mathcal{M})\otimes L^{2}(\mathcal{M})$. The functional $\tau\otimes\tau$ is a faithful normal trace, and so is $\mathrm{Tr}_{n}\otimes(\tau\otimes\tau)$ on $\mathcal{N}$. When $T$ is an arbitrary element of $\mathcal{N}$, the Fuglede-Kadison determinat $\Delta(T)$ with respect to $\mathrm{Tr}_{n}\otimes(\tau\otimes\tau)$ is at our disposal.

It is convenient to give a brief exposition on noncommutative derivations\index{derivation} in the abstract setting. Let $\mathcal{A}$ be a unital algebra and $x\in  \mathcal{A}$. We want to discuss a \textit{derivation} $\partial_{x} : \mathcal{A}\rightarrow \mathcal{C}$. The characteristic property of any derivation is the \textit{Leibniz rule}:\index{Leibniz rule}
\begin{equation*}
\partial_{x}(a_{1}a_{2})=(\partial_{x}a_{1})a_{2}+a_{1}\partial_{x}a_{2}\,.
\end{equation*}
Therefore we have to assume that $\mathcal{C}$ is a bimodule over $\mathcal{A}$. When $\mathcal{A}$ is a bimodule over $\mathcal{B}$ playing the role of ``scalars'', we can require that
\begin{equation*}
\partial_{x}(b_{1}ab_{2})=b_{1}(\partial_{x}a)b_{2}\,.
\end{equation*}
In particular, $\mathcal{C}$ has to be a $\mathcal{B}$-bimodule as well. We do not make precise the assumptions on $\mathcal{A},\,\mathcal{B}$ and $\mathcal{C}$; in the concrete situation in which we work below those will be clear. Further natural requirements are
\begin{equation*}
\partial_{x}x= \mathbf{1}, \qquad  \partial_{x}b=0  \quad \mathrm{for} \quad  b\in \mathcal{B}.
\end{equation*}
Clearly such a derivation $\partial_{x}$ exists uniquely when every element of $\mathcal{A}$ is uniquely written in the form $a=b_{1}xb_{2}x\cdots xb_{n}$, that is, $\mathcal{A}$ is algebraically freely generated by $x$ and $\mathcal{B}$. It is time to give an example where all of our assumptions are fulfilled. Consider the polynomial algebras $\mathcal{A}:=\mathbb{C}\langle X_{1}, X_{2}, \ldots, X_{N}\rangle,\ \mathcal{B} :=\mathbb{C}\langle X_{2}, \ldots, X_{N}\rangle$ and $\mathcal{C}:=\mathbb{C}\langle X_{1}, X_{2}, \ldots, X_{N}\rangle$ with noncommuting indeterminates $X_{1}, X_{2}, \ldots, X_{n}$. If we choose $x=X_{1}$, then $\mathcal{A}$ is algebraically freely generated by $x$ and $\mathcal{B}$. A repeated application of the Leibniz rule yields
\begin{equation*}
\partial_{X_{1}}(X_{2}X_{1}^{3}X_{3}X_{1})=3X_{2}X_{1}^{2}X_{3}X_{1}+X_{2}X_{1}^{3}X_{3}\,.
\end{equation*}
This example is not what we really need yet. In place of $\mathcal{C}$ we put the tensor product algebra $\mathbb{C}\langle X_{1}, X_{2}, \ldots, X_{N}\rangle\otimes \mathbb{C}\langle X_{1}, X_{2}, \ldots, X_{N}\rangle$, which has to be a bimodule. The $\mathbb{C}\langle X_{1}, X_{2}, \ldots, X_{N}\rangle$-bimodule structure of $\mathcal{C}$ is determined by the rule
\begin{equation*}
a_{1}(a_{2}\otimes a_{3})a_{4} :=(a_{1}a_{2})\,\otimes\,(a_{4}^{t}a_{3})\,,
\end{equation*}
where
\begin{equation*}
(X_{i(1)}X_{i(2)}\cdots X_{i(k)})^{t}=X_{i(k)}\cdots X_{i(2)}X_{i(1)}.
\end{equation*}
Now we consider the derivation $D_{X_{1}}$ with respect to the indeterminate $X_{1}$. This is uniquley defined by the requirements, and we have
\begin{equation*}
D_{X_{1}}(X_{2}X_{1}^{2}X_{3})=X_{2}\otimes X_{3}X_{1}+X_{2}X_{1}\otimes X_{3},
\end{equation*}
and generally
\begin{align}
& D_{X_{j}}(X_{i(1)}X_{i(2)}\cdots X_{i(k)}) \notag \\
& \qquad \ = \ \sum_{i(m)=j} X_{i(1)}X_{i(2)}\cdots X_{i(m-1)}\otimes(X_{i(m+1)}X_{i(m+2)}\cdots X_{i(k)})^{t}\,.
\label{ch06:eqn6.2.2}
\end{align}
Next we consider formal infinite power series
\begin{align}
& F(X_{1},\ X_{2}, \ldots, X_{N}) \notag\\
& \quad =\sum\ \mathrm{coef} \ [i(1), i(2), \ldots, i(k)](F)X_{i(1)}X_{i(2)}\cdots X_{i(k)}
\label{ch06:eqn6.2.3}
\end{align}
of noncommuting indeterminates $X_{1}, X_{2}, \ldots, X_{N}$ with complex coefficients. (The summation here and below is over $1 \leq i(1), i(2), \ldots, i(k)\leq N, \ k\in \mathbb{N}$.) The conjugate power series $F^{*}$ is given by
\begin{equation*}
F^{*}(X_{1}, X_{2}, \ldots, X_{N}) :=\sum\overline{\mathrm{coef}[i(1),i(2),\ldots,i(k)](F)}\,(X_{i(1)}X_{i(2)}\cdots X_{i(k)})^{t}.
\end{equation*}
The $N$-tuple $(R_{1}, \ldots, R_{N})$ of positive numbers is called a \textit{multi-radius of convergence}\index{multi-radius of convergence} for $F$ if the quantuty
\begin{equation*}
M(F;R_{1}, R_{2}, \ldots, R_{N}) :=\sum| \mathrm{coef} [i(1), i(2), \ldots, i(k)](F)|R_{i(1)}R_{i(2)}\cdots R_{i(k)}
\end{equation*}
is finite. When $a_{1}, a_{2}, \ldots, a_{N}$ are taken from $\mathcal{M}$ and $(\Vert a_{1}\Vert, \ldots, \Vert a_{N}\Vert)$ is a multiradius of convergence for $F$, one can define $F(a_{1}, \ldots, a_{N})\in \mathcal{M}$ by the absolutely convergent series
\begin{equation*}
\sum \mathrm{coef} [i(1), i(2), \ldots, i(k)](F)a_{i(1)}a_{i(2)}\cdots a_{i(k)}\,.
\end{equation*}
In particular, if $a_{1}, \ldots, a_{N}\in\mathcal{M}^{sa}, (\Vert a_{1}\Vert, \ldots, \Vert a_{N}\Vert)$ is a multi-radius of convergence for $F$ and $F^{*}=F$, then $F(a_{1}, \ldots, a_{N})\in\mathcal{M}^{sa}$.

Let $F(X_{1}, X_{2}, \ldots, X_{N})$ be the infinite power series (\ref{ch06:eqn6.2.3}). We set
\begin{equation*}
D_{j}F(X_{1}, \ldots, X_{N}) :=\sum \mathrm{coef} [i(1), i(2), \ldots, i(k)](F)D_{X_{j}}(X_{i(1)}X_{i(2)}\cdots X_{i(k)}),
\end{equation*}
where $D_{X_{j}}$ is given by (\ref{ch06:eqn6.2.2}). Then $D_{j}F(a_{1}, \ldots, a_{N})$ is a well-defined element of the von Neumann algebra $\mathcal{M}\otimes \mathcal{M}^{op}$ whenever $(\Vert a_{1}\Vert, \ldots, \Vert a_{N}\Vert)$ is a multi-radius of convergence for $F$ (hence for $D_{j}F$ too). It is explicitly written as
\begin{align*}
& D_{j}F(a_{1}, \ldots, a_{N}) \\
& \quad =\sum \mathrm{coef} [i(1), \ldots, i(k)](F)\sum_{i(m)=j}(a_{i(1)}\cdots a_{i(m-1)})\otimes(a_{i(m+1)}\cdots a_{i(k)})^{o}\,.
\end{align*}

Now let $(F_{1}, \ldots, F_{N})$ be an $N$-tuple of noncommutative power series of $N$ variables, and let $(R_{1}, \ldots, R_{N})$ with $R_{i}>0$ be a common multi-radius of convergence for $F_{i}$. The \textit{differential}\index{differential} $DF$ of $F=(F_{1}, \ldots, F_{N})$ is given by
\begin{equation}
DF(a_{1}, \ldots, a_{N}) :=\left[\begin{array}{ccc}
D_{1}F_{1}(a_{1}, \ldots, a_{N}) & \ldots & D_{N}F_{1}(a_{1}, \ldots, a_{N})\\
\vdots & \ddots & \ \vdots \\
 D_{1}F_{N} (a_1, \ldots, a_{N}) & \ldots & D_{N}F_{N}(a_{1}, \ldots, a_{N})
\end{array}\right],
\label{ch06:eqn6.2.4}
\end{equation}
which is well-defined for $a_{1}, \ldots, a_{N}\in \mathcal{M}$ with $\Vert a_{i}\Vert\leq R_{i}$ as an element of the tensor product $\mathcal{N}=M_{N}(\mathbb{C})\otimes(\mathcal{M}\otimes \mathcal{M}^{op})$.

Let $G=(G_{1}, \ldots, G_{N})$ be another $N$-tuple of power series. The formula
\begin{align*}
(G\circ F)(X_{1}, \ldots, X_{N}) & :=(G_{1}(F_{1}(X_{1}, \ldots, X_{N}), \ldots, F_{N}(X_{1}, \ldots, X_{N})), \\
& \qquad  \ldots, G_{N}(F_{1}(X_{1}, \ldots, X_{N}), \ldots, F_{N}(X_{1}, \ldots,\ X_{N})))
\end{align*}
can be used to define the composition of $N$-tuples of power series, since every coefficient of the composition is a finite sum. When there are $R_{i}>0$ such that $(R_{1}, \ldots, R_{N})$ is a common multi-radius of convergence for $F_{i}$ and $(R_{1}', \ldots, R_{N}')$, where $R_{i}' :=M(F_{i};R_{1}, \ldots, R_{N})$, is a common multi-radius of convergence for $G_{i}$, then $(R_{1}, \ldots, R_{N})$ is a common multi-radius of convergence for all components of the composition defined above. Under this condition we have the \textit{chain rule}\index{chain rule for differentials} as follows.

\begin{lemma}
\label{ch06:lem6.2.1}
In the above situation let $a_{1}, \ldots, a_{N}\in \mathcal{M}$ be such that $\Vert a_{i}\Vert\leq R_{i}$. Then
\begin{equation*}
D(G\circ F)(a_{1}, \ldots, a_{N})=DG(F(a_{1}, \ldots, a_{N}))\cdot DF(a_{1}, \ldots, a_{N})\, ;
\end{equation*}
that is,
\begin{equation*}
D_{j}(G\circ F)_{i}(a_{1}, \ldots, a_{N})=\sum_{l=1}^{N}D_{l}G_{i}(F(a_{1}, \ldots, a_{N}))D_{j}F_{l}(a_{1}, \ldots, a_{N})
\end{equation*}
for each $1\leq i, j\leq N$.
\end{lemma}

\begin{proof2}
Since the differential can be calculated term by term, it suffices to show that if $G(X_{1}, \ldots, X_{N})=X_{i_{1}}X_{i_{2}}\cdots X_{i_{k}}$ and $F_{l}(X_{1}, \ldots, X_{N})=X_{j_{l}(1)}X_{j_{l}(2)}\cdots X_{j_{l}(k(l))}$, then
\begin{equation*}
D_{j}(G\circ F)(a_{1}, \ldots, a_{N})=\sum_{l=1}^{N}D_{l}G(F(a_{1}, \ldots, a_{N}))D_{j}F_{l}(a_{1}, \ldots ,a_{N}).
\end{equation*}
Since
\begin{equation*}
(G\circ F)(a_{1}, \ldots, a_{N})=\prod_{m=1}^{k}F_{i_{m}}(a_{1}, \ldots, a_{N})=\prod_{l=i_{1},\ldots, i_{k}}(a_{j_{l}(1)}\cdots a_{j_{l}(k(l))}),
\end{equation*}
we get
\begin{align*}
& D_{j}(G\circ F)(a_{1}, \ldots, a_{N}) \\
& =\sum_{m=1}^{k}\sum_{s:j_{i_{m}}(s)=j}(F_{i_{1}}(a_{1}, \ldots, a_{N})\cdots F_{i_{m-1}}(a_{1}, \ldots, a_{N})a_{j_{i_{m}}(1)}\cdots a_{j_{i_{m}}(s-1)}) \\ & \qquad \qquad \quad \otimes(a_{j_{i_{m}}(s+1)}\cdots a_{j_{i_{m}}(k(i_{m}))}F_{i_{m+1}}(a_{1}, \ldots, a_{N})\cdots F_{i_{k}}(a_{1}, \ldots, a_{N}))^{o} \\
& =\sum_{m=1}^{k}(F_{i_{1}}(a_{1}, \ldots, a_{N})\cdots F_{i_{m-1}}(a_{1}, \ldots, a_{N})) \\
& \qquad \qquad \qquad \quad \otimes(F_{i_{m+1}}(a_{1}, \ldots, a_{N})\cdots F_{i_{k}}(a_{1}, \ldots, a_{N}))^{o} \\
& \qquad \times\sum_{s: j_{i_{m}}(s)=j}(a_{j_{i_{m}}(1)}\cdots a_{j_{i_{m}}(s-1)})\otimes(a_{j_{i_{m}}(s+1)}\cdots a_{j_{i_{m}}(k(i_{m}))})^{o} \\
& =\sum_{l=1}^{N}D_{l}G(F(a_{1}, \ldots, a_{N}))D_{j}F_{l}(a_{1}, \ldots, a_{N})\,.
\end{align*}
\end{proof2}

The (positive-valued) \textit{Jacobian}\index{Jacobian} of $F$ at $(a_{1}, \ldots, a_{N})$ is defined by
\begin{equation*}
|\mathcal{J}|(F)(a_{1}, \ldots, a_{N}) :=\Delta(DF(a_{1}, \ldots, a_{N}))\,.
\end{equation*}
(Recall that the Fuglede-Kadison determinant is taken with respect to the trace $\mathrm{Tr}_{n}\otimes\tau\otimes\tau$.) The next lemma gives the Jacobian in the matrix case $(\mathcal{M}=M_{n}(\mathbb{C}), \tau=\mathrm{tr}_{n})$.

\begin{lemma}
\label{ch06:lem6.2.2}
Let $F_{1}, \ldots, F_{N}$ be noncommutative power series of $N$ variables with $F_{i}=F_{i}^{*}$, and let $(R_{1}, \ldots, R_{N})$ be a common multi-radius of convergence for $F_{i}$. For the map $F:=(F_{1}, \ldots, F_{N})$ on $\{(A_{1}, \ldots, A_{N})\in(M_{n}(\mathbb{C})^{sa})^{N}:\Vert A_{i}\Vert<R_{i}\}$,
\begin{align}
\frac{d(\Lambda \circ F)}{d\Lambda}(A_{1}, \ldots, A_{N}) & \ =\ |\det(DF(A_{1}, \ldots, A_{N}))| \notag \\
& \ =\ (|\mathcal{J}|(F)(A_{1}, \ldots, A_{N}))^{n^{2}},
\label{ch06:eqn6.2.5}
\end{align}
where $\det$ means the usual determinant on $M_{N}(\mathbb{C})\otimes M_{n}(\mathbb{C})\otimes M_{n}(\mathbb{C})^{op}\cong M_{Nn^{2}}(\mathbb{C})$.
\end{lemma}

\begin{proof2}
When $\mathcal{M}=M_{n}(\mathbb{C})$, one can identify $\mathcal{M}\otimes \mathcal{M}^{op}$ with $M_{n}(\mathbb{C})\otimes M_{n}(\mathbb{C})= M_{n^{2}}(\mathbb{C})$ by $A\otimes B^{o}=A\otimes B^{t}$, where ${}^t$ stands for transpose. Identifying $(M_{n}(\mathbb{C})^{sa})^{N}$ with $\mathbb{R}^{Nn^{2}}$, let us show that the usual Jacobian matrix for $F\negthinspace(A_{1},\ldots,\ A_{N})$ is unitarily conjugate to $DF(A_{1}, \ldots, A_{N})\in M_{N}(\mathbb{C})\otimes M_{n^{2}}(\mathbb{C})$ given by (\ref{ch06:eqn6.2.4}). To show this, for $1\leq p, q\leq N$ fixed, write
\begin{equation*}
F_{p}(A_{1}, \ldots, A_{N})=\sum_{k=0}^{\infty}\sum_{\, i_{1},\ldots, i_{k}=1}^{N} \mathrm{coef} [i(1), \ldots, i(k)]A_{i(1)}\cdots A_{i(k)},
\end{equation*}
and put
\begin{align*}
x_{ii} & :=(A_{q})_{ii}, \quad x_{ij} :=\sqrt{2}\, \mathrm{Re}\, (A_{q})_{ij}, \quad x_{ji} :=\sqrt{2}\, \mathrm{Im}(A_{q})_{ij} \qquad (i<j), \\
y_{ii}& :=F_{p}(A_{1}, \ldots, A_{N})_{ii}, \\
y_{ij} & :=\sqrt{2}\, \mathrm{Re} \, F_{p}(A_{1}, \ldots, A_{N})_{ij}, \qquad y_{ji} :=\sqrt{2}\, \mathrm{Im} \, F_{p}(A_{1}, \ldots , A_{N})_{ij} \qquad (i<j).
\end{align*}
Then it is easy to check that for $i<j$ and $i'<j'$
\begin{align*}
\dfrac{\partial y_{ii}}{\partial x_{i'i'}} & =(D_{q}F_{p})_{ii,i'i'}, \\
\left[\dfrac{\partial y_{ii}}{\partial x_{i'j'}} \ \dfrac{\partial y_{ii}}{\partial x_{j'i'}}\right] & =[\ (D_{q}F_{p})_{ii,i'j'} \quad (D_{q}F_{p})_{ii,j'i'}\ ]\left[\begin{array}{ll}
1/\sqrt{2} & \mathrm{i}\,/\,\sqrt{2}\\
\\
1/\sqrt{2} & -\mathrm{i}\,/\,\sqrt{2}
\end{array}\right], \\
\left[\begin{array}{c}
\dfrac{\partial y_{ij}}{\partial x_{i'i'}} \\
\\
\dfrac{\partial y_{ji}}{\partial x_{i'i'}}
\end{array}\right]& =\left[\begin{array}{ll}
1\,/\,\sqrt{2} & 1/\sqrt{2}\\
\\
-\mathrm{i}\,/\,\sqrt{2} & \mathrm{i}\,/\,\sqrt{2}
\end{array}\right]\left[\begin{array}{l}
(D_{q}F_{p})_{ij,i'i'}\\
\\
(D_{q}F_{p})_{ji,i'i'}
\end{array}\right], \\
\left[\begin{array}{ll}
\dfrac{\partial y_{ij}}{\partial x_{i'j'}} & \dfrac{\partial y_{ij}}{\partial x_{j'i'}}\\
\\
\dfrac{\partial y_{ji}}{\partial x_{i'j'}} & \dfrac{\partial y_{ji}}{\partial x_{j'i'}}
\end{array}\right] & =\left[\begin{array}{ll}
1/\sqrt{2} & 1/\sqrt{2}\\
\\
-\mathrm{i}/\sqrt{2} & \mathrm{i}/\sqrt{2}
\end{array}\right]\left[\begin{array}{ll}
(D_{q}F_{p})_{ij,i'j'} & (D_{q}F_{p})_{ij,j'i'}\\
\\
(D_{q}F_{p})_{ji,i'j'} & (D_{q}F_{p})_{ji,j'i'}
\end{array}\right] \\
& \qquad \times\left[\begin{array}{ll}
1\,/\,\sqrt{2} & \mathrm{i}\,/\,\sqrt{2}\\
\\
1\,/\,\sqrt{2} & -\mathrm{i}\,/\,\sqrt{2}
\end{array}\right],
\end{align*}
where
\begin{align*}
(D_{q}F_{p})_{ij,i'j'} & \ = \ \sum_{k=1}^{\infty}\sum_{m=1}^{k}\sum_{\, i(m)=q}\mathrm{coef}[i(1), \ldots, i(k)] \\
& \qquad \qquad \ \times[(A_{i(1)}\cdots A_{i(m-1)})\otimes(A_{i(m+1)}\cdots A_{i(k)})^{t}]_{ij,i'j'} \\
& \ = \ \sum_{k=1}^{\infty}\sum_{m=1}^{k}\sum_{\, i(m)=q} \mathrm{coef} [i(1), \ldots,\ i(k)] \\
& \qquad \qquad \times(A_{i(1)}\cdots A_{\, i(m-1)})_{ii'}(A_{i(m+1)}\cdots A_{i(k)})_{j'j}\,.
\end{align*}
Arranging these formulas in the form of an $n^{2}\times n^{2}$ matrix, we get the desired conclusion. Hence we obtain the first equality of (\ref{ch06:eqn6.2.5}), while the second equality is just the definition of $|\mathcal{J}|(F)(A_{1}, \ldots, A_{N})$, where the power $n^{2}$ is due to $\mathrm{Tr}_{n}\otimes \mathrm{Tr}_{n}= n^{2}\mathrm{tr}_{n}\otimes \mathrm{tr}_{n}$.
\end{proof2}

\section{Change of variable formulas for free entropy}
\label{ch06:sec6.3}

The difference between the properties of free entropy and of Boltzmann-Gibbs entropy becomes considerable after the common subadditivity and upper semicontinuity. In this section we obtain transformation formulas for free entropy under some types of changes of the variables. Those formulas give an insight to free entropy, and will play crucial roles in the next section, where the additivity of free entropy will be discussed. When $a_{1}, a_{2}, \ldots, a_{n}$ are selfadjoint noncommutative random variables, the separate change of variable formula means that the quantity
\begin{equation*}
\chi(f_{1}(a_{1}), \ldots, f_{N}(a_{N}))-\sum_{i=1}^{N}\chi(f_{i}(a_{i}))
\end{equation*}
is independent of the continuous monotone functions $f_{i}$.

As for the Boltzmann-Gibbs entropy, when $\xi_{1}, \ldots, \xi_{N}$ are random variables with joint density $f$ on $\mathbb{R}^N$ and we transform $(\xi_{1}, \ldots, \xi_{N})$ to $(\eta_{1}, \ldots, \eta_{N})$ by a diffeomorphism $F$ from a region including the values of $(\xi_{1}, \ldots, \xi_{N})$, the density $g$ of $(\eta_{1}, \ldots, \eta_{N})$ is the transform of $f$ under the coordinate change by $F^{-1}$. Then the following formula for Boltzmann-Gibbs entropy is easy to obtain:
\begin{equation*}
S(\xi_{1}, \ldots, \xi_{N})=S(\eta_{1}, \ldots, \eta_{N})+\int_{\mathbb{R}^{N}}f(x)\log|J(x)|\,dx,
\end{equation*}
where $J(x)$ is the Jacobian of the coordinate change. Transformation formulas for free entropy are similar conceptually, but the functional calculus in noncommutative power series is involved, and the noncommutative Jacobian discussed in the previous section becomes important.

\begin{theorem}
\label{ch06:the6.3.1}
Let $F_{1}, \ldots, F_{N}$ and $G_{1}, \ldots, G_{N}$ be noncommutative power series with $F_{i}=F_{i}^{*}$ and $G_{i}=G_{i}^{*}$, such that
\begin{equation}
G_{i}(F_{1}(X_{1}, \ldots, X_{N}), \ldots, F_{N}(X_{1}, \ldots, X_{N}))=X_{i} \qquad (1 \leq i\leq N).
\label{ch06:eqn6.3.1}
\end{equation}
Let $(R_{1}, \ldots, R_{N})$ and $(R_{1}', \ldots, R_{N}')$ be common multi-radii of convergence for $F_{i}$ and $G_{i}$, respectively. If $a_{1}, \ldots, a_{N}\in \mathcal{M}^{sa}$ satsify
\begin{equation*}
\Vert a_{i}\Vert<R_{i}, \quad  M(F_{i};\Vert a_{1}\Vert, \ldots, \Vert a_{n}\Vert)<R_{i}'  \qquad (1\leq i\leq N),
\end{equation*}
then
\begin{align}
& \chi(F_{1}(a_{1}, \ldots, a_{N}), \ldots, F_{N}(a_{1}, \ldots, a_{N})) \notag \\
& \qquad \geq\chi(a_{1}, \ldots, a_{N})+\log|\mathcal{J}|(F)(a_{1}, \ldots, a_{N}).
\label{ch06:eqn6.3.2}
\end{align}
If in addition
\begin{equation*}
M(G_{i};\Vert F_{1}(a_{1}, \ldots, a_{N})\Vert, \ldots, \Vert F_{N}(a_{1}, \ldots, a_{N})\Vert)<R_{i} \qquad (1\leq i\leq N),
\end{equation*}
then equality holds in \emph{(\ref{ch06:eqn6.3.2})}.
\end{theorem}

\begin{proof2}
For $\rho_{1}, \ldots, \rho_{N}>0$, we can define $\Gamma_{\rho_{1},\ldots,\rho_{N}}(a_{1}, \ldots, a_{N};n, r, \varepsilon)$ similarly to $\Gamma_{R}(a_{1}, \ldots, a_{N};n, r, \varepsilon)$ by taking $\Vert A_{i}\Vert\leq\rho_{i}$ instead of $\Vert A_{i}\Vert\leq R$, and we have $\chi_{\rho_{1},\ldots,\rho_{N}}(a_{1}, \ldots, a_{N})$ like $\chi_{R}(a_{1}, \ldots, a_{N})$. Then the proof of Proposition~\ref{ch06:pro6.1.4} can be slightly modified to show that
\begin{equation*}
\chi(a_{1}, \ldots, a_{N})=\chi_{\rho_{1},\ldots, \rho_{N}}(a_{1}, \ldots, a_{N})
\end{equation*}
whenever $\Vert a_{i}\Vert<\rho_{i}$ for $1\leq i\leq N$.

Choose $\Vert a_{i}\Vert<\rho_{i}<R_{i}$ and $M(F_{i};\rho_{1}, \ldots, \rho_{N})<\rho_{i}'<R_{i}'$ for $1\leq i\leq N.$ (This is possible because $F_{i}$ is not constant, due to (\ref{ch06:eqn6.3.1}).) Given $r\in \mathbb{N}$ and $\varepsilon >0$, it is not difficult to see that there exist $r_{1}\in \mathbb{N}$ and $\varepsilon_1 >0$ such that
\begin{align*}
& F(\Gamma_{\rho_{1},\ldots,\rho_{N}}(a_{1}, \ldots,a_{N};n, r_{1}, \varepsilon_{1}) \\
& \quad \ \subset\Gamma_{\rho_{1}',\ldots,\rho_{N}'}(F_{1}(a_{1}, \ldots, a_{N}), \ldots, F_{N}(a_{1}, \ldots, a_{N});n, r, \varepsilon) \qquad (n\in \mathbb{N}),
\end{align*}
where $F:=(F_{1}, \ldots, F_{N})$ on $\{(A_{1}, \ldots, A_{N})\in(M_{n}(\mathbb{C})^{sa})^{N} : \Vert A_{i}\Vert<R_{i}\}$. By Lemma~\ref{ch06:lem6.2.2} this yields
\begin{align}
& \log\Lambda(\Gamma_{\rho_{1}',\ldots,\rho_{N}'}(F_{1}(a_{1}, \ldots, a_{N}), \ldots, F_{N}(a_{1}, \ldots, a_{N});n, r, \varepsilon)) \notag \\
& \qquad \geq\log\Lambda(\Gamma_{\rho_{1},\ldots,\rho_{N}}(a_{1}, \ldots, a_{N};n, r_{1},\varepsilon_{1})) \notag \\
& \qquad \qquad +n^{2}\inf\{\log|\mathcal{J}|(F)(A_{1}, \ldots, A_{N})\} \qquad (n\in \mathbb{N}),
\label{ch06:eqn6.3.3}
\end{align}
where the infimum is taken over $(A_{1}, \ldots, A_{N})\in\Gamma_{\rho_{1},\ldots, \rho_{N}}(a_{1}, \ldots, a_{N};n, r_{1},\varepsilon_{1})$.

In the following let $b_{1}, \ldots, b_{N}\in \mathcal{M}^{sa}$. By the assumption on the multi-radius of convergence, there exists $C\geq 1$ such that
\begin{align*}
\Vert DF(b_{1}, \ldots, b_{N})\Vert& \leq C \quad \mathrm{if} \quad \Vert b_{i}\Vert\leq\rho_{i}, \\
 \Vert DG(b_{1}, \ldots, b_{N})\Vert & \leq C \quad \mathrm{if}  \quad \Vert b_{i}\Vert\leq\rho_{i}'.
\end{align*}
By Lemma~\ref{ch06:lem6.2.1} applied to (\ref{ch06:eqn6.3.1}) we know that
\begin{equation*}
DG(F(b_{1}, \ldots, b_{N}))\cdot DF(b_{1}, \ldots, b_{N})=\mathbf{1},
\end{equation*}
and hence $C^{2}DF(b_{1}, \ldots, b_{N})^{*}DF(b_{1}, \ldots, b_{N})\geq \mathbf{1}$ in $M_{N}(\mathbb{C})\otimes \mathcal{M}\otimes \mathcal{M}^{op}$. So we get
\begin{equation*}
C^{-2}\mathbf{1}\leq DF(b_{1}, \ldots, b_{N})^{*}DF(b_{1}, \ldots, b_{N})\leq C^{2}\mathbf{1}  \quad \mathrm{if} \quad  \Vert b_{i}\Vert\leq\rho_{i}.
\end{equation*}
For any $\delta>0$ with $\delta(2C+\delta)<C^{-2}$, put $\delta_{1} :=\delta(2C+\delta)$ and $\delta_{2}:=\delta_{1}/(C^{-2}-\delta_{1})$. One can choose an $N\times N$ matrix $P$ whose entries are noncommutative polynomials of $2n$ variables and which satisfies
\begin{equation*}
\Vert DF(b_{1}, \ldots, b_{N})-P(b_{1}\otimes \mathbf{1}, \ldots, b_{N}\otimes \mathbf{1},\mathbf{1}\otimes b_{1}^{o}, \ldots, \mathbf{1}\otimes b_{N}^{o})\Vert<\delta,
\end{equation*}
and hence
\begin{align*}
& \Vert DF(b_{1}, \ldots, b_{N})^{*}DF(b_{1}, \ldots, b_{N})-P(b_{1}\otimes \mathbf{1}, \ldots, b_{N}\otimes \mathbf{1}, \\
& \quad \mathbf{1}\otimes b_{1}^{o}, \ldots, \mathbf{1}\otimes b_{N}^{o})^{*}P(b_{1}\otimes \mathbf{1}, \ldots, b_{N}\otimes \mathbf{1},\mathbf{1}\otimes b_{1}^{o}, \ldots, \mathbf{1}\otimes b_{N}^{o})\Vert\leq\delta_{1}
\end{align*}
whenever $\Vert b_{i}\Vert\leq\rho_{i}$. Furthermore, choose a polynomial $Q(t)$ such that
\begin{equation*}
|Q(t)-\log t|\leq\delta \quad \mathrm{for} \quad C^{-2}-\delta_{1}\leq t\leq C^{2}+\delta_{1}.
\end{equation*}
Here note that if $a, b\geq\alpha \mathbf{1}$ and $\Vert a-b\Vert<\alpha$, then
\begin{equation*}
\Vert\log a-\log b\Vert\leq\frac{\Vert a-b\Vert}{\alpha-\Vert a-b\Vert}\,.
\end{equation*}
Indeed, from the operator monotonicity of $\log$ we have
\begin{align*}
-\Vert a-b\Vert b^{-1} & \ \leq \ \log(b-\Vert a-b\Vert \mathbf{1})-\log b\leq\log a-\log b \\
& \ \leq \ \log(b+\Vert a-b\Vert \mathbf{1})-\log b\leq\Vert a-b\Vert b^{-1}
\end{align*}
and $b\geq(\alpha-\Vert a-b\Vert)\mathbf{1}$.

From the above estimates we have
\begin{align}
& \Vert\log DF(a_{1}, \ldots, a_{N})^{*}DF(a_{1}, \ldots, a_{N})-Q(P(a_{1}\otimes \mathbf{1}, \ldots, a_{N}\otimes \mathbf{1}, \notag \\
& \quad \mathbf{1}\otimes a_{1}^{o}, \ldots,  \mathbf{1}\otimes a_{N}^{o})^{*}P(a_{1}\otimes \mathbf{1}, \ldots, a_{N}\otimes \mathbf{1},\mathbf{1}\otimes a_{1}^{o}, \ldots, \mathbf{1}\otimes a_{N}^{o}))\Vert \notag \\
& \qquad \leq\delta+\delta_{2}\,.
\label{ch06:eqn6.3.4}
\end{align}
In the same way, in $M_{N}(\mathbb{C})\otimes M_{n}(\mathbb{C})\otimes M_{n}(\mathbb{C})^{op}\cong M_{Nn^{2}}(\mathbb{C})$ we have
\begin{align}
& \Vert\log DF(A_{1}, \ldots, A_{N})^{*}DF(A_{1}, \ldots, A_{N})-Q(P(A_{1}\otimes I, \ldots, A_{N}\otimes I, \notag \\
& \quad I\otimes A_{1}^{t}, \ldots,  I\otimes A_{N}^{t})^{*}P(A_{1}\otimes I, \ldots, A_{N}\otimes I, I\otimes A_{1}^{t}, \ldots, I\otimes A_{N}^{t}))\Vert \notag \\
& \qquad \leq\delta+\delta_{2}
\label{ch06:eqn6.3.5}
\end{align}
whenever $A_{i}\in M_{n}(\mathbb{C})^{sa},\,\Vert A_{i}\Vert\leq\rho_{i}$. Since
\begin{align*}
& (\mathrm{Tr}N\otimes \mathrm{tr}_{n}\otimes \mathrm{tr}_{n})(Q(P(A_{1}\otimes I, \ldots, A_{N}\otimes I, I\otimes A_{1}^{t}, \ldots, I\otimes A_{N}^{t})^{*} \\
& \qquad \qquad \qquad \qquad \quad \times P(A_{1}\otimes I, \ldots, A_{N}\otimes I, I\otimes A_{1}^{t}, \ldots, I\otimes A_{N}^{t})))
\end{align*}
is a polynomial (of degree $\leq 2$) in the multiple moments $\mathrm{tr}_{n}(A_{i_{1}}\cdots A_{i_{k}})$ and the same is true for $a_{1}, \ldots, a_{N}$ as well, it follows from (\ref{ch06:eqn6.3.4}) and (\ref{ch06:eqn6.3.5}) that if $r_{1}$ is large enough and $\varepsilon_{1}$ is small enough, then
\begin{align*}
& |(\mathrm{Tr}_{N}\otimes \mathrm{tr}_{n}\otimes \mathrm{tr}_{n})(\log(DF(A_{1}, \ldots, A_{N})^{*}DF(A_{1}, \ldots, A_{N}))) \\
& \quad -(\mathrm{Tr}_{N}\otimes\tau\otimes\tau)(\log(DF(a_{1}, \ldots, a_{N})^{*}DF(a_{1}, \ldots, a_{N})))| \\
& \qquad \leq\delta+2N(\delta+\delta_{2})
\end{align*}
for all $(A_{1}, \ldots, A_{N})\in\Gamma_{\rho_{1},\ldots \rho_{N}}(a_{1}, \ldots, a_{N};n, r_{1},\varepsilon_{1})$. The above inequality means that
\begin{equation*}
|\log|\mathcal{J}|(F)(A_{1}, \ldots, A_{N})-\log|\mathcal{J}|(F)(a_{1}, \ldots, a_{N})|\leq\frac{1}{2}\delta+N(\delta+\delta_{2})(=:\delta_{3}).
\end{equation*}
This and (\ref{ch06:eqn6.3.3}) imply that
\begin{align*}
& \chi_{\rho'_{1}, \ldots, \rho_{N}'}(F_{1}(a_{1}, \ldots, a_{N}), \ldots, F_{N}(a_{1}, \ldots, a_{N});r, \varepsilon) \\
& \qquad \geq\chi_{\rho_{1},\ldots,\rho_{N}}(a_{1}, \ldots, a_{N};r_{1}, \varepsilon_{1})+\log|\mathcal{J}|(F)(a_{1}, \ldots, a_{N})-\delta_{3},
\end{align*}
so that
\begin{align*}
& \chi_{\rho_{1}',\ldots, \rho_{N}'}(F_{1}(a_{1}, \ldots, a_{N}), \ldots, F_{N}(a_{1}, \ldots, a_{N})) \\
& \qquad \geq\chi_{\rho_{1},\ldots, \rho_{N}}(a_{1}, \ldots, a_{N})+\log|\mathcal{J}|(F)(a_{1}, \ldots, a_{N})-\delta_{3}.
\end{align*}
Since $\delta_{3}$ can be arbitrarily small, we obtain (\ref{ch06:eqn6.3.2}) from the fact noted at the beginning of the proof.

For the second assertion we need to show that
\begin{equation*}
F_{i}(G_{1}(X_{1}, \ldots, X_{N}), \ldots, G_{N}(X_{1}, \ldots, X_{N}))=X_{i} \qquad (1\leq i\leq N).
\end{equation*}
Consider $F$ on a neighborhood of $0$ in $(M_{n}(\mathbb{C})^{sa})^{N}\ (\cong \mathbb{R}^{Nn^{2}})$. Since $DF(0)$ is invertible by Lemma~\ref{ch06:lem6.2.1} and (\ref{ch06:eqn6.3.1}), it follows from the fact noticed in the proof of Lemma~\ref{ch06:lem6.2.2} that $F$ is invertible in a neighborhood of $0$, so $F\circ G= \mathrm{id} \ (G := (G_{1}, \ldots, G_{N}))$ in a neighborhood of $F(0)$. By analyticity we get $F\circ G= \mathrm{id}$ in a neighborhood of $0$ in $(M_{n}(\mathbb{C})^{sa})^{N}$. Since $n$ is arbitrary, one can easily see that $F\circ G= \mathrm{id}$ as noncommutative power series, as desired. Thus the first assertion gives
\begin{align*}
\chi(a_{1}, \ldots, a_{N}) & \ \geq \ \chi(F_{1}(a_{1}, \ldots, a_{N}), \ldots, F_{N}(a_{1}, \ldots, a_{N})) \\
& \qquad \ +\log|\mathcal{J}|(G)(F(a_{1}, \ldots, a_{N})).
\end{align*}
Moreover, the multiplicativity of the determinant $\Delta$ gives
\begin{equation*}
|\mathcal{J}|(G)(F(a_{1}, \ldots, a_{N})) \cdot |\mathcal{J}|(F)(a_{1}, \ldots, a_{N})=1,
\end{equation*}
so that equality is valid in (\ref{ch06:eqn6.3.2}).
\end{proof2}

The following is a simple application of the above theorem. The free entropy behaves in exactly the same way as the Boltzmann-Gibbs entropy under linear transformations of the variables.

\begin{corollary}
\label{ch06:cor6.3.2}
Let $a_{1}, \ldots, a_{N}\in \mathcal{M}^{sa}$.
\begin{enumerate}
\item[(1)] \textit{If} $A=[\alpha_{ij}]_{i,j=1}^{N}\in M_{N}(\mathbb{R})$ and $\beta_{1}, \ldots, \beta_{N}\in \mathbb{R}$, then
\begin{equation*}
\chi\left(\sum_{j}\alpha_{1j}a_{j}+\beta_{1}\mathbf{1}, \ldots, \sum_{j}\alpha_{Nj}a_{j}+\beta_{N}\mathbf{1}\right)=\chi(a_{1}, \ldots, a_{N})+\log|\det A|.
\end{equation*}
\item[(2)] If $a_{1}, \ldots, a_{N}$ are linearly dependent, then $\chi(a_{1}, \ldots, a_{N})=-\infty$.
\end{enumerate}
\end{corollary}

\begin{proof2}
\begin{enumerate}
\item[(1)] It is enough to treat the case $\beta_{i}=0 \ (1\leq i\leq N)$. If $F(X_{1}, \ldots, X_{N}) := (\sum_{j}\alpha_{1j}X_{j}, \ldots, \sum_{j}\alpha_{Nj}X_{j})$, then
\begin{equation*}
DF(a_{1}, \ldots, a_{N})=A\otimes \mathbf{1}_{\mathcal{M}\otimes \mathcal{M}^{op}},
\end{equation*}
\end{enumerate}
\noindent so that $|\mathcal{J}|(F)(a_{1}, \ldots, a_{N})=|\det A|$. Hence the result is obvious from Theorem~\ref{ch06:the6.3.1} if $A$ is invertible. When $A$ is singular, both sides are $-\infty$ from (2), below.
\begin{enumerate}
\item[(2)] We may assume that $a_{1}=\alpha_{2}a_{2}+\cdots+\alpha_{N}a_{N}(\alpha_{i}\in \mathbb{R})$. For every $\varepsilon>0$, since $a_{1}=\varepsilon a_{1}+(1-\varepsilon)\alpha_{2}a_{2}+\cdots+(1-\varepsilon)\alpha_{N}a_{N}$, it follows from (1) that
\begin{equation*}
\chi(a_{1}, \ldots, a_{N})=\chi(a_{1}, \ldots, a_{N})+\log\varepsilon,
\end{equation*}
\end{enumerate}
which means $\chi(a_{1}, \ldots, a_{N})=-\infty$.
\end{proof2}

An essential point in the following special case of Theorem~\ref{ch06:the6.3.1} is that the assumptions there are automatically fulfilled here.

\begin{proposition}
\label{ch06:pro6.3.3}
Let $a_{1}, \ldots, a_{N}\in \mathcal{M}^{sa}$, and let $P_{1}, \ldots, P_{N}$ be noncommutative polynomials of $N$ variables with $P_{i}=P_{i}^{*}$. Then for any $\rho\in \mathbb{R}$ sufficiently near $0$, if $F_{\rho,i}(X_{1}, \ldots, X_{n}) :=X_{i}+\rho P_{i}(X_{1}, \ldots, X_{n})$, then
\begin{align*}
& \chi(F_{\rho,1}(a_{1}, \ldots, a_{N}), \ldots, F_{\rho,N}(a_{1}, \ldots, a_{N})) \\
& \qquad =\chi(a_{1}, \ldots, a_{N})+\log|\mathcal{J}|(F_{\rho})(a_{1}, \ldots, a_{N})\,.
\end{align*}
\end{proposition}

\begin{proof2}
Let us apply Theorem~\ref{ch06:the6.3.1}. For this we have to show that for any $\rho\in \mathbb{R}$ near $0$ there are noncommutative power series $G_{\rho,i}\ (1\leq i\leq n)$ with $G_{\rho,i}=G_{\rho,i}^{*}$ such that
\begin{enumerate}
\item[(i)] $G_{\rho,i}(F_{\rho,1}(X_{1}, \ldots, X_{N}), \ldots, F_{\rho,n}(X_{1}, \ldots, X_{N}))=X_{i}\ (1\leq i\leq N)$,

\item[(ii)] $(M(F_{\rho,1};\, \Vert a_{1}\Vert+\varepsilon,\ \ldots, \Vert a_{n}\Vert+\varepsilon), \ldots, M(F_{\rho,n};\Vert a_{1}\Vert+\varepsilon, \ldots, \Vert a_{n}\Vert+\varepsilon))$ is a common multi-radius of convergence for the $G_{\rho,i}$ for some $\varepsilon >0$.
\end{enumerate}
Note that the other two conditions about the multi-radius of convergence for the $F_{\rho,i}$ are trivial because the $F_{\rho,i}$ are polynomials.

Now choose $R>0$ and $\varepsilon >0$ so that $\Vert a_{i}\Vert+2\varepsilon<R$. Let $\mathcal{A}_{R}$ denote the space of all noncommutative power series $F=F(X_{1}, \ldots, X_{N})$ such that $F=F^{*}$ and $ M(F;R, \ldots, R)<+\infty$. It is straightforward to see that $\mathcal{A}_{R}$ is an involutive Banach algebra with the norm $\Vert F\Vert_{R} :=M(F;R, \ldots, R)$. Consider a Banach space $(\mathcal{A}_{R})^{N}$ with the norm $\Vert(F_{1}, \ldots, F_{N})\Vert_{R}:=\max_{i}\Vert F_{i}\Vert_{R}$. Letting
\begin{equation*}
\mathcal{D}^{N} :=\{(F_{i})\in(\mathcal{A}_{R})^{N} : \Vert(F_{i})\Vert_{R}\leq R+\varepsilon, \ F_{i}=F_{i}^{*}, \ 1\leq i\leq N\},
\end{equation*}
one can define $\Psi_{\rho} : \mathcal{D}^{N}\rightarrow(\mathcal{A}_{R})^{N}$ for $\rho\in \mathbb{R}$ by
\begin{equation*}
\Psi_{\rho}(F_{1}, \ldots, F_{N}) :=(\tfrac{1}{2}(X_{i}+F_{i}-\rho P_{i}(F_{1}, \ldots, F_{N})))_{1\leq i\leq N},
\end{equation*}
because $P_{i}(F_{1}, \ldots, F_{N})^{*}=P_{i}(F_{1}, \ldots, F_{N})$. Then
\begin{equation}
\Vert\Psi_{\rho}(F_{1}, \ldots, F_{N})\Vert_{R}\leq\tfrac{1}{2}(2R+\varepsilon+|\rho|\Vert(P_{i})\Vert_{R+\varepsilon}) \quad \mathrm{for} \quad (F_{i})\in \mathcal{D}^{N},
\label{ch06:eqn6.3.6}
\end{equation}
and since $\Vert F_{i_{1}}\cdots F_{i_{k}}-F_{i_{1}}'\cdots F_{i_{k}}'\Vert_{R}\leq k(R+\varepsilon)^{k-1}\Vert(F_{i}-F_{i}')\Vert_{R}$, there exists $K>0$ such that
\begin{equation}
\Vert\Psi_{\rho}(F_{1},\ \ldots,\ F_{n})-\Psi_{\rho}(F_{1}', \ldots, F_{n}')\Vert_{R}\leq(\frac{1}{2}+K|\rho|)\Vert(F_{i}-F_{i}')\Vert_{R}
\label{ch06:eqn6.3.7}
\end{equation}
for $(F_{i}), (F_{i}')\in \mathcal{D}^{N}$. Now assume $|\rho|<1/(\varepsilon^{-1}\Vert(P_{i})\Vert_{R+\varepsilon}+2K)$. Then it follows from (\ref{ch06:eqn6.3.6}) and (\ref{ch06:eqn6.3.7}) that $\Psi_{\rho}$ is a contraction (with Lipschitz constant $<1$) of $\mathcal{D}^{N}$ into itself. Hence, by Banach's contraction lemma, it has a fixed point $(G_{\rho,i})\in \mathcal{D}^{N}$ so that $G_{\rho,i}+\rho P_{i}(G_{\rho,1}, \ldots, G_{\rho,n})=X_{i}$, i.e.
\begin{equation*}
\tag{i$'$} F_{\rho,i}(G_{\rho,1}(X_{1}, \ldots, X_{n}), \ldots, G_{\rho,n}(X_{1}, \ldots, X_{n}))=X_{i}\quad (1\leq i\leq N).
\end{equation*}
Furthermore, since
\begin{equation*}
M(F_{\rho,i};\Vert a_{1}\Vert+\varepsilon, \ldots, \Vert a_{N}\Vert+\varepsilon)\leq\Vert F_{\rho,i}\Vert_{R-\varepsilon}\leq R-\varepsilon +|\rho|\Vert P_{i}\Vert_{R-\varepsilon}\leq R,
\end{equation*}
the condition (ii) is satisfied. Finally, (i) can be obtained from $(\mathrm{i}')$ as in the last part of the proof of Theorem~\ref{ch06:the6.3.1}.
\end{proof2}

It is quite natural to expect that the free entropy $\chi(a_{1}, \ldots, a_{N})$ is $-\infty$ whenever some algebraic relation is satisfied among $a_{1}, \ldots, a_{N}$. Although it seems difficult to show this directly from the definition, one can benefit from the above proposition. We illustrate it in a simple example.

\begin{example}
\label{ch06:exa6.3.4}
 Let $a_{1}, a_{2}\in\mathcal{M}^{sa}$. If $a_{1}a_{2}=a_{2}a_{1}$, then $\chi(a_{1}, a_{2})=-\infty$.

In order to apply Proposition~\ref{ch06:pro6.3.3} we choose $P_{1}(X_{1}, X_{2}):=\mathrm{i}(X_{1}X_{2}-X_{2}X_{1})$ and $P_{2}(X_{1}, X_{2}):=0$. Since $F_{\rho,1}(a_{1},a_{2})=a_{1}+\rho P_{1}(a_{1}, a_{2})=a_{1}$ and $F_{\rho,2}(a_{1}, a_{2})= a_{2}$, we have
\begin{equation}
\chi(a_{1}, a_{2})=\chi(a_{1}, a_{2})+\log|\mathcal{J}|(F_{\rho})(a_{1}, a_{2}).
\label{ch06:eqn6.3.8}
\end{equation}
We compute
\begin{equation*}
DF_{\rho}(a_{1}, a_{2})=\left[\begin{array}{cc}
\mathbf{1}\otimes \mathbf{1}+\rho \mathrm{i}(\mathbf{1}\otimes a_{2}^{o}-a_{2}\otimes \mathbf{1}) & \rho \mathrm{i}(a_{1}\otimes \mathbf{1}-\mathbf{1}\otimes a_{1}^{o})\\
0 & \mathbf{1}\otimes \mathbf{1}
\end{array}\right]
\end{equation*}
and
\begin{equation*}
DF_{\rho}(a_{1}, a_{2})DF_{\rho}(a_{1}, a_{2})^{*}=\mathbf{1}+\rho x+\rho^{2}y,
\end{equation*}
where
\begin{align*}
x:=\left[\begin{array}{cc}
0 & \mathrm{i}\,(a_{1}\otimes \mathbf{1}-\mathbf{1}\otimes a_{1}^{o})\\
-\mathrm{i}(a_{1}\otimes \mathbf{1}-\mathbf{1}\otimes a_{1}^{o}) & 0
\end{array}\right], \\
y:=\left[\begin{array}{cr}
(a_{1}\otimes \mathbf{1}-\mathbf{1}\otimes a_{1}^{o})^{2}+(\mathbf{1}\otimes a_{2}^{o}-a_{2}\otimes \mathbf{1})^{2} & 0\\
0 & 0
\end{array}\right].
\end{align*}
Now we perform a direct computation by means of the Taylor series of the logarithm:
\begin{align*}
\log|\mathcal{J}|(F_{\rho})(a_{1}, a_{2}) & \ = \ \frac{1}{2}(\mathrm{Tr}_{2}\otimes\tau\otimes\tau)(\log(DF_{\rho}(a_{1}, a_{2})DF_{\rho}(a_{1}, a_{2})^{*})) \\
& \ = \ \frac{1}{2}\sum_{m=1}^{\infty}\frac{(-1)^{m-1}}{m}(\mathrm{Tr}_{2}\otimes\tau\otimes\tau)((\rho x+\rho^{2}y)^{m}) \\
& \ = \ \frac{1}{2}\rho^{2}(\tau\otimes\tau)((\mathbf{1}\otimes a_{2}^{o}-a_{2}\otimes \mathbf{1})^{2})+o(\rho^{2}) \\
& \ = \ \rho^{2}(\tau(a_{2}^{2})-\tau(a_{2})^{2})+o(\rho^{2})\, .
\end{align*}
If $\tau(a_{2}^{2})>\tau(a_{2})^{2}$, then we get $\log|\mathcal{J}|(F_{\rho})(a_{1}, a_{2})>0$ for small $\rho$. This forces $\chi(a_{1}, a_{2})=-\infty$, thanks to (\ref{ch06:eqn6.3.8}). Otherwise, $a_{2}$ must be a scalar and $\chi(a_{1}, a_{2})= -\infty$.
\end{example}

From the subadditivity and the previous example, we know that $\chi(a_{1}, \ldots, a_{N})= -\infty$ if there are two commuting elements among $a_{1}, \ldots, a_{N}$. The above argument should be available in principle when some algebraic relation is satisfied by the elements $a_{1}, \ldots, a_{N}$. For instance, in the case $a_{1}a_{2}a_{3}+a_{3}a_{2}a_{1}=0$, it is enough to apply Proposition~\ref{ch06:pro6.3.3} to the polynomials $P_{2}(X_{1}, X_{2}, X_{3})=X_{1}X_{2}X_{3}+X_{3}X_{2}X_{1}$ and $P_{1}=P_{3}=0$, and the argument of the example works.

Another kind of transformation formula will be also useful in the next section. It is restricted to separate change of each variable, but a larger class of continuous functions are available. We first give a lemma which provides a smoothing technique for the free entropy under the functional calculus.

\begin{lemma}
\label{ch06:lem6.3.5}
Let $a\in\mathcal{M}^{sa}$ with $\chi(a)>-\infty$, and let $f$ be a continuous and increasing function on $\mathbb{R}$. Then there exists a sequence $(f_{m})$ of $C^{\infty}$-functions on $\mathbb{R}$ such that $f_{m}'>0$ on $\mathbb{R}, \, \Vert f_{m}(a)-f(a)\Vert\rightarrow 0$ and $\chi(f_{m}(a))\rightarrow\chi(f(a))$.
\end{lemma}

\begin{proof2}
Let $\mu$ be the distribution of $a$. Since $\Sigma(\mu)>-\infty$, there are $0<\delta(m)< 1/m\,(m\in \mathbb{N})$ such that
\begin{align*}
& \iint_{|s-t|<\delta(m)}\log|s-t|\,d\mu(s)\,d\mu(t)\geq-\frac{1}{m},\\
&  \iint_{|s-t|<\delta(m)}\,d\mu(s)\,d\mu(t)\leq\frac{1}{m\log m}\, .
\end{align*}
For each $m$ choose a $C^{\infty}$-function $\phi_{m}\geq 0$ supported in a neighborhood of $0$ with $\int\phi_{m}\,dt=1$ such that
\begin{equation*}
|(f*\phi_{m})(t)-f(t)|\leq\frac{\delta(m)}{2m} \quad \mathrm{for} \quad t\in \mathrm{supp}\, \mu.
\end{equation*}
Define $C^{\infty}$-functions $f_{m}$ by
\begin{equation*}
f_{m}(t):=\frac{t}{m}+(f*\phi_{m})(t) \qquad (t\in \mathbb{R}).
\end{equation*}
Then it is obvious that $f_{m}'(t)\geq 1/m$ and $\Vert f_{m}(a)-f(a)\Vert\rightarrow 0$. If $s,  t\in \mathrm{supp}\,\mu$ and $|s-t|\geq\delta(m)$, then, assuming $s>t$, we estimate
\begin{align*}
& f_{m}(s)-f_{m}(t) \\
& \qquad =\frac{s-t}{m}+(f*\phi_{m})(s)-(f*\phi_{m})(t) \\
& \qquad \geq\frac{\delta(m)}{m}+f(s)-f(t)-|(f*\phi_{m})(s)-f(s)|-|(f*\phi_{m})(t)-f(t)| \\
& \qquad \geq f(s)-f(t).
\end{align*}
Hence we have
\begin{align*}
\Sigma(f_{m}(a)) & \ = \ \iint\log|f_{m}(s)-f_{m}(t)|\,d\mu(s)\,d\mu(t) \\
& \ \geq \ \iint_{|s-t|\geq\delta(m)}\log|f(s)-f(t)|\,d\mu(s)\,d\mu(t) \\
& \qquad \quad +\iint_{|s-t|<\delta(m)}\log\frac{|s-t|}{m}\,d\mu(s)\,d\mu(t) \\
& \ \geq \ \iint \log |f(s)-f(t)|\,d\mu(s)\,d\mu(t) \\
& \qquad \quad + \iint_{|s-t|<\delta(m)} \log|s-t|\,d\mu(s)\,d\mu(t) \\
& \qquad \quad -\log m\iint_{|s-t|<\delta(m)}\,d\mu(s)\,d\mu(t) \\
& \ \geq \ \Sigma(f(a))-\frac{2}{m}
\end{align*}
whenever $m$ is large, so that $|f(s)-f(t)|<1$ if $s,  t\in \mathrm{supp}\, \mu$ and $|s-t|<\delta(m)$. Therefore,
\begin{equation*}
\Sigma(f(a))\leq\liminf_{m\rightarrow\infty}\, \Sigma(f_{m}(a)).
\end{equation*}
This together with Proposition~\ref{ch05:pro5.3.2} shows that $\Sigma(f_{m}(a))\rightarrow\Sigma(f(a))$; that is, $\chi(f_{m}(a))\rightarrow\chi(f(a))$.
\end{proof2}

\begin{proposition}
\label{ch06:pro6.3.6}
Let $a_{1}, \ldots, a_{N}\in \mathcal{M}^{sa}$ with $\chi(a_{i})>-\infty$, and let $f_{1}, \ldots, f_{N}$ be continuous increasing functions on $\mathbb{R}$. Then
\begin{equation*}
\chi(f_{1}(a_{1}), \ldots, f_{N}(a_{N}))\geq\chi(a_{1}, \ldots, a_{N})+\sum_{i=1}^{N}[\chi(f_{i}(a_{i}))-\chi(a_{i})].
\end{equation*}
Moreover, equality holds here if $f_{1}, \ldots, f_{N}$ are strictly increasing,
\end{proposition}

\begin{proof2}
If $\chi(f_{i}(a_{i}))=-\infty$ for some $i$, then both sides of the required inequality are $-\infty$ thanks to Proposition~\ref{ch06:pro6.1.1}. Hence we may assume that $\chi(f_{i}(a_{i}))>-\infty$ for all $i$. Now it suffices to show that
\begin{equation}
\chi(f_{1}(a_{1}), a_{2}, \ldots, a_{N})\geq\chi(a_{1}, \ldots, a_{N})+\chi(f_{1}(a_{1}))-\chi(a_{1}).
\label{ch06:eqn6.3.9}
\end{equation}
In fact, repeated use of this inequality yields the conclusion. To show this, by Lemma~\ref{ch06:lem6.3.5} and Proposition~\ref{ch06:pro6.1.5} we may assume that $f_{1}$ is a $C^{\infty}$-function $f$ on $\mathbb{R}$ such that $f'>0$ on $\mathbb{R}$. Put
\begin{equation*}
K(s, t):=\left\{\begin{array}{ll}
\dfrac{f(s)-f(t)}{s-t} & \mathrm{if}\ s\neq t,\\
f'(t) & \mathrm{if}\ s=t,
\end{array}\right.
\end{equation*}
and $L(s, t) :=\log K(s, t)$, which is continuous on $\mathbb{R}^{2}$. Then, thanks to (\ref{ch06:eqn6.1.2}),
\begin{align}
\chi(f(a_{1}))-\chi(a_{1}) & \ =  \ \iint\log\left(\frac{f(s)-f(t)}{s-t}\right)d\mu_{1}(s)\,d\mu_{1}(t) \notag \\
& \ = \ (\tau\otimes\tau)(L(a_{1}\otimes \mathbf{1},\mathbf{1}\otimes a_{1}))\,,
\label{ch06:eqn6.3.10}
\end{align}
where $\mu_{1}$ is the distribution of $a_{1}$ and the latter is defined in $(\mathcal{M}\otimes \mathcal{M}, \tau\otimes\tau)$.

Write $F(A_{1}, \ldots, A_{N}) :=(f(A_{1}), A_{2}, \ldots, A_{N})$ on $(M_{n}(\mathbb{C})^{sa})^{N}$. Let $R_{1}\geq R> \max_{i}\Vert a_{i}\Vert$, for which $f([-R, R])\subset[-R_{1}, R_{1}]$. For each $r\in \mathbb{N}$ and $\varepsilon >0$, by a polynomial approximation of $f$ on $[-R, R]$, one can choose $r_{1}\in \mathbb{N}$ and $\varepsilon_{1} >0$ such that
\begin{align}
& F(\Gamma_{R}(a_{1}, \ldots, a_{N};n, r_{1}, \varepsilon_{1})) \notag \\
& \qquad \subset\Gamma_{R_{1}}(f(a_{1}), a_{2}, \ldots, a_{N};n, r, \varepsilon) \qquad (n\in \mathbb{N}).
\label{ch06:eqn6.3.11}
\end{align}
The Jacobian of $F(A_{1}, \ldots, A_{N})$ depends on $A_{1}$ only, and as in the proof of Proposition~\ref{ch06:pro6.1.4} we have
\begin{equation*}
\frac{d(\Lambda\circ f)}{d\Lambda}(A_{1})=\prod_{i<j}\left(\frac{f(t_{i})-f(t_{j})}{t_{i}-t_{j}}\right)^{2}\prod_{i=1}^{n}f'(t_{i})
\end{equation*}
under the coordinate change $M_{n}(\mathbb{C})^{sa}\leftrightarrow \mathcal{U}(n)/T\times \mathbb{R}_{\leq}^{n}$. Since the eigenvalues of $K(A_{1}\otimes I, I\otimes A_{1})$ are $K(t_{i}, t_{j})(1\leq i, j\leq n)$, this means that
\begin{align}
\frac{d(\Lambda\circ f)}{d\Lambda}(A_{1})& \ =\ \det K(A_{1}\otimes I, I\otimes A_{1}) \notag \\
& \ = \ \exp(\mathrm{Tr}_{n}\otimes \mathrm{Tr}_{n})(L(A_{1}\otimes I, I\otimes A_{1})).
\label{ch06:eqn6.3.12}
\end{align}
For any $\delta>0$ we choose a real polynomial $P(s, t)$ of two variables such that $|L(s, t)-P(s, t)|<\delta$ on $[-R, R]^{2}$. Then by (\ref{ch06:eqn6.3.10}) and (\ref{ch06:eqn6.3.12}), we have
\begin{align*}
& \left|\frac{1}{n^{2}}\log\frac{d(\Lambda\circ f)}{d\Lambda}(A_{1})-[\chi(f(a_{1}))-\chi(a_{1})]\right| \\
& \quad =|(\mathrm{tr}_{n}\otimes \mathrm{tr}_{n})(L(A_{1}\otimes I, I\otimes A_{1}))-(\tau\otimes\tau)(L(a_{1}\otimes \mathbf{1},\mathbf{1}\otimes a_{1}))| \\
& \quad \leq 2\delta+|(\mathrm{tr}_{n}\otimes \mathrm{tr}_{n})(P(A_{1}\otimes I, I\otimes A_{1}))-(\tau\otimes\tau)(P(a_{1}\otimes \mathbf{1},\mathbf{1}\otimes a_{1}))| \\
& \quad \leq 3\delta
\end{align*}
for all $(A_{1}, \ldots, A_{N})\in\Gamma_{R}(a_{1}, \ldots, a_{N};n, r_{1}, \varepsilon_{1}), \, n\in \mathbb{N}$, whenever $r_{1}$ is large enough and $\varepsilon_{1}$ is small enough. This and (\ref{ch06:eqn6.3.11}) imply that
\begin{align*}
& \chi_{R_{1}}(f(a_{1}), a_{2}, \ldots, a_{N};r, \varepsilon) \\
& \qquad \geq\chi_{R}(a_{1}, \ldots, a_{N};r_{1},\varepsilon_{1})-3\delta+\chi(f(a_{1}))-\chi(a_{1}) ,
\end{align*}
which gives (\ref{ch06:eqn6.3.9}) by Proposition~\ref{ch06:pro6.1.4}.

Finally, when $f_{1}, \ldots, f_{N}$ are strictly increasing, we can apply the already known inequality to the inverse functions, so the reverse inequality is obtained.
\end{proof2}

\section{Additivity of free entropy}
\label{ch06:sec6.4}\index{approximately free}\index{free!approximately}

The subadditivity of free entropy, $\chi(a_{1}, \ldots, a_{N})\leq\chi(a_{1})+\cdots+\chi(a_{N})$, holds for any $N$-tuple of selfadjoint random variables. A crucial problem is to characterize when equality holds here. This problem was settled by Voiculescu: The free relation of noncommutative selfadjoint random variables is equivalent to the additivity of their free entropy.

The highlight of the section is the following \textit{additivity theorem}.\index{free entropy!additivity theorem}\index{additivity theorem}

\begin{theorem}
\label{ch06:the6.4.1}
Let $a_{1}, \ldots, a_{N}\in \mathcal{M}^{sa}$. If $a_{1}, \ldots, a_{N}$ are in free relation, then
\begin{equation*}
\chi(a_{1}, \ldots, a_{N})=\chi(a_{1})+\cdots+\chi(a_{N})\,.
\end{equation*}
Conversely, if $\chi(a_{i})>-\infty$ for  $1 \leq i\leq N$ and the above equality holds, then $a_{1}, \ldots, a_{N}$ are in free relation.
\end{theorem}

The proof will be divided into two parts. First, we prepare an approximate freeness property of Haar distributed unitary matrices, which will be a key technique in proving that the free relation implies additivity.

The notion of \textit{approximate freeness} for matrices is introduced as follows. Let $(M_{n}(\mathbb{C})^{\star N}, \mathrm{tr}_{n}^{\star N})$ be the free product of $N$ copies of $(M_{n}(\mathbb{C}), \mathrm{tr}_{n})$, and let $j_{i}$ be the injection of $M_{n}(\mathbb{C})$ into its $i$th copy in $M_{n}(\mathbb{C})^{\star N}$. When $\Omega_{i}\subset M_{n}(\mathbb{C})(1\leq i\leq N), r\in \mathbb{N}$ and $\varepsilon>0$ are given, the subsets $\Omega_{1}, \ldots, \Omega_{N}$ are said to be $(r, \varepsilon)$-\textit{free} if
\begin{equation*}
|\mathrm{tr}_{n}(A_{1}A_{2}\ldots A_{k})-\mathrm{tr}_{n}^{\star N}(\tilde{A}_{1}\tilde{A}_{2}\cdots\tilde{A}_{k})|\leq\varepsilon
\end{equation*}
for all $A_{1}, A_{2}\ldots, A_{k}\in \bigsqcup_{i=1}^{N}\Omega_{i}, 1\leq k\leq r$, where $\tilde{A} :=j_{i}(A)$ for $A\in\Omega_{i}$.

\begin{lemma}
\label{ch06:lem6.4.2}
Let $r\in \mathbb{N}, \,\varepsilon >0, \, R>0$ and $\theta>0$ be given. Then there exists $n_{0}\in \mathbb{N}$ such that
\begin{align*}
& \gamma_{n}^{\otimes N}(\{(U_{1}, \ldots, U_{N})\in \mathcal{U}(n)^{N} : \{A_{1}^{(0)}, \ldots, A_{k_{0}}^{(0)}\}, \{U_{1}A_{1}^{(1)}U_{1}^{*}, \ldots, U_{1}A_{k_{1}}^{(1)}U_{1}^{*}\},\\
& \qquad \qquad \ldots, \{U_{N}A_{1}^{(N)}U_{N}^{*}, \ldots, U_{N}A_{k_{N}}^{(N)}U_{N}^{*}\} \ are \ (r, \varepsilon)\mbox{-}free\})\geq 1-\theta
\end{align*}
for all $n\geq n_{0}$ and for any choice of $1 \leq N\leq r, \, 1\leq k_{0}, k_{1}, \ldots, k_{N}\leq r$ and $A_{j}^{(i)}\in M_{n}(\mathbb{C})$ with $\Vert A_{j}^{(i)}\Vert\leq R\ (0\leq i\leq N, \, 1\leq j\leq k_{i})$.
\end{lemma}

\begin{proof2}
We may fix $N$ and $k_{0}, k_{1}, \ldots, k_{N}$. When $(U_{1}, \ldots, U_{N})\in \mathcal{U}(n)^{N}$ and $A_{j}^{(i)}\in M_{n}(\mathbb{C})$ are given as above, let $\Omega_{i}:=\{U_{i}A_{1}^{(i)}U_{i}^{*}, \ldots, U_{i}A_{k_{i}}^{(i)}U_{i}^{*}\}$ for $0\leq i\leq N$, with $U_{0}:=I_{n}$. Moreover, let $B^{(1)}, \ldots, B^{(p)}$ be an enumeration of
\begin{equation*}
T_{1}T_{2}\cdots T_{s}-\mathrm{tr}_{n}(T_{1}T_{2}\cdots T_{s})I_{n},
\end{equation*}
where $1\leq s\leq r$ and $T_{1}, \ldots, T_{s}$ are from $\{A_{1}^{(i)}, \ldots, A_{k_{i}}^{(i)}\}$ for some $0\leq i\leq N$. Then $\mathrm{tr}_{n}(B^{(t)})=0$ and $\Vert B^{(t)}\Vert\leq 2(1+R)^{r}$ for $1\leq t\leq p$. It can be easily seen that every joint moment $\mathrm{tr}_{n}(A_{1}A_{2}\cdots A_{k})$ , where $1\leq k\leq r$ and $A_{1}, \ldots, A_{k}\in \bigsqcup_{i=0}^{N}\Omega_{i}$, is a polynomial of $\mathrm{tr}_{n}(T_{1}T_{2}\cdots T_{s})$ as above and
\begin{equation}
\mathrm{tr}_{n}(U_{i_{1}}^{m_{1}}B_{1}U_{i_{2}}^{m_{2}}B_{2}\cdots U_{i_{l}}^{m_{l}}B_{l}),
\label{ch06:eqn6.4.1}
\end{equation}
where $l\leq 2r,\,1\leq i_{1}, \ldots, i_{l}\leq N, \, m_{1}, \ldots, m_{l}\in \mathbb{Z}\, \backslash \, \{\mathrm{0}\}$ and for $1\leq j\leq l$ either $B_{j}\in\{B^{(1)}, \ldots, B^{(p)}\}$ or $B_{j}=I_{n}$ and $i_{j}\neq i_{j+1}$ (with $i_{l+1}=i_{1}$). Here note that the polynomial is determined only by the pattern of $(A_{1}, \ldots, A_{k})$ from $\bigsqcup_{i=0}^{N}\Omega_{i}$, independently of $n$ and the choice of $U_{i}$ and $A_{j}^{(i)}$. According to Lemma~\ref{ch04:lem4.3.2}, the term (\ref{ch06:eqn6.4.1}) converges to $0$ as $ n\rightarrow\infty$ in $L^{2}$-mean and hence in probability with respect to $\gamma_{n}^{\otimes N}$, and the convergence is uniform for the choice of $A_{j}^{(i)}$ with $\Vert A_{j}^{(i)}\Vert\leq R$. Since $\mathrm{tr}_{n}^{\star N}(\tilde{A}_{1}\tilde{A}_{2}\cdots\tilde{A}_{k})$ is given by putting $0$ into all terms (\ref{ch06:eqn6.4.1}) in the polynomial, we observe that
\begin{equation*}
|\mathrm{tr}_{n}(A_{1}A_{2}\cdots A_{k})-\mathrm{tr}_{n}^{\star N}(\tilde{A}_{1}\tilde{A}_{2}\cdots\tilde{A}_{k})|\rightarrow 0 \quad \mathrm{as} \quad n\rightarrow\infty
\end{equation*}
in probability, and the convergence is uniform as above. Thus the conclusion follows because a finite number of $(A_{1}, \ldots, A_{k})$ are involved.
\end{proof2}

\begin{lemma}
\label{ch06:lem6.4.3}
Let $a_{1}, \ldots,  a_{N}\in \mathcal{M}^{sa}$ and $1\leq L<N$. Assume that $\{a_{1}, \ldots, a_{L}\}$ and $\{a_{L+1}, \ldots, a_{N}\}$ are free, $\chi(a_{1}, \ldots, a_{L})>-\infty$ and $\chi(a_{L+1}, \ldots , a_{N})>-\infty$. Then, for every $r\in \mathbb{N}, \, \varepsilon >0$ and $ R>\max_{i}\Vert a_{i}\Vert$, there exists $\varepsilon_{1}>0$ such that
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{\Lambda(\Xi_{n}(r,\varepsilon_1)\cap\Theta_{n}(r,\varepsilon))}{\Lambda(\Xi_n(r,\varepsilon_{1}))}=1,
\end{equation*}
where
\begin{align*}
& \Xi_{n}(r, \varepsilon_{1}) :=\Gamma_{R}(a_{1}, \ldots, a_{L};n, r, \varepsilon_{1})\times\Gamma_{R}(a_{L+1}, \ldots, a_{N};n, r, \varepsilon_{1}), \\ & \Theta_{n}(r, \varepsilon):=\Gamma_{R}(a_{1}, \ldots, a_{N};n, r, \varepsilon).
\end{align*}
\end{lemma}

\begin{proof2}
From the above definition of approximate freeness as well as the freeness of $\{a_{1}, \ldots, a_{L}\}$ and $\{a_{L+1}, \ldots, a_{N}\}$, for given $r,\,  \varepsilon$ and $R$ we can choose $\varepsilon_{1}> 0$ such that if $(A_{1}, \ldots, A_{L};A_{L+1}, \ldots, A_{N})\in\Xi_{n}(r, \varepsilon_{1})$ and if $\{A_{1}, \ldots, A_{L}\}$ and $\{A_{L+1}, \ldots, A_{N}\}$ are $(r, \varepsilon_{1})$-free, then $(A_{1}, \ldots, A_{L};A_{L+1}, \ldots, A_{N})\in\Theta_{n}(r, \varepsilon)$. For every $\theta>0$, by the previous lemma there is an $n_{0}\in \mathbb{N}$ such that
\begin{align}
& \gamma_{n}(\{U\in \mathcal{U}(n): \label{ch06:eqn6.4.2} \\
& \qquad \{A_{1}, \ldots, A_{L}\} \ \mathrm{and} \ \{UA_{L+1}U^{*}, \ldots, UA_{N}U^{*}\} \ \mathrm{are} \ (r, \varepsilon_{1})\mbox{-}\mathrm{free}\})\geq 1-\theta \notag
\end{align}
for all $n\geq n_{0}$ and for all $A_{i}\in M_{n}(\mathbb{C})^{sa}$ with $\Vert A_{i}\Vert\leq R(1\leq i\leq N)$. According to the assumption of finite free entropies and Proposition~\ref{ch06:pro6.1.4}, the $\Lambda$-measure of $\Xi_{n}(r, \varepsilon_{1})$ is positive whenever $n$ is large. For such $n$ the probability measure $\sigma_{n}$ on $\Xi_{n}(r, \varepsilon_{1})$ can be defined by normalizing the restriction of $\Lambda$ on $\Xi_{n}(r, \varepsilon_{1})$. Then, since $\sigma_{n}$ as well as the set $\Xi_{n}(r, \varepsilon_1)$ is invariant under the action of $\mathcal{U}(n)$ given by $(A_{1}, \ldots, A_{L};A_{L+1}, \ldots, A_{N})\mapsto(A_{1}, \ldots, A_{L};UA_{L+1}U^{*}, \ldots, UA_{N}U^{*})$ for $U\in \mathcal{U}(n)$, we have
\begin{align*}
& \frac{\Lambda(\Xi_{n}-(r,\varepsilon_{1})\cap\Theta_{n}(r,\varepsilon))}{\Lambda(\Xi_{n}-(r,\varepsilon_{1}))} \\
& \qquad =\int_{\Xi_n (r,\varepsilon_{1})} \left(\int_{\mathcal{U}(n)}\psi(A_{1}, \ldots, A_{L};UA_{L+1}U^{*}, \ldots, UA_{N}U^{*})\,d\gamma_{n}(U)\right)d\sigma_{n},
\end{align*}
where $\psi$ is the characteristic function of $\Xi_{n}(r, \varepsilon_{1})\cap\Theta_{n}(r, \varepsilon)$. The choice of $\varepsilon_1$ and (\ref{ch06:eqn6.4.2}) show that
\begin{equation*}
\int_{\mathcal{U}(n)}\psi(A_{1}, \ldots, A_{L};UA_{L+1}U^{*}, \ldots, UA_{N}U^{*})d\gamma_{n}(U)\geq 1-\theta
\end{equation*}
for all $(A_{1}, \ldots, A_{L};A_{L+1}, \ldots, A_{N})\in\Xi_{n}(r, \varepsilon_{1})$. Therefore, we infer that
\begin{equation*}
\frac{\Lambda(\Xi_{n}-(r,\varepsilon_1)\cap\Theta_{n}(r,\varepsilon))}{\Lambda(\Xi{n}(r,\varepsilon_{1}))}\geq 1-\theta
\end{equation*}
whenever $n$ is large enough, and the conclusion is obtained.
\end{proof2}

\begin{proof1}[Proof of Theorem~\ref{ch06:the6.4.1} (the first part)] We can prove a slightly stronger result, namely,
\begin{equation}
\chi(a_{1}, a_{2}, \ldots, a_{N})=\chi(a_{1})+\chi(a_{2}, \ldots, a_{N})
\label{ch06:eqn6.4.3}
\end{equation}
when $a_{1}$ and $\{a_{2}, \ldots, a_{N}\}$ are in free relation. By Proposition~\ref{ch06:pro6.1.3} it suffices to show that $\chi(a_{1}, a_{2}, \ldots, a_{N})\geq\chi(a_{1})+\chi(a_{2}, \ldots, a_{N})$, so we may assume that $\chi(a_{1})>-\infty$ and $\chi(a_{2}, \ldots, a_{N})>-\infty$. Hence for any $r\in \mathbb{N}, \, \varepsilon >0$ and $R> \max_{i}\Vert a_{i}\Vert$, there is an $\varepsilon >0$ for which the conclusion of Lemma~\ref{ch06:lem6.4.3} is satisfied when applied to $a_{1}$ and $\{a_{2}, \ldots, a_{N}\}$. Then, in the notation of Lemma~\ref{ch06:lem6.4.3},
\begin{align*}
& \chi_{R}(a_{1}, \ldots, a_{N};r, \varepsilon) \\
& \qquad =\underset{n\rightarrow\infty}{\limsup}\left[\frac{1}{n^{2}}\log\Lambda(\Theta_{n}(r, \varepsilon))+\frac{N}{2}\log n\right] \\
& \qquad \geq \underset{n\rightarrow\infty}{\limsup}\left[\frac{1}{n^{2}}\log\Lambda(\Xi_{n}(r, \varepsilon_{1}))+\frac{N}{2}\log n\right] \\
& \qquad =\underset{n\rightarrow\infty}{\limsup}\left[\frac{1}{n^{2}}\log(\Gamma_{R}(a_{1};n, r, \varepsilon_{1}))+\frac{1}{2}\log n \right.\\
& \qquad \qquad\quad  \ \left.+\frac{1}{n^{2}}\log\Lambda(\Gamma_{R}(a_{2}, \ldots, a_{N};n, r, \varepsilon_1)+\frac{N-1}{2}\log n\right] \\
& \qquad =\chi_{R}(a_{1};r, \varepsilon_{1})+\chi_{R}(a_{2}, \ldots, a_{N};r, \varepsilon_{1}) \\
& \qquad \geq\chi(a_{1})+\chi(a_{2}, \ldots, a_{N})\,.
\end{align*}
The above third equality is due to the existence of the limit in (\ref{ch06:eqn6.1.1}). Therefore, (\ref{ch06:eqn6.4.3}) is obtained.
\end{proof1}

The other part of the additivity theorem states that the additivity of free entropy implies the free relation. The strategy consists in proving this first for semicircular random variables. The discussion of the semicircular case is related to a maximization problem, more precisely, to the uniqueness of the maximizer.

Let $a_{1}, \ldots, a_{N}\in \mathcal{M}^{sa}$ be noncommutative random variables with $\tau(a_{i}^{2})=C_{i} (1\leq i\leq N)$. Proposition~\ref{ch06:pro6.1.1} tells us that
\begin{equation*}
\chi(a_{1}, \ldots, a_{N})\leq\sum_{i=1}^{N}\chi(a_{i})\leq\frac{1}{2}\sum_{i=1}^{N}\log(2\pi eC_{i})\leq\frac{N}{2}\log\left(\frac{2\pi e}{N}\sum_{i=1}^{N}C_{i}\right).
\end{equation*}
Equality holds in the first inequality above when $a_{1}, \ldots, a_{N}$ are in free relation. This is provided by the already proven part of the additivity theorem. The equality case in the second inequality is charaterized thanks to Proposition~\ref{ch05:pro5.3.4}; that is, equality takes place if and only if $a_{i}$ has the semicircle distribution $w_{2\sqrt{C_{i}}}$ for each $i$. The equality in the third inequality is equivalent to $C_{1}=\ldots=C_{N}$. In this way, we infer that under the constraint $\tau(a_{1}^{2}+\cdots+a_{N}^{2})\leq C$ the free entropy $\chi(a_{1}, \ldots, a_{N})$ attains the maximum $\frac{N}{2}\log(2\pi eC/N)$ if $a_{1}, \ldots, a_{N}$ are in free relation and the distribution of each $a_{i}$ is $w_{r}, \, r=2\sqrt{C/N}$. The content of the next theorem is the uniqueness of the maximizer.

\begin{theorem}
\label{ch06:the6.4.4}
If $a_{1}, \ldots, a_{N}\in \mathcal{M}^{sa}$ satisfy $\tau(a_{1}^{2}+\cdots+a_{N}^{2})\leq C$, then the free entropy $\chi(a_{1}, \ldots, a_{N})$ attains the maximum $\frac{N}{2}\log(2\pi eC/N)$ if and only if $a_{1}, \ldots, a_{N}$ are in free relation and the distribution of each $a_{i}$ is $w_{r}, \, r=2\sqrt{C/N}$.
\end{theorem}

\begin{proof2}
By the above argument it suffices to show that the joint distribution of $(a_{1}, \ldots, a_{N})$ for which the maximal value is attained is uniquely determined. We may assume that $C=N$ and the distribution of each $a_{i}$ is $w_{2}$ (this is necessary to attain the maximum). In particular, $\tau(a_{i})=0$ and $\tau(a_{i}^{2})=1$.

We are going to show that any local maximizer $(a_{1}, \ldots, a_{N})$ satisfying the above conditions must be free. We perturb $a_{i}$ by a noncommutative polynomial $P=P^{*}$. According to Proposition~\ref{ch06:pro6.3.3},
\begin{align*}
& \chi(a_{1}, \ldots, a_{i-1}, a_{i}+\rho P(a_{1}, \ldots, a_{N}), a_{i+1}, \ldots, a_{N})\\
& \qquad =\chi(a_{1}, \ldots, a_{N})+\log|\mathcal{J}|(F_{\rho})(a_{1}, \ldots, a_{N})
\end{align*}
for all $\rho\in \mathbb{R}$ near $0$. Since
\begin{equation*}
b_{i}(\rho) :=\tau((a_{i}+\rho P(a_{1}, \ldots, a_{N}))^{2})^{-1/2}(a_{i}+\rho P(a_{1},\ \ldots,\ a_{N}))
\end{equation*}
belongs to $\mathcal{M}^{sa}$ and $\tau(b_{i}(\rho)^{2})=1$, we have
\begin{align*}
\chi(a_{1}, \ldots, a_{N}) & \ \geq \ \chi(a_{1}, \ldots, a_{i-1}, b_{i}(\rho), a_{i+1}, \ldots, a_{N}) \\
& \ = \ \chi(a_{1}, \ldots, a_{i-1}, a_{i}+\rho P(a_{1}, \ldots, a_{N}), a_{i+1}, \ldots, a_{N}) \\
& \qquad \quad -\frac{1}{2}\log\tau((a_{i}+\rho P(a_{1}, \ldots, a_{N}))^{2}) \\
& = \ \chi(a_{1}, \ldots, a_{N})+\log|\mathcal{J}|(F_{\rho})(a_{1}, \ldots, a_{N}) \\
& \qquad \quad -\frac{1}{2}\log\tau((a_{i}+\rho P(a_{1}, \ldots, a_{N}))^{2})
\end{align*}
by assumption and Corollary~\ref{ch06:cor6.3.2}. Therefore,
\begin{equation}
\log|\mathcal{J}|(F_{\rho})(a_{1}, \ldots, a_{N})-\frac{1}{2}\log\tau((a_{i}+\rho P(a_{1}, \ldots, a_{N}))^{2})\leq 0
\label{ch06:eqn6.4.4}
\end{equation}
when $|\rho|$ is small. Furthermore, the following is immediate to check (cf. Example~\ref{ch06:exa6.3.4}):
\begin{equation*}
\log|\mathcal{J}|(F_{\rho})(a_{1}, \ldots, a_{N})=\frac{1}{2}(\mathrm{Tr}_{N}\otimes\tau\otimes\tau)(\log(\mathbf{1}+\rho x+\rho^{2}y)),
\end{equation*}
where $x$ and $y$ are selfadjoint elements of $M_{N}(\mathbb{C})\otimes(\mathcal{M}\otimes\mathcal{M}^{op})$ and
\begin{equation*}
(\tau\otimes\tau)(x)=2(\tau\otimes\tau)(D_{i}P(a_{1}, \ldots, a_{N}))\,.
\end{equation*}
Therefore, the Taylor expansion of the logarithm yields
\begin{equation*}
\log|\mathcal{J}|(F_{\rho})(a_{1}, \ldots, a_{N})=\rho(\tau\otimes\tau)(D_{i}P(a_{1}, \ldots, a_{N}))+O(\rho^{2})\,,
\end{equation*}
so that
\begin{equation*}
\left.\frac{d}{d\rho}\right|_{\rho=0} {\log|\mathcal{J}|(F_{\rho})(a_{1},\ldots,a_{N})=(\tau\otimes\tau)(D_{i}P(a_{1},\ldots,a_{N}))}\,.
\end{equation*}
On the other hand, since $\tau(a_{i}^{2})=1$, it is obvious that
\begin{equation*}
\left.\frac{d}{d\rho}\right|_{\rho=0}{\frac{1}{2}\log\tau((a_{i}+\rho P(a_{1},\ldots,a_{N}))^{2})=\tau(a_{i}P(a_{1},\ldots,a_{N}))}\,.
\end{equation*}
By (\ref{ch06:eqn6.4.4}) these imply that
\begin{equation}
\tau(a_{i}P(a_{1}, \ldots, a_{N}))=(\tau\otimes\tau)(D_{i}P(a_{1}, \ldots, a_{N}))\,.
\label{ch06:eqn6.4.5}
\end{equation}
Since both sides of (\ref{ch06:eqn6.4.5}) are linear in $P$, this relation must hold for any polynomial, not only for the selfadjoint ones.

Let $1\leq i_{1}, \ldots, i_{k}\leq N$ and choose $i=i_{1}$ and $P(X_{1}, \ldots, X_{N})=X_{i_{2}}\cdots X_{i_{k}}$. Then from (\ref{ch06:eqn6.4.5}) we have
\begin{equation*}
\tau(a_{i_{1}}a_{i_{2}}\cdots a_{i_{k}})=\sum_{s:i_{s}=i_{1}}\tau(a_{i_{2}}\cdots a_{i_{s-1}})\tau(a_{i_{s+1}}\cdots a_{i_{k}}).
\end{equation*}
This recursion formula uniquely determines the joint distribution of $(a_{1}, \ldots, a_{N})$, and it gives the conclusion.
\end{proof2}

We are now in a position to complete the proof of the additivity theorem. For standard semicircular variables the full statement is already shown; additivity implying free relation is contained in the previous theorem. The strategy is to treat the general random variables as functions of semicircular ones.

\begin{proof1}[Proof of Theorem~\ref{ch06:the6.4.1} (the second part)]
For each $i$, since the distribution of $a_{i}$ is nonatomic by the assumption $\chi(a_{i})>-\infty$, one can choose a continuous and increasing function $f_{i}: \mathbb{R}\rightarrow \mathbb{R}$ such that the distribution of $f_{i}(a_{i})$ is the standard semicircle law $w_{2}$. Then, by Proposition~\ref{ch06:pro6.3.6} and the additivity assumption, we have
\begin{equation*}
\chi(f_{1}(a_{1}), \ldots, f_{N}(a_{N}))\geq\sum_{i=1}^{N}\chi(f_{i}(a_{i}))=\frac{N}{2}\log(2\pi e),
\end{equation*}
so that $\chi(f_{1}(a_{1}), \ldots, f_{N}(a_{n}))$ takes the maximal value $\frac{N}{2}\log(2\pi e)$. Hence Theorem~\ref{ch06:the6.4.4} implies that $f_{1}(a_{1}), \ldots, f_{N}(a_{N})$ are in free relation, and so are $a_{1}, \ldots, a_{N}$ because $a_{i}\in\{f_{i}(a_{i})\}''$.
\end{proof1}

In the proof of the additivity theorem we obtained the equality (\ref{ch06:eqn6.4.3}) under the condition that $a_{1}$ and $\{a_{2}, \ldots, a_{N}\}$ are in free relation. This is slightly stronger than the statement of the first part of Theorem~\ref{ch06:the6.4.1}, so we may call it the \textit{strong additivity theorem}.\index{free entropy!strong additivity theorem}\index{strong additivity theorem} The converse is not known in this case, and it is an interesting question whether the equality
\begin{equation}
\chi(a_{1}, \ldots, a_{L}, a_{L+1}, \ldots, a_{N})=\chi(a_{1}, \ldots, a_{L})+\chi(a_{L+1}, \ldots, a_{N})
\label{ch06:eqn6.4.6}
\end{equation}
holds when $\{a_{1}, \ldots, a_{L}\}$ and $\{a_{L+1}, \ldots, a_{N}\}$ are in free relation. The question is open, but one can see from the above proof that the block additivity (\ref{ch06:eqn6.4.6}) holds if the first block $(a_{1},\ldots,a_{L})$ is \textit{ regular},\index{regular multivariable} that is, replacing $\lim \sup$ in the definition $\chi(a_{1},\ldots,a_{L})$ by $\lim \inf$ yields the same quantity.

\section{Free entropies of unitary and non-selfadjoint random variables}
\label{ch06:sec6.5}\index{noncommutative!random variables, regular}

The distribution measure of a unitary random variable is supported on the unit circle, and the large deviation theorem for random unitaries\index{free entropy!for unitaries} suggests that the free entropy must be the double logarithmic integral of the distribution. Given an $N$-tuple of unitaries, one should follow a different approach: The $N$-tuple can be approximated in distribution by unitary matrices; the measure of the approximating unitaries can be taken, and one can imitate the definition of the free entropy of an $N$-tuple of selfadjoint random variables. The free entropy of an $N$-tuple of non-selfadjoint random variables is similar. Simple non-selfadjoint matrices are used in the matricial approximation.

Let $u_{1}, \ldots, u_{N}\in \mathrm{M}$ be unitaries. For $n, r\in \mathbb{N}$ and $\varepsilon>0$ define
\begin{align*}
& \Gamma_{u}(u_{1},\ldots,u_{N};n, r, \varepsilon) :=\{(U_{1},\ldots,U_{N})\in \mathcal{U}(n)^{N}: \\
& \quad |\mathrm{tr}_{n}(U_{i_{1}}'\cdots U_{i_{k}}')-\tau(u_{i_{1}}'\cdots u_{i_{k}}')|\leq\varepsilon \ \mathrm{for \ all} \ 1\leq i_{1}, \ldots, i_{k}\leq 2N, \, 1\leq k\leq r\},
\end{align*}
where
\begin{align*}
& (U_{1}',\ldots,U_{2N}') :=(U_{1},\ldots,U_{N}, U_{1}^{*},\ldots,U_{N}^{*}), \\
& (u_{1}',\ldots,u_{2N}') :=(u_{1},\ldots,u_{N}, u_{1}^{*},\ldots, u_{N}^{*}).
\end{align*}
The \textit{free entropy} of the $N$-tuple $(u_{1}, \ldots, u_{N})$ of unitaries is defined as follows:
\begin{equation*}
\chi_{u}(u_{1},\ldots,u_{N};r, \varepsilon):=\limsup_{n\rightarrow\infty}\frac{1}{n^{2}}\log\gamma(\Gamma_{u}(u_{1},\ldots,u_{N};n, r, \varepsilon))
\end{equation*}
(here $\gamma$ is the Haar probability measure on $\mathcal{U}(n)^{N}$, i.e. $\gamma=\gamma_{n}^{\otimes N}$),
\begin{equation*}
\chi_{u}(u_{1},\ldots,u_{N}):= \lim_{\begin{subarray}{c}r\rightarrow\infty, \\ \varepsilon\rightarrow+0\end{subarray}}\chi_{u}(u_{1},\ldots,u_{N};r, \varepsilon).
\end{equation*}

For the case of a single unitary $u\in \mathcal{M}$ we have
\begin{align*}
& \Gamma_{u}(u;n, r, \varepsilon)=\{U\in \mathcal{U}(n) : |\mathrm{tr}_{n}(U^{k})-m_{k}(\mu)|\leq\varepsilon, \, -r\leq k\leq r\},\\
& \gamma_{n}(\Gamma_{u}(u;n, r, \varepsilon))=\overline{\gamma}_{n}(\{\zeta\in \mathbb{T}^{n}: \kappa_{n}(\zeta)\in F(r, \varepsilon)\}),
\end{align*}
where $\mu$ is the distribution on $\mathbb{T}$ of $u,\,\overline{\gamma}_{n}$ is the measure on $\mathbb{T}^{n}$ induced by $\gamma_{n}$ (having the density given in Lemma~\ref{ch04:lem4.2.1}), and
\begin{equation*}
F(r, \varepsilon) :=\{\nu \in \mathcal{M}(\mathbb{T}) : |m_{k}(\nu)-m_{k}(\mu)|\leq\varepsilon, \, -r\leq k\leq r\}.
\end{equation*}
To show that the one variable case reduces to the double logarithmic integral, one can proceed as in the proof of Theorem~\ref{ch05:the5.6.2}. The large deviation result in Theorem~\ref{ch05:the5.4.10} with $Q(\zeta)=0$ (then $B=0$) implies that
\begin{align*}
& \limsup_{n\rightarrow \infty}\frac{1}{n^{2}}\log\overline{\gamma}_{n}(\{(\in \mathbb{T}^{n}:\kappa_{n}(\zeta)\in F(r, \varepsilon)\}) \\
& \qquad \leq \ \sup\{\Sigma(\nu): \nu \in F(r, \varepsilon)\}= \sup\{\Sigma(\nu):\nu\in G(r, \varepsilon)\} \\
& \qquad \leq\liminf_{n\rightarrow\infty}\frac{1}{n^{2}}\log\overline{\gamma}_{n}\ (\{\zeta\in \mathbb{T}^{n} : \kappa_{n}(\zeta)\in G(r, \varepsilon)\}) ,
\end{align*}
where $G(r, \varepsilon)$ is gotten by replacing $\leq\varepsilon$ by $<\varepsilon$ in the definition of $F(r, \varepsilon)$. Therefore,
\begin{align}
\chi_{u}(u;r, \varepsilon) & \ = \ \lim_{n\rightarrow\infty}\frac{1}{n^{2}}\log\gamma_{n}(\Gamma_{u}(u;n, r, \varepsilon))
\label{ch06:eqn6.5.1} \\
& \  = \ \sup\{\Sigma(\nu):\nu \in F(r, \varepsilon)\}.\notag
\end{align}
Since the latter tends to $\Sigma(\mu)$ as $ r\rightarrow\infty$ and $\varepsilon\rightarrow+0$, we observe that
\begin{equation}
\chi_{u}(u)=\Sigma(\mu)=\iint_{\mathbb{T}^{2}}\log|\zeta-\eta|\,d\mu(\zeta)\,d\mu(\eta).
\label{ch06:eqn6.5.2}
\end{equation}

Several properties of $\chi_{u}(u_{1},\ldots,u_{N})$ are completely similar to those of the previous $\chi(a_{1},\ldots,a_{N})$. For example, one readily verifies

\begin{proposition}
\label{ch06:pro6.5.1}
The following hold for unitaries $u_{1}, \ldots, u_{N}\in \mathcal{M}$.
\begin{enumerate}
\item[(1)] Negativity: $\chi_{u}(u_{1},\ldots,u_{N})\leq 0$.

\item[(2)] Subadditivity: \textit{For every} $1\leq L<N$,
\begin{equation*}
\chi_{u}(u_{1},\ldots,u_{N})\leq\chi_{u}(u_{1},\ldots,u_{L})+\chi_{u}(u_{L+1},\ldots,u_{N}).
\end{equation*}
\item[(3)] Upper semicontinuity: Let $(u_{1},\ldots,u_{N})$ and $(u_{m,1},\ldots,u_{m,N})$ be $N$-tuples of unitaries in $\mathcal{M}$ for $m\in \mathbb{N}$. If $(u_{m,1},\ldots,u_{m,N})\rightarrow(u_{1}, \ldots, u_{N})$ in distribution, then
\begin{equation*}
\chi_{u}(u_{1},\ldots,u_{N})\geq\limsup_{m\rightarrow\infty}\chi_{u}(u_{m,1},\ldots,u_{m,N}).
\end{equation*}
In particular, this is the case if $u_{m,i}\rightarrow u_{i}$ weakly for $1\leq i\leq N$.
\end{enumerate}
\end{proposition}

The next result is a unitary counterpart of Proposition~\ref{ch06:pro6.1.6}.

\begin{proposition}
\label{ch06:pro6.5.2}
Let $u_{1}, \ldots, u_{N}, v_{1}, \ldots, v_{N}\in \mathcal{M}$ be unitaries. If $v_{1}=u_{1}$ and $v_{i}u_{i}^{*}\in\{u_{1},\ldots,u_{i-1}\}''$ for $2\leq i\leq N$, then
\begin{equation*}
\chi_{u}(u_{1},\ldots,u_{N})=\chi_{u}(v_{1},\ldots,v_{N}).
\end{equation*}
\end{proposition}

\begin{proof2}
Since the assumption implies also that $u_{i}v_{i}^{*}\in\{v_{1},\ldots,v_{i-1}\}''$ for $ 2\leq i\leq N$, it suffices to show that
\begin{equation*}
\chi_{u}(u_{1},\ldots,u_{N})\leq\chi_{u}(v_{1},\ldots,v_{N}).
\end{equation*}
One can choose selfadjoint noncommutative polynomials $P_{m,i}(X_{1}, X_{2},\ldots,X_{2i-2})$ for $2\leq i\leq N, \, m\in \mathbb{N}$ such that
\begin{equation*}
\exp\left(\mathrm{i}\,P_{m,i}\left(\frac{u_{1}+u_{1}^{*}}{2}, \frac{u_{1}-u_{1}^{*}}{2\mathrm{i}},\ldots,\frac{u_{i-1}+u_{i-1}^{*}}{2}, \frac{u_{i-1}-u_{i-1}^{*}}{2\mathrm{i}}\right)\right)\rightarrow v_{i}u_{i}^{*}
\end{equation*}
strongly$^*$ as $m\rightarrow\infty$. Set $v_{m,1} :=v_{1}=u_{1}$ and, for $2\leq i\leq N$,
\begin{equation*}
v_{m,i}:=\exp\left(\mathrm{i}\, P_{m,i}\left(\frac{u_{1}+u_{1}^{*}}{2}, \frac{u_{1}-u_{1}^{*}}{2\mathrm{i}},\ldots,\frac{u_{i-1}+u_{i-1}^{*}}{2}, \frac{u_{i-1}-u_{i-1}^{*}}{2\mathrm{i}}\right)\right)u_{i}.
\end{equation*}
Then $v_{m,i} \ \rightarrow \ v_{i}$ strongly$^*$ as $m \ \rightarrow \ \infty$. If a map $\Psi \ : \ \mathcal{U}(n)^{N} \ \rightarrow \ \mathcal{U}(n)^{N}, \Psi(U_{1},\ldots,U_{N})=(V_{1},\ldots,V_{N})$, is defined by $V_{1} :=U_{1}$ and, for $2\leq i\leq N$,
\begin{equation*}
V_{i}:=\exp\left(\mathrm{i}\, P_{m,i} \left(\frac{U_{1}+U_{1}^{*}}{2}, \frac{U_{1}-U_{1}^{*}}{2\mathrm{i}},\ldots,  \frac{U_{i-1}+U_{i-1}^{*}}{2}, \frac{U_{i-1}-U_{i-1}^{*}}{2\mathrm{i}}\right)\right)U_{i},
\end{equation*}
then it is obvious that $\gamma \circ \Psi=\gamma$ holds due to the multiplication invariance of $\gamma$. For any $m, r\in \mathbb{N}$ and $\varepsilon>0$ one can easily see that there exist $r_{1}\in \mathbb{N}$ and $\varepsilon_{1}>0$ such that
\begin{equation*}
\Psi(\Gamma_{u}(u_{1},\ldots,u_{N};n, r_{1}, \varepsilon_{1}))\subset \Gamma_{u}(v_{m,1},\ldots,v_{m,N};n, r, \varepsilon) \qquad (n\in \mathbb{N}).
\end{equation*}
This yields
\begin{equation*}
\chi_{u}(u_{1},\ldots,u_{N};r_{1}, \varepsilon_{1})\leq\chi_{u}(v_{m,1},\ldots,v_{m,N};r, \varepsilon),
\end{equation*}
so that $\chi_{u}(u_{1},\ldots,u_{N})\leq\chi_{u}(v_{m,1},\ldots,v_{m,N})$. Hence the required inequality follows as $m\rightarrow\infty$, thanks to the upper semicontinuity.
\end{proof2}

The additivity theorem for $\chi_{u}(u_{1},\ldots,u_{n})$ is completely analogous to the selfadjoint case in Theorem~\ref{ch06:the6.4.1}.

\begin{theorem}
\label{ch06:the6.5.3}
Let $u_{1}, \ldots, u_{N}\in \mathcal{M}$ be unitaries. If $u_{1}, \ldots, u_{N}$ are in $^*$-free relation $($i.e. $\{u_{1}, u_{1}^{*}\},\ldots,\{u_{N}, u_{N}^{*}\}$ are free$)$, then
\begin{equation*}
\chi_{u}(u_{1},\ldots,u_{N})=\chi_{u}(u_{1})+\cdots+\chi_{u}(u_{N}).
\end{equation*}
Conversely, if $\chi_{u}(u_{i})>-\infty$ for $1 \leq i\leq N$ and the above equality holds, then $u_{1}, \ldots, u_{N}$ are in $^*$-free relation.
\end{theorem}

The proof of the theorem will be given in Sec.~\ref{ch06:sec6.6}.

Now let us turn to the free entropy of an $N$-tuple $(a_{1},\ldots,a_{N})$ of non-selfadjoint random variables. One possibility is to use the Descartes decomposition $a_{i}= b_{i}+\mathrm{i}\,c_{i}$ and to pass to the $2N$-tuple $(b_{1}, b_{2},\ldots,b_{N}, c_{1}, c_{2}, \ldots, c_{N})$ of selfadjoint variables. The other possibility is to approximate the non-selfadjoint\index{free entropy!for non-selfadjoint} variables with non-selfadjoint matrices. We are first going to follow the latter idea, but it turns out soon that the two possible ways are actually equivalent.

The Lebesgue measure $\hat{\Lambda}_{n}$ on $M_{n}(\mathbb{C})$ given in (\ref{ch04:eqn4.1.25}) is induced by the natural isometry $M_{n}(\mathbb{C})\cong \mathbb{R}^{2n^{2}}$. Consider the map $A\in M_{n}(\mathbb{C})\mapsto(B, C)\in(M_{n}(\mathbb{C})^{sa})^{2}$ which is given by the Descartes decomposition $A=B+\mathrm{i}\,C$. Since
\begin{equation*}
\Vert A\Vert_{HS}^{2}=\mathrm{Tr}(A^{*}A)=\mathrm{Tr}(B^{*}B+C^{*}C)=\Vert B\Vert_{HS}^{2}+\Vert C\Vert_{HS}^{2}
\end{equation*}
($\Vert \cdot \Vert_{HS}$ denotes the Hilbert-Schmidt norm), this map induces a linear isometry on $\mathbb{R}^{2n^{2}}$ via $M_{n}(\mathbb{C})\cong \mathbb{R}^{2n^{2}}\cong(M_{n}(\mathbb{C})^{sa})^{2}$, so we have

\begin{lemma}
\label{ch06:lem6.5.4}
Under the map $A\in M_{n}(\mathbb{C})\mapsto(B, C)\in(M_{n}(\mathbb{C})^{sa})^{2}$ above, $\hat{\Lambda}_{n}$ on $M_{n}(\mathbb{C})$ corresponds to $\Lambda_{n}\otimes\Lambda_{n}$ on $(M_{n}(\mathbb{C})^{sa})^{2}$.
\end{lemma}

Let $a_{1}, \ldots, a_{N}$ be (non-selfadjoint) elements of $\mathcal{M}$. The definition of the \textit{free entropy} of the $N$-tuple $(a_{1},\ldots,a_{N})$ is a slight modification of the selfadjoint case. For $n, r\in \mathbb{N},\,\varepsilon>0$ and $R>0$ we define
\begin{align*}
& \hat{\Gamma}_{R}(a_{1},\ldots,a_{N};n, r, \varepsilon) :=\{(A_{1},\ldots,A_{N})\in M_{n}(\mathbb{C})^{N} : \Vert A_{i}\Vert\leq R, \\
& \quad |\mathrm{tr}_{n}(A_{i_{1}}'\cdots A_{i_{k}}')-\tau(a_{i_{1}}'\cdots a_{i_{k}}')|\leq \varepsilon \ \mathrm{for \ all} \ 1\leq i_{1}, \ldots, i_{k}\leq 2N, \, 1\leq k\leq r\},
\end{align*}
where
\begin{align*}
&(A_{1}',\ldots,A_{2N}') :=(A_{1},\ldots,A_{N}, A_{1}^{*},\ldots,A_{N}^{*}), \\
& (a_{1}',\ldots,a_{2N}') :=(a_{1},\ldots,a_{N}, a_{1}^{*},\ldots, a_{N}^{*}).
\end{align*}
Moreover,
\begin{equation*}
\hat{\chi}_{R}(a_{1},\ldots,a_{N};r, \varepsilon) := \limsup_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\hat{\Lambda}(\hat{\Gamma}_{R}(a_{1},\ldots,a_{N};n, r, \varepsilon))+N\log n\right],
\end{equation*}
where $\hat{\Lambda}$ is short for $\hat{\Lambda}_{n}^{\otimes N}$. Then the definition of $\hat{\chi}(a_{1},\ldots,a_{N})$ follows the selfadjoint case.

\begin{proposition}
\label{ch06:pro6.5.5}
Let $a_{1}, \ldots, a_{N}\in \mathcal{M}$. If $b_{i} :=(a_{i}+a_{i}^{*})/2$ and $c_{i} :=(a_{i}-a_{i}^{*})/2\mathrm{i}$,
then
\begin{align*}
& \chi_{R}(b_{1}, c_{1},\ldots,b_{N}, c_{N})\geq\hat{\chi}_{R}(a_{1},\ldots,a_{N})\geq\chi_{R/2}(b_{1}, c_{1},\ldots,b_{N}, c_{N}), \\ & \hat{\chi}(a_{1},\ldots,a_{N})=\chi(b_{1}, c_{1},\ldots,b_{N}, c_{N}).
\end{align*}
\end{proposition}

\begin{proof2}
It is easy to check that
\begin{align*}
& \Gamma_{R}(b_{1}, c_{1},\ldots,b_{N}, c_{N};n, r, \varepsilon) \\
& \qquad \supset\{(B_{1}, C_{1},\ldots,B_{N}, C_{N})\in(M_{n}(\mathbb{C})^{sa})^{2N}:\\
& \qquad \qquad \qquad \ (B_{1}+\mathrm{i}\,C_{1},\ldots,B_{N}+\mathrm{i}\,C_{N})\in\hat{\Gamma}_{R}(a_{1},\ldots,a_{N};n, r, \varepsilon)\}, \\
& \hat{\Gamma}_{2R}(a_{1},\ldots,a_{N};n, r, \varepsilon) \\
& \qquad \supset\{(B_{1}+\mathrm{i}\,C_{1},\ldots,B_{N}+\mathrm{i}\,C_{N})\in M_{n}(\mathbb{C})^{N}\ : \\
& \qquad \qquad \qquad \ (B_{1}, C_{1},\ldots,B_{N}, C_{N})\in\Gamma_{R}(b_{1}, c_{1},\ldots,b_{N}, c_{N};n, r, \varepsilon/2^{r})\}.
\end{align*}
By Lemma~\ref{ch06:lem6.5.4} these imply that
\begin{align*}
& \chi_{R}(b_{1}, c_{1},\ldots,b_{N}, c_{N};r, \varepsilon)\geq\hat{\chi}_{R}(a_{1},\ldots,a_{N};r, \varepsilon), \\
& \hat{\chi}_{2R}(a_{1},\ldots,a_{N};r, \varepsilon)\geq\chi_{R}(b_{1}, c_{1},\ldots,b_{N}, c_{N};r, \varepsilon/2^{r}).
\end{align*}
Hence we get the conclusions.
\end{proof2}

The above proposition enables us to extend some properties of free entropy of selfadjoint random variables to the non-selfadjoint case. For instance, we have

\begin{proposition}
\label{ch06:pro6.5.6} $\hat{\chi}(a_{1},\ldots,a_{N})=\hat{\chi}_{R}(a_{1}, \ldots, a_{N})$ whenever $ R>2\max_{i}\Vert a_{i}\Vert$.
\end{proposition}

\begin{proposition}
\label{ch06:pro6.5.7}
For $a_{1}, \ldots, a_{N}\in\mathcal{M}$,
\begin{enumerate}
\item[(1)] $\hat{\chi}(\sum_{j}\alpha_{1j}a_{j}+\beta_{1}\mathbf{1},\ldots,\sum_{j}\alpha_{Nj}a_{j}+\beta_{N}\mathbf{1})=\hat{\chi}(a_{1},\ldots,a_{N})+2\log|\det A|$ for every $A=[\alpha_{ij}]\in M_{n}(\mathbb{C})$ and $\beta_{1}, \ldots, \beta_{N}\in \mathbb{C}$,

\item[(2)] $\hat{\chi}(a_{1},\ldots,a_{N})=-\infty$ when $a_{1}, \ldots, a_{N}$ are linearly dependent, and

\item[(3)] $\hat{\chi}(a_{1},\ldots,a_{N})$ does not change when any $a_{i}$ is replaced by $a_{i}^{*}$.
\end{enumerate}
\end{proposition}
The non-selfadjoint extension $\hat{\chi}(a_{1}, \ldots, a_{N})$ has the subadditivity and upper semicontinuity properties as well. Furthermore, from Example~\ref{ch06:exa6.3.4} and the subadditivity we know that $\hat{\chi}(a_{1},\ldots,a_{N})=-\infty$ if there is a normal element\index{elliptic!element} among $a_{1}, \ldots, a_{N}$. This means that the free entropy $\hat{\chi}$ is of no use for normal random variables.

It is evident that $a_{1}, \ldots, a_{N}$ are $^*$-free circular elements with the same variance $C$ if and only if $b_{1}, c_{1}, \ldots, b_{N}, c_{N}$ are free semicircular elements with the same variance $C/2$ (i.e. with radius $\sqrt{2C}$). So the following is the translation of Theorem~\ref{ch06:the6.4.4}.

\begin{proposition}
\label{ch06:pro6.5.8}
If $a_{1}, \ldots, a_{N}\in \mathcal{M}$ satisfy $\tau(a_{1}^{*}a_{1}+\cdots+a_{N}^{*}a_{N})\leq C$, then $\hat{\chi}(a_{1},\ldots,a_{N})\leq N\log(\pi eC/N)$, and equality is attained if and only if $a_{1}, \ldots, a_{N}$ are $^*$-free circular elements of the same variance $C/N$.
\end{proposition}

\begin{example}
\label{ch06:exa6.5.9} Let $a$ be an \textit{elliptic element} in Example~\ref{ch02:exa2.6.7}, that is,
\begin{equation*}
a=\sqrt{\frac{1+\alpha}{2}}b_{0}+\mathrm{i}\,\sqrt{\frac{1-\alpha}{2}}c_{0} \, ,
\end{equation*}
where $b_{0}, c_{0}$ are free standard semicircular elements and $-1<\alpha<1$. Then by Proposition~\ref{ch06:pro6.5.5} and Theorem~\ref{ch06:the6.4.1} we compute
\begin{equation*}
\hat{\chi}(a)=\chi\left(\sqrt{\frac{1+\alpha}{2}}b_{0}\right)+\chi\left(\sqrt{\frac{1-\alpha}{2}}c_{0}\right)=\log(\pi e\sqrt{1-\alpha^{2}}).
\end{equation*}
Consider the functional
\begin{equation*}
\hat{\chi}(a)-\left(\frac{\tau(b^{2})}{1+\alpha}+\frac{\tau(c^{2})}{1-\alpha}\right) \quad \mathrm{for} \quad a=b+\mathrm{i}\,c \  (b, c\in  \mathcal{M}^{sa}).
\end{equation*}
By Theorem~\ref{ch06:the6.4.1} and Proposition~\ref{ch05:pro5.3.4} we observe that this functional attains its maximal value if and only if $a$ is the above elliptic element.
\end{example}

\section{Relation between different free entropies}
\label{ch06:sec6.6}

The aim of this section is to make a bridge between free entropy of unitary random variables and that of non-selfadjoint ones via the polar decomposition. A key idea is as follows. Let $u_{1}, \ldots, u_{N}$ be unitaries and $h_{1}, \ldots, h_{N}$ be positive variables. On the one hand, we have the free entropy  $\hat{\chi}(u_{1}h_{1},\ldots,u_{N}h_{N})$ of the $N$-tuple of non-selfadjoint variables, and on the other hand we may consider the $2N$-tuple $(u_{1}, \ldots, u_{N}, h_{1},\ldots,h_{N})$, or rather $(u_{1},\ldots,u_{N}, h_{1}^{2}, \ldots, h_{N}^{2})$, in which unitary and positive random variables are mixed. We define the \textit{free entropy of} such \textit{mixed tuples},\index{free entropy!for mixed tuples} and next obtain its connection with $\hat{\chi}(u_{1}h_{1},\ldots,u_{N}h_{N})$.

The measure $\Lambda_{+,n}$ on $M_{n}(\mathbb{C})^{+}$ introduced in Sec.~\ref{ch04:sec4.4} will be used below. According to Lemma~\ref{ch04:lem4.4.7} this measure is the restriction of the Lebesgue measure $\Lambda_{n}$ on $M_{n}(\mathbb{C})^{+}$ with a different normalizing constant. Let $u_{1}, \ldots, u_{N}\in \mathcal{M}$ be unitaries and $h_{1}, \ldots, h_{L}\in \mathcal{M}^{+}$. For $n, r\in \mathbb{N}, \, \varepsilon>0$ and $R>0$ we define
\begin{align*}
& \Gamma_{(u,+),R}(u_{1},\ldots,u_{N};h_{1},\ldots,h_{L};n, r, \varepsilon)\\
& \qquad :=\{(U_{1}, \ldots, U_{N};H_{1},\ldots,H_{L})\in \mathcal{U}(n)^{N}\times(M_{n}(\mathbb{C})^{+})^{L} : \Vert H_{i}\Vert\leq R, \\
& \qquad \qquad \qquad \qquad \quad \ |\mathrm{tr}_{n}(B_{i_{1}}\cdots B_{i_{k}})-\tau(b_{i_{1}}\cdots b_{i_{k}})|\leq\varepsilon \\
& \qquad \qquad \qquad \qquad \quad \ \mathrm{for \ all} \ 1\leq i_{1}, \ldots, i_{k}\leq 2N+L, \ 1\leq k\leq r\},
\end{align*}
where
\begin{align*}
& (B_{1},\ldots,B_{2N+L}) :=(U_{1},\ldots,U_{N}, U_{1}^{*},\ldots,U_{N}^{*}, H_{1},\ldots,H_{L})\, , \\
& (b_{1},\ldots,b_{2N+L}) :=(u_{1},\ldots,u_{N}, u_{1}^{*},\ldots,u_{N}^{*}, h_{1},\ldots,h_{L})\, ,
\end{align*}
and further define
\begin{align*}
& \chi_{(u,+),R}(u_{1},\ldots,u_{N};h_{1},\ldots,h_{L};r, \varepsilon) \\
& \quad \ \ :=\limsup_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log(\gamma\otimes\Lambda_{+})(\Gamma_{(u,+),R}(u_{1},\ldots,u_{N};h_{1},\ldots,h_{L};n, r, \varepsilon)) \right.\\
& \qquad \qquad \qquad \qquad \ \ \left. +L\log n\right],
\end{align*}
where $\gamma\otimes\Lambda_{+}$ is used for $\gamma_{n}^{\otimes N}\otimes\Lambda_{+,n}^{\otimes L}$ on $\mathcal{U}(n)^{N}\times(M_{n}(\mathbb{C})^{+})^{L}$. Now the \textit{free entropy} $\chi_{(u,+)}(u_{1},\ldots,u_{N};h_{1},\ldots,h_{L})$ \textit{of mixed type}\index{free entropy!of mixed type} is defined as earlier; we let $ r\rightarrow\infty, \varepsilon \rightarrow+0$ and then $ R\rightarrow\infty$. When the set $\{h_{1},\ldots,h_{L}\}$ of positive elements is empty, our definition reduces to $\chi_{u}(u_{1},\ldots,u_{N})$ given in the previous section. On the other hand, we write $\Gamma_{+,R}(h_{1},\ldots,h_{L};n, r, \varepsilon),\ \chi_{+}(h_{1},\ldots, h_{L})$, etc. when no unitaries are present.

It is obvious that the free entropy\index{free entropy} $\chi_{(u,+)}$ has the subadditivity property, like $\chi$ and $\chi_{u}$. Moreover, the following upper semicontinuity of $\chi_{(u,+)}$ can be shown in a way similar to Propositions~\ref{ch06:pro6.1.4} and \ref{ch06:pro6.1.5}.

\begin{proposition}
\label{ch06:pro6.6.1} Let $u_{1}, \ldots, u_{N}$ and $u_{m,1}, \ldots, u_{m,N}\ (m\in \mathbb{N})$ be unitaries in $\mathcal{M}$. Let $h_{1}, \ldots, h_{L}$ and $h_{m,1}, \ldots, h_{m,L}\ (m\in \mathbb{N})$ be in $\mathcal{M}^{+}$. If
\begin{equation*}
(u_{m,1},\ldots,u_{m,N}, h_{m,1},\ldots,h_{m,L})\rightarrow(u_{1},\ldots,u_{N}, h_{1},\ldots,h_{L})
\end{equation*}
in the distribution sense and $\sup_{m}\Vert h_{m,i}\Vert<+\infty\,(1\leq i\leq L)$, then
\begin{align*}
& \chi_{(u,+)}(u_{1},\ldots,u_{N};h_{1},\ldots,h_{L}) \\
& \quad \ \ \geq  \limsup_{m\rightarrow\infty}\chi_{(u,+)}(u_{m,1},\ldots,u_{m,N};h_{m,1},\ldots,h_{m,L}).
\end{align*}
\end{proposition}
For a single $h\in\mathcal{M}^{+}$ let $\mu$ be the distribution on $\mathbb{R}^{+}$ of $h$ and $ R\geq\Vert h\Vert$. Since
\begin{equation*}
\Gamma_{+,R}(h;n, r, \varepsilon)=\{H\in M_{n}(\mathbb{C})^{+}:\Vert H\Vert\leq R, \, |\mathrm{tr}_{n}(H^{k})-m_{k}(\mu)|\leq\varepsilon,\,k\leq r\},
\end{equation*}
it follows from (\ref{ch05:eqn5.6.8}), (\ref{ch05:eqn5.6.9}) and (\ref{ch05:eqn5.6.11}) that the limit
\begin{equation}
\chi_{+,R}(h;r, \varepsilon)=\lim_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\Lambda_{+,n}(\Gamma_{+,R}(h;n, r, \varepsilon))+\log n\right] \label{ch06:eqn6.6.1}
\end{equation}
exists for every $r\in \mathbb{N}$ and $\varepsilon>0$, and
\begin{align}
\chi_+(h)=\chi_{+,R}(h) & \ = \ \Sigma(\mu)+\log\pi+\frac{3}{2} \notag \\
& \ = \ \chi(h)+\frac{1}{2}\log\frac{\pi}{2}+\frac{3}{4}.
\label{ch06:eqn6.6.2}
\end{align}

Furthermore, from Proposition~\ref{ch05:pro5.3.5} we know that, among $h\in\mathcal{M}^{+}$ with $\tau(h)\leq C$, the free entropy $\chi_+(h)$ attains the maximal value $\log(\pi eC)$ if (and only if) $h$ has the distribution
\begin{equation*}
\frac{\sqrt{4Ct-t^{2}}}{2\pi Ct}\chi_{[0,4C]}(t)\,dt
\end{equation*}
or equivalently $h^{1/2}$ is a quarter-circular element of radius $2\sqrt{C}$. From the subadditivity we have
\begin{equation*}
\chi_+(h_{1},\ldots,h_{N})\leq\chi_+(h_{1})+\cdots+\chi_{+}(h_{N})\leq N\log\frac{\pi eC}{N}
\end{equation*}
whenever $h_{1}, \ldots, h_{N}\in \mathcal{M}^{+}$ satisfy $\tau(h_{1}^{2}+\cdots+h_{N}^{2})\leq C$.

The next proposition says that the free entropy $\chi_+$ is nothing but $\chi$ restricted on positive random variables up to additive constants, extending the relation between $\chi_+(h)$ and $\chi(h)$ in (\ref{ch06:eqn6.6.2}).

\begin{proposition}
\label{ch06:pro6.6.2}
The equality
\begin{equation*}
\chi_+(h_{1},\ldots,h_{N})=\chi(h_{1},\ldots,h_{N})+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right)
\end{equation*}
holds for every $N\in \mathbb{N}$ and $h_{1}, \ldots, h_{N}\in\mathcal{M}^{+}$.
\end{proposition}

\begin{proof2}
For $r\in \mathbb{N}, \, \varepsilon >0$ and $R>0$, it is obvious that
\begin{equation*}
\Gamma_{+,R}(h_{1},\ldots,h_{N};n, r, \varepsilon)\subset\Gamma_{R}(h_{1},\ldots,h_{N};n, r, \varepsilon)
\end{equation*}
(the right-hand side is taken in $(M_{n}(\mathbb{C})^{sa})^{N}\supset(M_{n}(\mathbb{C})^{+})^{N}$). Hence by Lemma~\ref{ch04:lem4.4.7} we get
\begin{align*}
& \chi_{+,R}(h_{1},\ldots,h_{N};r, \varepsilon) \\
& \qquad \leq\limsup_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\Lambda_+(\Gamma_{R}(h_{1},\ldots,h_{N};n, r, \varepsilon))+N\log n\right] \\
& \qquad =\limsup_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\Lambda(\Gamma_{R}(h_{1},\ldots,h_{N};n, r, \varepsilon))+\frac{N}{2}\log n \right.\\
& \qquad \qquad \qquad \qquad \qquad \ \ \left.+N\left(\frac{1}{n^{2}}\log C_{n}+\frac{1}{2}\log n\right)\right] \\
& \qquad =\chi_{R}(h_{1},\ldots,h_{N};r, \varepsilon)+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right),
\end{align*}
where $C_{n}$ is the constant in Lemma~\ref{ch04:lem4.4.7} and satisfies
\begin{equation*}
\lim_{n\rightarrow\infty}\left(\frac{1}{n^{2}}\log C_{n}+\frac{1}{2}\log n\right)=\frac{1}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right)
\end{equation*}
by the Stirling formula. Therefore, we have
\begin{equation*}
\chi_+(h_{1},\ldots,h_{N})\leq\chi(h_{1},\ldots,h_{N})+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right).
\end{equation*}

To show the reverse inequality, for $\delta>0$ we take $(h_{1}+\delta \mathbf{1},\ldots,h_{N}+\delta \mathbf{1})$ instead of $(h_{1},\ldots,h_{N})$, and also $R>\max_{i}\Vert h_{i}\Vert+\delta$. Thanks to Corollary~\ref{ch06:cor6.3.2} (1), Proposition~\ref{ch06:pro6.1.4} and the translation invariance of $\Lambda_{n}$, we can estimate
\begin{align*}
& \chi(h_{1},\ldots,h_{N})=\chi(h_{1}+\delta \mathbf{1},\ldots,h_{N}+\delta \mathbf{1}) \\
& =\lim_{\begin{subarray}{c}{r\rightarrow\infty}\\ \\ \varepsilon\rightarrow+0 \end{subarray}}\limsup_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\Lambda(\Gamma_{+,R}(h_{1}+\delta \mathbf{1},\ldots,h_{N}+\delta \mathbf{1};n, r, \varepsilon))+\frac{N}{2}\log n\right] \\
& =\lim_{\begin{subarray}{c}{r\rightarrow\infty} \\ \\ \varepsilon\rightarrow+0 \\ \end{subarray}}\limsup_{n\rightarrow \infty}\left[\frac{1}{n^{2}}\log\Lambda_{+}(\Gamma_{+,R}(h_{1}+\delta \mathbf{1},\ldots,h_{N}+\delta \mathbf{1};n, r, \varepsilon)) \right.\\
& \qquad \qquad \qquad \qquad \qquad \left.+N\log n-N\left(\frac{1}{n^{2}}\log C_{n}+\frac{1}{2}\log n\right)\right] \\
& \leq\chi_+(h_{1}+\delta \mathbf{1},\ldots,h_{N}+\delta \mathbf{1})-\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right).
\end{align*}
Using the upper semicontinuity (Proposition~\ref{ch06:pro6.6.1}) as $\delta\rightarrow+0$, we obtain the result.
\end{proof2}

The following relation between two free entropies $\hat{\chi}$ and $\chi_{(u,+)}$ might be expected from the definitions in the light of Lemma~\ref{ch04:lem4.4.7}.

\begin{theorem}
\label{ch06:the6.6.3}
If $u_{1}, \ldots, u_{N}\in\mathcal{M}$ are unitaries and $h_{1}, \ldots, h_{N}\in\mathcal{M}^{+}$, then
\begin{align*}
\hat{\chi}(u_{1}h_{1},\ldots,u_{N}h_{N}) & \ = \ \chi_{(u,+)}(u_{1},\ldots,u_{N};h_{1}^{2},\ldots,h_{N}^{2}) \\
& \ \leq \ \chi_{u}(u_{1},\ldots,u_{N})+\chi(h_{1}^{2},\ldots,h_{N}^{2})+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right).
\end{align*}
\end{theorem}
To prove the theorem, we need to approximate the unitary part of $A$ by polynomials of $A, A^{*}$. The approximation here must be uniform for $A\in M_{n}(\mathbb{C})$ with $\Vert A\Vert\leq R$ in some sense. The next lemma provides the right approximation procedure for our purpose.

Let $a\in \mathcal{M}$ and assume that the distribution of $|a|$ is nonatomic. Let $a=u|a|$ be the polar decomposition. Note that $\mathrm{ker}\, a=\{0\}$ by assumption, and hence $u\in \mathcal{M}$ must be a unitary (since $\mathcal{M}$ is a finite von Neumann algebra). Let $\Vert\cdot\Vert_{p}$ denote the Schatten $p$-norm with respect to $\tau$ as well as $\mathrm{tr}_{n}$.

\begin{lemma}
\label{ch06:lem6.6.4}
With the above assumption and notation, for every $p\geq 1, \, \varepsilon >0$ and $ R\geq\Vert a\Vert$, there exist $n_{0}, r\in \mathbb{N}, \, \delta>0$ and a real polynomial $P(t)$ such that $\Vert u-aP(a^{*}a)\Vert_{p}\leq\varepsilon$, and such that, for each $n\geq n_{0}$, if $A\in M_{n}(\mathbb{C})$ with $\Vert A\Vert\leq R$ is non-singular and $U$ is the unitary part of $A$, and if
\begin{equation}
|\mathrm{tr}_{n}((A^{*}A)^{k})-\tau((a^{*}a)^{k})|\leq\delta \qquad (1\leq k\leq r),
\label{ch06:eqn6.6.3}
\end{equation}
then $\Vert U-AP(A^{*}A)\Vert_{p}\leq\varepsilon$.
\end{lemma}

\begin{proof2}
Let $\mu$ be the distribution of $|a|$. For every $\alpha, \beta>0$, since
\begin{equation*}
u-a(|a|+\alpha \mathbf{1})^{-1}=u(\mathbf{1}-|a|(|a|+\alpha \mathbf{1})^{-1})=\alpha u(|a|+\alpha \mathbf{1})^{-1}\,,
\end{equation*}
we have
\begin{align*}
\Vert u-a(|a|+\alpha \mathbf{1})^{-1}\Vert_{p}^{p} & \ =\ \Vert\alpha(|a|+\alpha \mathbf{1})^{-1}\Vert_{p}^{p} \\
& \ = \ \int_{0}^{\infty}\left(\frac{\alpha}{t+\alpha}\right)^{p}d\mu(t)\leq\mu([0, \beta])+\left(\frac{\alpha}{\beta}\right)^{p}.
\end{align*}
Similarly, for any non-singular $A\in M_{n}(\mathbb{C})$ with $A=U|A|$, we have
\begin{equation*}
\Vert U-A(|A|+\alpha I)^{-1}\Vert_{p}^{p}=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{\alpha}{\lambda_{i}+\alpha}\right)^{p}\leq\frac{1}{n}\#\{i:\lambda_{i}\leq\beta\}+\left(\frac{\alpha}{\beta}\right)^{p},
\end{equation*}
where $(0<)\,\lambda_{1}\leq\lambda_{2}\leq\ldots\leq\lambda_{n}$ are the eigenvalues of $|A|$.

Now for each $n\in \mathbb{N}$, since $\mu$ is nonatomic, one can choose $0<\xi_{1}^{(n)}<\xi_{2}^{(n)}< \ldots <\xi_{n}^{(n)}=\Vert a\Vert$ such that $\mu([0, \xi_{i}^{(n)}])=i/n\,(1\leq i\leq n)$. Then it immediately follows that
\begin{equation*}
\tau((a^{*}a)^{k})=\int_{0}^{\infty}t^{2k}\,d\mu(t)=\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^{n}(\xi_{i}^{(n)})^{2k} \qquad (k\in \mathbb{N}).
\end{equation*}
Let $\beta>0$ be fixed so that $\mu([0,2\beta])<\varepsilon^{p}/2$. By Lemma~\ref{ch04:lem4.3.4} there are $r\in \mathbb{N}$ and $\delta>0$ such that, for every $n\in \mathbb{N}$, if $(\lambda_{1}, \ldots, \lambda_{n})\in(\mathbb{R}^{+})_{\leq}^{n}$ satisfies
\begin{equation*}
\left|\frac{1}{n}\sum_{i=1}^{n}\lambda_{i}^{2k}-\frac{1}{n}\sum_{i=1}^{n}(\xi_{i}^{(n)})^{2k}\right|\leq 2\delta \qquad (1\leq k\leq r),
\end{equation*}
then
\begin{equation}
\frac{1}{n}\sum_{i=1}^{n}(\lambda_{i}^{2}-(\xi_{i}^{(n)})^{2})^{2}\leq\beta^{4}\varepsilon^{p}\,.
\label{ch06:eqn6.6.4}
\end{equation}
Next, choose $n_{0}\in \mathbb{N}$ such that
\begin{equation*}
\left|\frac{1}{n}\sum_{i=1}^{n}(\xi_{i}^{(n)})^{2k}-\tau((a^{*}a)^{k})\right|\leq\delta \qquad (1\leq k\leq r)
\end{equation*}
whenever $n\geq n_{0}$. Then, for any $n\geq n_{0}, (\ref{ch06:eqn6.6.4})$ is valid if $A\in M_{n}(\mathbb{C})$ satisfies (\ref{ch06:eqn6.6.3}). Furthermore, when (\ref{ch06:eqn6.6.3}) is satisfied, we have
\begin{equation}
\frac{1}{n}\#\{i:\lambda_{i}\leq\beta\}\leq\frac{11}{18}\varepsilon^{p}.
\label{ch06:eqn6.6.5}
\end{equation}
Indeed, put $l :=\#\{i : \lambda_{i}\leq\beta\}$ and $m :=\#\{i : \xi_{i}^{(n)}\leq 2\beta\}$. If $m<i\leq l$, then $\lambda_{i}\leq\lambda_{l}\leq\beta$ but $2\beta<\xi_{m+1}^{(n)}\leq\xi_{i}^{(n)}$, so $(\lambda_{i}^{2}-(\xi_{i}^{(n)})^{2})^{2}\geq(4\beta^{2}-\beta^{2})^{2}=9\beta^{4}$. Hence (\ref{ch06:eqn6.6.4}) implies $\frac{1}{n}(l-m)\cdot 9\beta^{4}\leq\beta^{4}\varepsilon^{p}$, so that $l/n\leq m/n+\varepsilon^{p}/9$. Since
\begin{equation*}
\frac{m}{n}=\mu([0, \xi_{m}^{(n)}])\leq\mu([0,2\beta])\leq \frac{\varepsilon^p}{2} \, ,
\end{equation*}
we have $l/n\leq\varepsilon^{p}/2+\varepsilon^{p}/9$, showing (\ref{ch06:eqn6.6.5}).

From all the above estimates, we infer that, for each $\alpha>0$ and $n\geq n_{0}$, if $A\in M_{n}(\mathbb{C})$ with $\Vert A\Vert\leq R$ is non-singular and satisfies (\ref{ch06:eqn6.6.3}), then
\begin{equation*}
\Vert U-A(|A|+\alpha I)^{-1}\Vert_{p}^{p}\leq\frac{11}{18}\varepsilon^{p}+\left(\frac{\alpha}{\beta}\right)^{p}
\end{equation*}
and
\begin{equation*}
\Vert u-a(|a|+\alpha \mathbf{1})^{-1}\Vert_{p}^{p}\leq\frac{\varepsilon^{p}}{2}+\left(\frac{\alpha}{\beta}\right)^{p} \, .
\end{equation*}
Choose $\alpha>0$ such that $(\alpha/\beta)^{p}\leq \varepsilon^{p}/18$, and next choose a polynomial $P(t)$ such that
\begin{equation*}
|P(t)-(\sqrt{t}+\alpha)^{-1}|\leq\frac{1}{R}\left(1-\left(\frac{2}{3}\right)^{1/p}\right)\varepsilon \quad \mathrm{for}  \quad 0\leq t\leq R^{2}.
\end{equation*}
Then for each $n\geq n_{0}$ and $A$ as above, we obtain
\begin{align*}
& \Vert U-A(|A|+ \alpha I)^{-1}\Vert_{p}\leq\left(\frac{2}{3}\right)^{1/p}\varepsilon \, , \\
& \Vert AP(A^{*}A)-A(|A|+\alpha I)^{-1}\Vert_{p} \\
& \qquad \leq\Vert A\Vert \ \Vert P(A^{*}A)-(|A|+\alpha I)^{-1}\Vert\leq\left(1-\left(\frac{2}{3}\right)^{1/p}\right)\varepsilon \, ,
\end{align*}
so $\Vert U-AP(A^{*}A)\Vert_{p}\leq\varepsilon$ holds, and similary $\Vert u-aP(a^{*}a)\Vert_{p}\leq\varepsilon$.
\end{proof2}

\begin{proof1}[Proof of Theorem~\ref{ch06:the6.6.3}]
First, the inequality in the theorem is a consequence of the subadditivity of $\chi_{(u,+)}$ and Proposition~\ref{ch06:pro6.6.2}. Define $\Psi : M_{n}(\mathbb{C})\rightarrow \mathcal{U}(n)\times M_{n}(\mathbb{C})^{+}$ by $\Psi(A) :=(U, A^{*}A)$ , where $U$ is the unitary part of $A$. This is bijective except for the negligible singular elements (in $M_{n}(\mathbb{C})$ and $M_{n}(\mathbb{C})^{+}$). Put $a_{i} :=u_{i}h_{i}$, so that $h_{i}^{2}=a_{i}^{*}a_{i}$. Let $n, r\in \mathbb{N}, \, \varepsilon >0$ and $R>\max\{1,\, \Vert h_{1}\Vert,\ldots,\Vert h_{N}\Vert\}$. It is straightforward to see that there are $r_{1}\in \mathbb{N}$ and $\varepsilon_{1}>0$ such that
\begin{equation*}
\Psi(\hat{\Gamma}_{R}(a_{1},\ldots,a_{N};n, r_{1}, \varepsilon_1))\subset \mathcal{U}(n)\times\Gamma_{+,R^{2}}(h_{1}^{2},\ldots,h_{N}^{2};n, r, \varepsilon),
\end{equation*}
and by Lemma~\ref{ch04:lem4.4.7}
\begin{equation*}
\hat{\Lambda}(\hat{\Gamma}_{R}(a_{1},\ldots,a_{N};n, r_{1}, \varepsilon_{1}))\leq\Lambda_{+}(\Gamma_{+,R^{2}}(h_{1}^{2},\ldots,h_{N}^{2};n, r, \varepsilon))
\end{equation*}
for all $n\in \mathbb{N}$. This yields $\hat{\chi}(a_{1}, \ldots, a_{N})\leq\chi_{+}(h_{1}^{2},\ldots,h_{N}^{2})$. Hence we may assume that $\chi(h_{1}^{2},\ldots,h_{N}^{2})>-\infty$, so the distribution of each $h_{i}$ is nonatomic.

Let $\varepsilon_0>0$ be such that $r\varepsilon_0(R^{2}+\varepsilon_0)^{r-1}\leq\varepsilon/3$. By Lemma~\ref{ch06:lem6.6.4} there exist $n_{0}, r_{0}\in \mathbb{N}, \, \delta>0$ and real polynomials $P_{i}(t)\ (1\leq i\leq N)$ such that $\Vert u_{i}-a_{i}P_{i}(a_{i}^{*}a_{i})\Vert_{r}\leq\varepsilon_0$, and such that, for each 1 $\leq i\leq N$ and $n\geq n_{0}$, if $A_{i}\in M_{n}(\mathbb{C})$ is non-singular with $A_{i}=U_{i}|A_{i}|,\,\Vert A_{i}\Vert\leq R$, and
\begin{equation*}
|\mathrm{tr}_{n}((A_{i}^{*}A_{i})^{k})-\tau((a_{i}^{*}a_{i})^{k})|\leq\delta \qquad (1 \leq k\leq r_{0}),
\end{equation*}
then $\Vert U_{i}-A_{i}P_{i}(A_{i}^{*}A_{i})\Vert_{r}\leq\varepsilon_0$. For $A_{i}\in M_{n}(\mathbb{C}) \ (1\leq i\leq N)$ satisfying the above conditions, we set
\begin{align*}
(B_{1},\ldots,B_{3N}) & :=(U_{1},\ldots,U_{N}, U_{1}^{*},\ldots,U_{N}^{*}, A_{1}^{*}A_{1},\ldots,A_{N}^{*}A_{N}), \\
(B_{1}',\ldots,B_{3N}') & :=(A_{1}P_{1}(A_{1}^{*}A_{1}), \ldots, A_{N}P_{N}(A_{N}^{*}A_{N}), \\
& \qquad \ P_{1}(A_{1}^{*}A_{1})A_{1}^{*},\ldots,P_{N}(A_{N}^{*}A_{N})A_{N}^{*}, A_{1}^{*}A_{1},\ldots,A_{N}^{*}A_{N})\, ,
\end{align*}
as well as
\begin{align*}
(b_{1},\ldots,b_{3N}) & :=(u_{1},\ldots,u_{N}, u_{1}^{*},\ldots,u_{N}^{*}, a_{1}^{*}a_{1},\ldots,a_{N}^{*}a_{N}), \\
(b_{1}',\ldots,b_{3N}')&  :=(a_{1}P_{1}(a_{1}^{*}a_{1}),\ldots,a_{N}P_{N}(a_{N}^{*}a_{N}), \\
& \qquad \quad P_{1}(a_{1}^{*}a_{1})a_{1}^{*},\ldots,P_{N}(a_{N}^{*}a_{N})a_{N}^{*}, a_{1}^{*}a_{1},\ldots,a_{N}^{*}a_{N}).
\end{align*}
Then for any $n\geq n_{0}$ and $1\leq i_{1}, \ldots, i_{k}\leq 3N(1\leq k\leq r)$, by using the H\"{o}lder inequality, it can be verified that
\begin{align*}
& |\mathrm{tr}_{n}(B_{i_{1}}\cdots B_{i_{k}})-\mathrm{tr}_{n}(B_{i_{1}}'\cdots B_{i_{k}}')| \\
& \qquad \leq\Vert B_{i_{1}}\cdots B_{i_{k}}-B_{i_{1}}'\cdots B_{i_{k}}'\Vert_{1}\leq k\varepsilon_{0}(R^{2}+\varepsilon_0)^{k-1}\leq \frac{\varepsilon}{3},
\end{align*}
and similarly $|\tau(b_{i_{1}}\cdots b_{i_{k}})-\tau(b_{i_{1}}'\cdots b_{i_{k}}')|\leq\varepsilon/3$. Now choose $r_{1}(\geq 2r_{0})$ large enough and $\varepsilon_{1}(\leq\delta)$ small enough so that if $(A_{1},\ldots,A_{N})\in\hat{\Gamma}_{R}(a_{1},\ldots,a_{N};n, r_{1}, \varepsilon_{1})$ then $|\mathrm{tr}_{n}(B_{i_{1}}'\cdots B_{i_{k}}')-\tau(b_{i_{1}}'\cdots b_{i_{k}}')|\leq\varepsilon/3$ for all $1\leq i_{1}, \ldots, i_{k}\leq 3N\,(1\leq k\leq r)$. Therefore, for $n\geq n_{0}$ we obtain
\begin{equation*}
\Psi(\hat{\Gamma}_{R}(a_{1},\ldots,a_{N};n, r_{1},\varepsilon_{1}))\subset\Gamma_{(u,+),R^{2}}(u_{1}, \ldots, u_{N};h_{1}^{2},\ldots,h_{N}^{2}; n, r, \varepsilon)
\end{equation*}
(up to negligible sets), and hence by Lemma~\ref{ch04:lem4.4.7}
\begin{align*}
& \hat{\Lambda}(\hat{\Gamma}_{R}(a_{1},\ldots,a_{N};n, r_{1}, \varepsilon_{1})) \\
& \quad \ \ \leq(\gamma\otimes\Lambda_{+})(\Gamma_{(u,+),R^{2}} (u_{1},\ldots,u_{N};h_{1}^{2},\ldots,h_{N}^{2};n, r, \varepsilon)).
\end{align*}
This implies that
\begin{equation*}
\hat{\chi}(a_{1},\ldots,a_{N})\leq\chi_{(u,+)}(u_{1},\ldots,u_{N};h_{1}^{2},\ldots,h_{N}^{2}).
\end{equation*}

Conversely, given $r\in \mathbb{N}, \, \varepsilon >0$ and $R>0$, by approximating $\sqrt{t}$ on $[0, R^{2}]$ by a polynomial, it is seen that there are $r_{1}\in \mathbb{N}$ and $\varepsilon_{1}>0$ such that
\begin{equation*}
\Gamma_{(u,+),R^{2}} (u_{1},\ldots,u_{N};h_{1}^{2},\ldots,h_{N}^{2};n, r_{1}, \varepsilon_1)\subset\Psi(\hat{\Gamma}_{R}(a_{1},\ldots,a_{N};n, r, \varepsilon))
\end{equation*}
(up to negligible sets) for all $n\in \mathbb{N}$. This gives the reverse inequality.
\end{proof1}

Theorem~\ref{ch06:the6.6.3} gives
\begin{equation*}
\hat{\chi}(a_{1},\ldots,a_{N})\leq\chi_{u}(u_{1},\ldots,u_{N})+\chi(a_{1}^{*}a_{1},\ldots,a_{N}^{*}a_{N})+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right)
\end{equation*}
for every $a_{1}, \ldots,  a_{N}\in \mathcal{M}$ and all unitaries $u_{1}, \ldots, u_{N}\in\mathcal{M}$ satisfying $a_{i}=u_{i}|a_{i}|$. In particular, we have the following corollary. Its proof was indeed included in the first paragraph of the proof of Theorem~\ref{ch06:the6.6.3}.

\begin{corollary}
\label{ch06:cor6.6.5}
Let $a_{1}, \ldots, a_{N}\in\mathcal{M}$. If $\hat{\chi}(a_{1},\ldots,a_{N})>-\infty$, then the distribution of $a_{i}^{*}a_{i}$ is nonatomic $($hence $\mathrm{ker}\, a_{i}=\{0\})$ for every $1\leq i\leq N$.
\end{corollary}
Next, we show that the inequality in Theorem~\ref{ch06:the6.6.3} can be replaced by equality in some cases of free relation. First, we take a free family $\{h_{1}, \ldots, h_{N}\}$ which is also free from $\{u_{1}, \ldots, u_{N}, u_{1}^{*},\ldots,u_{N}^{*}\}$. Then an exact relation among $\hat{\chi}, \, \chi_{u}$ and $\chi$ is obtained as follows. Thus we have a formula for $\chi_{u}$ in terms of $\hat{\chi}$ (hence $\chi$).

\begin{theorem}
\label{ch06:the6.6.6}
Let $u_{1}, \ldots, u_{N}\in \mathcal{M}$ be unitaries and $h_{1}, \ldots, h_{N}\in\mathcal{M}^{+}$. If
$\{u_{1},\ldots,u_{N}, u_{1}^{*}, \ldots, u_{N}^{*}\}, \, h_{1}, \ldots, h_{N}$ are free, then
\begin{equation*}
\hat{\chi}(u_{1}h_{1},\ldots,u_{N}h_{N})=\chi_{u}(u_{1},\ldots,u_{N})+\sum_{i=1}^{N}\chi(h_{i}^{2})+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right).
\end{equation*}
In particular, if $h_{1}, \ldots, h_{N}$ are free standard $($i.e. of radius $2)$ quarter-circular elements and they are free from $\{u_{1},\ldots, u_{N}, u_{1}^{*},\ldots,u_{N}^{*}\}$, then
\begin{align*}
\chi_{u}(u_{1},\ldots,u_{N}) & \ = \ \hat{\chi}(u_{1}h_{1},\ldots,u_{N}h_{N})-N\log(\pi e) \\
& \ = \ \chi(b_{1}, c_{1},\ldots,b_{N}, c_{N})-N\log(\pi e),
\end{align*}
where $u_{i}h_{i}=b_{i}+\mathrm{i}\,c_{i}$ with selfadjoint $b_{i}, c_{i}$.
\end{theorem}

To prove the theorm we show the next lemma, which will play in the present situation the same role as Lemma~\ref{ch06:lem6.4.3} did in the proof of Theorem~\ref{ch06:the6.4.1}.

\begin{lemma}
\label{ch06:lem6.6.7}
Let $u_{1}, \ldots, u_{N}, h_{1}, \ldots, h_{N}$ be as in Theorem~\emph{\ref{ch06:the6.6.6}}, and assume that $\chi_{u}(u_{1},\ldots,u_{N})>-\infty$ and $\chi_+(h_{i}^{2})>-\infty(1\leq i\leq N)$. Then, for every $r\in \mathbb{N}, \, \varepsilon>0$ and $R>\max_{i}\Vert h_{i}\Vert^{2}$, there exists $\varepsilon_1>0$ such that
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{(\gamma\otimes\Lambda_{+})(\Xi_n(r,\varepsilon_{1})\cap\Theta_{n}(r,\varepsilon))}{(\gamma\otimes\Lambda_{+})(\Xi_{n}(r,\varepsilon_{1}))}=1,
\end{equation*}
where
\begin{align*}
& \Xi_{n}(r, \varepsilon_1) :=\Gamma_{u}(u_{1},\ldots,u_{N};n, r, \varepsilon_{1})\times\prod_{i=1}^{N}\Gamma_{+,R}(h_{i}^{2};n, r, \varepsilon_{1}) \, , \\ & \Theta_{n}(r, \varepsilon) :=\Gamma_{(u,+),R}(u_{1}, \ldots, u_{N};h_{1}^{2},\ldots,h_{N}^{2};n, r, \varepsilon)\,.
\end{align*}
\end{lemma}

\begin{proof2}
Thanks to the freeness of $\{u_{1},\ldots,u_{N}, u_{1}^{*},\ldots,u_{N}^{*}\}, \ h_{1}^{2}, \ldots, h_{N}^{2}$, one can choose $\varepsilon_{1}>0$ such that if $(U_{1},\ldots,U_{N};H_{1},\ldots,H_{N})\in\Xi_{n}(r, \varepsilon_{1})$ and
$\{U_{1}, \ldots, U_{N}, U_{1}^{*}, \ldots, U_{N}^{*}\}, \ \{H_{1}\}, \ldots, \{H_{N}\}$ are $(r, \varepsilon_{1})$-free, then $(U_{1},\ldots,U_{N};H_{1},\ldots,H_{N})\in \Theta_{n}(r, \varepsilon)$. For every $\theta>0$, according to Lemma~\ref{ch06:lem6.4.2} there exists $n_{0}\in \mathbb{N}$ such that
\begin{align}
\gamma(\{(V_{1},\ldots,V_{N})\in \mathcal{U}(n)^{N} & : \{U_{1},\ldots,U_{N}, U_{1}^{*},\ldots,U_{N}^{*}\}, \{V_{1}H_{1}V_{1}^{*}\}, \ldots, \notag\\
& \quad \{V_{N}H_{N}V_{N}^{*}\}\ \mathrm{are} \ (r, \varepsilon_1)\mbox{-}\mathrm{free}\}) \geq 1-\theta
\label{ch06:eqn6.6.6}
\end{align}
for all $n\geq n_{0}$ and for any choice of $U_{i}\in \mathcal{U}(n)$ and $H_{i}\in M_{n}(\mathbb{C})^{+}$ with $\Vert H_{i}\Vert\leq R (1\leq i\leq N)$. Since $\chi_{u}(u_{1},\ldots,u_{N})>-\infty$ and $\chi_+(h_{i}^{2})>-\infty$, the $\gamma\otimes\Lambda+$-measure of $\Xi_{n}(r, \varepsilon_{1})$ is positive for large $n$, so we define the probability measure $\sigma_{n}$ on $\Xi_{n}(r, \varepsilon_{1})$ by normalizing the restriction of $\gamma\otimes\Lambda_+$ on $\Xi_{n}(r, \varepsilon_{1})$. Since $\sigma_{n}$ is invariant under the action of $\mathcal{U}(n)^{N}$ on $\Xi_{n}(r, \varepsilon_{1})$ given by $(U_{1},\ldots,U_{N};H_{1}, \ldots, H_{N})\mapsto (U_{1},\ldots,U_{N};V_{1}H_{1}V_{1}^{*},\ldots,V_{N}H_{N}V_{N}^{*})$ for $(V_{1},\ldots, V_{N})\in \mathcal{U}(n)^{N}$, we have
\begin{align*}
& \frac{(\gamma\otimes\Lambda_{+})(\Xi_n(r,\varepsilon_{1})\cap\Theta_{n}(r,\varepsilon))}{(\gamma\otimes\Lambda_{+})(\Xi_{n}(r,\varepsilon_{1}))} \ =  \ \int_{\Xi_{n(r,\varepsilon_{1})}}\left(\int_{\mathcal{U}(n)^{N}}\psi(U_{1}, \ldots, U_{N}; \right.\\
& \qquad \qquad \qquad \qquad \qquad \qquad \left. V_{1}H_{1}V_{1}^{*},\ldots,V_{N}H_{N}V_{N}^{*})\,d\gamma(V_{1},\ldots,V_{N})\right)d\sigma_{n}\, ,
\end{align*}
where $\psi$ is the characteristic function of $\Xi_{n}(r, \varepsilon_{1})\cap\Theta_{n}(r, \varepsilon)$. From (\ref{ch06:eqn6.6.6}) we get
\begin{equation*}
\int_{\mathcal{U}(n)^{N}}\psi(U_{1},\ldots,U_{N};V_{1}H_{1}V_{1}^{*},\ldots,V_{N}H_{N}V_{N}^{*})\,d\gamma(V_{1},\ldots,V_{N})\geq 1-\theta
\end{equation*}
for all $(U_{1},\ldots,U_{N};H_{1},\ldots,H_{N})\in\Xi_{n}(r, \varepsilon_{1})$. Therefore,
\begin{equation*}
\frac{(\gamma\otimes\Lambda_{+})(\Xi_{n}(r,\varepsilon_{1})\cap\Theta_{n}(r,\varepsilon))}{(\gamma\otimes\Lambda_{+})(\Xi_n(r,\varepsilon_{1}))}\geq 1-\theta
\end{equation*}
whenever $n$ is large, and we have the result.
\end{proof2}

\begin{proof1}[Proof of Theorem~\ref{ch06:the6.6.6}]
By Theorem~\ref{ch06:the6.6.3} and (\ref{ch06:eqn6.6.2}) it suffices to show that
\begin{equation}
\chi_{(u,+)}(u_{1},\ldots,u_{N};h_{1}^{2},\ldots,h_{N}^{2})\geq\chi_{u}(u_{1},\ldots,u_{N})+\sum_{i=1}^{N}\chi+(h_{i}^{2})\, ,
\label{ch06:eqn6.6.7}
\end{equation}
so we may assume that $\chi_{u}(u_{1},\ldots,u_{N})>-\infty$ and $\chi_+(h_{i}^{2})>-\infty\ (1\leq i\leq N)$. For any $r\in \mathbb{N}, \, \varepsilon>0$ and $R>\max_{i}\Vert h_{i}\Vert^{2}$, let $\varepsilon_1>0$ be as in Lemma~\ref{ch06:lem6.6.7}. Then we have
\begin{align*}
& \chi_{(u,+),R}(u_{1},\ldots,u_{N};h_{1}^{2},\ldots,h_{N}^{2};r, \varepsilon) \\
& \quad \ \ \geq\limsup_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log(\gamma\otimes\Lambda_{+})(\Xi_{n}(r, \varepsilon_{1}))+N\log n\right] \\
& \quad \ \ =\limsup_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log\gamma(\Gamma_{u}(u_{1},\ldots,u_{N};n, r, \varepsilon_{1})) \right.\\
& \qquad \qquad \qquad \left.+\sum_{i=1}^{N}\left(\frac{1}{n^{2}}\log\Lambda_{+,n}(\Gamma_{+,R}(h_{i}^{2};n, r, \varepsilon_{1}))+\log n\right)\right] \\
& \quad \ \ =\chi_{u}(u_{1},\ldots,u_{N};r, \varepsilon_{1})+\sum_{i=1}^{N}\chi_{+,R}(h_{i}^{2};r, \varepsilon_{1}).
\end{align*}
Above we used the fact that lim sup becomes $\lim$ in (\ref{ch06:eqn6.6.1}). Thus (\ref{ch06:eqn6.6.7}) is shown. The second part is clear from the fact mentioned before Proposition~\ref{ch06:pro6.6.2} and Proposition~\ref{ch06:pro6.5.5}.
\end{proof1}

When the roles of $u_{1}, \ldots, u_{N}$ and $h_{1}, \ldots, h_{N}$ are exchanged in Theorem~\ref{ch06:the6.6.6}, we have

\begin{theorem}
\label{ch06:the6.6.8}
Let $u_{1}, \ldots, u_{N}\in \mathcal{M}$ be unitaries and $h_{1}, \ldots, h_{N}\in\mathcal{M}^{+}$. If $\{u_{1}, u_{1}^{*}\}, \ldots,\{u_{N}, u_{N}^{*}\},\,  \{h_{1},\ldots,h_{N}\}$ are free, then
\begin{equation*}
\hat{\chi}(u_{1}h_{1},\ldots,u_{N}h_{N})=\sum_{i=1}^{N}\chi_{u}(u_{i})+\chi(h_{1}^{2},\ldots,h_{N}^{2})+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right).
\end{equation*}
If $u_{1}, \ldots, u_{N}$ are Haar unitaries in addition, then
\end{theorem}
\begin{equation*}
\hat{\chi}(u_{1}h_{1},\ldots,u_{N}h_{N})=\chi(h_{1}^{2},\ldots,h_{N}^{2})+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right).
\end{equation*}

\begin{proof2}
By Theorem~\ref{ch06:the6.6.3} and Proposition~\ref{ch06:pro6.6.2} it suffices to show that
\begin{equation}
\chi_{(u,+)} (u_{1},\ldots,u_{N};h_{1}^{2},\ldots,h_{N}^{2})\geq\sum_{i=1}^{N}\chi_{u}(u_{i})+\chi_+(h_{1}^{2},\ldots,h_{N}^{2}) \, ,
\label{ch06:eqn6.6.8}
\end{equation}
and we may assume $\chi_{u}(u_{i})>-\infty$ and $\chi_+(h_{1}^{2},\ldots,h_{N}^{2})>-\infty$. For $n, r\in \mathbb{N},\,\varepsilon >0$ and $R>0$ we set
\begin{equation*}
\Xi_{n}(r, \varepsilon) :=\prod_{i=1}^{N}\Gamma_{u}(u_{i};n, r, \varepsilon) \times\Gamma_{+,R}(h_{1}^{2},\ldots,h_{N}^{2};n, r, \varepsilon),
\end{equation*}
and $\Theta_{n}(r, \varepsilon)$ is the same as in Lemma~\ref{ch06:lem6.6.7}. By the freeness assumption there is $\varepsilon_{1}> 0$ such that if $(U_{1},\ldots, U_{N};H_{1},\ldots,H_{N})\in\Xi_{n}(r, \varepsilon_{1})$ and $\{U_{1}, U_{1}^{*}\},\ldots,\{U_{N}, U_{N}^{*}\},  \{H_{1},\ldots,H_{N}\}$ are $(r, \varepsilon_{1})$-free, then $(U_{1},\ldots,U_{N};H_{1},\ldots,H_{N})\in\Theta_{n}(r, \varepsilon)$. For every $\theta>0$, by Lemma~\ref{ch06:lem6.4.2} there exists $n_{0}\in \mathbb{N}$ such that
\begin{align*}
\gamma(\{(V_{1},\ldots,V_{N})\in(\mathcal{U}(n))^{N} & : \{V_{1}U_{1}V_{1}^{*}, V_{1}U_{1}^{*}V_{1}^{*}\},\ldots,\{V_{N}U_{N}V_{N}^{*}, V_{N}U_{N}^{*}V_{N}^{*}\}, \\
& \quad \{H_{1},\ldots,H_{N}\} \ \mathrm{are} \ (r, \varepsilon_{1})\mbox{-}\mathrm{free}\}) \geq 1-\theta
\end{align*}
for all $n \geq n_{0}$ and for all $U_{i}\in \mathcal{U}(n)$ and $H_{i}\in M_{n}(\mathbb{C})^{+}$ with $\Vert H_{i}\Vert\leq R(1\leq i\leq N)$. Then, as in the proof of Lemma~\ref{ch06:lem6.6.7}, we have
\begin{equation*}
\frac{(\gamma\otimes\Lambda_{+})(\Xi_n(r,\varepsilon_{1})\cap\Theta_{n}(r,\varepsilon))}{(\gamma\otimes\Lambda_{+})(\Xi_n(r,\varepsilon_{1}))}\geq 1-\theta
\end{equation*}
for large $n$. Therefore,
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{(\gamma\otimes\Lambda_{+})(\Xi_n(r,\varepsilon_{1})\cap\Theta_{n}(r,\varepsilon))}{(\gamma\otimes\Lambda_{+})(\Xi_n(r,\varepsilon_{1}))}=1.
\end{equation*}
This implies that
\begin{align*}
& \chi_{(u,+),R}(u_{1},\ldots,u_{N};h_{1}^{2},\ldots,h_{N}^{2}:r, \varepsilon) \\
& \qquad \geq\limsup_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\log(\gamma\otimes\Lambda_{+})(\Xi_{n}(r, \varepsilon_{1}))+N\log n\right] \\
& \qquad =\limsup_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\sum_{i=1}^{N}\log\gamma_{n}(\Gamma_{u}(u_{i};n, r, \varepsilon_{1})) \right. \\
& \qquad \qquad \qquad \ \left.+\frac{1}{n^{2}}\log\Lambda_{+} (\Gamma_{+,R}(h_{1}^{2},\ldots,h_{N}^{2};n, r, \varepsilon_{1}))+N\log n\right] \\
& \qquad =\sum_{i=1}^{N}\chi_{u}(u_{i};r, \varepsilon_{1})+\chi_{+,R}(h_{1}^{2},\ldots,h_{N}^{2};r, \varepsilon_{1}),
\end{align*}
thanks to (\ref{ch06:eqn6.5.1}), and we obtain (\ref{ch06:eqn6.6.8}).
\end{proof2}

Next, we apply the relation shown above to get the additivity properties of the free entropies $\chi_{u}$ and $\hat{\chi}$. We first give the change of variable formula similar to Proposition~\ref{ch06:pro6.3.6} for $\chi_{(u,+)}$. To do so, we need a smoothing technique like Lemma~\ref{ch06:lem6.3.5}. We denote by $\mathcal{F}_{\mathbb{T}}$ the set of all functions $f : \mathbb{T}\rightarrow \mathbb{T}$ which are given as $f(e^{\mathrm{i}\, t})=e^{\mathrm{i}\, \phi(t)}$ by a continuous increasing function $\phi$ on $[0,2\pi]$ with $\phi(0)=0, \, \phi(2\pi)=2\pi$. An $f\in \mathcal{F}_{\mathbb{T}}$ is said to be $C^{\infty}$ if $\phi$ is. Note that if $\phi$ is differentiable at $t\in[0,2\pi]$, then
\begin{equation*}
\lim_{\eta\rightarrow\zeta}\left|\frac{f(\eta)-f(\zeta)}{\eta-\zeta}\right|=\phi'(t) \quad \mathrm{for} \quad \zeta=e^{\mathrm{i}\, t}.
\end{equation*}
In this case we write $|f'(e^{\mathrm{i}\, t})|$ instead of $\phi'(t)$. For each unitary $u\in \mathcal{M}$ and $f\in \mathcal{F}_{\mathbb{T}}$ one can define the unitary $f(u)$ by functional calculus, that is, $f(u) :=\int_{\mathbb{T}}f(\zeta)\, de(\zeta)$ for the spectral decomposition $u=\int_{\mathbb{T}}\zeta \, de(\zeta)$.

\begin{lemma}
\label{ch06:lem6.6.9}
Let $u\in \mathcal{M}$ be a unitary with $\chi_{u}(u)>-\infty$, and let $f\in \mathcal{F}_{\mathbb{T}}$. Then there exists a sequence $(f_{m})$ of $C^{\infty}$-functions in $\mathcal{F}_{\mathbb{T}}$ such that $|f_{m}'|>0$ on $\mathbb{T}, \Vert f_{m}(u)-f(u)\Vert\rightarrow 0$ and $\chi_{u}(f_{m}(u))\rightarrow\chi_{u}(f(u))$.
\end{lemma}
On the other hand, we denote by $\mathcal{F}_{\mathbb{R}+}$ the set of all continuous increasing functions $g:\mathbb{R}^{+}\rightarrow \mathbb{R}^{+}$ with $g(0)=0$.

\begin{lemma}
\label{ch06:lem6.6.10}
Let $h\in\mathcal{M}^{+}, \, \chi(h)>-\infty$, and $g\in \mathcal{F}_{\mathbb{R}+}$. Then there exists a sequence $(g_{m})$ of $C^{\infty}$-functions in $\mathcal{F}_{\mathbb{R^{+}}}$ such that $g_{m}'>0$ on $\mathbb{R}^{+}, \Vert g_{m}(h)-g(h)\, \Vert\rightarrow 0$ and $\chi(g_{m}(h))\rightarrow\chi(g(h))$.
\end{lemma}

Lemma~\ref{ch06:lem6.6.10} is essentially included in Lemma~\ref{ch06:lem6.3.5}, so we omit the proof. The proof of Lemma~\ref{ch06:lem6.6.9} is similar, with some modifications as follows.

\begin{proof1}[Proof of Lemma~\ref{ch06:lem6.6.9}] Let $\mu$ be the distribution of $u$. Write $f(e^{\mathrm{i}\,t})=e^{\mathrm{i}\, \phi(t)}$ with $\phi: [0,2\pi]\rightarrow[0,2\pi]$, and extend $\phi$ to $\mathbb{R}$ periodically, that is, $\phi(-t) :=\phi(2\pi-t)-2\pi$ and $\phi(2\pi+t) :=\phi(t)+2\pi$ for $t\in[0,2\pi]$, and so on. Since $\Sigma(\mu)=\chi_{u}(u)>-\infty$ by (\ref{ch06:eqn6.5.2}), there are $0<\delta(m)<1/m\, (m\in\mathbb{N})$ such that
\begin{align}
& \iint_{|\zeta-\eta|<\delta(m)}\log|\zeta-\eta|\, \mu(\zeta)\, d\mu(\eta)\geq-\frac{1}{m}\, ,
\label{ch06:eqn6.6.9} \\
& \iint_{|\zeta-\eta|<\delta(m)}d\mu(\zeta)\, d\mu(\eta)\leq\frac{1}{m\log m}.
\label{ch06:eqn6.6.10}
\end{align}
For each $m$ choose a $C^{\infty}$-function $\psi_{m}\geq 0$ with compact support in $(-\pi, \pi)$ and $\int\psi_{m}\, dt=1$ such that
\begin{equation}
|\tilde{\phi}_{m}(t)-\phi(t)|\leq\frac{\delta(m)}{2m} \quad \mathrm{for}\quad t\in \mathbb{R},
\label{ch06:eqn6.6.11}
\end{equation}
where $\tilde{\phi}_{m}$ is defined by
\begin{equation*}
\tilde{\phi}_{m}(t):=(\phi*\psi_{m})(t)-(\phi*\psi_{m})(0).
\end{equation*}
Note that $\tilde{\phi}_{m}(0)=0,\, \tilde{\phi}_{m}(2\pi)=2\pi$ and $\tilde{\phi}_{m}$ is periodic with period $ 2\pi$. Now define
\begin{equation*}
\phi_{m}(t):=\frac{1}{m}t+\left(1-\frac{1}{m}\right)\tilde{\phi}_{m}(t) \qquad (t\in \mathbb{R}),
\end{equation*}
and $f_{m}(e^{\mathrm{i}\, t}) :=e^{\mathrm{i}\, \phi_{m}(t)}$ for $e^{\mathrm{i}\, t}\in \mathbb{T}$. Then $f_{m}$ is a $C^{\infty}$-function in $\mathcal{F}_{\mathbb{T}}$ and $|f_{m}'|\geq 1/m$ on $\mathbb{T}$. Moreover, we get $\Vert f_{m}-f\Vert$ (the $\sup$ norm on $\mathbb{T}) \rightarrow 0$, and hence $\Vert f_{m}(u)-f(u)\Vert\rightarrow 0$.

Let $m$ be so large that $\Vert f_{m}-f\Vert\leq(\sqrt{2}-1)/2$ and $|f(\zeta)-f(\eta)|<1$ if $|\zeta-\eta|< \delta(m)$. Note that if $|f(\zeta)-f(\eta)|<1$, then
\begin{equation*}
|f_{m}(\zeta)-f_{m}(\eta)|\leq|f(\zeta)-f(\eta)|+2\Vert f_{m}-f\Vert<\sqrt{2}.
\end{equation*}
Write
\begin{align*}
\chi_{u}(f_{m}(u)) & \ = \ \iint\log|f_{m}(\zeta)-f_{m}(\eta)|\,d\mu(\zeta)\,d\mu(\eta) \\
& \ = \ \left(\iint_{|\zeta-\eta|<\delta(m)}+\iint_{|f(\zeta)-f(\eta)|\geq 1}+\iint_{\begin{subarray}{c}|\zeta-\eta|\geq\delta(m) \\ |f(\zeta)-f(\eta)|<1 \end{subarray}}\right) \\
& \qquad \ \log|f_{m}(\zeta)-f_{m}(\eta)|\,d\mu(\zeta)\,d\mu(\eta).
\end{align*}
When $|\zeta-\eta|<\delta(m)$, since $|f_{m}(\zeta)-f_{m}(\eta)|<\sqrt{2}$, we can write $f_{m}(\zeta)=e^{\mathrm{i}\, \phi_{m}(t)} (\zeta=e^{\mathrm{i}\, t})$ and $f_{m}(\eta)=e^{\mathrm{i}\, \phi_{m}(s)}(\eta=e^{\mathrm{i}\, s})$ with $|\phi_{m}(t)-\phi_{m}(s)|<\pi/2$. Since
\begin{align*}
|f_{m}(\zeta)-f_{m}(\eta)| & \ \geq\ \frac{2\sqrt{2}}{\pi}|\phi_{m}(t)-\phi_{m}(s)| \\
& \ \geq \ \frac{2\sqrt{2}}{\pi} \cdot \frac{|t-s|}{m}\geq\frac{2\sqrt{2}}{\pi} \cdot \frac{|\zeta-\eta|}{m},
\end{align*}
we get by (\ref{ch06:eqn6.6.9}) and (\ref{ch06:eqn6.6.10})
\begin{align*}
& \iint_{|\zeta-\eta|<\delta(m)}\log|f_{m}(\zeta)-f_{m}(\eta)|\, d\mu(\zeta)\, d\mu(\eta) \\
& \qquad \geq\left(\log\frac{2\sqrt{2}}{\pi}-\log m\right)\frac{1}{m\log m}-\frac{1}{m}=\left(\log\frac{2\sqrt{2}}{\pi}\right)\frac{1}{m\log m}-\frac{2}{m}\, ,
\end{align*}
implying
\begin{equation}
\liminf_{m\rightarrow\infty}\iint_{|\zeta-\eta|<\delta(m)}\log|f_{m}(\zeta)-f_{m}(\eta)|\, d\mu(\zeta)\, d\mu(\eta)\geq 0.
\label{ch06:eqn6.6.12}
\end{equation}
When $|f(\zeta)-f(\eta)|\geq 1$, since
\begin{align*}
|f_{m}(\zeta)-f_{m}(\eta)| & \ \geq \ |f(\zeta)-f(\eta)|-|f_{m}(\zeta)-f(\zeta)|-|f_{m}(\eta)-f(\eta)| \\
& \ > \  1-(\sqrt{2}-1)>\frac{1}{2}\, ,
\end{align*}
we get $|\log|f_{m}(\zeta)-f_{m}(\eta)||\leq\log 2$ and
\begin{align}
& \lim_{m\rightarrow\infty}\iint_{|f(\zeta)-f(\eta)|\geq 1}\log|f_{m}(\zeta)-f_{m}(\eta)|\, d\mu(\zeta)\, d\mu(\eta) \notag\\
& \qquad =\iint_{|f(\zeta)-f(\eta)|\geq 1}\log|f(\zeta)-f(\eta)|\, d\mu(\zeta)\, d\mu(\eta)
\label{ch06:eqn6.6.13}
\end{align}
by the bounded convergence theorem. Next, when $|\zeta-\eta|\geq\delta(m)$ and $|f(\zeta)-f(\eta)|< 1$, we can write $f_{m}(\zeta)=e^{\mathrm{i}\, \phi_{m}(t)}\ (\zeta=e^{\mathrm{i}\, t})$ and $f_{m}(\eta)=e^{\mathrm{i}\, \phi_{m}(s)}(\eta=e^{\mathrm{i}\, s})$ with $|\phi_{m}(t)-\phi_{m}(s)|<\pi/2$. Letting $t>s$ and noting that $t-s\geq|\zeta-\eta|\geq\delta(m)$, we get
\begin{align*}
\frac{\pi}{2} & \ > \ \phi_{m}(t)-\phi_{m}(s) \\
& = \ \frac{1}{m}(t-s)+\left(1-\frac{1}{m}\right)(\tilde{\phi}_{m}(t)-\tilde{\phi}_{m}(s)) \\
& \geq \ \frac{\delta(m)}{m}+\left(1-\frac{1}{m}\right)(\phi(t)-\phi(s)-|\tilde{\phi}_{m}(t)-\phi(t)|-|\tilde{\phi}_{m}(s)-\phi(s)|) \\
& \geq \ \left(1-\frac{1}{m}\right)(\phi(t)-\phi(s))
\end{align*}
by (\ref{ch06:eqn6.6.11}), and hence
\begin{align*}
|f_{m}(\zeta)-f_{m}(\eta)| & \ \geq \ |e^{\mathrm{i}\,(1-\frac{1}{m})(\phi(t)-\phi(s))}-1| \\
& \ \geq \ \left(1-\frac{1}{m}\right)|e^{\mathrm{i}\,(\phi(t)-\phi(s))}-1| \\
& \ = \ \left(1-\frac{1}{m}\right)|f(\zeta)-f(\eta)|.
\end{align*}
(For the second inequality above, note that $|e^{\mathrm{i}\,\alpha\theta}-1|\geq\alpha|e^{\mathrm{i}\,\theta}-1|$ for $0\leq\alpha\leq 1$ and $0\leq\theta\leq\pi/2.)$ Therefore we have
\begin{align*}
& \iint_{\begin{subarray}{c}|\zeta-\eta|\geq\delta(m) \\ |f(\zeta)-f(\eta)|<1 \\ \end{subarray}} \log|f_{m}(\zeta)-f_{m}(\eta)|\, d\mu(\zeta)\, d\mu(\eta) \\
& \qquad \geq\log\left(1-\frac{1}{m}\right)\iint_{|\zeta-\eta|\geq\delta(m)}\,d\mu(\zeta)\,d\mu(\eta) \\
& \qquad \qquad \qquad \quad +\iint_{\begin{subarray}{c}|\zeta - \eta|\geq \delta (m) \\ |f (\zeta)- f(\eta)|<1 \end{subarray}} \log|f(\zeta)-f(\eta)|\,d\mu(\zeta)\,d\mu(\eta) \\
& \qquad \geq\log\left(1-\frac{1}{m}\right)\cdot \frac{1}{m\log m}+\iint_{|f(\zeta)-f(\eta)|<1}\log|f(\zeta)-f(\eta)|\,d\mu(\zeta)\,d\mu(\eta)\, ,
\end{align*}
implying
\begin{align}
& \liminf_{m\rightarrow\infty}\iint_{\begin{subarray}{c}|\zeta-\eta|\geq\delta(m) \\ |f(\zeta)-f(\eta)|<1 \\ \end{subarray}} \log|f_{m}(\zeta)-f_{m}(\eta)|\, d\mu(\zeta)\, d\mu(\eta) \notag \\
& \qquad \geq\iint_{|f(\zeta)-f(\eta)|<1}\log|f(\zeta)-f(\eta)|\, d\mu(\zeta)\, d\mu(\eta).
\label{ch06:eqn6.6.14}
\end{align}
The above estimates (\ref{ch06:eqn6.6.12})--(\ref{ch06:eqn6.6.14}) together imply that
\begin{equation*}
\liminf_{m\rightarrow \infty}\chi_{u}(f_{m}(u))\geq\chi_{u}(f(u)).
\end{equation*}
This and the upper semicontinuity give the conclusion.
\end{proof1}

\begin{lemma}
\label{ch06:lem6.6.11} Let $u_{1},\ldots,u_{N}\in \mathcal{M}$ be unitaries with $\chi_{u}(u_{i})>-\infty$ and $h_{1},\ldots,h_{L}\in \mathcal{M}^{+}$ with $\chi_+(h_{j})>-\infty$. Then
\begin{align*}
& \chi_{(u,+)}(f_{1}(u_{1}),\ldots,f_{N}(u_{N});g_{1}(h_{1}),\ldots,g_{L}(h_{L})) \\
& \qquad \geq\chi_{(u,+)}(u_{1},\ldots,u_{N};h_{1},\ldots,h_{L}) \\
& \qquad \qquad +\sum_{i=1}^{N}[\chi_{u}(f_{i}(u_{i}))-\chi_{u}(u_{i})]+\sum_{j=1}^{L}[\chi(g_{j}(h_{j}))-\chi(h_{j})]
\end{align*}
for every $f_{1},\ldots,f_{N}\in \mathcal{F}_{\mathbb{T}}$ and $g_{1},\ldots,g_{L}\in \mathcal{F}_{\mathbb{R}^{+}}$.
\end{lemma}

\begin{proof2}
By Lemmas~\ref{ch06:lem6.6.9} and \ref{ch06:lem6.6.10} together with Proposition~\ref{ch06:pro6.6.1} we may show the following two cases:
\begin{enumerate}
\item[(a)] If $f$ is a $C^{\infty}$-function in $\mathcal{F}_{\mathbb{T}}$ with $|f'|>0$ on $\mathbb{T}$, then
\begin{align*}
& \chi_{(u,+)}(f(u_{1}), u_{2},\ldots,u_{N};h_{1},\ldots,h_{N}) \\
& \qquad \geq\chi_{(u,+)} (u_{1},\ldots,u_{N};h_{1},\ldots,h_{N})+\chi_{u}(f(u_{1}))-\chi_{u}(u_{1}).
\end{align*}
\item[(b)] If $g$ is a $C^{\infty}$-function in $\mathcal{F}_{\mathbb{R}^{+}}$ with $g'>0$ on $\mathbb{R}^{+}$, then
\begin{align*}
& \chi_{(u,+)}(u_{1},\ldots,u_{N};g(h_{1}), h_{2},\ldots,h_{N}) \\
& \qquad \geq\chi_{(u,+)} (u_{1},\ldots,u_{N};h_{1},\ldots,h_{N})+\chi(g(h_{1}))-\chi(h_{1})\,.
\end{align*}
\end{enumerate}
The proof of (b) is the same as Proposition~\ref{ch06:pro6.3.6}. We sketch the similar proof of (a). For $\zeta, \eta\in \mathbb{T}$ define
\begin{equation*}
K(\zeta, \eta):=\left\{\begin{array}{ll}
\left|\dfrac{f(\zeta)-f(\eta)}{\zeta-\eta}\right| & \mathrm{if}\ \zeta\neq\eta,\\
|f'(\zeta)| & \mathrm{if}\ \zeta =\eta.
\end{array}\right.
\end{equation*}
Then $L(\zeta, \eta) :=\log K(\zeta, \eta)$ is continuous on $\mathbb{T}^{2}$, and
\begin{equation*}
\chi_{u}(f(u_{1}))-\chi_{u}(u_{1})=(\tau\otimes\tau)(L(u_{1}\otimes \mathbf{1},\mathbf{1}\otimes u_{1}))\, .
\end{equation*}
Write $F(U_{1},\ldots,U_{N};H_{1},\ldots,H_{L}) :=(f(U_{1}), U_{2},\ldots,U_{N};H_{1},\ldots,H_{L})$ on $\mathcal{U}(n)^{N}\times (M_{n}(\mathbb{C})^{+})^{L}$. For every $r\in \mathbb{N}$ and $\varepsilon >0$, by approximating $f$ by a trigonometric polynomial, we see that
\begin{align*}
& F(\Gamma_{(u,+),R}(u_{1},\ldots,u_{N};h_{1},\ldots,h_{L};n, r_{1}, \varepsilon_{1})) \\
& \qquad \subset\Gamma_{(u,+),R}(f(u_{1}), u_{2},\ldots,u_{N};h_{1},\ldots,h_{L};n, r, \varepsilon) \qquad (n\in \mathbb{N})
\end{align*}
for some $r_{1}\in \mathbb{N}$ and $\varepsilon_{1}>0$. Since
\begin{align*}
\frac{d(\gamma_{n}\circ f)}{d\gamma_{n}}(U_{1}) & \ = \ \prod_{i<j}\left|\frac{f(\zeta_{i})-f(\zeta_{j})}{\zeta_{i}-\zeta_{j}}\right|^{2}\prod_{i=1}^{n}|f'(\zeta_{i})| \\
& \ = \ \exp(\mathrm{Tr}_{n}\otimes \mathrm{Tr}_{n})(L(U_{1}\otimes I, I\otimes U_{1}))
\end{align*}
$(\zeta_{1},\ldots,\zeta_{N}$ are the eigenvalues of $U_{1})$, we can show as in the proof of Proposition~\ref{ch06:pro6.3.6} that for any $\delta>0$ there are $r_{1}\in \mathbb{N}$ and $\varepsilon_{1}>0$ such that
\begin{equation*}
\left|\frac{1}{n^{2}}\log\frac{d(\gamma_{n}\circ f)}{d\gamma_{n}}(U_{1})-[\chi_{u}(f(u_{1}))-\chi_{u}(u_{1})]\right|\leq 3\delta
\end{equation*}
for all $(U_{1},\ldots,U_{N};H_{1},\ldots,H_{L})\in\Gamma_{(u,+),R}(u_{1},\ldots,u_{N};n, r_{1}, \varepsilon_{1}), \, n\in \mathbb{N}$, and the inequality in (a) is obtained.
\end{proof2}

If $f_{1},\ldots,f_{N}\in \mathcal{F}_{\mathbb{T}}$ and $g_{1},\ldots,g_{L}\in \mathcal{F}_{\mathbb{R}^{+}}$ are strictly increasing (in terms of angle for $f_{i}$), then the inequality in Lemma~\ref{ch06:lem6.6.11} can be replaced by equality.

\begin{proposition}
\label{ch06:pro6.6.12}
If $ u_{1}\cdots  u_{N}\in \mathcal{M}$ are unitaries, then $\chi_{u}(u_{1},\ldots,u_{N})=0$ if and only if $u_{1},\ldots,u_{N}$ are $^*$-free Haar unitaries.
\end{proposition}

\begin{proof2}
 Choose free standard quarter-circular elements $h_{1},\ldots,h_{N}$ which are free from $\{u_{1},\ldots,u_{N}, u_{1}^{*},\ldots,u_{N}^{*}\}$. Theorem~\ref{ch06:the6.6.6} says that $\chi_{u}(u_{1},\ldots,u_{N})=0$ if and only if $\hat{\chi}(u_{1}h_{1},\ldots,u_{N}h_{N})=N\log(\pi e)$. According to Proposition~\ref{ch06:pro6.5.8} the latter equality holds if and only if $u_{1}h_{1},\ldots,u_{N}h_{N}$ are $^*$-free circular elements, which is equivalent to $u_{1},\ldots,u_{N}$ being $^*$-free Haar unitaries.
\end{proof2}

Now we are in a position to supply the promised proof of Theorem~\ref{ch06:the6.5.3}.

\begin{proof1}[Proof of Theorem~\ref{ch06:the6.5.3}] The proof of the first part is essentially included in the proof of Theorem~\ref{ch06:the6.6.8}. In fact, when $(h_{1},\ldots,h_{N})$ is empty in the proof of (\ref{ch06:eqn6.6.8}), it can read as a proof of the first part here. So the details are left to the reader. To prove the second, assume that $\chi_{u}(u_{i})>-\infty$ for $1\leq i\leq N$ and the additivity holds. For each $i$, since the distribution of $u_{i}$ is nonatomic, there is a (unique) $f_{i}\in \mathcal{F}_{\mathbb{T}}$ such that the distribution of $f_{i}(u_{i})$ is the Haar probability measure on $\mathbb{T}$, so $\chi_{u}(f_{i}(u_{i}))=0$. Then, by Lemma~\ref{ch06:lem6.6.11} (in the case of the set $\{h_{j}\}$ being empty) and the additivity assumption, we get
\begin{equation*}
\chi_{u}(f_{1}(u_{1}),\ldots,f_{N}(u_{N}))\geq\sum_{i=1}^{N}\chi_{u}(f_{i}(u_{i}))=0.
\end{equation*}
Hence Proposition~\ref{ch06:pro6.6.12} implies that $f_{1}(u_{1}), \ldots, f_{N}(u_{N})$ are $^*$-free, and so are $u_{1},\ldots,u_{N}$ because $u_{i}\in\{f_{i}(u_{i})\}^{\prime\prime}$.
\end{proof1}

\begin{theorem}
\label{ch06:the6.6.13}
Let $a_{1},\ldots,a_{N}\in \mathcal{M}$ be such that $a_{i}=u_{i}h_{i}$ with a $^*$-free pair of a unitary $u_{i}\in \mathcal{M}$ and $h_{i}\in\mathcal{M}^{+}$. If $a_{1},\ldots,a_{N}$ are $^*$-free, then
\begin{equation*}
\hat{\chi}(a_{1},\ldots,a_{N})=\hat{\chi}(a_{1})+\cdots+\hat{\chi}(a_{N})\, .
\end{equation*}
Conversely, if $\hat{\chi}(a_{i})>-\infty$ for  $1 \leq i\leq N$ and the above equality holds, then $a_{1},\ldots,a_{N}$ are $^*$-free.
\end{theorem}

\begin{proof2}
If $a_{1},\ldots,a_{N}$ are $^*$-free, then $u_{1},\ldots,u_{N}, h_{1},\ldots,h_{N}$ are $^*$-free due to the $^*$-freeness of $u_{i}, h_{i}$. Hence Theorems~\ref{ch06:the6.6.6} and \ref{ch06:the6.5.3} imply that
\begin{equation*}
\hat{\chi}(a_{1},\ldots, a_{N})=\sum_{i=1}^{N}\chi_{u}(u_{i})+\sum_{i=1}^{N}\chi(h_{i}^{2})+\frac{N}{2}\left(\log\frac{\pi}{2}+\frac{3}{2}\right)=\sum_{i=1}^{N}\hat{\chi}(a_{i})\, .
\end{equation*}
Conversely, assume that $\hat{\chi}(a_{i})>-\infty$ for $1\leq i\leq N$ and the additivity holds. Since $\chi_{u}(u_{i})>-\infty$ and $\chi(h_{i}^{2})>-\infty$, one can choose $f_{i}\in \mathcal{F}_{\mathbb{T}}$ and $g_{i}\in \mathcal{F}_{\mathbb{R}^{+}}$ such that $f_{i}(u_{i})$ is a Haar unitary and $g_{i}(h_{i})^{2}$ is a standard quarter-circular. Then, letting $b_{i} :=f_{i}(u_{i})g_{i}(h_{i})$ and using Theorem~\ref{ch06:the6.6.3}, Lemma~\ref{ch06:lem6.6.11} (applied to $f_{i},\,g_{i}(t^{1/2})^{2}$) and Theorem~\ref{ch06:the6.6.6}, we get
\begin{align*}
\hat{\chi}(b_{1},\ldots,b_{N}) & \ = \ \chi_{(u,+)}(f_{1}(u_{1}),\ldots,f_{N}(u_{N});g_{1}(h_{1})^{2},\ldots,g_{N}(h_{N})^{2}) \\
& \ \geq \ \hat{\chi}_{(u,+)}(u_{1},\ldots,u_{N};h_{1}^{2},\ldots,h_{N}^{2}) \\
& \qquad \ \ +\sum_{i=1}^{N}[\chi_{u}(f_{i}(u))-\chi_{u}(u_{i})]+\sum_{i=1}^{N}[\chi(g_{i}(h_{i})^{2})-\chi(h_{i}^{2})] \\
& \ = \ \hat{\chi}(a_{1},\ldots,a_{N})-\sum_{i=1}^{N}\hat{\chi}(a_{i})+N\log(\pi e)=N\log(\pi e)\, .
\end{align*}
Hence Proposition~\ref{ch06:pro6.5.8} implies that $b_{1}, \ldots, b_{N}$ are $^*$-free standard circulars, so that $a_{1},\ldots,a_{N}$ are $^*$-free because $a_{i}\in\{b_{i}, b_{i}^{*}\}''$.
\end{proof2}

Theorem~\ref{ch06:the6.6.13} can be applied in particular when $a_{1},\ldots,a_{N}$ are $R$-diagonal elements. Specializing to the case $\hat{\chi}(a)$ of a single non-selfadjoint $a\in \mathcal{M}$, we state

\begin{proposition}
\label{ch06:pro6.6.14}
Let $a\in \mathcal{M}$ with $\hat{\chi}(a)>-\infty$, and let $a=uh$ be the polar decomposition. Then
\begin{equation*}
\hat{\chi}(a)\leq\chi_{u}(u)+\chi(a^{*}a)+\frac{1}{2}\log\frac{\pi}{2}+\frac{3}{4}\, ,
\end{equation*}
and equality is attained if and only if $u, h$ are $^*$-free. Moreover, the equality
\begin{equation*}
\hat{\chi}(a)=\chi(a^{*}a)+\frac{1}{2}\log\frac{\pi}{2}+\frac{3}{4}
\end{equation*}
holds if and only if a is $R$-diagonal.
\end{proposition}

\begin{proof2}
Theorem~\ref{ch06:the6.6.6} contains the ``if'' part of the first assertion. To see the ``only if'', choose $f\in \mathcal{F}_{\mathbb{T}}$ and $g\in \mathcal{F}_{\mathbb{R}^{+}}$ such that $f(u)$ is a Haar unitary and $g(h)^{2}$ is a standard quarter-circular. Then the equality $\hat{\chi}(uh)=\chi_{u}(u)+\chi_+(h^{2})$ implies $\hat{\chi}(f(u)g(h))=\log(\pi e)$ as in the proof of Theorem~\ref{ch06:the6.6.13}, and this means that $f(u)g(h)$ is a standard circular and hence $u, h$ are $^*$-free. The second assertion is immediate from the first.
\end{proof2}

By (\ref{ch06:eqn6.6.2}) we restate the above proposition as follows.

\begin{corollary}
\label{ch06:cor6.6.15}
Let $\mu\in \mathcal{M}(\mathbb{R}^{+})$ have compact support and $\Sigma(\mu)>-\infty$. If $a\in\mathcal{M}$ is such that $a^{*}a$ has the distribution $\mu$, then
\begin{equation*}
\hat{\chi}(a)\leq\Sigma(\mu)+\log\pi+\frac{3}{2} \, ,
\end{equation*}
and equality is attained if and only if a is $R$-diagonal.
\end{corollary}

\begin{example}
\label{ch06:exa6.6.16} Let $\mu_{\lambda}$ be the free Poisson distribution (i.e. the Marchenko-Pastur distribution) for $\lambda\geq 1$. If $a$ is an $R$-diagonal element such that $a^{*}a$ has the distribution $\mu_{\lambda}$, then Corollary~\ref{ch06:cor6.6.15} and (\ref{ch05:eqn5.5.9}) give
\begin{equation*}
\hat{\chi}(a)=\log\pi+\frac{1}{2}(1+\lambda+\log\lambda+(\lambda-1)^{2}\log(1-\lambda^{-1}))\, .
\end{equation*}
Consider the functional
\begin{equation*}
\hat{\chi}(a)-\tau(a^{*}a)+2(\lambda-1)\Delta(a) \quad \mathrm{for} \quad  a\in \mathcal{M},
\end{equation*}
where $\Delta(a)$ is the Fuglede-Kadison determinant of $a$. If $\mu$ is the distribution of $a^{*}a$, then by Corollary~\ref{ch06:cor6.6.15} we have
\begin{align*}
\hat{\chi}(a)& -\tau(a^{*}a)+2(\lambda-1)\Delta(a) \\
& \leq\Sigma(\mu)-\int(t-(\lambda-1)\log t)\, d\mu(t)+\log\pi+\frac{3}{2}\, ,
\end{align*}
because $2\Delta(a)=\int\log t\, d\mu(t)$. According to Proposition~\ref{ch05:pro5.3.7}, the above functional attains the maximal value if and only if $a$ is an $R$-diagonal element $a$ such that $a^{*}a$ has the distribution $\mu_{\lambda}$.
\end{example}

\subsection*{Notes and Remarks} After the case of a single selfadjoint random variable in [\citen{bib202}], the (multivariate) free entropy $\chi(a_{1},\ldots,a_{N})$ was extensively developed in a series of papers by Voiculescu [\citen{bib203}], [\citen{bib205}]--[\citen{bib208}].

Basic properties such as subadditivity and upper semicontinuity in Sec.~\ref{ch06:sec6.1} and noncommutative functional calculus for power series in Sec.~\ref{ch06:sec6.2} were presented in [\citen{bib203}]. As for the change of variable formulas, Theorem~\ref{ch06:the6.3.1} was proved in [\citen{bib203}], while Propositions~\ref{ch06:pro6.3.3} and \ref{ch06:pro6.3.6} are taken from [\citen{bib205}].

The first part of Theorem~\ref{ch06:the6.4.1} is contained in [\citen{bib203}]. The proof based on the approximate freeness property (Lemma~\ref{ch06:lem6.4.2}) is taken from [\citen{bib208}], and the strong additivity (\ref{ch06:eqn6.4.3}) was given there in a slightly more general form. On the other hand, the converse part together with Theorem~\ref{ch06:the6.4.4} was in [\citen{bib205}]. [\citen{bib208}] contains several results like Lemma~\ref{ch06:lem6.4.2} on the approximate freeness of Haar distributed unitary matrices; the concentration technique of Gromov and Milman is essential in that paper. The proof here is a direct application of the $L^{2}$-convergence in Lemma~\ref{ch04:lem4.3.2}; thus the result of Gromov and Milman is not used.

In the classical case, when a random variable $X$ has a continuously differentiable density $f$, the Fisher information of $X$ is defined as
\begin{equation*}
I(X) :=\int\left(\frac{d}{dt}\log f(t)\right)^{2}f(t)\, dt=\int\frac{f'(t)^{2}}{f(f)}\, dt.
\end{equation*}
For any random variable $X$ with finite variance, one can reformulate the differential formula in [\citen{bib14}] as follows:
\begin{equation*}
S(X+\sqrt{s}Z)-\frac{1}{2}\int_{0}^{s}I(X+\sqrt{}Z)\, dt=S(X) \qquad (s\geq 0),
\end{equation*}
where $Z$ is a standard Gaussian random variable such that $X$ and $Z$ are independent. The free analogue of the Fisher information for a single selfadjoint variable $a$ was introduced in [\citen{bib202}] as $\Phi(a) :=\frac{4}{3}\pi^{2}\int f(t)^{3}\, dt$ if the distribution of $a$ has the density $f$ (otherwise $\Phi(a) :=+\infty$). The free analogue of the above differential formula was also obtained:
\begin{equation*}
\Sigma(a+\sqrt{s}S)-\frac{1}{2}\int_{0}^{s}\Phi(a+\sqrt{t}S)\, dt=\Sigma(a) \qquad (s\geq 0),
\end{equation*}
where $S$ is a standard semicircular variable free from $a$. (Constant coefficients in this formula and in the definition of $\Phi$ are properly changed from those in [\citen{bib202}]; see also [\citen{bib207}].) The above formula yields
\begin{equation*}
\chi(a)=\frac{1}{2}\int_{0}^{\infty}\left(\frac{1}{1+t}-\Phi(a+\sqrt{t}S)\right)dt+\frac{1}{2}\log(2\pi e) \, .
\end{equation*}

By using the notion of noncommutative Hilbert transform, Voiculescu introduced the (relative) free information $\Phi^{*}(a_{1},\ldots,a_{N}:B)$ of selfadjoint variables $a_{1},\ldots,a_{N}$ with respect to a subalgebra $B$ in [\citen{bib207}]. Furthermore, he defined the (relative) free entropy of $(a_{1},\ldots,a_{N})$ with respect to $B$ as
\begin{align*}
& \chi^{*}(a_{1},\ldots,a_{N}:B) \\
& \quad :=\frac{1}{2}\int_{0}^{\infty} \left(\frac{N}{1+t}-\Phi^{*}\ (a_{1}+\sqrt{t}S_{1},\ldots,a_{N}+\sqrt{t}S_{N}:B)\right)dt+\frac{N}{2}\log(2\pi e) \, ,
\end{align*}
where $S_{1},\ldots,S_{N}$ are standard semicircular variables such that $B, S_{1}, \ldots, S_{N}$ are free. For a single selfadjoint $a$, it was shown that $\Phi^{*} (a:\mathbb{C})$ coincides with $\Phi(a)$ whenever the distribution of $a$ has a density, and hence $\chi^{*}(a:\mathbb{C})=\chi(a)$. It is not known whether $\chi$ and $\chi^{*}$ coincide in the general multivariable case. The matricial entropy $\chi$ is also called the ``microstates'' free entropy, while $\chi^{*}$ is often called the ``microstates-free'' free entropy. The free informations $\Phi$ and $\Phi^{*}$ and the free entropy $\chi^{*}$ are not treated in this book.

There are some important inequalities known for free entropy and for free information. For instance, the free Cram\'{e}r-Rao inequality shown in [\citen{bib202}] says that $\Phi(a)\tau(a^{2})\geq 1$, and equality holds if and only if $a$ is centered semicircular. The free entropy power inequality in [\citen{bib186}] says that $e^{2\chi(a+b)}\geq e^{2\chi(a)}+e^{2\chi(b)}$ holds for selfadjoint variables $a, b$; its extension to the multivariable case is not known. The similar inequalities for $\Phi^{*}$ and $\chi^{*}$ are in [\citen{bib207}].

The contents of Sections~\ref{ch06:sec6.5} and \ref{ch06:sec6.6} are taken from [\citen{bib103}]. The interrelation among different types of free entropies given in Theorems~\ref{ch06:the6.6.3} and \ref{ch06:the6.6.6} makes it easy for us to transform properties of the selfadjoint free entropy into properties of the unitary free entropy, and vice versa. The maximization problems at the end of this chapter were treated in [\citen{bib135}] too. The same kind of maximization problems for $\Phi^{*}$ and $\chi^{*}$ were discussed in [\citen{bib134}].

\textit{Brown's spectral distribution measure}\index{distribution!measure, Brown} of a noncommutative random variable in a tracial $W^{*}$-probability space is based on the \textit{ Fuglede-Kadison determinant}.\index{Fuglede-Kadison determinant} The function $\lambda\mapsto(2\pi)^{-1}\log\Delta(a-\lambda \mathbf{1})$ is subharmonic on $\mathbb{C}$, and the representing Riesz measure $\mu_{a}$ is called the \textit{Brown measure}\index{Brown measure} for $a$. For an $R$-\textit{diagonal element}\index{$R$-diagonal!element} the Brown measure is supported on an annulus or on a disk, see Haagerup and Larsen [\citen{bib97}]. In some examples the Brown measure coincides with the limiting eigenvalue density of the corresponding random matrix model; the complexified Wishart matrix is such a model.


\chapter{Relation to Operator Algebras}
\label{ch07:chap07}

This chapter is mostly concerned with relations and applications of free probability theory to von Neumann algebras, in particular, to free group factors. We take a noncommutative probability space $(\mathcal{M}, \tau)$ of a von Neumann algebra $\mathcal{M}$ with a faithful normal tracial state $\tau$. A selfadjoint element $a$ in $\mathcal{M}$ with nonatomic distribution generates a von Neumann algebra isomorphic to $\mathcal{L}(\mathbb{Z})$. If $a_{1},\ldots,a_{n}$ are such elements in free relation, then the generated von Neumann algebra $\{a_{1},\ldots,a_{n}\}''$ is isomorphic to $\star_{i=1}^{n}\mathcal{L}(\mathbb{Z})=\mathcal{L}(\mathbf{F}_{n})$, the free group factor associated with the free group with $n$ generators. In this way, the free group factors are naturally realized via free families of noncommutative random variables, and it turns out that three concepts of freeness, free product and free group factors have the same essence.

The long-standing and famous isomorphism problem of free group factors is whether it is possible to have $\mathcal{L}(\mathbf{F}_{n})\cong \mathcal{L}(\mathbf{F}_{m})$ if $n\neq m$. Great progress was recently made on this problem by F.R\u{a}dulescu and by K. Dykema as a continuation of Voiculescu's work; however, the final solution has not yet been achieved. Random matrix models provide a powerful machinery in their studies. R\u{a}dulescu and Dykema independently discovered a one-parameter family of type $\mathrm{II}_{1}$ factors interpolating free group factors, which is central in the recent development of theory of free group factors.

There are remarkable applications of free entropy to the theory of factors akin to free group factors. Several open questions on factors were recently answered by using the free entropy technique. For instance, the non-existence of Carten subalgebras in free group factors was shown by Voiculescu, and L. Ge proved that free group factors are \textit{ prime}\index{factor!prime} in the sense that they are not isomorphic to the tensor product of two factors of type $\mathrm{II}_{1}$.

Voiculescu introduced the notion of free entropy dimension by differentiating free entropy in a certain way. The advantage is that the free entropy dimension is more sensitive to freeness than the free entropy itself. There is another related notion of free dimension defined by Dykema for some class of von Neumann algebras.

\section{Free group factors and semicircular systems}
\label{ch07:sec7.1}

First of all, recall some basic terminology on von Neumann algebras,\index{hyperfinite!von Neumann algebra}\index{injective von Neumann algebra}\index{von Neumann algebra} though we already used it in previous chapters. A \textit{ von Neumann algebra} (or $W^{*}$-\textit{algebra})\index{$W^{*}$-algebra} $\mathcal{M}$ on a Hilbert space $\mathcal{H}$ is a $^*$-subalgebra of $B(\mathcal{H})$ which is closed in the strong (or equivalently weak) operator topology and contains the identity operator $\mathbf{1}$. The theory of von Neumann algebras was initiated by the monumental work of Murray and von Neumann [\citen{bib122}]. The \textit{double commutant theorem}\index{double commutant theorem}\index{commutant} of von Neumann says that $\mathrm{a}^{\ *}$-subalgebra $\mathcal{M}$ of $B(\mathcal{H})$ is a von Neumann algebra if and only if $\mathcal{M} =\mathcal{M}''$, where $\mathcal{M}':=\{a\in B(\mathcal{H}): ab =ba,\,b\in \mathcal{M}\}$, the \textit{commutant}\index{von Neumann algebra!commutant} of $\mathcal{M}$, and $\mathcal{M}'' :=(\mathcal{M}')'$. A von Neumann algebra $\mathcal{M}$ is called a \textit{factor} if its \textit{center}\index{von Neumann algebra!center}\index{center} is trivial, i.e. $\mathcal{M} \cap \mathcal{M}'= \mathbb{C}\mathbf{1}$. Factors are classified into \textit{types}\index{factor!type} $\mathrm{I}_{n}\,(n=1,2,\ldots,\infty), \mathrm{II}_{1}, \ \mathrm{II} \infty,\ \mathrm{III}_{\lambda}\,(0\leq\lambda\leq 1)$. We do not enter into the details on the classification of factors. But we give a few remarks about type I and type II factors here. A type $\mathrm{I}_{n}\,(n<\infty)$ factor\index{factor} is just the matrix algebra $M_{n}(\mathbb{C})$, and a type $\mathrm{I}_{\infty}$ factor is $B(\mathcal{H})$ with $\dim \mathcal{H}=\infty$. Type $\mathrm{II}_{1}$ factors are characterized as factors which have a faithful normal tracial state (unique and faithful automatically). This type of factors will be the main topic of our subsequent discussions. Any type $\mathrm{II}_{\infty}$ factor is represented as the tensor product $\mathcal{M} \otimes B(\mathcal{H})$ of a type $\mathrm{II}_{1}$ factor $\mathcal{M}$ and $B(\mathcal{H})$ of type $\mathrm{I}_{\infty}$.

A von Neumann algebra $\mathcal{M}$ (acting on a separable Hilbert space) is said to be \textit{hyperfinite} or $AFD$\index{von Neumann algebra!AFD} if there is an increasing sequence $(\mathcal{M}_{n})$ of finite-dimensional $^*$-subalgebras of $\mathcal{M}$ which generates $\mathcal{M}$, i.e. $\mathcal{M} =(\bigcup_{n}\mathcal{M}_{n})''$. A remarkable result due to Murray and von Neumann [\citen{bib123}] is that a hyperfinite\index{von Neumann algebra!hyperfinite} type $\mathrm{II}_{1}$ factor is unique (up to isomorphism). It is constructed, for example, by the closure of the infinite tensor $M_{2}(\mathbb{C})\otimes M_{2}(\mathbb{C})\otimes\cdots$ via the GNS representation by $\mathrm{tr}_{2}\otimes \mathrm{tr}_{2}\otimes\cdots(\mathrm{tr}_{2}$ being the tracial state on $M_{2}(\mathbb{C}))$. The hyperfinite $\mathrm{II}_{1}$ factor is usually denoted by $R$. As was finally proved in epoch-making work of Connes [\citen{bib52}], several conditions equivalent to AFD for von Neumann algebras are known. Among others, a von Neumann algebra $\mathcal{M} \subset B(\mathcal{H})$ is AFD if and only if it is \textit{ injective},\index{von Neumann algebra!injective} that is, there exists a conditional expectation (or norm one projection) from $B(\mathcal{H})$ onto $\mathcal{M}$. Furthermore, Connes (and Haagerup for type $\mathrm{III}_{1}$) proved that an AFD factor of each type except type $\mathrm{III}_{0}$ is unique.

There is another important notion for $\mathrm{II}_{1}$ factors. Let $\mathcal{M}$ be a type $\mathrm{II}_{1}$ factor with a normal tracial state $\tau$, and $B(\mathcal{H})$ a $\mathrm{I}_{\infty}$ factor with the usual trace $\mathrm{Tr}$. For every $ 0<t<\infty$ choose a projection $p$ in $\mathcal{M} \otimes B(\mathcal{H})$ such that $(\tau\otimes \mathrm{Tr})(p)=t$. Then a $\mathrm{II}_{1}$ factor $\mathcal{M}_{t}$ is defined as $p(\mathcal{M} \otimes B(\mathcal{H}))p$. The isomorphism class of $\mathcal{M}_{t}$ is independent of the choice of $p$ having a given trace value $t$. When $0<t\leq 1$, we may define $\mathcal{M}_{t}=p\mathcal{M} p$ with a projection $p\in\mathcal{M}$ such that $\tau(p)=t$. One can readily see that
\begin{equation}
\mathcal{M}_{t_{1}t_{2}}\cong(\mathcal{M}_{t_{1}})_{t_{2}} \qquad (t_{1}, t_{2}>0),
\label{ch07:eqn7.1.1}
\end{equation}
and hence
\begin{equation*}
\{t\in(0, \infty):\mathcal{M}_{t}\cong \mathcal{M}\}
\end{equation*}
forms a subgroup of the multiplicative group $(0, \infty)$. This is the so-called \textit{ fundamental group}\index{von Neumann algebra!group} of $\mathcal{M}$. One can equivalently define it as the set of $t\in(0, \infty)$ such that there exists an automorphism $\alpha$ of $\mathcal{M} \otimes B(\mathcal{H})$ satisfying $(\tau\otimes \mathrm{Tr})\circ\alpha=t(\tau\otimes \mathrm{Tr})$. Concerning the hyperfinite (or injective) $\mathrm{II}_{1}$ factor $R$, its uniqueness implies that the fundamental group\index{fundamental group} of $R$ is the whole $(0, \infty)$.

The subjects of this chapter are more or less related to free group factors. So let us first recall the construction of free group factors for the reader's convenience. Free group factors are typical examples of the group von Nuemann algebra associated with a discrete group.

Let $G$ be a general discrete group. The Hilbert space $\ell^{2}(G)$ consists of $\xi : G\rightarrow \mathbb{C}$ such that $\sum_{g\in G}|\xi(g)|^{2}<+\infty$, whose inner product is $\langle\xi, \eta\rangle :=\sum_{g\in G}\xi(g)\,\overline{\eta(g)}$. The left and right regular representations of $G$ on $\ell^{2}(G)$ are defined by
\begin{equation*}
(L_{g}\xi)(h) :=\xi(g^{-1}h), \quad (R_{g}\xi)(h) :=\xi(hg) \qquad (\xi\in\ell^{2}(G), \ g, h\in G).
\end{equation*}
Then the (left) \textit{group von Neumann algebra}\index{group!von Neumann algebra} $\mathcal{L}(G)$ of $G$ is generated by $L_{G} := \{L_{g}:g\in G\}$, i.e. $\mathcal{L}(G) :=(L_{G})''$, and the right group von Neumann algebra is $\mathcal{R}(G):=(R_{G})''$. It is obvious that $\mathcal{L}(G)$ and $\mathcal{R}(G)$ are commuting, namely,
\begin{equation}
\mathcal{L}(G)\subset \mathcal{R}(G)'\,.
\label{ch07:eqn7.1.2}
\end{equation}
Define a vector state $\tau$ on $\mathcal{L}(G)$ by
\begin{equation*}
\tau(a):=\langle a\delta_{e}, \delta_{e}\rangle \qquad (a\in \mathcal{L}(G)) ,
\end{equation*}
where $e$ is the identity of $G$ and $\delta_{g}(h)=\delta_{gh}$ for $g, h\in G$. It is clear that $\delta_{e}$ is cyclic for both $\mathcal{L}(G)$ and $\mathcal{R}(G)$. Hence, thanks to (\ref{ch07:eqn7.1.2}), $\delta_{e}$ is a cyclic and separating vector for $\mathcal{L}(G)$, so $\tau$ is faithful on $\mathcal{L}(G)$.  Since
\begin{equation*}
\tau(L_{g}L_{h})=\delta_{e}(gh)=\delta_{e}(hg)=\tau(L_{h}L_{g})\,,
\end{equation*}
we see that $\tau$ is a faithful normal tracial state on $\mathcal{L}(G)$. A conjugate-linear isometry $J$ on $\ell^{2}(G)$ with $J^{2}=\mathbf{1}$ is given by
\begin{equation*}
(J\xi)(g):=\overline{\xi(g-1)} \qquad (\xi\in \ell^{2}(G),\ g\in G),
\end{equation*}
which is a particular case of the so-called \textit{modular conjugation}.\index{modular conjugation} In fact, it is easy to check that
\begin{align*}
&Ja\delta_{e}=a^{*}\delta_{e} \qquad (a\in \mathcal{L}(G)),\\
&\mathcal{L}(G)'=J\mathcal{L}(G)J=\mathcal{R}(G)\,.
\end{align*}
This means that equality indeed holds in (\ref{ch07:eqn7.1.2}).

Now let $G$ be an infinite discrete group, and we determine when $\mathcal{L}(G)$ becomes a factor. For any $a\in \mathcal{L}(G)$, since
\begin{equation*}
(L_{h}^{*}aL_{h}\delta_{e})(g)=\tau(L_{g}^{*}L_{h}^{*}aL_{h})=\tau(L_{hgh^{-1}}^{*}a)=(a\delta_{e})(hgh^{-1})
\end{equation*}
and $\delta_{e}$ is separating for $\mathcal{L}(G)$, it is seen that $a$ belongs to $\mathcal{L}(G)'$ if and only if $(a\delta_{e})(hgh^{-1})=(a\delta_{e})(g)$ for all $g, h\in G$, that is, $a\delta_{e}\in \ell^{2}(G)$ is constant on each conjugacy class of $G$. This says that if the conjugacy class of each $g\in G\,\backslash\,\{e\}$ is infinite, then $a\delta_{e}\in \mathbb{C}\delta_{e}$ or $a\in \mathbb{C} \mathbf{1}$ for any $a$ in the center of $\mathcal{L}(G)$. Hence $\mathcal{L}(G)$ becomes a factor if $G$ is an \textit{ ICC group},\index{group!ICC}\index{ICC group} and vice versa. The free group\index{factor!free group} $\mathbf{F}_{n}$ with $n$ generators $(n=2,3,\ldots,\infty)$ is a typical example of ICC groups. In this way, we have the type $\mathrm{II}_{1}$ factors $\mathcal{L}(\mathbf{F}_{n})$, called the \textit{free group factors}.\index{free!group factor} It is worth noting that the free group factors are not hyperfinite; indeed, $\mathcal{L}(\mathbf{F}_{2})$ was the first example of non-hyperfinite von Neumann algebras discovered by Murray and von Neumann.

Next, for convenience, let us recall and fix terms about free families of some special kinds. Let $(\mathcal{A}, \varphi)$ be a $C^{*}$-probability space. Let $(a_{i})_{i\in I}$ be a finite or countable family of noncommutative random variables in $\mathcal{A}$. We say that $(a_{i})_{i\in I}$ is a \textit{semicircular system}\index{semicircular!system} if it is a free family of standard semicircular elements (of distribution $w_{2}$). A semicircular system is sometimes assumed to have the distribution $w_{1}$, but we prefer $w_{2}$ in accordance with the limit distribution of the standard Gaussian random matrix model (cf. Theorem~\ref{ch04:the4.1.7}). Also, we say that $(a_{i})_{i\in I}$ is a \textit{circular system}\index{circular!system} if it is a $^*$-free family of standard circular elements, that is, $((a_{i}+a_{i}^{*})/\sqrt{2})_{i\in I}\,\cup\,((a_{i}-a_{i}^{*})/\sqrt{2}\,\mathrm{i})_{i\in I}$ is a semicircular system (cf. Example~\ref{ch02:exa2.6.2}).

The next proposition tells us that the von Neumann algebra generated by a semicircular or circular system is isomorphic to a free group factor.

\begin{proposition}
\label{ch07:pro7.1.1}
Let $(\mathcal{M}, \varphi)$ be a $W^{*}$-probability space with a faithful normal state $\varphi$ on $\mathcal{M}$.
\begin{enumerate}
\item[(1)] If $(a_{i})_{i\in I}$ is a semicircular system in $\mathcal{M}$, then the generated von Neumann algebra $\{a_{i}:i\in I\}''$ is isomorphic to $\mathcal{L}(\mathbf{F}_{|I|})$.

\item[(2)] If $(a_{i})_{i\in I}$ is a circular system in $\mathcal{M}$, then the von Neumann algebra generated by $(a_{i})_{i\in I}$ is isomorphic to $\mathcal{L}(\mathbf{F}_{2|I|})$.

\item[(3)] If $(u_{i})_{i\in I}$ is a finite or countable free family of Haar unitaries in $\mathcal{M}$, then $\{u_{i}:i\in I\}''$ is isomorphic to $\mathcal{L}(\mathbf{F}_{|I|})$.
\end{enumerate}
\end{proposition}

\begin{proof2}
(2) is an immediate consequence of (1). If $a=a^{*}$ has the distribution $w_{2}$ and $f: [-2, 2]\rightarrow[0,1]$ is defined by $f'(t)=\frac{1}{2\pi}\sqrt{4-t^{2}}$, then $u :=\exp(2\pi \mathrm{i}\,f(a))$ is a Haar unitary. So, it is enough to show (3) only. Let $g_{i}\ (i\in I)$ be the generators of $\mathbf{F}_{|I|}$, and let $L$ be the left regular representation of $\mathbf{F}_{|I|}$ on $\ell^{2}(\mathbf{F}_{|I|})$, as explained above. Let $\mathcal{M}$ and $\mathcal{B}$ denote the (algebraic) $^*$-algebras generated by $(L_{g_{i}})_{i\in I}$ and $(u_{i})_{i\in I}$, respectively. A ${^*}$-homomorphism $\rho : \mathcal{A}\rightarrow \mathcal{B}$ is defined by
\begin{equation*}
\rho (L_{g_{i_1}}^{k_1} L_{g_{i_2}}^{k_2} \cdots L_{g_{i_n}}^{k_n}) = u_{i_1}^{k_1} u_{i_2}^{k_2} \cdots u_{i_n}^{k_n}
\end{equation*}
for $i_{1}\neq i_{2}\neq\ldots\neq i_{n}$ and $k_{j}\in \mathbb{Z}\,\backslash\,\{0\}$ for $1\leq j\leq n$. Since
\begin{equation*}
\tau(L_{g_{i_{1}}}^{k_{1}}\cdots L_{g_{i_{n}}}^{k_{n}})=\tau(L_{g_{i_{1}}^{k_{1}} \cdots g_{i_{n}}^{k_{n}}})=0=\varphi(u_{i_{1}}^{k_{1}}\cdots u_{i_{n}}^{k_{n}})
\end{equation*}
by the assumption of free Haar unitaries, we have
\begin{equation*}
\tau(a)=\varphi\circ\rho(a) \qquad (a\in \mathcal{A}).
\end{equation*}
Thanks to the faithfulness of $\varphi$, this implies that $\rho$ is a $^*$-isomorphism between $\mathcal{A}$ and $\mathcal{B}$. When $\pi_{(\mathcal{A},\tau)}$ and $\pi_{(\mathcal{B},\varphi)}$ denote the GNS representations of $(\mathcal{A}, \tau)$ and $(\mathcal{B}, \varphi)$, we have
\begin{equation*}
\{u_{i}:i\in I\}''\cong\pi_{(\mathcal{B},\varphi)}(\mathcal{B})^{\prime \prime}\cong\pi_{(\mathcal{A},\tau)}(\mathcal{A})^{\prime \prime}=\mathcal{L}(\mathbf{F}_{|I|})\,.
\end{equation*}
\end{proof2}

The above proof shows that if $(a_{i})_{i\in I}$ is a semicircular system in $(\mathcal{M}, \varphi)$, then $\varphi$ is automatically tracial on $\{a_{i}:i\in I\}^{\prime \prime}$. This was indeed shown in Proposition~\ref{ch02:pro2.2.6}.

The free product of noncommutative probability spaces was explained in Sec.~\ref{ch02:sec2.1} at the algebraic level. In the following let us discuss the free product at a more analytic level to complement the contents of Sec.~\ref{ch02:sec2.1}. To do so, we first need to define the free product of Hilbert spaces.\index{free!product of Hilbert spaces} Let $(\mathcal{H}_{i}, \xi_{i})_{i\in I}$ be a family of Hilbert spaces with distiguished vectors $\xi_{i}\in \mathcal{H}_{i}\ (\Vert\xi_{i}\Vert=1)$. Set $\mathcal{H}_{i}^{0} :=\mathcal{H}_{i}\ominus \mathbb{C}\xi_{i}$. The \textit{Hilbert space free product} $\mathcal{H}$ with a distinguished vector $\xi \ (\Vert\xi\Vert=1)$ is the direct sum of $\mathbb{C}\xi$ and the tensor products $\mathcal{H}_{i_{1}}^{0}\otimes \mathcal{H}_{i_{2}}^{0}\otimes\cdots\otimes \mathcal{H}_{i_{n}}^{0}$ for all $i_{1}\neq i_{2}\neq\ldots\neq i_{n}$, that is,
\begin{equation*}
\mathcal{H}:=\mathbb{C}\xi\oplus\bigoplus_{n=1}^{\infty}\left(\bigoplus_{i_{1}\neq i_{2}\neq \ldots \neq i_{n}} \mathcal{H}_{i_{1}}^{0}\otimes \mathcal{H}_{i_{2}}^{0}\otimes\cdots\otimes \mathcal{H}_{i_{n}}^{0}\right).
\end{equation*}
We write $(\mathcal{H}, \xi)=\star_{i\in I}(\mathcal{H}_{i}, \xi_{i})$. For each $i\in I$, take a subspace $\mathcal{H}(i)$ of $\mathcal{H}$ as
\begin{equation*}
\mathcal{H}(i):=\mathbb{C}\xi\oplus\bigoplus_{n=1}^{\infty}\left(\bigoplus_{i\neq i_{1}\neq i_{2}\neq \ldots \neq i_{n}} \mathcal{H}_{i_{1}}^{0}\otimes \mathcal{H}_{i_{2}}^{0}\otimes\cdots\otimes \mathcal{H}_{i_{n}}^{0}\right),
\end{equation*}
and define a unitary operator $V_{i}:\mathcal{H}_{i}\otimes \mathcal{H}(i)\rightarrow \mathcal{H}$ as follows:
\begin{equation}
V_{i}:\left\{\begin{array}{ll}
\xi_{i}\otimes\xi\mapsto\xi, & \\
\eta\otimes\xi\mapsto\eta & \mathrm{for} \ \eta\in \mathcal{H}_{i}^{0},\\
\xi_{i}\otimes\zeta\mapsto \zeta & \mathrm{for} \ \zeta\in \mathcal{H}_{i_{1}}^{0}\otimes\cdots\otimes \mathcal{H}_{i_{n}}^{0},\\
\eta\otimes\zeta\mapsto\eta\otimes\zeta & \mathrm{for}\ \eta\in \mathcal{H}_{i}^{0},\,\zeta\in \mathcal{H}_{i_{1}}^{0}\otimes\cdots\otimes \mathcal{H}_{i_{n}}^{0}.
\end{array}\right.
\label{ch07:eqn7.1.3}
\end{equation}
Then a $^{*}$-representation $\lambda_{i}:B(\mathcal{H}_{i})\rightarrow B(\mathcal{H})$ can be defined by
\begin{equation}
\lambda_{i}(a)\ :=V_{i}(a\otimes \mathbf{1}_{\mathcal{H}(i)})V_{i}^{*} \qquad (a\in B(\mathcal{H}_{i})).
\label{ch07:eqn7.1.4}
\end{equation}
When $(\mathcal{M}_{i})_{i\in I}$ is a family of von Neumann algebras\index{free!product of von Neumann algebras} $\mathcal{M}_{i}$ on $(\mathcal{H}_{i}, \xi_{i})$, where $\xi_{i}$ is cyclic and separating for $\mathcal{M}_{i}$, the \textit{von Neumann algebra free product} $\star_{i\in I}\mathcal{M}_{i}$ is generated by $\lambda_{i}(\mathcal{M}_{i}), i\ \in\ I$:
\begin{equation*}
\star_{i\in I}\mathcal{M}_{i}:=\left(\bigcup_{i\in I}\lambda_{i}(\mathcal{M}_{i})\right)''.
\end{equation*}

We can also introduce the free product\index{free!product of states} of $^*$-representations\index{free!product of representations} using the above setting. For $i\in I$ let $\mathcal{A}_{i}$ be a $^{*}$-algebra and $\pi_{i}$ a $^{*}$-representation of $\mathcal{A}_{i}$ on $(\mathcal{H}_{i}, \xi_{i})$. Define the algebraic free product $\star_{i\in I}\mathcal{A}_{i}$ as in Sec.~\ref{ch02:sec2.1}:
\begin{equation*}
\star_{i\in I}\mathcal{A}_{i}:=\mathbb{C}\mathbf{1}\oplus\bigoplus_{n=1}^{\infty}\left(\bigoplus_{i_{1}\neq i_{2}\neq \ldots \neq i_{n}}\mathcal{A}_{i_{1}}^{0}\mathcal{A}_{i_{2}}^{0}\cdots \mathcal{A}_{i_{n}}^{0}\right)
\end{equation*}
with the product given in (\ref{ch02:eqn2.1.1}) and (\ref{ch02:eqn2.1.2}). Using the notation in (\ref{ch07:eqn7.1.4}), we define a $^{*}$-representation $\lambda_{i}:\mathcal{A}_{i}\rightarrow B(\mathcal{H})$ by
\begin{equation*}
\lambda_{i}(a):=V_{i}(\pi_{i}(a)\otimes \mathbf{1}_{\mathcal{H}(i)})V_{i}^{*} \qquad (a\in \mathcal{A}_{i}).
\end{equation*}
Then the \textit{free product representation} $\pi=\star_{i\in I}\,\pi_{i}$ of $\mathcal{A}=\star_{i\in i}\mathcal{A}_{i}$ is defined by
\begin{equation*}
\pi(a_{1}a_{2}\cdots a_{n}) :=\lambda_{i_{1}}(a_{1})\lambda_{i_{2}}(a_{2})\cdots\lambda_{i_{n}}(a_{n})
\end{equation*}
for $a_{j}\in \mathcal{A}_{i_{j}}, \,i_{1}\neq i_{2}\neq \ldots \neq i_{n}$. For instance, let $\varphi_{i}$ be a state on $\mathcal{A}_{i}$ and $(\pi_{i}, \mathcal{H}_{i}, \xi_{i})$ the GNS cyclic representation of $(\mathcal{A}_{i}, \varphi_{i})$, so that $\varphi_{i}(a)=\langle \pi_{i}(a)\xi_{i}, \xi_{i}\rangle  (a\in \mathcal{A}_{i})$ and $\mathcal{H}_{i}=\overline{\pi_{i}(\mathcal{A}_{i})\xi_{i}}$. Then the \textit{free product state} $\varphi$ on $\mathcal{A} =\star_{i\in I}\mathcal{A}_{i}$ is given as $\varphi(a) := \langle \pi(a)\xi, \xi\rangle$, where $(\mathcal{H}, \xi)=\star_{i\in I}\,(\mathcal{H}_{i}, \xi_{i})$ and $\pi=\star_{i\in I}\pi_{i}$. Then one can easily show that $(\pi, \mathcal{H}, \xi)$ is the GNS representation of $(\mathcal{A}, \varphi)$ and $\pi(\mathcal{A})''=\star_{i\in I}\pi_{i}(\mathcal{A}_{i})''$.

The following is a von Neumann algebra version of Example~\ref{ch02:exa2.1.2}.

\begin{example}
\label{ch07:exa7.1.2}
Let $(G_{i})_{i\in I}$ be a family of discrete groups and $G=\star_{i\in I}G_{i}$ the free product group. With the identities $e_{i}$ of $G_{i}$ and $e$ of $G$ we have
\begin{equation*}
(\ell^{2}(G), \delta_{e})=\underset{i\in I}{\star}(\ell^{2}(G_{i}), \delta_{e_{i}})
\end{equation*}
under the identification
\begin{equation*}
\delta_{g_{1}\cdots g_{n}}=\delta_{g_{1}}\otimes\cdots\otimes\delta_{g_{n}}\in \ell^{2}(G_{i_{1}})^{0}\otimes\cdots\otimes\ell^{2}(G_{i_{n}})^{0}
\end{equation*}
for $g_{j}\in G_{i_{j}}\,\backslash\,\{e_{i_{j}}\},\,i_{1}\neq i_{2}\neq\ldots\neq i_{n}$. Let $L^{(i)}$ denote the regular representation of $G_{i}$ on $\ell^{2}\thinspace(G_{i})$ and $L$ that of $G$ on $\ell^{2}(G)$ . Then it is straightforward to check that $\lambda_{i}(L_{g}^{(i)})=L_{g}$ for all $g\in  G_{i}$. Hence
\begin{equation*}
\mathcal{L}(G)=\left(\bigcup_{i\in I}\lambda_{i}(\mathcal{L}(G_{i}))\right)''=\underset{i\in I}{\star} \mathcal{L}(G_{i}).
\end{equation*}
In particular, since $\mathbf{F}_{|I|}=\star_{i\in I} \mathbb{Z}$, we have
\begin{equation}
\mathcal{L}(\mathbf{F}_{|I|})=\underset{i\in I}{\star} \mathcal{L}(\mathbb{Z})\,.
\label{ch07:eqn7.1.5}
\end{equation}
\end{example}

The definition of free relation modeled the free product construction, so that we have the following property as a matter of course.

\begin{proposition}
\label{ch07:pro7.1.3}
Let $\mathcal{M}_{i}$ be a von Neumann algebra on $(\mathcal{H}_{i}, \xi_{i})$ for $i\in I$, and let $\mathcal{M} =\star_{i\in I}\mathcal{M}_{i}$ be the von Neumann algebra free product on $(\mathcal{H}, \xi)=\star_{i\in I}(\mathcal{H}_{i}, \xi_{i})$. Then $(\mathcal{M}_{i})_{i\in I}$ is a free family in $(\mathcal{M}, \varphi)$ , where $\varphi=\langle\cdot\xi, \xi\rangle$ and $\mathcal{M}_{i}$ is identified with $\lambda_{i}(\mathcal{M}_{i})$.
\end{proposition}

\begin{proof2}
 Let $ i_{1}\neq i_{2}\neq \ldots \neq i_{n}$ and $a_{j}\in \mathcal{M}_{i_j}$, and assume that $\varphi(\lambda_{i_{j}}(a_{j}))= \langle a_{j}\xi_{i_{j}}, \xi_{i_{j}}\rangle =0$ or $\mathrm{v}a_{j}\xi_{i_{j}}\in \mathcal{H}_{i_{j}}^{0}$ for $1\leq j\leq n$. Then it is readily seen that
\begin{equation*}
\lambda_{i_{1}}(a_{1})\cdots\lambda_{i_{n}}(a_{n})\xi=a_{1}\xi_{i_{1}}\otimes\cdots\otimes a_{n}\xi_{i_{n}}\in  \mathcal{H}_{i_{1}}^{0}\otimes\cdots\otimes \mathcal{H}_{i_{n}}^{0},
\end{equation*}
and hence $\varphi(\lambda_{i_{1}}(a_{1})\cdots\lambda_{i_{n}}(a_{n}))=0$.
\end{proof2}

We explained in Sec.~\ref{ch04:sec4.2} that the standard selfadjoint (or real symmetric) Gaussian random matrices form an asymptotic model of a semicircular system. On the other hand, the full Fock space picture provides a direct model of a semicircular system, as can be seen more or less from Theorem~\ref{ch01:the1.1.5} and Example~\ref{ch02:exa2.2.1}. This is fully summarized in the next theorem.

\begin{theorem}
\label{ch07:the7.1.4}
Let $\mathcal{H}$ be a Hilbert space with an orthonormal basis $(f_{i})_{i\in I}$. Set $s(f_{i}) :=\ell(f_{i})^{*}+\ell(f_{i})$ and $\mathcal{M} :=\{s(f_{i}):i\in I\}''$ on the full Fock space $\mathcal{F}(\mathcal{H})$. Then:
\begin{enumerate}
\item[(1)] The vacuum vector $\Phi$ is cyclic and separating for $\mathcal{M}$.

\item[(2)] $\tau :=\langle \cdot\Phi, \Phi\rangle$ is a faithful normal tracial state on $\mathcal{M}$.

\item[(3)] $(s(f_{i}))_{i\in I}$ is a semicircular system in $(\mathcal{M}, \tau)$.

\item[(4)] $\mathcal{M} \cong \mathcal{L}(\mathbf{F}_{|I|})$.
\end{enumerate}
\end{theorem}
The assertion (3) is already known from Theorem~\ref{ch01:the1.1.5} and Example~\ref{ch02:exa2.2.1} (the argument of Example~\ref{ch02:exa2.2.1} is valid for many $\ell(f_{i})$'s) . The proof below is based on a general property of free products (Proposition~\ref{ch07:pro7.1.3}). We give two lemmas to prove the theorem. The first lemma treats the particular case where $\dim \mathcal{H}=1$.


\begin{lemma}
\label{ch07:lem7.1.5}
Let $\mathcal{H}=\mathbb{C}f$ be one-dimensional and $\mathcal{M} :=\{s(f)\}''$ on $\mathcal{F}(\mathcal{H})$. Then $(\mathcal{M}, \mathcal{F}(\mathcal{H}), \Phi)$ is unitarily conjugate to $(\mathcal{L}(\mathbb{Z}), \ell^{2}(\mathbb{Z}), \delta_{0})$.
\end{lemma}

\begin{proof2}
Since $\mathcal{F}(\mathcal{H})=\bigoplus_{n=0}^{\infty}\mathbb{C}f^{\otimes n}$ with $ f^{\otimes 0}=\Phi$ and
\begin{equation*}
(\ell(f)^{*}+\ell(f))^{n}\Phi-f^{\otimes n}\in\bigoplus_{k=0}^{n-1}\mathbb{C}f^{\otimes k} \qquad (n\geq 1),
\end{equation*}
it follows that $\Phi$ is cyclic for $\mathcal{M}$. Hence $\Phi$ is cyclic and separating for $\mathcal{M}$, because $\mathcal{M}$ is commutative. Since the distribution of $s(f)$ with respect to $\tau=\langle\cdot\Phi, \Phi\rangle$ is $w_{2}$ by Theorem~\ref{ch01:the1.1.5}, $\mathcal{M}$ is generated by a Haar unitary $U$ (see the proof of Proposition~\ref{ch07:pro7.1.1}). Thus we see that
\begin{equation*}
(\mathcal{M}, \mathcal{F}(\mathcal{H}), \Phi)\cong(L^{\infty}(\mathbb{T}), L^{2}(\mathbb{T}), 1)\,,
\end{equation*}
and the latter is unitarily conjugate to $(\mathcal{L}(\mathbb{Z}), \ell^{2}(\mathbb{Z}), \delta_{0})$ via the Fourier transform.
\end{proof2}

\begin{lemma}
\label{ch07:lem7.1.6}
Let $\mathcal{M}_{i}$ be a von Neumann algebra on $(\mathcal{H}_{i}, \xi_{i})$ for $i\in I$, and let $(\mathcal{H}, \xi)=\star_{i\in I}(\mathcal{H}_{i}, \xi_{i})$. If $\xi_{i}$ is cyclic and separating for $\mathcal{M}_{i}$ for each $i\in I$, then $\xi$ is cyclic and separating for $\star_{i\in I}\mathcal{M}_{i}$ too.
\end{lemma}

\begin{proof2}
Besides the ``left'' free product $\mathcal{M} =\star_{i\in I}\mathcal{M}_{i}$ one can define the ``right'' free product as follows: Set
\begin{equation*}
\mathcal{H}_{r}(i):=\mathbb{C}\xi\oplus\bigoplus_{n=1}^{\infty}\left(\bigoplus_{i_{1}\neq i_{2}\neq\ldots\neq i_{n}\neq i}\mathcal{H}_{i_{1}}^{0}\otimes \mathcal{H}_{i_{2}}^{0}\otimes\cdots\otimes \mathcal{H}_{i_{n}}^{0}\right)
\end{equation*}
and define a unitary operator $W_{i}: \mathcal{H}_{r}(i)\otimes \mathcal{H}_{i}\rightarrow \mathcal{H}$ analogously to (\ref{ch07:eqn7.1.3}). Let $\rho_{i}:B(\mathcal{H}_{i})\rightarrow B(\mathcal{H})$ be a ${^*}$-representation, given as
\begin{equation*}
\rho_{i}(a):=W_{i}(\mathbf{1}_{\mathcal{H}_{r}(i)}\otimes a)W_{i}^{*} \qquad (a\in B(\mathcal{H}_{i})).
\end{equation*}
Now let $\mathcal{N}_{i} :=\mathcal{M}_{i}'$ on $\mathcal{H}_{i}$ and define the ``right'' free product
\begin{equation*}
\mathcal{N}:=\left(\bigcup_{i\in I}\rho_{i}(\mathcal{N}_{i}\right)''.
\end{equation*}
Since we immediately see that $\lambda_{i}(a)\rho_{j}(b)=\rho_{j}(b)\lambda_{i}(a)$ for all $i, j\in I$ and $a\in \mathcal{M}_{i}, b\in \mathcal{N}_{j}$, we have $\mathcal{M} \subset \mathcal{N}'$. It is clear by construction that $\xi$ is cyclic for $\mathcal{N}$ as well as for $\mathcal{M}$. So $\xi$ is also separating for $\mathcal{M}$.
\end{proof2}

It is worth noting that $\mathcal{M} =\mathcal{N}'$ indeed holds under the assumption of the above lemma.

\begin{proof2}[Proof of Theorem~\ref{ch07:the7.1.4}]
For $ i\in I$ set $\mathcal{H}_{i} :=\mathbb{C}f_{i}$. Since $\mathcal{H}=\bigoplus_{i\in I}\mathcal{H}_{i}$ and $\mathcal{F}(\mathcal{H}_{i})=\mathbb{C}\Phi_{i}\oplus\bigoplus_{n=1}^{\infty}\mathbb{C}f_{i}^{\otimes n}$, we have
\begin{align*}
\bigoplus_{n=1}^{\infty}\mathcal{H}^{\otimes n} & \ = \ \bigoplus_{n=1}^{\infty}\bigoplus_{\,i_{1},\ldots,i_{n}\in I}\mathcal{H}_{i_{1}}\otimes\cdots\otimes \mathcal{H}_{i_{n}} \\
& \ = \ \bigoplus_{n,k=1}^{\infty} \bigoplus_{\begin{subarray}{c}\,i_{1}\neq i_{2}\neq\ldots \neq i_{k} \\ n_{j}\in \mathrm{N},n_{1}+\ldots +n_{k}=n \\ \end{subarray}} \mathbb{C}f_{i_{1}}^{\otimes n_{1}}\otimes\cdots\otimes \mathbb{C}f_{i_{k}}^{\otimes n_{k}} \\
& \ = \ \bigoplus_{k=1}^{\infty}\bigoplus_{\,i_{1}\neq i_{2}\neq\ldots\neq i_{k}}\mathcal{F}(\mathcal{H}_{i_{1}})^{0}\otimes\cdots\otimes \mathcal{F}(\mathcal{H}_{i_{k}})^{0},
\end{align*}
which shows that
\begin{equation*}
(\mathcal{F}(\mathcal{H}), \Phi)=\underset{i\in I}{\star}(\mathcal{F}(\mathcal{H}_{i}), \Phi_{i}).
\end{equation*}
It is readily verified that for each $i\in I$
\begin{equation*}
\lambda_{i}(\ell_{i}(f_{i}))=V_{i}(\ell_{i}(f_{i})\otimes \mathbf{1}_{\mathcal{F}(\mathcal{H})(i)})V_{i}^{*}=\ell(f_{i}),
\end{equation*}
and hence $\lambda_{i}(s_{i}(f_{i}))=s(f_{i})$, where $\ell_{i}(\cdot)$ and $s_{i}(\cdot)$ mean $\ell(\cdot)$ and $s(\cdot)$ defined on $\mathcal{F}(\mathcal{H}_{i})$. Therefore, we have
\begin{equation*}
\mathcal{M} =\{\lambda_{i}(s_{i}(f_{i})):i\in I\}^{\prime \prime}=\underset{i\in I}{\star}\mathcal{M}_{i},
\end{equation*}
where $\mathcal{M}_{i}=\{s_{i}(f_{i})\}^{\prime \prime}$ on $\mathcal{F}(\mathcal{H}_{i})$. Now (1) follows from Lemmas~\ref{ch07:lem7.1.5} and \ref{ch07:lem7.1.6}. Proposition~\ref{ch07:pro7.1.3} together with Theorem~\ref{ch01:the1.1.5} implies (3), so Proposition~\ref{ch07:pro7.1.1} and its remark yield (4) and (2). Also, (4) can be more directly shown as
\begin{equation*}
\mathcal{M} =\underset{i\in I}{\star} \mathcal{M}_{i}\cong\underset{i\in I}{\star} \mathcal{L}(\mathbb{Z})=\mathcal{L}(\mathbf{F}_{|I|})
\end{equation*}
by Lemma~\ref{ch07:lem7.1.5} and (\ref{ch07:eqn7.1.5}).
\end{proof2}

Furthermore, a circular system can be canonically realized on the full Fock space. Set $c_{i} :=\ell(f_{i})^{*}+\ell(g_{i})$ and $\tilde{c}_{i} :=(s(f_{i})+\mathrm{i}\ s(g_{i}))/\sqrt{2}$ on $\mathcal{F}(\mathcal{H})$, where $(f_{i})_{i\in I}\cup(g_{i})_{i\in I}$ is an orthonormal basis of $\mathcal{H}$. Theorem~\ref{ch07:the7.1.4} shows that $(\tilde{c}_{i})_{i\in I}$ is a circular system in $(\mathcal{M} =\{s(f_{i}), s(g_{i}):i\in I\}^{\prime \prime}, \tau= \langle\,\cdot \,\Phi, \Phi\rangle)$, and $\mathcal{M}$ is isomorphic to $\mathcal{L}(\mathbf{F}_{2|I|})$. Also, $(c_{i})_{i\in I}$ is a circular system, as we remarked before Corollary~\ref{ch04:cor4.3.8} (see also Example~\ref{ch02:exa2.6.2}). The unitary operator $\mathcal{F}(U)$ on $\mathcal{F}(\mathcal{H})$ preserving the vacuum vector is induced from the unitary operator $U$ on $\mathcal{H}$ given by $U\,f_{i} :=(f_{i}-\mathrm{i}\ g_{i})/\sqrt{2}, \ Ug_{i} :=(f_{i}+\mathrm{i}\ g_{i})/\sqrt{2}\ (i\in I)$. Then one gets
\begin{equation*}
\mathcal{F}(U)\ell(f_{i})\mathcal{F}(U)^{*}=\frac{\ell(f_{i})-\mathrm{i}\ \ell(g_{i})}{\sqrt{2}}, \quad \mathcal{F}(U)\ell(g_{i})\mathcal{F}(U)^{*}=\frac{\ell(f_{i})+\mathrm{i}\ \ell(g_{i})}{\sqrt{2}}\,,
\end{equation*}
so $\mathcal{F}(U)c_{i}\mathcal{F}(U)^{*}=\tilde{c}_{i}$ and $\{c_{i}:i\in I\}^{\prime \prime}$ is unitarily conjugate to $\{\tilde{c}_{i}:i\in I\}''$.

\section{Interpolated free group factors}
\label{ch07:sec7.2}

The discussions in the previous section tell us that semicircular and circular systems are essentially the same as free group factors from the viewpoint of von Neumann algebras. Indeed, in this section it will turn out that the technique using (semi)circular systems and their random matrix models is quite useful in the analysis of free group factors.

From now on we always consider a tracial $W^{*}$-probability space $(\mathcal{M}, \tau)$ of a von Neumann algebra $\mathcal{M}$ and a faithful normal tracial state $\tau$. We first give some lemmas whose proofs are typical applications of Gaussian random matrix models.

\begin{lemma}
\label{ch07:lem7.2.1}
In $(\mathcal{M}, \tau)$ let $(a_{s})_{s\in S}$ be a semicircular system and $(e_{ij})_{i,j=1}^{N}$ a system of matrix units with $\tau(e_{ii})=1/N$ such that $\{a_{s}:s\in S\}$ and $\{e_{ij} : 1\leq i, j\leq N\}$ are in free relation. Define
\begin{align*}
\Omega_{1} & \ := \{N^{1/2}e_{1i}a_{s}e_{i1}:1\leq i\leq N,\ s\in S\}, \\
\Omega_{2} & \ := \{N^{1/2}e_{1i}a_{s}e_{j1}:1\leq i<j\leq N,\ s\in S\}.
\end{align*}
Then the following hold in $(e_{11}\mathcal{M}e_{11}, N\tau|_{e_{11}\mathcal{M}{e_{11}}})$:
\begin{enumerate}
\item[(1)] $\Omega_{1}$ is a semicircular system.

\item[(2)] $\Omega_{2}$ is a circular system.

\item[(3)] $\Omega_{1}\cup\Omega_{2}$ is a $^*$-free family.
\end{enumerate}
\end{lemma}
\begin{proof2}
For $n\in N\mathbb{N}$ let $(H(s, n))_{s\in S}$ be an independent family of $n\times n$ standard selfadjoint Gaussian matrices. Also, let $(E_{ij})_{i,j=1}^{N}$ be the usual matrix units of $M_{N}(\mathbb{C})$ , and let $E_{ij}(n)\ (n\in N\mathbb{N})$ be the $n\times n$ block-diagonal matrix all of whose diagonals are $E_{ij}$. Then Corollary~\ref{ch04:cor4.3.6} says that
\begin{equation*}
\left((\{H(s, n)\})_{s\in S}, \ \{E_{ij}(n):1\leq i, j\leq N\}\right)
\end{equation*}
is asymptotically free as $ n\rightarrow\infty$ (through multiples of $N$), and its limit distribution is equal to the distribution of
\begin{equation*}
\left((\{a_{s}\})_{s\in S}, \ \{e_{ij}:1\leq i,j\leq N\}\right)
\end{equation*}
due to the assumptions on $(a_{s})$ and $(e_{ij})$. So, for any $1\leq i, j\leq N$ the distribution of $e_{1i}a_{s}e_{j1}$ is equal to the limit distribution of $E_{1i}(n)H(s, n)E_{j1}(n)$. Note that $(N^{1/2}E_{1i}(n)H(s, n)E_{j1}(n))_{1\leq i\leq j\leq N,\ s\in S}$ is an independent family of $n/N\times n/N$ standard selfadjoint (for $i=j$) or non-selfadjoint (for $i<j$) Gaussian matrices (if the trivial summand of size $n-n/N$ is neglected). Hence the result follows from Corollaries~\ref{ch04:cor4.3.6} and \ref{ch04:cor4.3.8} together.
\end{proof2}

In previous chapters we sometimes discussed compressions of noncommutative random variables, cf. Examples~\ref{ch02:exa2.4.7}, \ref{ch02:exa2.6.9} and Lemma~\ref{ch04:lem4.4.10}. The following is another result of the same kind.

\begin{lemma}
\label{ch07:lem7.2.2} In $(\mathcal{M}, \tau)$ let $R$ be a copy of the hyperfinite $II_{1}$ factor, and let $(a_{s})_{s\in S}$ be a semicircular system such that $R$ and $\{a_{s}:s\in S\}$ are in free relation. Let $p\in R$ be a nonzero projection. Then in $(p\mathcal{M}p, \tau(p)^{-1}\tau|_{p\mathcal{M}p})$, $(\tau(p)^{-1/2}pa_{s}p)_{s\in S}$ is a semicircular system which is free from $pRp$.
\end{lemma}

\begin{proof2}
First assume that $\tau(p)=l/2^{k}$, a dyadic rational number. Note that $(R, (\{ua_{s}u^{*}\})_{s\in S})$ is free again if $u\in R$ is a unitary. Furthermore, note that if $p, q\in R$ are projections with $\tau(p)=\tau(q)$, then $u^{*}pu=q$ for some unitary $u\in R$. So we may show the result for a specific $p$ having the above trace value. Represent $R=M_{2^{k}}(\mathbb{C})\otimes M_{2}(\mathbb{C})\otimes M_{2}(\mathbb{C})\otimes\cdots$ and choose a diagonal $p\in M_{2^{k}}(\mathbb{C})\ (\subset R)$ with $\tau(p)=l/2^{k}$. Let $N\in \mathbb{N}$ be given, and for $n=2^{k+N}, 2^{k+N+1}, \ldots$ let $H(s, n) (s\in S)$ be independent $n\times n$ standard Gaussian matrices. Also, for such $n$ and $A\in M_{2^{k+N}}(\mathbb{C})$ let $D(A, n)$ be the block-diagonal matrix all of whose diagonals are $A$. Then it is obvious that the distribution of $(D(A, n))_{A\in M_{2^{k+N}}(\mathbb{C})}$ is equal to that of $(A)_{A\in M_{2^{k+N}}(\mathbb{C})}$ as a family in $R$. Hence Corollary~\ref{ch04:cor4.3.6} shows that
\begin{equation*}
\left((\{H(s, n)\})_{s\in S}, \{D(A, n):A\in M_{2^{k+N}}(\mathbb{C})\}\right)
\end{equation*}
is asymptotically free as $n=2^{k+N+j},  \ j\rightarrow\infty$, and its limit distribution is the distribution of
\begin{equation*}
\big((\{a_{s}\})_{s\in S},\ M_{2^{k+N}}(\mathbb{C})\big)
\end{equation*}
(where $M_{2^{k+N}}(\mathbb{C})\subset R$). Note that $\tau(p)^{-1/2}D(p, n)H(s, n)D(p, n)\,(s\in S)$ are the same kind of random matrices of size $nl/2^{k}$ together with block-diagonal matrices $D(p, n)D(A, n)D(p, n)=D(pAp, n)$ (if the trivial summand of size $n-nl/2^{k}$ is neglected). Hence Corollary~\ref{ch04:cor4.3.6} can be used again to see that $(\tau(p)^{-1/2}pa_{s}p)_{s\in S}$ is a semicircular system free from $pM_{2^{k+N}}(\mathbb{C})p\ (\subset pRp)$. Letting $ N\rightarrow\infty$ gives the result.

For general $p$, choose a sequence $(p_{m})$ of nonzero projections in $R$ such that $\tau(p_{m})$ are dyadic rationals and $p_{m}\rightarrow p$ strongly. Then
\begin{equation*}
((\tau(p_{m})^{-1/2}p_{m}a_{s}p_{m})_{s\in S},\ (p_{m}xp_{m})_{x\in R})\rightarrow((\tau(p)^{-1/2}pa_{s}p)_{s\in S},  (pxp)_{x\in R})
\end{equation*}
in distribution as $ m\rightarrow\infty$. Since the conclusion holds for each $p_{m}$, we can pass to the limit.
\end{proof2}

Below, for a family $\Omega\ (\subset\mathcal{M})$ containing non-selfadjoint elements, we shall often say simply that $\Omega$ is free when it is $^*$-free.

\begin{lemma}
\label{ch07:lem7.2.3} In $(\mathcal{M}, \tau)$ let $(a_{1}, a_{2})$ be a semicircular pair, $h$ a quarter-circular element and $b_{1}, b_{2}$ normal elements. Assume that $(a_{1}, a_{2}, h, b_{1}, b_{2})$ is a free family. In $(\mathcal{M}\otimes M_{2}(\mathbb{C}), \tau\otimes \mathrm{tr}_{2})$ set
\begin{equation*}
y:=\frac{1}{\sqrt{2}} \left[\begin{array}{cc}
a_{1} & h\\
h & a_{2}
\end{array}\right],\quad b:=\left[\begin{array}{cc}
b_{1} & 0\\
0 & b_{2}
\end{array}\right].
\end{equation*}
Then $y$ is semicircular and $y, b$ are free
\end{lemma}

\begin{proof2}
 Let $u, v$ be Haar unitaries in $\mathcal{M}$ such that $(a_{1}, a_{2}, h, u, v)$ is free (we may enlarge $\mathcal{M}$ if necessary to choose such $u, v$). Since $(a_{1}, a_{2}, h, v, u^{*}vu)$ is free, we may assume by functional calculus that $b_{1}=f_{1}(v)$ and $b_{2}=f_{2}(u^{*}vu)$ with some measurable functions $f_{1}, f_{2} : \mathbb{T}\rightarrow \mathbb{C}$. Then $b_{1}$ and $ub_{2}u^{*}=f_{2}(v)$ are commuting. It is enough to show the required conclusion for
\begin{align*}
\tilde{y} & :=\left[\begin{array}{cc}
\mathbf{1} & 0\\
0 & u
\end{array}\right] y \left[\begin{array}{cc}
\mathbf{1} & 0\\
0 & u^{*}
\end{array}\right]=\frac{1}{\sqrt{2}} \left[\begin{array}{cc}
a_{1} & hu^{*}\\
uh & ua_{2}u^{*}
\end{array}\right], \\
\tilde{b} & :=\left[\begin{array}{cc}
\mathbf{1} & 0\\
0 & u
\end{array}\right] b \left[\begin{array}{cc}
\mathbf{1} & 0\\
0 & u^{*}
\end{array}\right]=\left[\begin{array}{cc}
b_{1} & 0\\
0 & ub_{2}u^{*}
\end{array}\right]\,.
\end{align*}
It is easy to see that $(a_{1}, ua_{2}u^{*}, h, u, v)$ is a free family. Moreover, Proposition~\ref{ch04:pro4.4.2} implies that $uh$ is circular. Now the proof is an application of the random matrix model. For $n\in \mathbb{N}$ let
\begin{equation*}
H(2n)=\left[\begin{array}{cc}
H_{1}(n) & X(n)^{*}\\
X(n) & H_{2}(n)
\end{array}\right]
\end{equation*}
be a $2n\,\times\,2n$ standard selfadjoint Gaussian matrix with $n\,\times\,n$ blocks $H_{1}(n), H_{2}(n), X(n)$. Also, let $D(2n)\ (n\in \mathbb{N})$ be constant diagonal matrices with $n{\times} n$ diagonal blocks $D_{1}(n), D_{2}(n)$ such that the limit distribution of $(D_{1}(n), D_{2}(n))$ is the distribution of $(b_{1}, ub_{2}u^{*})$. Then, applying Corollaries~\ref{ch04:cor4.3.6} and \ref{ch04:cor4.3.8} to $n\times n$ blocks shows that
\begin{equation*}
\left(\{\sqrt{2}\ H_{1}(n)\},\,\{\sqrt{2}\ H_{2}(n)\},\,\{\sqrt{2}\ X(n), \sqrt{2}\ X(n)^{*}\},\,\{D_{1}(n), D_{2}(n)\}\right)
\end{equation*}
is asymptotically free and its limit distribution is equal to the distribution of $(\{a_{1}\},\,\{ua_{2}u^{*}\},\,\{uh, hu^{*}\},\,\{b_{1}, ub_{2}u^{*}\})$. (Here the multiple constant $\sqrt{2}$ is just to adjust variances of matrix entries to $n$.) In this way, it is shown that the distribution of $(\tilde{y},\tilde{b})$ coincides with the limit distribution of $(H(2n), D(2n))$ . Now the desired conclusion follows from Corollary~\ref{ch04:cor4.3.6}.
\end{proof2}

\begin{theorem}
\label{ch07:the7.2.4}
Let $R$ be a hyperfinite $II_{1}$ factor. For $n=1,2,\ldots,\infty$,
\begin{equation*}
R\star \mathcal{L}(\mathbf{F}_{n})\cong \mathcal{L}(\mathbf{F}_{n+1}),
\end{equation*}
where $\mathbf{F}_{1}$ means $\mathbb{Z}$.
\end{theorem}

\begin{proof2}
According to (\ref{ch07:eqn7.1.5}) it suffices to show that
\begin{equation*}
R\star \mathcal{L}(\mathbb{Z})\cong \mathcal{L}(\mathbf{F}_{2}).
\end{equation*}
Write $\mathcal{M} :=R\star \mathcal{L}(\mathbb{Z})$ with the canonical (free product) trace $\tau$. Let $a$ be a semicircular element generating $\mathcal{L}(\mathbb{Z})$. Represent $ R=M_{2}(\mathbb{C})\otimes M_{2}(\mathbb{C})\otimes\cdots$ with matrix units $(e_{ij}^{m})_{i,j=0}^{1}$ of the $m\mathrm{th} \ M_{2}(\mathbb{C})$, and for each $k\in \mathbb{N}$ write
\begin{equation*}
e_{k}(i_{1}\ldots i_{k}, j_{1}\ldots j_{k}) :=e_{i_{1}j_{1}}^{1}\otimes\cdots\otimes e_{i_{k}j_{k}}^{k} \qquad (i_{m}, j_{m}=0,1),
\end{equation*}
which are matrix units of $\bigotimes_{1}^{k}M_{2}(\mathbb{C}) \ (\subset R)$. In
\begin{equation*}
\mathcal{A}_{k} :=e_{k}(0\ldots 0,0\ldots 0)\mathcal{M}e_{k}(0\ldots 0,0\ldots 0),
\end{equation*}
set
\begin{align*}
a_{k}(i_{1}\ldots i_{k})&  \ := \ 2^{k/2}e_{k}(0\ldots 0, i_{1}\ldots i_{k})ae_{k}(i_{1}\ldots i_{k}, 0\ldots 0), \\
c_{k}(i_{1}\ldots i_{k}, j_{1}\ldots j_{k}) & \ := \ 2^{k/2}e_{k}(0\ldots 0, i_{1}\ldots i_{k})ae_{k}(j_{1}\ldots j_{k}, 0\ldots 0)
\end{align*}
for $ i_{1}\ldots  i_{k}<j_{1}\ldots j_{k}$ (lexicographic order). Then Lemma~\ref{ch07:lem7.2.1} implies that in $(\mathcal{A}_{k}, 2^{k}\tau|_{A_{k}})$
\begin{equation*}
\Omega_{1} :=\{a_{k}(i_{1}\ldots i_{k}):i_{1},\ldots,i_{k}=0,1\}
\end{equation*}
and
\begin{equation*}
\Omega_{2}:=\{c_{k}(i_{1}\ldots i_{k}, j_{1}\ldots j_{k}) :i_{m}, j_{m}=0,1  \ \mathrm{with} \  i_{1} \ldots  i_{k}<j_{1}\ldots j_{k}\}
\end{equation*}
are a semicircular system and a circular system, respectively, and $\Omega_{1}\cup\Omega_{2}$ is a free family. Let $c_{k}(0\ldots 0,0\ldots 01)=u_{k}h_{k}$ be the polar decomposition. Then $u_{k}$ is a Haar unitary and $h_{k}$ is quarter-circular by Proposition~\ref{ch04:pro4.4.2}. Now define a selfadjoint $y$ and a normal $b$ in $\mathcal{M}$ by
\begin{align*}
y & :=\sum_{k=1}^{\infty}[e_{k}(0\ldots 01,0\ldots 01)ae_{k}(0\ldots 01,0\ldots 01) \\
& \qquad \quad \ +2^{-k/2}e_{k}(0\ldots 0,0\ldots 0)h_{k}e_{k}(0\ldots 0,0\ldots 01) \\
& \qquad \quad \ +2^{-k/2}e_{k}(0\ldots 01, 0\ldots 0)h_{k}e_{k}(0\ldots 0,0\ldots 0)], \\
b & :=\sum_{k=1}^{\infty}\frac{1}{k}e_{k}(0\ldots 01,0\ldots 0)u_{k}e_{k}(0\ldots 0,0\ldots 01).
\end{align*}
Let us see that $y$ and $b$ generate $\mathcal{M}$. Take spectral projections of $b$ to get $ e_{k}(0\ldots 01, 0\ldots 01)\ (k\geq 1)$. These extract $e_{k}(0\ldots 0,0\ldots 0)h_{k}e_{k}(0\ldots 0,0\ldots 01)\ (k\geq 1)$ from $y$, whose polar parts are $e_{k}(0\ldots 0,0\ldots 01)$. These generate all matrix units $e_{k}(i_{1}\ldots i_{k}, j_{1}\ldots j_{k})$. Hence we get all $e_{k}(0\ldots 01, 0\ldots 01)ae_{k}(0\ldots 01,0\ldots 01)$ and $c_{k}(0\ldots 0,0\ldots 01)$, generating $a$.

Note that the spectral measure of $b$ has no atoms, because the $u_{k}$ are Haar unitaries. So it suffices by Proposition~\ref{ch07:pro7.1.1} to show that $y$ is semicircular and $y, b$ are free. To do so, set
\begin{align*}
y_{n} & :=\sum_{k=1}^{n}[e_{k}(0\ldots 01, 0\ldots 01)ae_{k}(0\ldots 01, 0\ldots 01) \\
& \qquad \quad +2^{-k/2}e_{k}(0\ldots 0,0\ldots 0)h_{k}e_{k}(0\ldots 0,0\ldots 01) \\
& \qquad \quad +2^{-k/2}e_{k}(0\ldots 01, 0\ldots 0)h_{k}e_{k}(0\ldots 0,0\ldots 0)] \\
& \qquad \quad +e_{n}(0\ldots 0,0\ldots 0)ae_{n}(0\ldots 0,0\ldots 0), \\
b_{n} & :=\sum_{k=1}^{n}\frac{1}{k}e_{k}(0\ldots 01, 0\ldots 0)u_{k}e_{k}(0\ldots 0,0\ldots 01)+\frac{1}{n+1}e_{n}(0\ldots 0,0\ldots 0).
\end{align*}
Since $\Omega_{1}$ and $\Omega_{2}$ are uniformly bounded, it is clear that $\Vert y_{n}-y\Vert\rightarrow 0$. Also $\Vert b_{n}-b\Vert\rightarrow 0$ is clear. Hence it is enough to show the same conclusion for $y_{n}$ and $b_{n}$ for each $n$. We proceed by induction. For $n=1$ we may write
\begin{equation*}
y_{1}=\frac{1}{\sqrt{2}} \left[\begin{array}{cc}
a_{1}(0) & h_{1}\\
h_{1} & a_{1}(1)
\end{array}\right], \ \ b_{1}=\left[\begin{array}{cc}
\frac{1}{2}\mathbf{1} & 0\\
0 & u_{1}
\end{array}\right] \quad \mathrm{in} \ \ \mathcal{A}_{1}\otimes M_{2}(\mathbb{C}),
\end{equation*}
so $y_{1}$ is semicircular and $y_{1}, b_{1}$ are free by Lemma~\ref{ch07:lem7.2.3}. Suppose the conclusion holds for $n-1$, and set
\begin{align*}
y_{n}' & :=\sqrt{2}(y_{n}-e_{1}(1,1)ae_{1}(1,1)-e_{1}(0,0)h_{1}e_{1}(0,1)-e_{1}(1,0)h_{1}e_{1}(0,0)), \\
b_{n}' & :=b_{n}-e_{1}(1, 0)u_{1}e_{1}(0,1).
\end{align*}
Note that $y_{n}'$ and $b_{n}'$ correspond to $y_{n-1}$ and $b_{n-1}$ taken in $(\mathcal{A}_{1},2\tau|_{A_{1}})$ for $a_{1}(0)= \sqrt{2}e_{1}(0,0)ae_{1}(0,0)$ instead of $a$. Hence, applying the induction hypothesis to $\mathcal{A}_{1}$, we see that $y_{n}'$ is semicircular and $y_{n}', b_{n}'$ are free. Since
\begin{equation*}
y_{n}=\frac{1}{\sqrt{2}} \left[\begin{array}{cc}
y_{n}' & h_{1}\\
h_{1} & a_{1}(1)
\end{array}\right], \quad b_{n}=\left[\begin{array}{cc}
b_{n}' & 0\\
0 & u_{1}
\end{array}\right] \quad \mathrm{in} \quad \mathcal{A}_{1}\otimes M_{2}(\mathbb{C}),
\end{equation*}
it remains, thanks to Lemma~\ref{ch07:lem7.2.3}, to show that $(y_{n}', a_{1}(1), h_{1}, b_{n}', u_{1})$ is a free family. For this, note that
\begin{equation*}
\left(\{e_{n}(0i_{2}\ldots i_{n}, 0j_{2}\ldots j_{n}):i_{m}, j_{m}=0,1\},\ \{a_{1}(0)\},\ \{a_{1}(1)\},\ \{c_{1}(0,1)\}\right)
\end{equation*}
is free in $(\mathcal{A}_{1},2\tau|_{\mathcal{A}_{1}})$, which can be shown like Lemma~\ref{ch07:lem7.2.1} by using the random matrix model. Since $y_{n}'$ and $b_{n}'$ are generated by $\{e_{n}(0i_{2}\ldots i_{n}, 0j_{2}\ldots j_{n})\}$ and $a_{1}(0)$, it follows that $(\{y_{n}', b_{n}'\}, \{a_{1}(1)\}, \{h_{1}, u_{1}\})$ is free. So we have the required assertion because both $(y_{n}', b_{n}')$ and $(h_{1}, u_{1})$ are free pairs.
\end{proof2}

Besides the above theorem there are several known relations involving free group factors and free products. For instance, Voiculescu proved that for every $N\in \mathbb{N}$ and $n\in \mathbb{N}\cup\{\infty\}$
\begin{equation}
\mathcal{L}(\mathbf{F}_{n+1})_{1/N}\cong \mathcal{L}(\mathbf{F}_{N^{2}n+1}) \quad \mathrm{or}  \quad \mathcal{L}(\mathbf{F}_{n+1})\cong \mathcal{L}(\mathbf{F}_{N^{2}n+1})\otimes M_{N}(\mathbb{C})
\label{ch07:eqn7.2.1}
\end{equation}
and
\begin{equation}
\mathcal{L}(\mathbf{F}_{n}\star \mathbb{Z}/N\mathbb{Z})_{1/N}\cong \mathcal{L}(\mathbf{F}_{N^{2}n-N+1}).
\label{ch07:eqn7.2.2}
\end{equation}
The following, as well as Theorem~\ref{ch07:the7.2.4}, is due to Dykema: For every $N\in \mathbb{N}$ and $n\in \mathbb{N}\cup\{\infty\}$,
\begin{align}
& (\mathcal{L}(\mathbf{F}_{n})\otimes M_{N}(\mathbb{C}))_{1/N}\cong \mathcal{L}(\mathbf{F}_{N^{2}n}) \notag \\
& \qquad \quad \mathrm{or} \quad \mathcal{L}(\mathbf{F}_{n})\otimes M_{N}(\mathbb{C})\cong \mathcal{L}(\mathbf{F}_{N^{2}n})\otimes M_{N}(\mathbb{C}).
\label{ch07:eqn7.2.3}
\end{align}
All of these formulas can be proved by manipulating semicircular and circular systems approximated by Gaussian random matrix models.

It follows from (\ref{ch07:eqn7.2.1}) for $n=\infty$ that the fundamental group of $\mathcal{L}(\mathbf{F}_{\infty})$ contains all positive rational numbers. In fact, R\u{a}dulescu proved more strongly that the fundamental group of $\mathcal{L}(\mathbf{F}_{\infty})$ is the whole $(0, \infty)$. This fact will be seen below as a by-product of the construction of interpolated free group factors. It is remarkable that we still have almost no exact information about fundamental groups of $\mathrm{II}_{1}$ factors except for $R$ and $\mathcal{L}(\mathbf{F}_{\infty})$.

Next let us show that $R\star R\cong \mathcal{L}(\mathbf{F}_{2})$. To prove this, the following lemma is useful.

\begin{lemma}
\label{ch07:lem7.2.5}
Let $(p, q)$ be a free pair of projections in $(\mathcal{M}, \tau)$ such that $\tau(p)= \tau(q)=1/2$. Let $\mathcal{N} :=\{p, q\}''$. Then
\begin{equation*}
\mathcal{N}\cong L^{\infty}([0, \pi/2], 2d\theta/\pi)\otimes M_{2}(\mathbb{C}),
\end{equation*}
where $\tau|_{N}=(\frac{2}{\pi}\int_{0}^{\pi/2}\cdot d\theta)\otimes \mathrm{tr}_{2}$ and $p, q$ are represented as
\begin{equation}
p=\left[\begin{array}{ll}
1 & 0\\
0 & 0
\end{array}\right], \quad q= \left[\begin{array}{cc}
\cos^{2} \theta & \cos \theta \sin \theta\\
\cos \theta \sin \theta & \sin^{2}\theta
\end{array}\right].
\label{ch07:eqn7.2.4}
\end{equation}
Moreover, the distribution of $pqp$ in $(p\mathcal{N}p, 2\tau|_{pNp})$ is
\begin{equation*}
\frac{1}{\pi\sqrt{t(1-t)}}\chi_{(0,1)}(t)\,dt.
\end{equation*}
\end{lemma}

\begin{proof2}
Since $p$ and $q$ are free, the distribution of $p+q$ (also $p+(\mathbf{1}-q))$ is $\frac{1}{2}(\delta(0)+\delta(1))\boxplus\frac{1}{2}(\delta(0)+\delta(1))$. Hence, thanks to Example~\ref{ch03:exa3.2.2},  $p\wedge q,\,p\wedge(\mathbf{1}-q),\ (\mathbf{1}-p)\wedge q$ and $(\mathbf{1}-p)\wedge(\mathbf{1}-q)$ are all $\{0\}$. So the structure theorem for two projections says (cf. [\citen{bib188}], pp. 306--308) that
\begin{equation*}
\mathcal{N}\cong L^{\infty}([0, \pi/2], \nu) \otimes M_{2}(\mathbb{C})
\end{equation*}
with identifications $\tau|_{N}=(\int_{0}^{\pi/2}\cdot\,d\nu) \otimes \mathrm{tr}_{2}$ and (\ref{ch07:eqn7.2.4}), where $\nu$ is a probability measure on $[0, \pi/2]$. Then we have
\begin{equation*}
(2p-1)(2q-1)=\left[\begin{array}{cc}
\cos 2\theta & \sin 2\theta\\
-\sin 2\theta & \cos 2\theta
\end{array}\right],
\end{equation*}
which is a Haar unitary. Therefore,
\begin{equation*}
\int_{0}^{\pi/2}\cos 2n\theta\,d\nu (\theta)=0 \qquad (n\in \mathbb{Z}, n\neq 0),
\end{equation*}
which forces $\nu$ to equal $2 \ d\theta/\pi$. Moreover, the latter assertion is true because
\begin{equation*}
\tau((pqp)^{n})=\frac{1}{\pi}\int_{0}^{\pi/2}\cos^{2n}\theta\,d\theta=\frac{1}{2\pi}\int_{0}^{1}t^{n}\frac{dt}{\sqrt{t(1-t)}} \qquad (n\in \mathbb{N}).
\end{equation*}
\end{proof2}

\begin{theorem}
\label{ch07:the7.2.6}
Let $R$ and $\tilde{R}$ be hyperfinite $II_{1}$ factors. Then
\begin{equation*}
R\star\tilde{R}\cong R\star \mathcal{L}(\mathbb{Z})
\end{equation*}
by an isomorphism mapping $R$ identically to itself.
\end{theorem}

\begin{proof2}
The proof will be divided into several steps.

\textit{Step} 1. First, let $\mathcal{M} :=R\star\tilde{R}$, and let $\tau$ be the canonical (free product) trace. Choose projections $p$ in $R$ and $q$ in $\tilde{R}$ such that $\tau(p)=\tau(q)=1/2$. Let $u\in R$ and $v\in\tilde{R}$ be partial isometries such that
\begin{equation*}
u^{*}u=p, \quad uu^{*}=\mathbf{1}-p, \quad v^{*}v=q, \quad vv^{*}=\mathbf{1}-q.
\end{equation*}
Represent $\mathcal{N} :=\{p, q\}''$ as in the above lemma, and set
\begin{align*}
x &  :=\left[\begin{array}{cc}
0 & 0\\
1 & 0
\end{array}\right], \quad  y:=\left[\begin{array}{cc}
-\cos \theta \sin \theta & -\sin^{2} \theta\\
\cos^{2} \theta & \cos \theta \sin \theta \end{array}\right], \\
w & :=\left[\begin{array}{cc}
\cos \theta & -\sin \theta\\
\sin \theta & \cos \theta \end{array}\right].
\end{align*}
Then $x, y$ are partial isometries and $w$ is a unitary such that
\begin{align*}
& x^{*}x=p, \quad xx^{*}= \mathbf{1}-p, \quad  y^{*}y=q, \quad yy^{*}=\mathbf{1}-q, \\
& p=w^{*}qw, \quad x=w^{*}yw.
\end{align*}

\textit{Step} 2. Show that $p\mathcal{M}p$ is generated by
\begin{equation*}
\Omega_{0}:=pRp\cup w^{*}q\tilde{R}qw\cup\{pqp,\ x^{*}u,\ w^{*}y^{*}vw\}.
\end{equation*}
Set
\begin{equation*}
\Omega:=pRp\cup w^{*}q\tilde{R}qw\cup\{q, u, w^{*}y^{*}vw\}.
\end{equation*}
Since $\Omega$ contains $p$ and $q$, it is obvious that $\Omega$ generates $\mathcal{M}$. Let $e_{11}=p,\,e_{12}=x^{*}, e_{21}=x$, and $e_{22}=\mathbf{1}-p$, which form matrix units. For $\Sigma\subset \mathcal{M}$ the linear span of $\bigcup_{j,k=1}^{2}e_{1j}\Sigma e_{k1}$ is denoted by $\Theta(\Sigma)$. Note that $\Theta(\Sigma^{*})=\Theta(\Sigma)^{*}$ and $\Theta(\Sigma_{1}\Sigma_{2})\subset \Theta(\Sigma_{1})\Theta(\Sigma_{2})$. Hence one can easily see that $\Theta(\Omega)$ generates $e_{11}\mathcal{M}e_{11}=p \mathcal{M}p$. It remains to show that $\Theta(\Omega)$ is generated by $\Omega_{0}$. But this holds because $pqx$ and $x^{*}qx$ are generated by $pqp$, and $pu=ux=0,\ up=u$, etc.

\textit{Step} 3. For $\Sigma_{1}, \Sigma_{2}\subset \mathcal{M}$ we use the notation $\Xi(\Sigma_{1}, \Sigma_{2})$ for the set of alternating products from $\Sigma_{1}$ and $\Sigma_{2}$, that is, the set of $a_{1}a_{2}\cdots a_{n}$ where $a_{j}\in\Sigma_{i_{j}}$ and $i_{1}\neq i_{2}\neq\ldots\neq i_{n}$, containing $\mathbf{1}$ (the trivial product). Let $a:=2p-\mathbf{1}$ and $b:=2q-\mathbf{1}$. Show that if $z\in(\tilde{R}\cup \mathcal{N})''\,(=(\tilde{R}\cup\{a\})'')$ and $\tau(z)=\tau(pz)=0$, then $z$ is the strong limit of a bounded sequence in the linear span of $\Xi(\tilde{R}^{0}, \{a\})\backslash \{\mathbf{1}, a\}$, where $\tilde{R}^{0}$ denotes the set of elements of $\tilde{R}$ with trace zero. Since the linear span of $\Xi(\tilde{R}^{0}, \{a\})$ is a dense $^*$-subalgebra of $(\tilde{R}\cup \mathcal{N})^{\prime},\ z$ is the strong limit of a bounded sequence $(z_{n})$ in span $\Xi(\tilde{R}^{0}, \{a\})$ by the Kaplansky density theorem (see [\citen{bib188}], II.4.8). Write $z_{n}=\alpha_{n}\mathbf{1}+\beta_{n}a+z_{n}^{'}$, where $ z_{n}'\in$ span $(\Xi(\tilde{R}^{0}, \{a\})\,\backslash\,\{\mathbf{1}, a\})$. From the freeness of $p$ and $\tilde{R}$ it is easily checked that $\tau(z')=\tau(pz')=0$ for all $z'\in\Xi(\tilde{R}^{0}, \{a\})\,\backslash\,\{\mathbf{1}, a\}$. Hence $\alpha_{n}=\tau(z_{n})\rightarrow 0$ and $\alpha_{n}+\beta_{n}=2\tau(pz_{n})\rightarrow 0$, implying $z_{n}'\rightarrow z$ strongly. Similarly, if $z\in(R\cup \mathcal{N})''$ and $\tau(z)=\tau(qz)=0$, then $z$ is the strong limit of a bounded sequence in the linear span of $\Xi(R^{0}, \{b\})\,\backslash\,\{\mathbf{1},b\}$.

\textit{Step} 4. Show that $x^{*}u$ is a Haar unitary in $p\mathcal{M}p$. Note that $\tau(x^{*})=0$ and $px^{*}=0$. So by Step 3, to prove that $\tau((x^{*}u)^{n})=0$ for $n\geq 1$, it suffices to show that $\tau(z_{1}uz_{2}u\cdots z_{n}u)=0$ for every $z_{i}\in\Xi(\tilde{R}^{0}, \{a\})\backslash \{\mathbf{1}, a\}$. But since $ua=u$ and $au=-u$, the array inside the trace is an alternating product from $\{a, u\}$ and $\tilde{R}^{0}$, so zero trace follows from freeness and $\tau(a)=\tau(u)=0$. Hence $x^{*}u$ is a Haar unitary in $p\mathcal{M}p$. Similarly, by using the second assertion in Step 3, it follows that $y^{*}v$ is a Haar unitary in $q\mathcal{M}q$, so $w^{*}y^{*}vw$ is one in $p\mathcal{M}p$.

\textit{Step} 5. Show that $(pRp,\,\{pqp\},\,\{x^{*}u\})$ is free in $p\mathcal{M}p$. Set
\begin{equation*}
g_{k}:=(pqp)^{k}-2\tau((pqp)^{k}) \qquad (k\geq 1).
\end{equation*}
Let $S_{1} :=\{z\in pRp:\tau(z)=0\},\,S_{2} :=\{g_{k}:k\geq 1\}$ and $S_{3}:=\{(x^{*}u)^{j} : j\in \mathbb{Z},\, j\neq 0\}$. We need to show that $\tau(a_{1}a_{2}\cdots a_{n})=0$ for every $a_{j}\ \in\ S_{i_{j}},\,i_{1}\neq i_{2}\neq\ldots\neq i_{n}$. It is clear that such $a_{1}a_{2}\cdots a_{n}$ can be written as an alternating product from
\begin{equation*}
\Phi :=\{u, u^{*}\}\cup\{z,\,uz,\,zu^{*},\,uzu^{*}:z\in pRp,\,\tau(z)=0\}
\end{equation*}
and
\begin{equation*}
\Psi :=\{x, x^{*}\}\cup\{g_{k},\,xg_{k},\,g_{k}x^{*},\,xg_{k}x^{*}:k\geq 1\}.
\end{equation*}
Here $\Psi\subset \mathcal{N}$. Since $px=0$ and $px^{*}=x^{*},\,\tau(z)=\tau(pz)=0$ for all $ z\in\Psi$. So by Step 3 we may show that an alternatling product from $\Phi$ and $\Xi(\tilde{R}^{0}, \{a\})\,\backslash\,\{\mathbf{1}, a\}$ has trace zero. Note that $\tau(uz)=\tau(zu^{*})=0, \tau(uzu^{*})=\tau(z)$ and $za=az=z$ for all $z\in pRp$. Also, note that $ua=u, au^{*}=u^{*}, au=-u$ and $u^{*}a=-u^{*}$. Therefore, an alternating product from $\Phi$ and $\Xi(\tilde{R}^{0}, \{a\})\,\backslash\,\{\mathbf{1}, a\}$ becomes an alternating product from $R^{0}$ and $\tilde{R}^{0}$, which has trace zero by freeness.

\textit{Step} 6. Show that
\begin{equation*}
(pRp,\,w^{*}q\tilde{R}qw,\,\{pqp\},\,\{x^{*}u\},\,\{w^{*}y^{*}vw\})
\end{equation*}
is free in $p\mathcal{M}p$. Set
\begin{equation*}
\tilde{\mathcal{N}}_{0}:=(pRp\cup\{pqp,\ x^{*}u\})''\,(\subset p\mathcal{M}p), \qquad \tilde{\mathcal{N}}_{1}:=w\tilde{\mathcal{N}}_{0}w^{*}\ (\subset q\mathcal{M}q).
\end{equation*}
Note that $\tilde{\mathcal{N}}_{1}\subset(R\cup \mathcal{N})''$. We may show that $(\tilde{\mathcal{N}}_{1},\,q\tilde{R}q, \{y^{*}v\})$ is free in $q\mathcal{M}q$. Let $\tilde{S}_{1} :=\{z\in\tilde{\mathcal{N}}_{1}:\tau(z)=0\},\,\tilde{S}_{2}:=\{z\in q\tilde{R}q:\tau(z)=0\}$ and $\tilde{S}_{3}:=\{(y^{*}v)^{j}:j\in \mathbb{Z},\,j\neq 0\}$. A product $a_{1}a_{2}\cdots a_{n}$ of $a_{j}\in\tilde{S}_{i_{j}},\,i_{1}\neq i_{2}\neq\ldots\neq i_{n}$, is written as an alternating product from
\begin{equation*}
\tilde{\Phi} :=\{v, v^{*}\}\cup\{z,\,vz,\,zv^{*},\,vzv^{*}:z\in q\tilde{R}q,\,\tau(z)=0\}
\end{equation*}
and
\begin{equation*}
\tilde{\Psi}:=\{y, y^{*}\}\cup\tilde{S}_{1}\cup y\tilde{S}_{1}\cup\tilde{S}_{1}y^{*}\cup y\tilde{S}_{1}y^{*}.
\end{equation*}
Since $qy=0$ and $qy^{*}=y^{*},\,\tau(z)=\tau(qz)=0$ for all $z\in\tilde{\Psi}$. Moreover, $\tilde{\Psi}\subset (R\cup \mathcal{N})''$. Hence it suffices by Step 3 to show that an alternating product from $\tilde{\Phi}$ and $\Xi(R^{0}, \{b\})\,\backslash\,\{\mathbf{1}, b\}$ has trace zero. As in Step 5, such a product is an alternating product from $\tilde{R}^{0}$ and $R^{0}$, so the conclusion is obtained.

\textit{Step} 7. Since the distribution of $pqp$ has no atoms by Lemma~\ref{ch07:lem7.2.5}, $\{pqp\}''$ is isomorphic to $\mathcal{L}(\mathbb{Z})$. Hence Steps 2, 4 and 6 imply by Proposition~\ref{ch07:pro7.1.1} that
\begin{equation*}
p(R\star\tilde{R})p\cong pRp\star q\tilde{R}q\star \mathcal{L}(\mathbf{F}_{3}),
\end{equation*}
so by Theorem~\ref{ch07:the7.2.4}
\begin{equation*}
p(R\star\tilde{R})p\cong pRp\star \mathcal{L}(\mathbf{F}_{4}),
\end{equation*}
where the isomorphism maps $pRp\ (\subset p(R\star\tilde{R})p)$ identically to itself.

\textit{Step} 8. Next, let $\mathcal{M} :=R\star \mathcal{L}(\mathbb{Z})$, and let $\tau$ be the free product trace on $\mathcal{M}$. Let $p, u\in R$ be as in Step 1, and let $q$ be a projection in $\mathcal{A}:=\mathcal{L}(\mathbb{Z})$ such that $\tau(q)=1/2$. Also, let $\mathcal{N}:=\{p, q\}'',\,x, y$ and $w$ be as in Step 1. It is seen as in Step 2 that $p\mathcal{M}p$ is generated by
\begin{equation*}
pRp\cup w^{*}q\mathcal{A}w\cup w^{*}y^{*}\mathcal{A}yw\cup\{pqp,\,x^{*}u\}.
\end{equation*}
The assertions in Step 3 hold in the present case where $\tilde{R}$ is replaced by $\mathcal{A}$. Then the proofs of Step 4 (for $x^{*}u$) and Step 5 are the same with $\mathcal{A}$ in place of $\tilde{R}$. Furthermore, it can be shown that
\begin{equation*}
\left(pRp, w^{*}q\mathcal{A}w,\ w^{*}y^{*}\mathcal{A}yw,\,\{pqp\},\,\{x^{*}u\}\right)
\end{equation*}
is free in $p\mathcal{M}p$. The proof here is a slight modification of Step 6, so the full details are left to the reader. Since $w^{*}q\mathcal{A}w$ and $w^{*}y^{*}\mathcal{A}yw$ are isomorphic to $\mathcal{L}(\mathbb{Z})$, we have
\begin{equation*}
p(R\star \mathcal{L}(\mathbb{Z}))p\cong pRp\star \mathcal{L}(\mathbf{F}_{4})
\end{equation*}
by an isomorphism mapping $pRp\ (\subset p(R\star \mathcal{L}(\mathbb{Z}))p)$ onto itself. Combining this with the isomorphism in Step 7, we have
\begin{equation*}
p(R\star\tilde{R})p\cong p(R\star \mathcal{L}(\mathbb{Z}))p,
\end{equation*}
and the result follows by tensoring with $M_{2}(\mathbb{C})$.
\end{proof2}

Now we are in a good position to introduce the \textit{interpolated free group factors}.\index{free!group factor, interpolated}\index{interpolated free group factor} The one-parameter family $\mathcal{L}(\mathbf{F}_{r})\ (1<r\leq\infty)$ of $\mathrm{II}_{1}$ factors possesses the following properties:
\begin{enumerate}
\item[(i)] When $r=n\in\{2,3,\ldots,\infty\},\,\mathcal{L}(\mathbf{F}_{r})$ is isomorphic to the free group factor $\mathcal{L}(\mathbf{F}_{n})$.

\item[(ii)] \textit{Compression formula}:\index{compression formula, for free group factors} For every $ 1<r\leq\infty$ and $ 0<t<\infty$,
\begin{equation*}
\mathcal{L}(\mathbf{F}_{r})_{t}\cong \mathcal{L}(\mathbf{F}_{1+(r-1)/t^{2}}).
\end{equation*}
\item[(iii)] \textit{Addition formula}:\index{addition formula, for free group factors} For every $1<r,  r'\leq\infty$,
\begin{equation*}
\mathcal{L}(\mathbf{F}_{r})\star \mathcal{L}(\mathbf{F}_{r'})\cong \mathcal{L}(\mathbf{F}_{r+r'})\,.
\end{equation*}
The properties (i) and (ii) may be used as a definition of $\mathcal{L}(\mathbf{F}_{r})$ for non-integer $r>1$, namely,
\begin{equation*}
\mathcal{L}(\mathbf{F}_{r})=\mathcal{L}(\mathbf{F}_{n})_{t} \quad \mathrm{with} \quad t=\left(\frac{n-1}{r-1}\right)^{1/2}.
\end{equation*}
\end{enumerate}
The formula (\ref{ch07:eqn7.2.1}) is a special case of (ii). (Needless to say, the notation $\mathbf{F}_{r}$ itself makes no sense.)

In the following we give two equivalent definitions of interpolated free group factors. The first is R\u{a}dulescu's definition and the second is due to Dykema. The two definitions are a bit different, but a semicircular system plays an essential role in both.

In $(\mathcal{M}, \tau)$ let $(a_{s})_{s\in S}$ be an infinite semicircular system, and let $R$ be a copy of the hyperfinite $\mathrm{II}_{1}$ factor such that $R$ and $\{a_{s}:s\in S\}$ are in free relation. (In the first definition below $R$ is of no use.)
\begin{enumerate}
\item[$1^{\circ}$] Let two distinct $\sigma_{0}, \sigma_{1}\in S$ be fixed, and let $h$ be a nonzero projection in $\{a_{\sigma_{1}}\}''$.  Let $p_{s},q_{s}\ (s\in S\, \backslash\,\{\sigma_{0}, \sigma_{1}\})$ be projections in $\{a_{\sigma_{1}}\}''$ such that $p_{s}=q_{s}$ or $p_{s}q_{s}=0$. Let
\begin{equation*}
r:=1+2\tau(h)\tau(\mathbf{1}-h)+\sum_{s\in S\backslash \{\sigma_{0},\sigma_{1}\}}k_{s}\tau(p_{s})\tau(q_{s}),
\end{equation*}
where $k_{s} :=1$ if $p_{s}=q_{s}$ and $k_{s} :=2$ if $p_{s}q_{s}=0$. Then $\mathcal{L}(\mathbf{F}_{r})$ is a type $\mathrm{II}_{1}$ factor isomorphic to
\begin{equation*}
(\{a_{\sigma_{1}},\ ha_{\sigma_{0}}(\mathbf{1}-h)\}\cup\{p_{s}a_{s}q_{s}:s\in S\, \backslash\,\{\sigma_{0}, \sigma_{1}\}\})''.
\end{equation*}
(Note that $\{a_{\sigma_{1}}, ha_{\sigma_{0}}(\mathbf{1}-h)\}''$ and also the above double commutant become factors.)

\item[$2^{\circ}$] Let $ 1<r\leq\infty$, and let the $p_{s}\ (s\in S)$ be projections in $R$ such that
\begin{equation*}
r=1+\sum_{s\in S}\tau(p_{s})^{2}.
\end{equation*}
Then $\mathcal{L}(\mathbf{F}_{r})$ is a type $\mathrm{II}_{1}$ factor isomorphic to
\begin{equation*}
(R\cup\{p_{s}a_{s}p_{s}:s\in S\})''\,.
\end{equation*}
\end{enumerate}

Our first nontrivial task is to show that the above definition of $\mathcal{L}(\mathbf{F}_{r})$ depends only on $r$, and is independent of the choice of the projections. From now on let us follow Dykema's approach.

\begin{lemma}
\label{ch07:lem7.2.7}
In the definition $2^{\circ}$ let $q_{s} \ (s\in S)$ be another choice of projections in $R$ such that $r=1+\sum_{s\in S}\tau\left(q_{s}\right)^{2}$. Then
\begin{equation*}
(R\cup\{p_{s}a_{s}p_{s}:s\in S\})''\cong(R\cup\{q_{s}a_{s}q_{s}:s\in S\})''
\end{equation*}
by an isomorphism mapping $R$ identically to itself
\end{lemma}

\begin{proof2}
We consider a certain standard form. Represent $R=M_{2}(\mathbb{C})\otimes M_{2}(\mathbb{C})\otimes\cdots$, and set $f_{0} :=1$ and $f_{k} :=e_{k}(0\ldots 1,0\ldots 1)$ for $k\in \mathbb{N}$ with the notation in the proof of Theorem~\ref{ch07:the7.2.4}. Then $(f_{k})_{k\geq 1}$ is an orthogonal sequence of projections with $\tau(f_{k})=2^{-k}$. When 1 $<r<\infty$, expand $r-1$ on the base 4: $r-1=  N_{0}+N_{1}4^{-1}+N_{2}4^{-2}+\cdots$, where $N_{0}\in\{0\}\cup \mathbb{N}$ and $N_{j}\in\{0,1,2,3\}$ for $j\geq 1$. When $ r=\infty$, let $ N_{0}=\infty$ and $N_{j}=0$ for $j\geq 1$. Choose mutually disjoint $S_{0}, S_{1}, \ldots\subset S$ such that $\# S_{j}=N_{j}\ (j\geq 0)$, and let $k(s)=j$ for $s\in S_{j}$. Obviously,
\begin{equation*}
r=1+\sum_{s\in\tilde{S}}\tau(f_{k(s)})^{2}, \quad \mathrm{where} \quad \tilde{S}:=\bigcup_{j=0}^{\infty}S_{j}\,.
\end{equation*}
Then it is enough to show that
\begin{equation*}
\mathcal{N}_{1}:=(R\cup\{p_{s}a_{s}p_{s}:s\in\hat{S}\})''\cong \mathcal{N}_{0}:=(R\cup\{f_{k(s)}a_{s}f_{k(s)}:s\in\tilde{S}\})'',
\end{equation*}
where $\hat{S} :=\{s\in S:p_{s}\neq 0\}$ (the above right-hand side may be considered as a standard form). Here and below, an isomorphism is always supposed to map $R$ identically to itself.

Note that if $u_{s} \in R \ (s\ \in\ S)$ are unitaries, then $(R, (\{u_{s}a_{s}u_{s}^{*}\})_{s\in S})$ is free again. This can be seen by a simple induction argument. Indeed, to see that $(R, \{u_{1}a_{s(1)}u_{1}^{*}\},\ldots,\{u_{n}a_{s(n)}u_{n}^{*}\})$ is free, we may show the freeness of $(R, \{a_{s(1)}\},\ \{u_{2}a_{s(2)}u_{2}^{*}\},\ldots,\{u_{n}a_{s(n)}u_{n}^{*}\})$ with $u_{k}$ for $u_{1}^{*}u_{k}$. But this reduces to the freeness of $(R,\ \{u_{2}a_{s(2)}u_{2}^{*}\},\ldots,\{u_{n}a_{s(n)}u_{n}^{*}\})$, because $a_{s(1)}$ is free from $R\,\cup\,\{a_{s(2)},\ldots,a_{s(n)}\}$. Thus we may assume that each $p_{s}$ is a (possibly infinite) sum of projections from $(f_{k})_{k\geq 0}$. So write $p_{s}=\sum_{k\in K_{s}}f_{k}$, where $K_{s}\subset \mathbb{N}$ if $p_{s}\neq 1$ and $K_{s}=\{0\}$ if $p_{s}=1$. Then we have
\begin{equation*}
\mathcal{N}_{1}=(R\cup\{f_{k}a_{s}f_{k'}:k, k'\in K_{s},\,s\in\hat{S}\})''\,.
\end{equation*}
The rest of the proof will be divided into three parts.

\textit{Step} 1. Show that
\begin{align}
& \mathcal{N}_{1}\cong \mathcal{N}_{2}:=(R\cup\{f_{k}a_{\beta(k,k,s)}f_k:k\in K_{s},\, s\in\hat{S}\}
\label{ch07:eqn7.2.5} \\
& \qquad \cup\{f_{k}a_{\beta(k,k',s)}f_{k'},\,f_{k'}a_{\beta(k,k',s)}f_k:k, k'\in K_{s},\, k'<k,\,s\in\hat{S}\})''\,, \notag
\end{align}
where $\beta : \{(k, k', s):k, k'\in K_{s}, k'\leq k,s\in\hat{S}\}\rightarrow S$ is an injection. First assume that $\hat{S}$ and all the $K_{s}$ are finite, and let $K :=\max\{k:k\in K_{s}, s\in\hat{S}\}$. For $n=2^{K+N}, 2^{K+N+1}, \ldots$ let $H(s, n)\ (s\in S)$ and $D(A, n)\ (A\in M_{2^{K+N}}(\mathbb{C}))$ be as in the proof of Lemma~\ref{ch07:lem7.2.2}. Then the distribution of
\begin{equation}
\left(M_{2^{K+N}}(\mathbb{C}), \ (f_{k}a_{s}f_{k})_{k\in K_{s},\,s\in\overline{S}},\ (f_{k}a_{s}f_{k'},\,f_{k'}a_{s}f_{k})_{k,k'\in K_{s},\,k'<k,\,s\in\hat{S}}\right)
\label{ch07:eqn7.2.6}
\end{equation}
(where $M_{2^{K+N}}(\mathbb{C})\subset R$) is the limit distribution of
\begin{align*}
& \left((D(A, n))_{A\in M_{2^{K+N}}(\mathbb{C})},\ \left(D(f_{k}, n)H(s, n)D(f_{k}, n)\right)_{k\in K_{s},\,s\in\hat{S}}\right. , \\
& \quad \left. (D(f_{k}, n)H(s, n)D(f_{k'}, n), D(f_{k'}, n)H(s, n)D(f_{k}, n))_{k,k'\in K_{s},\,k'<k,\,s\in \hat{S}}\right).
\end{align*}
Here, due to the orthogonality of $(f_{k})$, one can replace $H(s, n)$ against $D(f_{k}, n)$ and $D(f_{k'}, n)$ by $H(\beta(k, k', s), n)$ for each $k, k'\in K_{s},\ k'\leq k$. Therefore, the distribution of (\ref{ch07:eqn7.2.6}) is equal to that of
\begin{align*}
& \left(M_{2^{K+N}}(\mathbb{C}),\,(f_{k}a_{\beta(k,k,s)}f_{k})_{k\in K_{s},\,s\in\hat{S}}\right., \\
& \quad \left. (f_{k}a_{\beta(k,k',s)}f_{k'}, \ f_{k'}a_{\beta(k,k',s)}f_{k})_{k,k'\in K_{s},\,k'<k,\,s\in\hat{S}}\right),
\end{align*}
and this implies that (\ref{ch07:eqn7.2.5}) is satisfied when the $R$'s in $\mathcal{N}_{1}$ and in $\mathcal{N}_{2}$ are replaced by $M_{2^{K+N}}(\mathbb{C})$. The limit as $ N\rightarrow\infty$ gives the assertion in the case of finite $\hat{S}$ and $K_{s}$. The general case then follows by taking a suitable inductive limit.

\textit{Step} 2. Show that $\mathcal{N}_{2}\cong \mathcal{N}_{3} := (R\cup\{f_{l(s)}a_{s}f_{l(s)}:s\in T\})''$ for some $T\subset S$ and some $l(s)\in\{0\}\cup \mathbb{N}$ for $s\in T$. As in Step 1 we may show the assertion when $\hat{S}$ and all the $K_{s}$ are finite. For each $k'<k, f_{k'}$ is the sum of $2^{k-k'}$ orthogonal projections which are equivalent in $M_{2^{k}}(\mathbb{C})\ (\subset R)$ to $f_{k}$ (by permutation matrices), so one can write
\begin{equation*}
f_{k'}=\sum_{j=1}^{2^{k-k'}}\Pi_{kk'j}f_{k}\Pi_{kk'j}^{-1},
\end{equation*}
where the $\Pi_{kk'j}$ are permutation matrices in $M_{2^{k}}(\mathbb{C})$. Then the right-hand side of (\ref{ch07:eqn7.2.5}) is
\begin{align*}
& (R\cup\{f_{k}a_{\beta(k,k,s)}f_k:k\in K_{s},\,s\in\hat{S}\} \\
& \quad \cup\{f_{k}a_{\beta(k,k',s)}\Pi_{kk'j}f_{k},\,f_{k}\Pi_{kk'j}^{-1}a_{\beta(k,k',s)}f_{k}: \\
& \qquad \qquad \qquad \qquad \qquad k, k'\in K_{s},\ k'<k,\,1\leq j\leq 2^{k-k'},\, s\in\hat{S}\})'' \\
& =(R\cup\{f_{k}a_{\beta(k,k,s)}f_{k}:k\in K_{s},\ s\in\hat{S}\} \\
& \qquad \cup\{f_{k}a_{\beta(k,k',s),j}^{(1)}f_{k},\,f_{k}a_{\beta(k,k',s),j}^{(2)}f_{k}: \\
& \qquad \qquad \qquad \qquad \qquad k, k'\in K_{s}, k'<k,\,1\leq j\leq 2^{k-k'},\, s\in\hat{S}\})'',
\end{align*}
where
\begin{align*}
a_{\beta(k,k',s),j}^{(1)} & \ := \ \frac{a_{\beta(k,k',s)}\Pi_{kk'j}+\Pi_{kk'j}^{-1}a_{\beta(k,k',s)}}{\sqrt{2}}\,, \\
a_{\beta(k,k',s),j}^{(2)} & \ := \ \frac{a_{\beta(k,k',s)}\Pi_{kk'j}-\Pi_{kk'j}^{-1}a_{\beta(k,k',s)}}{\sqrt{2}\ \mathrm{i}}\, .
\end{align*}
The distribution of $(f_{k}a_{\beta(k,k',\,s),j}^{(i)}f_{k})_{k,k'\in K_{s},\,k'<k,\,1\leq j\leq 2^{k-k'}, i=1,2,\,s\in\hat{S}}$ is the limit distribution of $(D(f_{k}, n)H_{kk'sj}^{(i)}(n)D(f_{k}, n))_{k,k'\in K_{s},k'<k,\ 1\leq j\leq 2^{k-k'},\,i=1,2,\, s\in\hat{S}}$, where
\begin{align*}
H_{kk'\mathrm{s}j}^{(1)}(n) & :=\frac{H(\beta(k,k',s),n)D(\Pi_{kk'j},n)+D(\Pi_{kk'j}^{-1},n)H(\beta(k,k',s),n)}{\sqrt{2}},\\ H_{kk'sj}^{(2)}(n)& :=\frac{H(\beta(k,k',s),n)D(\Pi_{kk'j},n)-D(\Pi_{kk'j}^{-1},n)H(\beta(k,k',s),n)}{\sqrt{2}\ \mathrm{i}}.
\end{align*}
Since $D(f_{k}, n)H_{kk'\,sj}^{(i)}(n)D(f_{k}, n)\ (i=1,2)$ are regarded as submatrices of the real and imaginary parts of a standard non-selfadjoint Gaussian matrix, one can replace $H_{kk'\,sj}^{(i)}(n)$ by $H(t_{kk'\,sj}^{(i)}, n)$, where $t_{kk'\,sj}^{(1)}, t_{kk'\,sj}^{(2)}\in S\ (k, k'\in K_{s}, k'<k,\,1\leq j\leq 2^{k-k'}, s\in\hat{S})$ are all distinct. Now the proof proceeds as in Step 1. Note that the value $r$ is preserved in the procedure of Steps 1 and 2:
\begin{equation*}
r=1+\sum_{s\in T}\tau(f_{l(s)})^{2}.
\end{equation*}

\textit{Step} 3. Finally, show that $\mathcal{N}_{3}\cong \mathcal{N}_{0}$. One can apply the random matrix model as in Step 2 to get
\begin{equation}
(R\cup\{f_{k}a_{s_{i}}f_k:1\leq i\leq 4\})''\cong(R\cup\{f_{k-1}a_{s}f_{k-1}\})'',
\label{ch07:eqn7.2.7}
\end{equation}
whenever $s_{1},\ldots,s_{4}\in S$ are distinct, $s\in S$ and $k\geq 1$. When $T$ is finite, $\mathcal{N}_{3}\cong \mathcal{N}_{0}$ follows from a finite number of applications of (\ref{ch07:eqn7.2.7}). Assume that $T$ is infinite. If $ r<\infty$ and $r$ is not rational on the basis 4, then there exists an increasing sequence $T_{m}\ (m\in \mathbb{N})$ of finite subsets of $T$ such that
\begin{equation*}
\sum_{s\in T_{m}}4^{-l(s)}=\sum_{j=0}^{m}N_{j}4^{-j}, \quad \bigcup_{m=1}^{\infty}T_{m}=T \,.
\end{equation*}
By (\ref{ch07:eqn7.2.7}) we have a compatible sequence of isomorphisms
\begin{equation*}
(R\cup\{f_{l(s)}a_{s}f_{l(s)}:s\in T_{m}\})''\cong\left(R\cup\left\{f_{k{(s)}}a_{s}f_{k(s)}:s\in\bigcup_{j=0}^{m}S_{j}\right\}\right)''.
\end{equation*}
Taking the inductive limit gives $\mathcal{N}_{3}\cong \mathcal{N}_{0}$. If $r$ is rational on the base 4 and $m_{0}:=\max\{j:N_{j}\neq 0\}$, then there exists an increasing sequence $T_{m}\ (m>m_{0})$ of subsets of $T$ such that
\begin{equation*}
\sum_{s\in T_{m}}4^{-l(s)}=\sum_{j=0}^{m_{0}}N_{j}4^{-j}-4^{-m}, \quad \bigcup_{m>m_{0}}T_{m}=T\,.
\end{equation*}
Choose $\sigma\in S_{m_{0}}$ (so $k(\sigma)=m_{0}$). Application of (\ref{ch07:eqn7.2.7}) gives
\begin{align}
& (R\cup\{f_{l(s)}a_{s}f_{l(s)}:s\in T_{m}\})'' \label{ch07:eqn7.2.8} \\
& \cong(R\cup\{f_{k(s)}a_{s}f_{k(s)}:s\in\tilde{S}\,\backslash\,\{\sigma\}\}\cup\{f_{j}a_{t_{ji}}f_{j}:m_{0}<j\leq m,\ 1\leq i\leq 3\})''\,, \notag
\end{align}
where the $t_{ji}\in S\,\backslash\,\tilde{S} \ (j>m_{0},\,1\leq i\leq 3)$ are all distinct. Moreover, similarly to the arguments in Steps 1 and 2 we can see that the right-hand side of (\ref{ch07:eqn7.2.8}) is isomorphic to
\begin{equation*}
(R\ \cup\{f_{k(s)}a_{s}f_{k(s)}:s\in\tilde{S}\,\backslash\,\{\sigma\}\}\cup\{f_{m_{0}}a_{\sigma}f_{m_{0}}-g_{m}a_{\sigma}g_{m}\})'' \ (\subset \mathcal{N}_{0}),
\end{equation*}
where $g_{m}\in R$ are projections such that $g_{m}\leq f_{m_{0}}$ and $\tau(g_{m})=2^{-m}$. Then $\mathcal{N}_{3}\cong \mathcal{N}_{0}$ follows by taking an inductive limit. If $ r=\infty$ then, since $\sum_{s\in T_{m}}4^{-l(s)}\geq 1$ for disjoint finite subsets $T_{m}$ of $T\ (m\in \mathbb{N})$, the situation can be transformed to the case $\# T(0) =\infty$ by an isomorphism using (\ref{ch07:eqn7.2.7}) infinitely many times, where $T(k) :=\{s\in T:l(s)=k\}$. This can be further transformed to the case $\# T(k)=\infty$ for all $k\geq 0$, and finally to the case $\# T(0) =\infty$ and $\# T(k)=0$ for all $k\geq 1$, so $\mathcal{N}_{3}\cong \mathcal{N}_{0}=\mathcal{L}(\mathbf{F}_{\infty})$.
\end{proof2}

Now property (i) is shown as follows. When $r\in\{2,3,\ldots,\infty\}$ and $n=r-1$, we may choose a semicircular system $(a_{1},\ldots,a_{n})$ and $p_{i}=1\ (1\leq i\leq n)$ in the definition $2^{\circ}$. Then by Proposition~\ref{ch07:pro7.1.1} and Theorem~\ref{ch07:the7.2.4} we have
\begin{equation*}
\mathcal{L}(\mathbf{F}_{r})=R\star\{a_{i}:1\leq i\leq n\}''=R\star \mathcal{L}(\mathbf{F}_{n})\cong \mathcal{L}(\mathbf{F}_{n+1}).
\end{equation*}
Property (ii) and the fact that $\mathcal{L}(\mathbf{F}_{r})$ is a type $\mathrm{II}_{1}$ factor are proved as follows.

\begin{theorem}
\label{ch07:the7.2.8}
For every $ 1<r\leq\infty,\ \mathcal{L}(\mathbf{F}_{r})$ is a type $II_{1}$ factor and
\begin{equation*}
\mathcal{L}(\mathbf{F}_{r})_{t}\cong \mathcal{L}(\mathbf{F}_{1+(r-1)/t^{2}}) \qquad (0<t<\infty).
\end{equation*}
\end{theorem}

\begin{proof2}
Let $\mathcal{L}(\mathbf{F}_{r})=\left(R\cup\{p_{s}a_{s}p_{s}:s\in S\}\right)''$ as in the definition $2^{\mathrm{o}}$ with $r= 1+\sum_{s\in S}\tau(p_{s})^{2}$. Let $p\in R$ be a nonzero projection with $\tau(p)=t$. By Lemma~\ref{ch07:lem7.2.7} we may assume that $p_{s}\leq p$ for all $s\in S$. Since
\begin{align*}
p\mathcal{L}(\mathbf{F}_{r})p  & \ = \ \left(pRp\cup\{p_{s}a_{s}p_{s}:s\in S\}\right)'' \\
& = \ \left(pRp\cup\{p_{s}(\tau(p)^{-1/2}pa_{s}p)p_{s}:s\in S\}\right)'',
\end{align*}
we have, thanks to Lemma~\ref{ch07:lem7.2.2},
\begin{equation*}
p\mathcal{L}(\mathbf{F}_{r})p\cong \mathcal{L}(\mathbf{F}_{r'}),
\end{equation*}
where
\begin{equation*}
r'=1+\sum_{s\in S}\left(\frac{\tau(p_{s})}{\tau(p)}\right)^{2}=1+\frac{r-1}{t^{2}}\,.
\end{equation*}
Let $t=((r-1)/(n-1))^{1/2}$ in the above, where $r\leq n\in \mathbb{N}$. Then, by property (i), $p\mathcal{L}(\mathbf{F}_{r})p$ is a factor for any projection $p\in R$ with $\tau(p)=t$. This means that $\mathcal{L}(\mathbf{F}_{r})$ is a factor, and the compression formula holds for $0<t\leq 1$. The case $t>1$ follows from $\mathcal{L}(\mathbf{F}_{r})\cong \mathcal{L}(\mathbf{F}_{1+(r-1)/t^{2}})_{1/t}$ and (\ref{ch07:eqn7.1.1}).
\end{proof2}

Property (iii) is shown as follows.

\begin{theorem}
\label{ch07:the7.2.9}
For every $1<r,  r'\leq\infty$,
\begin{equation*}
\mathcal{L}(\mathbf{F}_{r})\star \mathcal{L}(\mathbf{F}_{r'})\cong \mathcal{L}(\mathbf{F}_{r+r'}).
\end{equation*}
\end{theorem}

\begin{proof2}
In $(\mathcal{M}, \tau)$ let $(a_{s})_{s\in S\cup T}$ be a semicircular system where $S, T$ are disjoint sets, and let $R,\tilde{R}$ be copies of the hyperfinite $\mathrm{II}_{1}$ factors such that $R,\,\tilde{R}$ and $\{a_{s}:s\in S\cup T\}$ are in free relation. Write
\begin{align*}
\mathcal{L}(\mathbf{F}_{r}) & \cong\left(R\cup\{p_{s}a_{s}p_{s}\}_{s\in S}\right)'', \\
\mathcal{L}(\mathbf{F}_{r'})& \cong\left(\tilde{R}\cup\{\tilde{p}_{t}a_{t}\tilde{p}_{t}\}_{t\in T}\right)'',
\end{align*}
where $p_{s}\in R\ (s\in S)$ and $\tilde{p}_{t}\in\tilde{R}\ (t\in T)$ are projections such that $r=1+ \sum_{s\in S}\tau(p_{s})^{2}$ and $r'=1+\sum_{t\in T}\tau(\tilde{p}_{t})^{2}$. Since $R\cup\{p_{s}a_{s}p_{s}:s\in S\}$ and $\tilde{R}\cup\{\tilde{p}_{t}a_{t}\tilde{p}_{t}: t\in T\}$ are in free relation, we have
\begin{equation*}
\mathcal{L}(\mathbf{F}_{r})\star \mathcal{L}(\mathbf{F}_{r'})\cong(R\cup\tilde{R}\cup\{p_{s}a_{s}p_{s} :s\in S\}\cup\{\tilde{p}_{t}a_{t}\tilde{p}_{t}:t\in T\})''\,.
\end{equation*}
Since $(R\cup\tilde{R})''\cong R\star\tilde{R}\cong R\star \mathcal{L}(\mathbb{Z})$ by Theorem~\ref{ch07:the7.2.6}, we can choose a semicircular element $a$ in $(R\cup\tilde{R})''$ such that $R$ and $a$ are in free relation and $R\cup\{a\}$ generates $(R\cup\tilde{R})''$. Furthermore, choose unitaries $u_{t}\in(R\cup\tilde{R})''\ (t\in T)$ such that $q_{t} := u_{t}\tilde{p}_{t}u_t^*\in R$. Then
\begin{align}
& \left(R\cup\tilde{R}\cup\{p_{s}a_{s}p_{s} :s\in S\}\cup\{\tilde{p}_{t}a_{t} \tilde{p}_{t}:t\in T\}\right)'' \notag \\
& \qquad = \ \left(R\cup\{a\}\cup\{p_{s}a_{s}p_{s}\ :s\in S\}\cup\{q_{t}u_{t}a_{t}u_{t}^{*}q_{t}\ :t\in T\}\right)''
\label{ch07:eqn7.2.9}
\end{align}
and $(a, a_{s}, u_{t}a_{t}u_{t}^{*})_{s\in S,\,t\in T}$ is a semicircular system free from $R$. Since
\begin{equation*}
r+r'=1+1+\sum_{s\in S}\tau(p_{s})^{2}+\sum_{t\in T}\tau(q_{t})^{2}\,,
\end{equation*}
it follows that $\mathcal{L}(\mathbf{F}_{r+r'})$ is given as (\ref{ch07:eqn7.2.9}), and the addition formula is proved.
\end{proof2}

The properties (ii) and (iii) supply important knowledge concerning ismorphism between the (interpolated) free group factors. The following is a direct consequence of (ii) in the case of $ r=\infty$.

\begin{corollary}
\label{ch07:cor7.2.10}
The fundamental group of $\mathcal{L}(\mathbf{F}_{\infty})$ is $(0, \infty)$.
\end{corollary}

\begin{corollary}
\label{ch07:cor7.2.11} $\mathcal{L}(\mathbf{F}_{r})\ (1<r<\infty)$ are all stably isomorphic, that is,
\begin{equation*}
\mathcal{L}(\mathbf{F}_{r})\otimes B(\mathcal{H})\cong \mathcal{L}(\mathbf{F}_{s})\otimes B(\mathcal{H})
\end{equation*}
for every $1<r,  s<\infty$, where $B(\mathcal{H})$ is of type $I_{\infty}$.
\end{corollary}

\begin{proof2}
Note that $\mathcal{M}_{t}\otimes B(\mathcal{H})\cong\mathcal{M}\otimes B(\mathcal{H})$ for any $\mathrm{II}_{1}$ factor $\mathcal{M}$ and any $t>0$. So the corollary is obvious from property (ii).
\end{proof2}

\begin{corollary}
\label{ch07:cor7.2.12}
For every $1<r,  s\leq\infty$ the isomorphism class of $\mathcal{L}(\mathbf{F}_{r})\otimes \mathcal{L}(\mathbf{F}_{s})$ depends only on $(r-1)(s-1)$. In particular,
\begin{equation*}
\mathcal{L}(\mathbf{F}_{r})\otimes \mathcal{L}(\mathbf{F}_{\infty})\cong \mathcal{L}(\mathbf{F}_{s})\otimes \mathcal{L}(\mathbf{F}_{\infty})
\end{equation*}
for every $r, s>1$.
\end{corollary}

\begin{proof2}
Note that if $\mathcal{M}$ and $\mathcal{N}$ are general $\mathrm{II}_{1}$ factors, then $\mathcal{M}_{t}\otimes \mathcal{N}_{1/t}\cong\mathcal{M} \otimes \mathcal{N}$ for any $t>0$. This can be easily seen by taking a tensor product of projections $p\in \mathcal{M}$ and $q\in  \mathcal{N}\otimes B(\mathcal{H})$ such that $\tau_{\mathcal{M}}(p)=t(<1)$ and $(\tau \mathcal{N}\otimes\ \mathrm{Tr})(q)=1/t$. Hence the proof is a simple application of (ii).
\end{proof2}

\begin{corollary}
\label{ch07:cor7.2.13}
One $($and only one$)$ of the following two statements is true:
\begin{enumerate}
\item[(1)] $\mathcal{L}(\mathbf{F}_{r})\ (1<r<\infty)$ are all isomorphic, and the fundamental group of $\mathcal{L}(\mathbf{F}_{r})$ is $(0, \infty)$ for any $ 1<r<\infty$.

\item[(2)] $\mathcal{L}(\mathbf{F}_{r})\ (1<r<\infty)$ are mutually non-isomorphic, and the fundamental group of $\mathcal{L}(\mathbf{F}_{r})$ is $\{1\}$ for any $1<r<\infty$.
\end{enumerate}
\end{corollary}

\begin{proof2}
Note that, by property (ii), if $\mathcal{L}(\mathbf{F}_{r})\cong \mathcal{L}(\mathbf{F}_{r'})$ for some $r\neq r'$, then the fundamental group of $\mathcal{L}(\mathbf{F}_{r})$ is non-trivial. So, suppose that the fundamental group of $\mathcal{L}(\mathbf{F}_{r})$ contains $\alpha\neq 1$ for some $ 1<r<\infty$. Choose $1<s,  s'<\infty$ such that $((s-1)/(s'-1))^{1/2}=\alpha$. Since (ii) implies that
\begin{equation*}
\mathcal{L}(\mathbf{F}_{s})\cong \mathcal{L}(\mathbf{F}_{r})_{\beta}\cong \mathcal{L}(\mathbf{F}_{r})_{\alpha\beta}\cong \mathcal{L}(\mathbf{F}_{s'}), \end{equation*}
where $\beta=((r-1)/(s-1))^{1/2}$, we have $\mathcal{L}(\mathbf{F}_{s+t})\cong \mathcal{L}(\mathbf{F}_{s'+t})$ for all $t>1$ by property $(\mathrm{iii})$ . One can choose $s, s'$ arbitrarily near 1 and conclude that the statement (1) holds true.
\end{proof2}

Moreover, it is known that if (1) holds true in the above, then we also have $\mathcal{L}(\mathbf{F}_{r})\cong \mathcal{L}(\mathbf{F}_{\infty})$ for every $r>1$.

Concerning the long-standing isomorphism problem of free group factors, the last corollary says that we have one of two extreme cases, according to whether the fundamental group is trivial or not. The problem is still open.

\section{Free entropy dimension}
\label{ch07:sec7.3}

In this section let $(\mathcal{M}, \tau)$ be a tracial $W^{*}$-probability space as before. Based on the free entropy studied in the previous chapter, Voiculescu further introduced the notion of free entropy dimension\index{free entropy!dimension} for $N$-tuples of selfadjoint elements of $\mathcal{M}$. This dimension $\delta(a_{1},\ldots,a_{N})$ is defined as a certain kind of differential of the free entropy $\chi(a_{1},\ldots,a_{N})$ when perturbed by a semicircular system. It will turn out that the value of $\delta(a_{1},\ldots,a_{N})$ is closely related to the freeness of $a_{1},\ldots,a_{N}$, and so is the additivity of $\chi(a_{1},\ldots,a_{N})$.

Let $a_{1},\ldots,a_{N}\in\mathcal{M}^{sa}$, and let $(S_{1},\ldots,S_{N})$ be a semicircular system in $\mathcal{M}$ such that $\{a_{1},\ldots,a_{N}\}$ and $\{S_{1},\ldots,S_{N}\}$ are in free relation. Then the \textit{free entropy dimension} $\delta(a_{1},\ldots,a_{N})$ is defined by
\begin{equation*}
\delta(a_{1},\ldots,a_{N}):=N+\limsup_{\varepsilon\rightarrow+0}\frac{\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N})}{|\log\varepsilon|}\,.
\end{equation*}
(The above $S_{1},\ldots,S_{N}$ always exist when we enlarge $\mathcal{M}$ by taking a free product with another von Neumann algebra.) Note that the joint distribution of $a_{1}+ \varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}$ is independent of the choice of $S_{1},\ldots,S_{N}$, and so $\delta(a_{1},\ldots,a_{N})$ is uniquely determined.

The following is obvious from the definition and Proposition~\ref{ch06:pro6.1.3}.

\begin{proposition}
\label{ch07:pro7.3.1}
For every $1\leq L<N$,
\begin{equation*}
\delta(a_{1},\ldots,a_{N})\leq\delta(a_{1},\ldots,a_{L})+\delta(a_{L+1},\ldots,a_{N}).
\end{equation*}
\end{proposition}
For the case of a single $a\in\mathcal{M}^{sa}$ we can exactly compute $\delta(a)$ as follows.


\begin{theorem}
\label{ch07:the7.3.2}
Let $a, S\in\mathcal{M}^{sa}$, where $S$ is standard semicircular and free from $a$. If $\mu$ is the distribution of $a$, then
\begin{equation*}
\lim_{\varepsilon\rightarrow+0}\frac{\chi(a+\varepsilon S)}{|\log\varepsilon|}=-\sum_{t\in \mathbb{R}}\mu(\{t\})^{2}=-(\mu\otimes\mu)(\Delta),
\end{equation*}
and hence
\begin{equation*}
\delta(a)=1-\sum_{t\in \mathbb{R}}\mu(\{t\})^{2},
\end{equation*}
where $\Delta :=\{(s, t)\in \mathbb{R}^{2}:s=t\}$.
\end{theorem}

Before proving the theorem let us recall some monotonicity (or subordination) properties of additive free convolution without proof. Let $\mu_{1}, \mu_{2}$ be compactly supported probability measures on $\mathbb{R}$ and $\mu_{3} :=\mu_{1}  \boxplus \mu_{2}$. If $\mu_{1}$ has density $f_{1}= d\mu_{1}/dt$ belonging to $L^{p}(\mathbb{R})$ for some $ 1<p\leq\infty$, then $\mu_{3}$ has density $f_{3}$ satisfying
\begin{equation}
\Vert f_{3}\Vert_{p}\leq\Vert f_{1}\Vert_{p}\,.
\label{ch07:eqn7.3.1}
\end{equation}
The proof is based on an analytic subordination principle in the upper half-plane $\mathbb{C}^{+}$, which says that if $F(x+\mathrm{i}\,y)$ is a harmonic function on $\mathbb{C}^{+}$ and $\omega: \mathbb{C}^{+}\rightarrow \mathbb{C}^{+}$ is an analytic function satisfying $\lim_{z\rightarrow\infty}|\omega(z)|=\infty$ and $\mathrm{Im}\, \omega(z)\geq\mathrm{Im}\,z$, then
\begin{equation*}
\int|F(\omega(x+\mathrm{i}\,y))|^{p}\,dx\leq\int|F(x+\mathrm{i}\,y)|^{p}\,dx
\end{equation*}
for every $ 1\leq p\leq\infty$ and every $y>0$. This is an analogue of the known subordination principle in the disk ([\citen{bib60}], Chap.~\ref{ch06:chap06}). In addition, although it will not be needed here, it is noteworthy that the following monotonicity can be proved by applying the above subordination principle:
\begin{equation}
\Sigma (\mu_{1} \boxplus \mu_{2}) \geq\Sigma(\mu_{1})
\label{ch07:eqn7.3.2}
\end{equation}
for any compactly supported probability measures $\mu_{1}, \mu_{2}$ on $\mathbb{R}$.

\begin{proof2}[Proof of Theorem~\ref{ch07:the7.3.2}]
First, note that the limit in question does not change when $S$ is replaced by $tS\ (t>0)$ . So in the proof below we use $S$ having the distribution $w_{1}$ (instead of $w_{2}$) for notational convenience. For $\varepsilon >0$ let $\mu_{\varepsilon}$ be the distribution of $a+\varepsilon S$. By (\ref{ch06:eqn6.1.2}) we need to show that
\begin{equation*}
\lim_{\varepsilon\rightarrow+0}\frac{\Sigma(\mu_{\varepsilon})}{|\log\varepsilon|}=-(\mu\otimes\mu)(\Delta).
\end{equation*}
It suffices to prove the following two estimtes:
\begin{align}
& \lim_{\varepsilon \rightarrow+0}\frac{1}{|\log\varepsilon|}\left|\Sigma(\mu_{\varepsilon})-\iint\log|s-t+\mathrm{i}\ \varepsilon|\ d\mu_{\varepsilon}(s)\,d\mu_{\varepsilon}(t)\right|=0, \label{ch07:eqn7.3.3} \\
& \lim_{\varepsilon \rightarrow+0}\frac{1}{|\log\varepsilon|}\iint\log|s-t+\mathrm{i}\, \varepsilon|\,d\mu_{\varepsilon}(s)\,d\mu_{\varepsilon}(t)=-(\mu\otimes\mu)(\Delta). \label{ch07:eqn7.3.4}
\end{align}
By the monotonicity property mentioned above, $\mu_{\varepsilon}$ has the density $f_{\varepsilon}=d\mu_{\varepsilon}/dt$ and
\begin{align*}
\Vert f_{\varepsilon}\Vert_{2}\leq\Vert w_{\varepsilon}\Vert_{2} & \ =\ \frac{2}{\pi\varepsilon^{2}}\left[\int_{-\varepsilon}^{\varepsilon}(\varepsilon^{2}-t^{2})\,dt\right]^{1/2} \\
& \ =\ \frac{2}{\pi\varepsilon^{1/2}}\left[\int_{-1}^{1}(1-t^{2})\, dt\right]^{1/2}=\mathrm{const}\cdot\varepsilon^{-1/2}.
\end{align*}
Put $g_{\varepsilon}(x) :=\log|1+\mathrm{i}\,\varepsilon/x|$. Then
\begin{align*}
\Vert g_{\varepsilon}\Vert_{2} & = \left[2\int_{0}^{\infty}\left(\log\left(1+\frac{\varepsilon^{2}}{x^{2}}\right)^{1/2}\right)^{2}dx\right]^{1/2} \\
& = \ \left[\frac{\varepsilon}{2}\int_{0}^{\infty}\left(\log\left(1+\frac{1}{x^{2}}\right)\right)^{2}dx\right]^{1/2}=\mathrm{const}\cdot\varepsilon^{1/2}.
\end{align*}
Hence we have
\begin{align*}
0 & \ \leq \ \iint\log|s-t+\mathrm{i}\,\varepsilon|\,d\mu_{\varepsilon}(s)\, d\mu_{\varepsilon}(t)-\Sigma(\mu_{\varepsilon}) \\
& \ = \ \iint f_{\varepsilon}(s)f_{\varepsilon}(t)\log\left|1+\mathrm{i}\, \frac{\varepsilon}{s-t}\right|\,ds\,dt \\
& \ \leq \ \int f_{\varepsilon}(s)\Vert f_{\varepsilon}\Vert_{2}\Vert g_{\varepsilon}\Vert_{2}\,ds\leq  \ \mathrm{const}.
\end{align*}
This implies (\ref{ch07:eqn7.3.3}).

Next, let us prove (\ref{ch07:eqn7.3.4}). Concerning the spectral measures $e_{a}$ and $e_{a+\varepsilon S}$ of $a$ and $a+\varepsilon S$, since $\Vert S\Vert=1$, we get
\begin{align*}
e_{a}((-\infty, t-\varepsilon))\wedge e_{a+\varepsilon S}([t, \infty))& =0,\\
e_{a+\varepsilon S}((-\infty, t))\wedge e_{a}([t+\varepsilon, \infty)) & =0,
\end{align*}
which imply that $\tau\left(e_{a}\left(\left(-\infty, t-\varepsilon\right)\right)\right)\leq\tau\left(e_{a+\varepsilon S}\left(\left(-\infty, t\right)\right)\right)\leq\tau\left(e_{a}\left(\left(-\infty, t+\varepsilon\right)\right)\right)$, that is, $\mu\left(\left(-\infty, t-\varepsilon\right)\right)\leq\mu_{\varepsilon}\left(\left(-\infty, t\right)\right)\leq\mu\left(\left(-\infty, t+\varepsilon\right)\right)$, and hence, for any $s<t$,
\begin{equation*}
\mu_{\varepsilon}((s, t))\leq\mu((s-\varepsilon, t+\varepsilon)), \quad \mu((s, t))\leq\mu_{\varepsilon}((s-\varepsilon,\,t+\varepsilon)).
\end{equation*}
For $0\leq s\leq t$ write
\begin{equation*}
\Delta(s, t):=\{(x, y)\in \mathbb{R}^{2}:s\leq|x-y|<t\}.
\end{equation*}
Then for any $r>0$ we can estimate
\begin{align*}
& (\mu_{\varepsilon}\otimes\mu_{\varepsilon})(\Delta(0, r)) \\
& \qquad \leq(\mu_{\varepsilon}\otimes\mu_{\varepsilon})\left(\bigcup_{n\in \mathbb{Z}}(n\varepsilon-r, (n+1)\varepsilon+r)\times\left[n\varepsilon, (n+1)\varepsilon)\right.\right) \\
& \qquad \leq(\mu_{\varepsilon} \otimes\mu_{\varepsilon}) \left(\bigcup_{n\in \mathbb{Z}}(n\varepsilon - (r+\varepsilon), (n+1)\varepsilon + (r + \varepsilon)) \times \left[n \varepsilon,  (n + 1) \varepsilon)\right. \right)\\
& \qquad \leq(\mu\otimes\mu_{\varepsilon})(\Delta(0, r+2\varepsilon)) \\
& \qquad \leq \ (\mu\otimes\mu_{\varepsilon})\left(\bigcup_{n\varepsilon \mathbb{Z}}[n\varepsilon, (n+1)\varepsilon)\times(n\varepsilon  -(r+2\varepsilon), (n+1)\varepsilon+(r+2\varepsilon))\right) \\
& \qquad \leq(\mu\otimes\mu)\left(\bigcup_{n\in \mathbb{Z}}[n\varepsilon, (n+1)\varepsilon)\times(n\varepsilon -(r+3\varepsilon), (n+1)\varepsilon+(r+3\varepsilon))\right) \\
& \qquad \leq(\mu\otimes\mu)(\Delta(0, r+4\varepsilon)),
\end{align*}
and similarly
\begin{equation*}
(\mu\otimes\mu)(\Delta(0, r))\leq(\mu_{\varepsilon}\otimes\mu_{\varepsilon})(\Delta(0,\ r+4\varepsilon)).
\end{equation*}

Now, for any $0<\delta<1$, let $ 0<\varepsilon <1$ be such that $5\varepsilon <\varepsilon^{\delta}$, and let $R:=\Vert a\Vert+1$ be such that $\mathrm{supp}\, \mu_{\varepsilon}\subset[-R, R]$. If $s, t\in[-R, R]$ and $|s-t|\geq\varepsilon^{\delta}$, then
\begin{equation*}
\delta\log\varepsilon \leq\log|s-t+\mathrm{i}\,\varepsilon|\leq\log(2R+\varepsilon).
\end{equation*}
Hence we have
\begin{equation*}
\left|\iint_{|s-t|\geq\varepsilon^{\delta}}\log|s-t+\mathrm{i}\,\varepsilon|\, d\mu_{\varepsilon}(s)\, d\mu_{\varepsilon}(t)\right|\leq\log(2R+\varepsilon)+\delta|\log\varepsilon|,
\end{equation*}
so that
\begin{equation}
\limsup_{\varepsilon\rightarrow+0}\frac{1}{|\log\varepsilon|} \left|\iint_{|s-t|\geq\varepsilon^{\delta}}\log|s-t+\mathrm{i}\,\varepsilon|\, d\mu_{\varepsilon}(s)\,d\mu_{\varepsilon}(t)\right|\leq\delta.
\label{ch07:eqn7.3.5}
\end{equation}
Since
\begin{equation*}
\log\varepsilon \leq\log|s-t+\mathrm{i}\, \varepsilon|\leq\log(1+\varepsilon)\leq-\log\varepsilon \quad \mathrm{if} \quad |s-t|<1,
\end{equation*}
we have
\begin{align*}
& \frac{1}{|\log\varepsilon|}\left|\iint_{\Delta(5\varepsilon,\varepsilon^{\delta})} \log|s-t+\mathrm{i}\,\varepsilon|\,d\mu_{\varepsilon}(s)\,d\mu_{\varepsilon}(t)\right| \\
& \qquad \leq(\mu_{\varepsilon}\otimes\mu_{\varepsilon})(\Delta(5\varepsilon, \varepsilon^{\delta}))\leq(\mu\otimes\mu)(\Delta(\varepsilon, \varepsilon^{\delta}+4\varepsilon)),
\end{align*}
so that
\begin{equation}
\lim_{\varepsilon\rightarrow+0}\frac{1}{|\log\varepsilon|}\left|\iint_{\Delta(5\varepsilon,\varepsilon^{\delta})}\log|s-t+\mathrm{i}\, \varepsilon|\,d\mu_{\varepsilon}(s)\,d\mu_{\varepsilon}(t)\right|=0. \label{ch07:eqn7.3.6}
\end{equation}
Finally, since $\log\varepsilon \leq\log|s-t+\mathrm{i}\, \varepsilon|\leq\log(6\varepsilon)$ if $(s, t)\in\Delta(0,5\varepsilon)$, we have
\begin{align*}
-(\mu\otimes\mu)(\Delta(0,9\varepsilon)) & \ \leq \ -(\mu_{\varepsilon}\otimes\mu_{\varepsilon})(\Delta(0,5\varepsilon)) \\
& \ \leq \ \frac{1}{|\log\varepsilon|}\iint_{\Delta(0,5\varepsilon)}\log|s-t+\mathrm{i}\, \varepsilon|\,d\mu_{\varepsilon}(s)\,d\mu_{\varepsilon}(t) \\
& \ \leq \ \left(-1+\frac{6}{|\log\varepsilon|}\right)(\mu_{\varepsilon}\otimes\mu_{\varepsilon})(\Delta(0,5\varepsilon)) \\
& \ \leq \ \left(-1+\frac{6}{|\log\varepsilon|}\right)(\mu\otimes\mu)(\Delta(0, \varepsilon)).
\end{align*}
Therefore,
\begin{equation}
\lim_{\varepsilon \rightarrow+0}\frac{1}{|\log\varepsilon|}\iint_{\Delta(0,5\varepsilon)}\log|s-t+\mathrm{i}\, \varepsilon|\,d\mu_{\varepsilon}(s)\,d\mu_{\varepsilon}(t)=-(\mu\otimes\mu)(\Delta).
\label{ch07:eqn7.3.7}
\end{equation}
Combining (\ref{ch07:eqn7.3.5})--(\ref{ch07:eqn7.3.7}), we obtain
\begin{equation*}
\limsup_{\varepsilon\rightarrow+0}\left|\frac{1}{|\log\varepsilon|}\iint\log|s-t+\mathrm{i}\, \varepsilon|\,d\mu_{\varepsilon}(s)\,d\mu_{\varepsilon}(t)+(\mu\otimes\mu)(\Delta)\right|\leq\delta\,,
\end{equation*}
which implies (\ref{ch07:eqn7.3.4}).
\end{proof2}

The above theroem implies the lower semicontinuity of $\delta(a)$ as follows.

\begin{corollary}
\label{ch07:cor7.3.3}
Let $a, a_{m}\in\mathcal{M}^{sa}$ for $m\in \mathbb{N}$. If $\sup_{m}\Vert a_{m}\Vert<+\infty$ and $a_{m}\rightarrow a$ in distribution, then
\begin{equation*}
\delta(a)\leq\liminf_{m\rightarrow \infty}\delta(a_{m}).
\end{equation*}
In particular, this is the case if $a_{m}\rightarrow a$ strongly.
\end{corollary}

\begin{proof2}
The assumption implies that $(\mu_{m}\otimes\mu_{m})(f)\rightarrow(\mu\otimes\mu)(f)$ for every continuous function $f$ on $\mathbb{R}^{2}$, where $\mu, \mu_{m}$ are the distributions of $a, a_{m}$. Then it is straightforward to see that
\begin{equation*}
(\mu\otimes\mu)(\Delta)\geq\limsup_{m\rightarrow\infty}(\mu_{m}\otimes\mu_{m})(\Delta).
\end{equation*}
This gives the result by Theorem~\ref{ch07:the7.3.2}.
\end{proof2}

\begin{corollary}
\label{ch07:cor7.3.4}
Let $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$, and let $\mu_{i}$ be the distribution of $a_{i}$. Then:
\begin{enumerate}
\item[(1)] $\delta(a_{1},\ldots,a_{N})\leq N$.

\item[(2)] If $\delta(a_{1},\ldots,a_{N})=N$, then all $\mu_{i}$ are nonatomic.

\item[(3)] If $a_{1},\ldots,a_{N}$ are in free relation, then
\end{enumerate}
\begin{equation*}
\delta(a_{1},\ldots,a_{N})=\delta(a_{1})+\cdots+\delta(a_{N})=N-\sum_{i=1}^{N}\sum_{t\in \mathbb{R}}\mu_{i}(\{t\})^{2}\,.
\end{equation*}
\end{corollary}

\begin{proof2}(1) and (2) immediately follow from Proposition~\ref{ch07:pro7.3.1} and Theorem~\ref{ch07:the7.3.2}.
(3) follows from Theorems~\ref{ch06:the6.4.1} and \ref{ch07:the7.3.2}.
\end{proof2}

In view of its name, one may expect that the free entropy dimension admits a nonnegative value. Although its nonnegativity will be intrinsically characterized in a future theorem, we first prove


\begin{proposition}
\label{ch07:pro7.3.5}
If $a_{1},\ldots,a_{N}$ belong to a von Neumann subalgebra of $\mathcal{M}$ isomorphic to $\mathcal{L}(\mathbf{F}_{M})$ for some $M\in \mathbb{N}$, then
\begin{equation*}
\delta(a_{1},\ldots,a_{N})\geq 0.
\end{equation*}
\end{proposition}

\begin{proof2}
The assumption means that there exists a semicircular system $(b_{1},\ldots,b_{M})$ in $\mathcal{M}$ such that $a_{i}\in\{b_{1},\ldots,b_{M}\}''$ for $1\leq i\leq N$. Choose another semicircular system $(S_{1},\ldots,S_{N})$ which is in free relation to $\{b_{1},\ldots,b_{M}\}$. By Propositions~\ref{ch06:pro6.1.3}, \ref{ch06:pro6.1.6} and Theorem~\ref{ch06:the6.4.1} we have
\begin{align*}
& \chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}) \\
& \quad \geq\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}, b_{1},\ldots,b_{M})-\sum_{j=1}^{M}\chi(b_{j}) \\
& \quad =\chi(\varepsilon S_{1},\ldots,\varepsilon S_{N}, b_{1},\ldots,b_{M})-\sum_{j=1}^{M}\chi(b_{j}) \\
& \quad =\sum_{i=1}^{N}\chi(\varepsilon S_{i})=\sum_{i=1}^{N}\chi(S_{i})+N\log\varepsilon\,,
\end{align*}
which implies that
\begin{equation*}
\limsup_{\varepsilon\rightarrow +0}\frac{\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N})}{|\log\varepsilon|}\geq-N,
\end{equation*}
and hence the result follows.
\end{proof2}

Next we prepare a technical result on restricted\index{Minkowski sum!restricted} Minkowski sums.\index{restricted Minkowski sum}\index{Minkowski sum} For a while, all subsets of $\mathbb{R}^{n}$ are assumed to be measurable, and the same $\lambda$ denotes the Lebesgue measure on $\mathbb{R}^{n}$ of different dimensions. The \textit{Minkowski sum} of $A, B\subset \mathbb{R}^{n}$ is $A+B:=\{x+y:a\in A,\ b\in B\}$. The \textit{ Brunn-Minkowski inequality}\index{Brunn-Minkowski inequality}
\begin{equation*}
\lambda(A+B)^{1/n}\geq\lambda(A)^{1/n}+\lambda(B)^{1/n}
\end{equation*}
is known (see [\citen{bib151}]). Given a subset $\Theta\subset A\times B$, the \textit{ restricted Minkowski sum} is defined as
\begin{equation*}
A+_{\Theta} B :=\{a+b:(a, b)\in\Theta\}.
\end{equation*}
The following is a modified Brunn-Minkowski inequality in which the exponent $1/n$ is replaced by $2/n$.

\begin{lemma}
\label{ch07:lem7.3.6}
There exists a universal constant $c>0$ such that, for any $0<\rho<1,\ n\in \mathbb{N}$ and $A, B\subset \mathbb{R}^{n}$, if
\begin{equation*}
\rho\leq\left(\frac{\lambda(A)}{\lambda(B)}\right)^{1/n}\leq\rho^{-1}
\end{equation*}
and if $\Theta\subset A\times B \ (\subset \mathbb{R}^{2n})$ satisfies
\begin{equation*}
\lambda(\Theta)\geq(1-c\min\{\rho\sqrt{n}, 1\})\lambda(A\times B),
\end{equation*}
then
\begin{equation*}
\lambda(A+_{\Theta} {B)^{2/n}}\geq\lambda(A)^{2/n}+\lambda(B)^{2/n}.
\end{equation*}
\end{lemma}

\begin{proof2}
We divide the proof into two parts.

\textit{Step} 1. Let $0<\rho<1$ and
\begin{equation*}
\Theta :=\{(a, b)\in \mathbb{B}^{n}\times\rho \mathbb{B}^{n}:\Vert a+b\Vert\leq\sqrt{1+\rho^{2}}\}\,,
\end{equation*}
where $\mathbb{B}^{n}$ denotes the unit ball of $\mathbb{R}^{n}$. Then we show that
\begin{equation}
\lambda(\Theta)\leq(1-c\min\{\rho\sqrt{n}, 1\})\lambda(\mathbb{B}^{n}\times\rho \mathbb{B}^{n})
\label{ch07:eqn7.3.8}
\end{equation}
for some constant $c>0$ independent of $n$ and $\rho$. To do so, it suffices to show that, for a constant $c'>0$, if $1\geq\Vert a\Vert\geq 1-s/n$ where $s:=\frac{1}{2}\min\{\rho\sqrt{n},\ 1\}$, then one gets
\begin{equation}
\lambda (\{b\in\rho \mathbb{B}^{n}:\Vert a+b\Vert>\sqrt{1+\rho^{2}}\})\geq c'\lambda(\rho \mathbb{B}^{n})
\label{ch07:eqn7.3.9}
\end{equation}
uniformly for $n$ and $\rho$. Indeed, we then have
\begin{align*}
\lambda(\Theta) & \ = \ \lambda(\mathbb{B}^{n}\times\rho \mathbb{B}^{n})-\lambda((\mathbb{B}^{n}\times\rho \mathbb{B}^{n})\,\backslash\,\Theta) \\
& \ \leq \ \lambda(\mathbb{B}^{n}\times\rho \mathbb{B}^{n}) \\
& \qquad \ -\int_{1\geq\Vert a\Vert\geq 1-s/n}\lambda(\{b\in\rho \mathbb{B}^{n}:\Vert a+b\Vert>\sqrt{1+\rho^{2}}\})\ d\lambda(a) \\
& \ \leq \ \left(1-\left[1-\left(1-\frac{s}{n}\right)^{n}\right]c'\right)\lambda(\mathbb{B}^{n}\times\rho \mathbb{B}^{n}),
\end{align*}
which implies (\ref{ch07:eqn7.3.8}) because
\begin{equation*}
1-\left(1-\frac{s}{n}\right)^{n}\geq\frac{s}{2} \qquad (0\leq s\leq 1,\ n\in \mathbb{N}).
\end{equation*}
To prove (\ref{ch07:eqn7.3.9}), we may assume that $a=(r, 0,\ldots,0)$ , where $1\geq r\geq 1-s/n$ and $n\geq 2$ (the case $n=1$ is easily treated). For $b=(t, b')\ (b'\in \mathbb{R}^{n-1})$, the conditions $b\in\rho \mathbb{B}^{n}$ and $\Vert a+b\Vert\leq\sqrt{1+\rho^{2}}$ mean that
\begin{equation*}
t^{2}+\Vert b'\Vert^{2}\leq\rho^{2} \quad \mathrm{and} \quad (r+t)^{2}+\Vert b'\Vert^{2}\leq 1+\rho^{2},
\end{equation*}
that is,
\begin{equation*}
\Vert b'\Vert^{2}\leq\left\{\begin{array}{ll}
\rho^{2}-t^{2} & \qquad \mathrm{for}-\rho\leq t\leq u_{1},\\
1+\rho^{2}-(r+t)^{2} & \qquad \mathrm{for}\ u_{1}\leq t\leq u_{2},
\end{array}\right.
\end{equation*}
where
\begin{equation*}
u_{1}:=\frac{1-r^{2}}{2r}, \quad u_{2}:=\sqrt{1+\rho^{2}}-r.
\end{equation*}
Note that $u_{2}<p$ and
\begin{equation*}
u_{1}\leq\frac{1-\left(1-\frac{\rho}{2\sqrt{n}}\right)^{2}}
{2\left(1-\frac{\rho}{2\sqrt{n}}\right)}\leq\frac{\rho}{2\sqrt{n}-\rho}\leq\frac{\rho}{\sqrt{n}}\,.
\end{equation*}
Therefore, we have
\begin{align}
& \lambda\left(\left\{b\in\rho \mathbb{B}^{n}:\Vert a+b\Vert\leq\sqrt{1+\rho^{2}}\right\}\right) \notag \\
& \leq\lambda(\mathbb{B}^{n-1})\left[\int_{-\rho}^{u_{1}}(\rho^{2}-t^{2})^{\frac{n-1}{2}}\,dt+\int_{u_{1}}^{u_{2}}(1+\rho^{2}-(r+t)^{2})^{\frac{n-1}{2}}\,dt\right]\notag \\
& \leq\lambda(\mathbb{B}^{n-1})\left[\int_{-\rho}^{\rho/\sqrt{n}}(\rho^{2}-t^{2})^{\frac{n-1}{2}}\,dt+\int_{\rho/\sqrt{n}}^{u_{2}}(1+\rho^{2}-(r+t)^{2})^{\frac{n-1}{2}}\,dt\right]. \label{ch07:eqn7.3.10}
\end{align}
The two integrals in (\ref{ch07:eqn7.3.10}) can be compared with $\int_{-\rho}^{\rho}(\rho^{2}-t^{2})^{\frac{n-1}{2}}\,dt$. For the first integral, one has
\begin{equation*}
\frac{\int_{-\rho}^{\rho/\sqrt{n}}(\rho^{2}-t^{2})^{\frac{n-1}{2}}dt}{\int_{-\rho}^{\rho}(\rho^{2}-t^{2})^{\frac{n-1}{2}}dt} = \frac{\int_{-\sqrt{n}}^{1}(1-\frac{t^{2}}{n})^{\frac{n-1}{2}}dt}{\int_{-\sqrt{n}}^{\sqrt{n}}(1-\frac{t^{2}}{n})^{\frac{n-1}{2}}dt},
\end{equation*}
which is stricly smaller than 1 uniformly for $n$. When $\rho/\sqrt{n}\leq t\leq u_{2}$, since $(1+\rho^{2}-(r+t)^{2})/(\rho^{2}-t^{2})$ is decreasing in $t$ and $r\geq 1-\rho/2\sqrt{n}$, one gets
\begin{align*}
\frac{1+\rho^{2}-(r+t)^{2}}{\rho^{2}-t^{2}} & \leq \frac{1+\rho^{2}-(r+\frac{\rho}{\sqrt{n}})^{2}}{\rho^{2}-\frac{\rho^{2}}{n}} \\
& \leq \ \frac{1+\rho^{2}-(1+\frac{\rho}{2\sqrt{n}})^{2}}{\rho^{2}-\frac{\rho^{2}}{n}}\leq\frac{n-\sqrt{n}-\frac{1}{4}}{n-1}\,.
\end{align*}
Therefore, the second integral in (\ref{ch07:eqn7.3.10}) is dominated by
\begin{equation*}
\left(\frac{n-\sqrt{n}-\frac{1}{4}}{n-1}\right)^{\frac{n-1}{2}}\int_{-\rho}^{\rho}(\rho^{2}-t^{2})^{\frac{n-1}{2}}\,dt.
\end{equation*}
Since
\begin{equation*}
\left(\frac{n-\sqrt{n}-\frac{1}{4}}{n-1}\right)^{\frac{n-1}{2}}\rightarrow 0 \quad \mathrm{as} \quad n\rightarrow\infty,
\end{equation*}
the estimates of two integrals imply that
\begin{equation*}
\lambda(\{b\in\rho \mathbb{B}^{n}:\Vert a+b\Vert\leq\sqrt{1+\rho^{2}}\})\leq c''\lambda(\rho \mathbb{B}^{n})
\end{equation*}
for some $0<c''<1$ uniformly for $n$ and $\rho$, yielding (\ref{ch07:eqn7.3.9}).

\textit{Step} 2. Step 1 shows that the conclusion of the lemma holds for the particular case
\begin{equation}
A=\rho_{1}\mathbb{B}^{n}, \quad B=\rho_{2}\mathbb{B}^{n}, \quad \Theta=\{(a, b)\in A\times B:a+b\in\rho_{3}\mathbb{B}^{n}\},
\label{ch07:eqn7.3.11}
\end{equation}
where $\rho_{1}, \rho_{2}, \rho_{3}>0$. In fact, the case $\rho_{1}=1>\rho_{2}=\rho$ is a direct consequence, and the above case follows by symmetry and homogeneity. Now let $A, B\subset \mathbb{R}^{n}$ and $\Theta\subset A\times B$ be general as stated in the lemma. It suffices to show that there are $A_{0},\,B_{0}$ and $\Theta_{0}$ of the form (\ref{ch07:eqn7.3.11}) such that
\begin{align*}
\lambda(A_{0}) & =\lambda(A), \quad \lambda(B_{0})=\lambda(B)\,, \\
\lambda(\Theta_{0})& \geq\lambda(\Theta), \quad \lambda(A_{0}+_{ \Theta_{0}}B_{0})\leq\lambda(A+_{\Theta} B)\,.
\end{align*}
Set
\begin{equation*}
C :=A+_{\Theta} B, \quad \Theta_{1} :=\{(a, b)\in A\times B:a+b\in C\},
\end{equation*}
and take $\rho_{1}, \rho_{2}, \rho_{3}>0$ so that
\begin{equation*}
\lambda(A)=\lambda(\rho_{1}\mathbb{B}^{n}), \quad \lambda(B)=\lambda(\rho_{2}\mathbb{B}^{n}), \quad \lambda(C)=\lambda(\rho_{3}\mathbb{B}^{n})\,.
\end{equation*}
Then $\Theta\subset\Theta_{1}$, and one can estimate
\begin{align}
\lambda(\Theta_{1}) & \ = \ \int_{\mathbb{R}^{n}}\int_{\mathbb{R}^{n}}\chi_{A}(a)\chi_{B}(b)\chi_{C}(a+b)da\,db \notag\\
& \ \leq \ \int_{\mathbb{R}^{n}}\int_{\mathbb{R}^{n}}\chi_{\rho_{1}\mathbb{B}^{n}}(a)\chi_{\rho_{2}
\mathbb{B}^{n}}(b)\chi_{\rho_{3}\mathbb{B}^{n}}(a+b)da\,db \label{ch07:eqn7.3.12} \\
& \ = \ \lambda(\{(a, b)\in\rho_{1}\mathbb{B}^{n}\times\rho_{2}\mathbb{B}^{n} : a+b\in\rho_{3}\mathbb{B}^{n}\}).\notag
\end{align}
This is due to a rearrangement inequality for an integral of a product of nonnegative functions by spherical symmetrizations (see [\citen{bib43}], Theorem 3.4). Thus the required conditions are satisfied for $A_{0} :=\rho_{1}\mathbb{B}^{n},\,B_{0} :=\rho_{2}\mathbb{B}^{n}$ and $\Theta_{0} :=\{(a,b)\in A_{0}\times B_{0} : a+b\in\rho_{3}\mathbb{B}^{n}\}$, so the proof is completed.
\end{proof2}

Here it is convenient to introduce the following terminology of Voiculescu. For $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$, it is said that the $N$-tuple $(a_{1},\ldots,a_{N})$ has \textit{finite-dimensional approximants}\index{finite-dimensional approximants} (or $f.d.a.$, for short) if, for every $r\in \mathbb{N},\,\varepsilon >0$ and $ R>\max_{i}\Vert a_{i}\Vert,\ \Gamma_{R}(a_{1},\ldots,a_{N};n, r, \varepsilon) \neq\emptyset$ for some $n$ (equivalently for any sufficiently large $n$). Evidently, this condition is necessary for $\chi(a_{1},\ldots,a_{N})>-\infty$.

The next proposition gives the free entropy power inequality under perturbation by a semicircular system.

\begin{proposition}
\label{ch07:pro7.3.7}
Let $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$ and assume that $(a_{1},\ldots,a_{N})$ has finite-dimensional approximants. If $(S_{1},\ldots,S_{N})$ is a semicircular system free from $\{a_{1},\ldots,a_{N}\}$, then for every $t_{1},\ldots,t_{N}>0$
\begin{align*}
& \exp\left(\frac{2}{N}\chi(a_{1}+t_{1}S_{1},\ldots,a_{N}+t_{N}S_{N})\right) \\
& \quad \geq\exp \left(\frac{2}{N}\chi(a_{1},\ldots,a_{N})\right)+\exp\left(\frac{2}{N}\chi(t_{1}S_{1},\ldots,t_{N}S_{N})\right) \\
& \quad =\exp\left(\frac{2}{N}\chi(a_{1},\ldots,a_{N})\right)+2\pi e\left(\prod_{i=1}^{N}t_{i}\right)^{2/N}.
\end{align*}
\end{proposition}

\begin{proof2}
Let $R>\max_{i}\Vert a_{i}\Vert+2\max_{i}t_{i}$. For $n, r\in \mathbb{N}$ and $\varepsilon >0$ set
\begin{align*}
A_{n}(r, \varepsilon) & := \Gamma_{R}(a_{1},\ldots,a_{N};n, r, \varepsilon), \\
B_{n}(r, \varepsilon) & := \prod_{i=1}^{N}\Gamma_{R}(t_{i}S_{i};n, r, \varepsilon), \\
\Omega_{n}(r, \varepsilon) & := \Gamma_{R}(a_{1},\ldots,a_{N}, t_{1}S_{1},\ldots,t_{N}S_{N};n, r,\varepsilon), \\
C_{n}(r, \varepsilon) & := \Gamma_{2R}(a_{1}+t_{1}S_{1},\ldots,a_{N}+t_{N}S_{N};n, r, \varepsilon).
\end{align*}
For any given $r\in \mathbb{N}$ and $\varepsilon >0$, choose $\varepsilon'>0$ such that
\begin{align}
& \{(A_{1}+B_{1},\ldots,A_{N}+B_{N}):(A_{1},\ldots,A_{N}, B_{1},\ldots,B_{N})\in\Omega_{n}(r, \varepsilon')\} \notag \\
& \qquad \subset C_{n}(r, \varepsilon)\,.
\label{ch07:eqn7.3.13}
\end{align}
Moreover, by Lemma~\ref{ch06:lem6.4.3} there exists $\varepsilon_1>0$ such that
\begin{equation}
\lim_{n\rightarrow\infty}\frac{\Lambda((A_{n}(r,\varepsilon_{1})\times B_{n}(r,\varepsilon_{1}))\cap\Omega_{n}(r,\varepsilon'))}{\Lambda(A_{n}(r,\varepsilon_{1})\times B_{n}(r,\varepsilon_{1}))}=1\,.
\label{ch07:eqn7.3.14}
\end{equation}
(Note that Lemma~\ref{ch06:lem6.4.3} is valid under the assumption of f.d.a. in place of finite free entropy, as is clear from the proof.) Set
\begin{equation*}
\Theta_{n}:=(A_{n}(r, \varepsilon_{1})\times B_{n}(r, \varepsilon_{1}))\cap\Omega_{n}(r, \varepsilon')\,.
\end{equation*}
Then (\ref{ch07:eqn7.3.13}) and (\ref{ch07:eqn7.3.14}) imply that
\begin{align}
A_{n}(r, \varepsilon_{1})+_{\Theta_{n}} B_{n} (r, \varepsilon_{1})\subset C_{n}(r, \varepsilon)\,, \notag\\
\lim_{n\rightarrow\infty}\frac{\Lambda(\Theta_{n})}{\Lambda(A_{n}(r,\varepsilon_{1})\times B_{n}(r,\varepsilon_{1}))}=1\,.
\label{ch07:eqn7.3.15}
\end{align}
Note by (\ref{ch06:eqn6.1.1}) that the limit
\begin{equation}
\lim_{n\rightarrow\infty}\left[\frac{1}{n^{2}}\Lambda(B_{n}(r, \varepsilon_1))+\frac{N}{2}\log n\right]=\sum_{i=1}^{N}\chi_{R}(t_{i}S_{i};r, \varepsilon_{1})
\label{ch07:eqn7.3.16}
\end{equation}
exists.

First assume that $\chi(a_{1},\ldots,a_{N})>-\infty$. Choose a subsequence $\{n_{k}\}\subset\{n\}$ such that
\begin{equation}
\lim_{k\rightarrow\infty}\left[\frac{1}{n_{k}^{2}}\log\Lambda(A_{n_{k}}(r, \varepsilon_{1}))+\frac{N}{2}\log n_{k}\right]=\chi_{R}(a_{1},\ldots,a_{N};r, \varepsilon_{1})\,.
\label{ch07:eqn7.3.17}
\end{equation}
Hence we get
\begin{equation*}
\lim_{k\rightarrow\infty}\frac{1}{n_{k}^{2}}\log\frac{\Lambda(A_{n_{k}}(r,\varepsilon_{1}))}{\Lambda(B_{n_{k}}(r,\varepsilon_{1}))}=\chi_{R}(a_{1},\ldots, a_{N};r,\,\varepsilon_{1})-\sum_{i=1}^{N}\chi_{R}(t_{i}S_{i};r,\,\varepsilon_{1}),
\end{equation*}
which is a finite real value (because $\chi_{R}(a_{1},\ldots,a_{N};r, \varepsilon_1)\geq\chi(a_{1},\ldots,a_{N})>-\infty)$. So one can apply Lemma~\ref{ch07:lem7.3.6} to get
\begin{equation*}
\Lambda(C_{n_{k}}(r, \varepsilon))^{2/Nn_{k}^{2}}\geq\Lambda(A_{n_{k}}(r, \varepsilon_{1}))^{2/Nn_{k}^{2}}+\Lambda(B_{n_{k}}(r, \varepsilon_{1}))^{2/Nn_{k}^{2}}
\end{equation*}
for large $k$. This implies that
\begin{align*}
& \exp\left(\frac{2}{N}\chi_{2R}(a_{1}+t_{1}S_{1},\ldots,a_{N}+t_{N}S_{N};r, \varepsilon)\right) \\
& \qquad \ =\limsup_{ n\rightarrow\infty}n\Lambda(C_{n}(r, \varepsilon))^{2/Nn^{2}} \\
& \qquad \ \geq \limsup_{ k\rightarrow\infty} n_{k}\Lambda(C_{n_{k}}(r, \varepsilon))^{2/Nn_{k}^{2}} \\
& \qquad \ \geq\lim_{k\rightarrow\infty}\left[n_{k}\Lambda(A_{n_{k}}(r, \varepsilon_1))^{2/Nn_{k}^{2}}+n_{k}\Lambda(B_{n_{k}}(r, \varepsilon_1))^{2/Nn_{k}^{2}}\right] \\
& \qquad \ =\exp\left(\frac{2}{N}\chi_{R}(a_{1},\ldots,a_{N};r, \varepsilon_{1})\right)+\exp\left(\frac{2}{N}\sum_{i=1}^{N}\chi_{R}(t_{i}S_{i};r, \varepsilon_{1})\right) \\
& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \ (\mathrm{by} \ (\ref{ch07:eqn7.3.16}),\,(\ref{ch07:eqn7.3.17})) \\
& \qquad \ \geq\exp\left(\frac{2}{N}\chi_{R}(a_{1},\ldots,a_{N})\right)+\exp\left(\frac{2}{N}\sum_{i=1}^{N}\chi_{R}(t_{i}S_{i})\right).
\end{align*}
Since
\begin{equation*}
\chi(t_{1}S_{1},\ldots,t_{N}S_{N})=\sum_{i=1}^{N}\chi(t_{i}S_{i})=\frac{1}{2}\sum_{i=1}^{N}\log(2\pi et_{i}^{2})\,,
\end{equation*}
we have the result.

Next assume that $\chi(a_{1},\ldots,a_{N})=-\infty$. For any $0<\theta<1$ fixed, by (\ref{ch07:eqn7.3.15})
\begin{equation*}
\Lambda(\Theta_{n})\geq(1-\theta)\Lambda(A_{n}(r, \varepsilon_{1})\times B_{n}(r, \varepsilon_{1}))
\end{equation*}
for large $n$. For such $n$ there is $(A_{1},\ldots,A_{N})\in A_{n}(r, \varepsilon_{1})$ such that
\begin{align*}
& (1-\theta)\Lambda(B_{n}(r, \varepsilon_{1})) \\
& \qquad \leq\Lambda(\{(B_{1},\ldots,B_{N}) : (A_{1}\ldots, A_{N}, B_{1}\ldots, B_{N})\in\Theta_{n}\}) \\
& \qquad \leq\Lambda(\{(B_{1},\ldots,B_{N}) : (A_{1}+B_{1},\ldots,A_{N}+B_{N})\in C_{n}(r, \varepsilon)\}) \\
& \qquad =\Lambda(C_{n}(r, \varepsilon))\,.
\end{align*}
This implies that
\begin{equation*}
\chi(a_{1}+t_{1}S_{1},\ldots,a_{N}+t_{N}S_{n})\geq\chi(t_{1}S_{1},\ldots,t_{N}S_{N})
\end{equation*}
which is the conclusion in this case.
\end{proof2}

\begin{corollary}
\label{ch07:cor7.3.8}
If $a_{1},\ldots,a_{N} \in \mathcal{M}^{sa}$ and $\chi(a_{1},\ldots,a_{N}) > -\infty$, then $\delta(a_{1},\ldots, a_{N})=N$.
\end{corollary}

\begin{proof2}
This is obvious because the previous proposition gives
\begin{equation*}
\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N})\geq\chi(a_{1},\ldots, a_{N})>-\infty
\end{equation*}
for every $\varepsilon >0$.
\end{proof2}

\begin{theorem}
\label{ch07:the7.3.9}
Let $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$, and let $(S_{1},\ldots,S_{N})$ be a semicircular system free from $\{a_{1},\ldots,a_{N}\}$. Then the following are equivalent:
\begin{enumerate}
\item[(i)] $(a_{1},\ldots,a_{N})$ has finite-dimensional approximants;

\item[(ii)] $\chi(a_{1}+tS_{1},\ldots,a_{N}+tS_{N})>-\infty$ for all $t>0$;

\item[(iii)] $\delta(a_{1},\ldots,a_{N})\geq 0$;

\item[(iv)] $\delta(a_{1}+tS_{1},\ldots,a_{N}+tS_{N})=N$ for all $t>0$.
\end{enumerate}

Furthermore, if $(a_{1},\ldots,a_{N})$ does not have finite-dimensional approximants, then
\begin{equation*}
\chi(a_{1}+tS_{1},\ldots,a_{N}+tS_{N})=-\infty \qquad (0\leq t\leq\varepsilon)
\end{equation*}
for some $\varepsilon>0$, and $\delta(a_{1},\ldots,a_{N})=-\infty$.
\end{theorem}

\begin{proof2}
$(\mathrm{i})\Rightarrow(\mathrm{ii})$ is obvious from Proposition~\ref{ch07:pro7.3.7}. Conversely, (ii) implies that $(a_{1}+tS_{1},\ldots, a_{N}\,+\,tS_{N})$ has f.d.a. for all $t>0$, so (i) follows by approximation. Under the assumption (i), Proposition~\ref{ch07:pro7.3.7} yields
\begin{equation*}
\limsup_{\varepsilon \rightarrow+0}\frac{\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N})}{|\log\varepsilon|}\geq\limsup_{\varepsilon\rightarrow+0}\frac{\frac{N}{2}\log(2\pi e\varepsilon^{2})}{|\log\varepsilon|}=-N\,.
\end{equation*}
Hence $(\mathrm{i})\Rightarrow\mathrm{(iii)}$. Conversely, (iii) implies that $\chi(a_{1}\,+\,\varepsilon_{k}S_{1},\ldots,a_{N}\,+\,\varepsilon_{k}S_{N})>-\infty$, and hence $(a_{1}+\varepsilon kS_{1},\ldots,a_{N}+\varepsilon_{k}S_{N})$ has f.d.a. for some $\varepsilon_{k} \rightarrow+0$. This gives $\mathrm{(i)}.\,\mathrm{(ii)} \Rightarrow \mathrm{(iv)}$ follows from the above corollary, and $\mathrm{(iv)} \Rightarrow(\mathrm{i})$ is shown similarly to $(\mathrm{iii}) \Rightarrow(\mathrm{i})$.

Next, assume that $(a_{1},\ldots,a_{N})$ does not have f.d.a. It is clear from the above proof that $\chi(a_{1}+tS_{1},\ldots, a_{N}+tS_{N})=-\infty$ for all $t$ near $0$. This gives $\delta(a_{1},\ldots,a_{N})=-\infty$.
\end{proof2}

According to Proposition~\ref{ch07:pro7.3.5} and Theorem~\ref{ch07:the7.3.9}, if $a_{1},\ldots,a_{N}$ are in a von Neumann algebra isomorphic to $\mathcal{L}(\mathbf{F}_{M})$, then $(a_{1},\ldots,a_{N})$ has f.d.a. But this is easy to check directly; in fact, if $(b_{1},\ldots,b_{M})$ has f.d.a. and $a_{1},\ldots,a_{N}\in\{b_{1},\ldots,b_{M}\}''$, then $(a_{1},\ldots,a_{N})$ has f.d.a. too.

The behavior of the free entropy dimension under linear transformations of the variables is similar to that of the free entropy in Corollary~\ref{ch06:cor6.3.2}.

\begin{proposition}
\label{ch07:pro7.3.10}
Let $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$, let $A=[\alpha_{ij}]_{i,j=1}^{N}$ be an invertible real matrix, and let $\beta_{1},\ldots,\beta_{N}\in \mathbb{R}$. Then
\begin{equation*}
\delta(a_{1},\ldots,a_{N})=\delta\left(\sum_{j=1}^{N}\alpha_{1j}a_{j}+\beta_{1} \mathbf{1},\ldots,\sum_{j=1}^{N}\alpha_{Nj}a_{j}+\beta_{N}\mathbf{1}\right).
\end{equation*}
\end{proposition}

\begin{proof2}
It is obvious that $\delta(a_{1}+\beta_{1}\mathbf{1},\ldots,a_{N}+\beta_{N}\mathbf{1})=\delta(a_{1},\ldots,a_{N})$. So the case $\beta_{i}=0\ (1\leq i\leq N)$ is enough. First assume that $A$ is an orthogonal matrix. Let $(S_{1},\ldots,S_{N})$ be as before and set $S_{i}' :=\sum_{j=1}^{N}\alpha_{ij}S_{j}$. Then $(S_{1}',\ldots,S_{N}')$ is a semicircular system again (cf. Proposition~\ref{ch02:pro2.6.6}), and by Corollary~\ref{ch06:cor6.3.2}
\begin{equation*}
\chi \left(\sum_{j=1}^{N}\alpha_{1j}a_{j}+\varepsilon S_{1}',\ldots,\sum_{j=1}^{N}\alpha_{Nj}a_{j}+\varepsilon S_{N}'\right)=\chi(a_{1}+\varepsilon S_{1},\ldots, a_{N}+\varepsilon S_{N})\,,
\end{equation*}
so we have $\delta (\sum_{j=1}^{N}\alpha_{1j}a_{j},\ldots,\sum_{j=1}^{N}\alpha_{Nj}a_{j})=\delta(a_{1},\ldots,a_{N})$. Now we may assume that $(a_{1},\ldots,a_{N})$ has f.d.a. Let $\lambda_{1}\geq\ldots\geq\lambda_{N}\ (>0)$ be the eigenvalues of $(A^{t}A)^{1/2}$. Since there are orthogonal matrices $T_{1}, T_{2}$ such that $T_{1}A= \mathbf{Diag} (\lambda_{1},\ldots,\lambda_{N})T_{2}$, it suffices to show the case $A= \mathbf{Diag} (\lambda_{1},\ldots, \lambda_{N})$. For $\varepsilon >0$ we have
\begin{align*}
& \chi(\lambda_{1}a_{1}+\varepsilon S_{1},\ldots,\lambda_{N}a_{N}+\varepsilon S_{N}) \\
& \qquad =\chi \left(a_{1}+\frac{\varepsilon}{\lambda_{1}}S_{1},\ldots,a_{N}+\frac{\varepsilon}{\lambda_{N}}S_{N}\right)+\log(\lambda_{1}\cdots\lambda_{N})\,.
\end{align*}
So it remains to check that
\begin{equation}
\delta(a_{1},\ldots,a_{N})=N+\lim_{\varepsilon\rightarrow+0}\frac{\chi(a_{1}+\varepsilon t_{1}S_{1},\ldots,a_{N}+\varepsilon t_{N}S_{N})}{|\log\varepsilon|}
\label{ch07:eqn7.3.18}
\end{equation}
for any constants $t_{1},\ldots,t_{N}>0$. For $0<t<\min_{i}t_{i}$ we can write
\begin{align*}
& \chi(a_{1}+\varepsilon t_{1}S_{1},\ldots,a_{N}+\varepsilon t_{N}S_{N}) \\
& \qquad =\chi \left(a_{1}+\varepsilon tS_{1}+\varepsilon\sqrt{t_{1}^{2}-t^{2}}\tilde{S}_{1},\ldots,a_{N}+\varepsilon tS_{N}+\varepsilon\sqrt{t_{N}^{2}-t^{2}}\tilde{S}_{N}\right),
\end{align*}
where $(\tilde{S}_{1}, \ldots,\tilde{S}_{N})$ is a semicircular system free from $\{a_{1},\ldots,a_{N}\}\cup\{S_{1},\ldots,S_{N}\}$. Hence Proposition~\ref{ch07:pro7.3.7} yields
\begin{align*}
& \exp\left(\frac{2}{N}\chi(a_{1}+\varepsilon t_{1}S_{1},\ldots,a_{N}+\varepsilon t_{N}S_{N})\right) \\
& \qquad \geq\exp\left(\frac{2}{N}\chi(a_{1}+\varepsilon tS_{1},\ldots,a_{N}+\varepsilon tS_{N})\right) \\
& \qquad \qquad +\exp \left(\frac{2}{N}\chi(\varepsilon\sqrt{t_{1}^{2}-t^{2}}\tilde{S}_{1},\ldots,\varepsilon\sqrt{t_{N}^{2}-t^{2}}\tilde{S}_{N})\right),
\end{align*}
so that
\begin{equation*}
\chi(a_{1}+\varepsilon t_{1}S_{1},\ldots,a_{N}+\varepsilon t_{N}S_{N})\geq\chi(a_{1}+\varepsilon tS_{1},\ldots,a_{N}+\varepsilon tS_{N})\,.
\end{equation*}
Similarly, for $t'>\max_{i}t_{i}$ we have
\begin{equation*}
\chi(a_{1}+\varepsilon t_{1}S_{1},\ldots,a_{N}+\varepsilon t_{N}S_{N})\leq\chi(a_{1}+\varepsilon t'S_{1},\ldots,a_{N}+\varepsilon t'S_{N})\,.
\end{equation*}
Now (\ref{ch07:eqn7.3.18}) immediately follows from the above estimates.
\end{proof2}

It is worth noting that the free entropy dimension does not change when trivial elements (i.e. scalars) are added. Indeed, for $a_{1},\ldots, a_{N}\in\mathcal{M}^{sa}$ one gets
\begin{align*}
& \delta(a_{1},\ldots,a_{N}, 0) \\
& \qquad =N+1+\limsup_{\varepsilon \rightarrow+0}\frac{\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N},\varepsilon S_{N+1})}{|\log\varepsilon|} \\
& \qquad =N+1+\limsup_{\varepsilon \rightarrow+0}\frac{\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N})+\chi(\varepsilon S_{N+1})}{|\log\varepsilon|} \\
& \qquad =\delta(a_{1},\ldots,a_{N})\,.
\end{align*}
The above second equality is due to the strong additivity (\ref{ch06:eqn6.4.3}).

The propositions below are generalizations of the above with some additional assumptions.

\begin{proposition}
\label{ch07:pro7.3.11}
Let $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$ be such that $\chi(a_{1},\ldots,a_{N})>-\infty$. If $b_{j}=b_{j}^{*}\in \{a_{1},\ldots,a_{N}\}''$ for $1\leq j\leq M$, then
\begin{equation*}
\delta(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{M})\geq\delta(a_{1},\ldots,a_{N})=N\,.
\end{equation*}
\end{proposition}

\begin{proof2}
Let $(S_{1},\ldots,S_{N+M})$ be a semicircular system which is in free relation to $\{a_{1},\ldots,a_{N}\}$. By Propositions~\ref{ch06:pro6.1.3} and \ref{ch06:pro6.1.6} we have
\begin{align*}
& \chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+ \varepsilon S_{N}, b_{1}+\varepsilon S_{N+1},\ldots,b_{M}+\varepsilon S_{N+M})\\
& \quad \geq\chi(S_{1},\ldots,S_{N}, a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}, b_{1}+\varepsilon S_{N+1},\ldots,b_{M}+\varepsilon S_{N+M}) \\
& \qquad \quad -\chi(S_{1},\ldots,S_{N}) \\
& \quad =\chi(S_{1},\ldots,S_{N}, a_{1},\ldots,a_{N}, b_{1}+\varepsilon S_{N+1},\cdots,b_{M}+\varepsilon S_{N+M})-\sum_{i=1}^{N}\chi(S_{i}) \\
& \quad =\chi(S_{1},\ldots,S_{N}, a_{1},\ldots,a_{N}, \varepsilon S_{N+1},\ldots,\varepsilon S_{N+M})-\sum_{i=1}^{N}\chi(S_{i}) \\
& \quad =\chi(a_{1},\ldots,a_{N})+\sum_{j=1}^{M}\chi(S_{N+j})+M\log\varepsilon.
\end{align*}
The last equality is due to repeated use of the strong additivity property (\ref{ch06:eqn6.4.3}). Therefore,
\begin{equation*}
\delta(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{M})\geq(N+M)-M=N\,.
\end{equation*}
This, together with Corollary~\ref{ch07:cor7.3.8}, gives the result.
\end{proof2}

Under a stronger assumption than the previous we have

\begin{proposition}
\label{ch07:pro7.3.12}
Let $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$. Let $F_{j}(X_{1},\ldots,X_{N})\ (1\leq j\leq M)$ be noncommutative power series with a common multi-radius $(R_{1},\ldots,R_{N})$ of convergence such that $ R_{i}>\Vert a_{i}\Vert$ for $1\leq i\leq N.$ If $b_{j} :=F_{j}(a_{1},\ldots,a_{N})$ and $b_{j}=b_{j}^{*}$ for $1\leq j\leq M$, then
\begin{equation*}
\delta(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{M})\leq\delta(a_{1},\ldots,a_{N})\,.
\end{equation*}
If $\chi(a_{1},\ldots,a_{N})>-\infty$ in addition, then
\begin{equation}
\delta(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{M})=\delta(a_{1},\ldots,a_{N})=N\,.
\label{ch07:eqn7.3.19}
\end{equation}
\end{proposition}

\begin{proof2}
Let $(S_{1},\ldots,S_{N+M})$ be as in the previous proof. Since $\phi_{j}(z) := F_{j}(a_{1}+zS_{1},\ldots,a_{N}+zS_{N})-b_{j}$ is analytic in a neighborhood of $0\in \mathbb{C}$ and $\phi_{j}(0)=0$, we get
\begin{equation}
\inf\{\Vert b_{j}-x\Vert:x\in\{a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}\}''\}\leq\Vert\phi_{j}(\varepsilon)\Vert=O(\varepsilon)\,. \label{ch07:eqn7.3.20}
\end{equation}
If $x_{j}\in\{a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}\}''$ for $1\leq j\leq M$, then
\begin{align*}
& \chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}, b_{1}+\varepsilon S_{N+1},\ldots,b_{M}+\varepsilon S_{N+M}) \\
& \quad \ =\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}, b_{1}-x_{1}+\varepsilon S_{N+1},\ldots,b_{M}-x_{M}+\varepsilon S_{N+M}) \\
& \quad \ \leq\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}) \\
& \qquad \quad  \ +\chi(\varepsilon^{-1}(b_{1}-x_{1})+S_{N+1},\ldots,\varepsilon^{-1}(b_{M}-x_{M})+S_{N+M})+M\log\varepsilon \\
& \quad \ \leq\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}) \\
& \qquad \quad \ +\frac{M}{2}\log\left[\frac{2\pi e}{M}\sum_{j=1}^{M}\left(\frac{\Vert b_{j}-x_{j}\Vert}{\varepsilon}+2\right)^{2}\right]+M\log\varepsilon
\end{align*}
by Propositions~\ref{ch06:pro6.1.6}, \ref{ch06:pro6.1.3}, Corollary~\ref{ch06:cor6.3.2} and Proposition~\ref{ch06:pro6.1.1}. Hence, thanks to (\ref{ch07:eqn7.3.20}), we have
\begin{align*}
& \chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}, b_{1}+\varepsilon S_{N+1},\ldots,b_{M}+\varepsilon S_{N+M}) \\
& \quad \ \leq\chi(a_{1}+\varepsilon  S_{1},\ldots,a_{N}+\varepsilon S_{N})+ \mathrm{const} +M\log\varepsilon
\end{align*}
for sufficiently small $\varepsilon >0$. This implies that
\begin{equation*}
\delta(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{M})\leq\delta(a_{1},\ldots,a_{N})\,.
\end{equation*}
When $\chi(a_{1},\ldots,a_{N}) > -\infty$, the reverse inequality is also valid by Proposition~\ref{ch07:pro7.3.11}.
\end{proof2}

Here a quite interesting problem arises: is $\delta(a_{1},\ldots,a_{N})$ lower semicontinuous in the sense that if $a_{i}, a_{m,i}\in  \mathcal{M}^{sa}$ and $a_{m,i}\rightarrow a_{i}\ (m\rightarrow\infty)$ strongly for $1\leq i\leq N$, then
\begin{equation*}
\delta(a_{1},\ldots,a_{N})\leq\liminf_{m\rightarrow\infty}\delta(a_{m,1},\ldots,a_{m,N})\,?
\end{equation*}
For the single variable case this was proven in Corollary~\ref{ch07:cor7.3.3}. When this happens to hold true in the multivariable case too, it is evident from Propositions~\ref{ch07:pro7.3.11} and \ref{ch07:pro7.3.12} that we indeed have (\ref{ch07:eqn7.3.19}) under the assumption of Proposition~\ref{ch07:pro7.3.11}. Then the isomrophism problem for free group factors would be solved. In fact, if $\mathcal{L}(\mathbf{F}_{N})\cong \mathcal{L}(\mathbf{F}_{M})$ for $N, M\in \mathbb{N}$, then there exist two semicircular systems $(a_{1},\ldots,a_{N})$ and $(b_{1},\ldots,b_{M})$ in $\mathcal{L}(\mathbf{F}_{N})$ such that $\{a_{1},\ldots, a_{N}\}''=\{b_{1},\ldots,b_{M}\}''$, and so (\ref{ch07:eqn7.3.19}) implies that
\begin{equation*}
N=\delta(a_{1},\ldots,a_{N})=\delta(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{M})=\delta(b_{1},\ldots,b_{M})=M\,.
\end{equation*}
In this way, the lower semicontinuity of $\delta(a_{1},\ldots,a_{N})$ would imply that case (2) of Corollary~\ref{ch07:cor7.2.13} is indeed true. But the above lower semicontinuity problem may be extremely difficult to solve even though it is true.

Another notion named free dimension\index{free!dimension} was introduced by K. Dykema in the course of his study on free products of von Neumann algebras. He defined the \textit{ free dimension} for a certain class of finite von Neumann algebras, including free group factors, finite-dimensional algebras and also hyperfinite finite von Neumann algebras. Von Neumann algebras treated here are of finite type, so they are tracial $W^{*}$-probability spaces equipped with a faithful normal tracial state. The following notation is convenient: When $\mathcal{M}_{i} \ (i=1,2, \ldots)$ are von Neumann algebras with associated trace $\tau_{i}$ and $\alpha_{i}\geq 0$ are such that $\sum_{i}\alpha_{i}=1$, let
\begin{align*}
& \mathcal{M}_{1}\oplus \mathcal{M}_{2}\oplus\cdots \\
& \alpha_{1} \qquad \alpha_{2}
\end{align*}
denote the direct sum von Neumann algebra whose associated trace is
\begin{equation*}
\tau(x_{1}\oplus x_{2}\oplus\cdots)=\sum_{i}\alpha_{i}\tau_{i}(x_{i})\,,
\end{equation*}
where the summands $\mathcal{M}_{i}$ with $\alpha_{i}=0$ are considered to be removed. Dykema's definition of free dimension is given in the following way:
\begin{enumerate}
\item[(1)] The free dimension of
\begin{align*}
\mathcal{M} & \cong \mathcal{L}(\mathbf{F}_{r})\oplus M_{n_{1}}(\mathbb{C})\oplus M_{n_{2}}(\mathbb{C})\oplus \cdots \\
& \qquad \alpha_{0} \qquad \quad \alpha_{1} \qquad \qquad \alpha_{2}
\end{align*}
is equal to
\begin{equation*}
\mathbf{fdim}(\mathcal{M}): =\alpha_{0}^{2}r+\sum_{i\geq 1}\alpha_{i}^{2}(1-n_{i}^{-2})+\sum_{\begin{subarray}{c}i,j\geq 0 \\ i\neq j \\ \end{subarray}} \alpha_{i}\alpha_{j}\,.
\end{equation*}
In particular,
\begin{align*}
& \mathbf{fdim} (\mathcal{L}(\mathbb{Z}))=1,  \quad \mathbf{fdim} \ (\mathcal{L}(\mathbf{F}_{r}))=r \quad \mathrm{for} \quad r>1, \\
& \mathbf{fdim} (M_{n}(\mathbb{C}))=1-n^{-2}\,.
\end{align*}
\item[(2)] Let $\mathcal{M}$ be a hyperfinite von Neumann algebra with a trace specified. Decomposing it into its nonatomic and atomic parts, we may represent $\mathcal{M}$ as
\begin{align*}
\mathcal{M} & \cong \mathcal{M}_{0}\oplus M_{n_{1}}(\mathbb{C})\oplus M_{n_{2}}(\mathbb{C})\oplus \cdots, \\
& \quad \ \ \alpha_{0} \qquad \ \ \alpha_{1} \qquad \quad  \ \ \alpha_{2}
\end{align*}
where $\mathcal{M}_{0}$ is a nonatomic (or diffuse) finite von Neumann algebra. Then the free dimension of $\mathcal{M}$ is equal to
\begin{equation*}
\mathbf{fdim} (\mathcal{M}):=\alpha_{0}^{2}+\sum_{i\geq 1}\alpha_{i}^{2}(1-n_{i}^{-2})+\sum_{\begin{subarray}{c}i, j\geq 0, \\ i \neq j\\ \end{subarray}} \alpha_{i}\alpha_{j}\,.
\end{equation*}
\end{enumerate}
In particular, $\mathbf{fdim}(R)=1$ for the hyperfinite $\mathrm{II}_{1}$ factor $R$.

Of course, if the free group factors happen to be mutually isomorphic (see Corollary~\ref{ch07:cor7.2.13}), then the above definition is not well-defined. However, in this case, it is needless to determine the free dimension itself (for instance, in the theorems below).

The free dimension defined above is of essential use in the next theorems, which were discovered by Dykema.

\begin{theorem}
\label{ch07:the7.3.13}
Let
\begin{align*}
\mathcal{M} & \cong \mathcal{L}(\mathbf{F}_{r})\oplus \mathbb{C}p_{1}\oplus \mathbb{C}p_{2}\oplus\cdots  \qquad (r\geq 1, \  \alpha_{i}\geq 0), \\
& \qquad \alpha_{0} \qquad \alpha_{1} \qquad \alpha_{2} \\
\mathcal{N}& \cong \mathcal{L}(\mathbf{F}_{s})\oplus \mathbb{C}q_{1}\oplus \mathbb{C}q_{2}\oplus\ldots \qquad (s\geq 1,\,\beta_{i}\geq 0), \\
& \qquad \ \beta_{0} \qquad \beta_{1} \qquad  \beta_{2}
\end{align*}
where $\alpha_{0}+\beta_{0}>0$. Then
\begin{align*}
\mathcal{M}\star \mathcal{N}& \cong \mathcal{L}(\mathbf{F}_{t})\oplus\bigoplus\nolimits_{i,j}\mathbb{C}(p_{i}\wedge q_{j}), \\
& \qquad \ \gamma \qquad \qquad \qquad \gamma_{ij}
\end{align*}
where $\gamma_{ij} :=\max\{\alpha_{i}+\beta_{j}-1,0\}$ $($and $\gamma=1-\sum_{i,j}\gamma_{ij})$ and $t$ is determined so that $\mathbf{fdim}(\mathcal{M}\star \mathcal{N})=\mathbf{fdim}(\mathcal{M})+\mathbf{fdim}(\mathcal{N})$.
\end{theorem}

\begin{theorem}
\label{ch07:the7.3.14}
Let $\mathcal{M}$ and $\mathcal{N}$ be hyperfinte $($possibly finite-dimensional$)$ von Neumann algebras having the linear dimensions $\dim(\mathcal{M})\geq 2$ and $\dim(\mathcal{N})\geq 3$. Represent
\begin{align*}
\mathcal{M} & \cong \mathcal{M}_{0}\oplus\bigoplus\nolimits_{i\in I}M_{m_{i}}(\mathbb{C})\, , \\
& \quad \ \alpha_{0} \qquad \qquad \qquad \alpha_{i} \\
\mathcal{N}& \cong \mathcal{N}_{0}\oplus\bigoplus\nolimits_{j\in J}M_{n_{j}}(\mathbb{C})\, , \\
& \quad \ \beta_{0} \qquad\qquad \quad \ \beta_{j}
\end{align*}
where $\mathcal{M}_{0}$ and $\mathcal{N}_{0}$ are diffuse von Neumann algebras. Then
\begin{align*}
\mathcal{M} \star \mathcal{N} & \cong \mathcal{L}(\mathbf{F}_{r})\oplus\bigoplus\nolimits_{(i,j)\in I\times J}M_{N(i,j)}(\mathbb{C}),\\
& \qquad \gamma \qquad \qquad \qquad \qquad \quad \ \gamma_{ij}
\end{align*}
where $N(i, j) :=\max\{m_{i}, n_{j}\},\,\gamma_{ij} :=N(i, j)^{2}\max\{\alpha_{i}m_{i}^{-2}+\beta_{j}n_{j}^{-2}-1,0\}$, and $r$ is determined so that $\mathbf{fdim} (\mathcal{M}\star \mathcal{N})= \mathbf{fdim} (\mathcal{M})+ \mathbf{fdim} (\mathcal{N})$. $($Note that $\gamma_{ij}>0$ implies $m_{i}=1$ or $n_{j}=1$, and that $\gamma_{ij}>0$ only for finitely many pairs $(i, j).)$
\end{theorem}

Let $\mathcal{M}$ and $\mathcal{N}$ be von Neumann algebras as in Theorem~\ref{ch07:the7.3.14}. Then the theorem implies that $\mathcal{M} \star \mathcal{N}$ is a factor if and only if
\begin{equation*}
\max_{i\in I}\frac{\alpha_{i}}{m_{i}^{2}}+\max_{j\in J}\frac{\beta_{j}}{n_{j}^{2}}\leq 1\,.
\end{equation*}
This tells us that the free product of finite (particularly finite-dimensional) von Neumann algebras mostly becomes a type $\mathrm{II}_{1}$ factor.

One may expect that the two notions of free entropy dimension and free dimension are naturally related to each other. The next theorem shows that this is indeed so when $a_{1},\ldots,a_{N}$ are free.

\begin{theorem}
\label{ch07:the7.3.15}
Let $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$ be in free relation. Then
\begin{equation*}
\delta(a_{1},\ldots,a_{N})= \mathbf{fdim} (\{a_{1},\ldots,a_{N}\}'')\,.
\end{equation*}
Moreover, if $\{a_{1},\ldots,a_{N}\}''$ is a factor, then
\begin{equation*}
\{a_{1},\ldots,a_{N}\}''\cong \mathcal{L}(\mathbf{F}_{\delta(a_{1},\ldots, a_{N})}),.
\end{equation*}
\end{theorem}

\begin{proof2}
First we show that $\delta(a)= \mathbf{fdim} (\{a\}'')$ for any $a\in \mathcal{M}^{sa}$. If $p_{1},p_{2}, \ldots$ are the atoms of $\{a\}''$ and $\alpha_{i}:=\tau(p_{i})$, then
\begin{align*}
\{a\}''& \cong \mathcal{L}(\mathbb{Z})\oplus \mathbb{C}p_{1}\oplus \mathbb{C}p_{2}\oplus\ldots, \\
& \qquad \alpha_{0} \qquad \alpha_{1} \qquad \alpha_{2}
\end{align*}
and Theorem~\ref{ch07:the7.3.2} implies that
\begin{equation*}
\delta(a)=1-\sum_{i\geq 1}\alpha_{i}^{2}=\alpha_{0}^{2}+\sum_{\begin{subarray}{c}i,j\geq 0 \\ i\neq j \end{subarray}} \alpha_{i}\alpha_{j}= \mathbf{fdim} (\{a\}'')\,.
\end{equation*}
Since $a_{1},\ldots,a_{N}$ are free, we can write
\begin{equation*}
\{a_{1},\ldots,a_{N}\}''=\star_{i=1}^{n}(\{a_{i}\}'',\,\tau|_{\{a_{i}\}''})\,.
\end{equation*}
Hence Corollary~\ref{ch07:cor7.3.4} (3) and Theorem~\ref{ch07:the7.3.13} imply that
\begin{equation*}
\delta(a_{1},\ldots,a_{N})=\sum_{i=1}^{N}\delta(a_{i})=\sum_{i=1}^{N} \mathbf{fdim} (\{a_{i}\}'')= \mathbf{fdim} (\{a_{1},\ldots,a_{N}\}'')\,.
\end{equation*}
Moreover, assume that $\{a_{1},\ldots,a_{N}\}''$ is a factor. Repeated application of Theorem~\ref{ch07:the7.3.13} implies that $\{a_{1},\ldots, a_{N}\}''\cong \mathcal{L}(\mathbf{F}_{t})$, where
\begin{equation*}
t :=\sum_{i=1}^{N} \mathbf{fdim} (\{a_{i}\}'')=\delta(a_{1},\ldots,a_{N})\,.
\end{equation*}
\end{proof2}

\section{Applications of free entropy}
\label{ch07:sec7.4}

\noindent The subject of this section is the study of applications of Voiculescu's free entropy to von Neumann algebras, in particular, to the structure theory on free group factors. The first success in this direction was obtained by Voiculescu himself. Let $\mathcal{M}$ be a general von Neumann algebra (always acting on a separable Hilbert space). A von Neumann subalgebra\index{regular von Neumann subalgebra} $\mathcal{A}$ of $\mathcal{M}$ is said to be \textit{regular} (in $\mathcal{M}$) if the \textit{normalizer}\index{normalizer}
\begin{equation*}
\mathcal{N}(\mathcal{A}):=\{u\in \mathcal{M} : \mathrm{unitary}, \ u\mathcal{A}u^{*}=\mathcal{A}\}
\end{equation*}
generates $\mathcal{M}$. When $\mathcal{A}$ is a \textit{maximal abelian subalgebra}\index{maximal abelian subalgebra} (abbreviated as $MASA$) of $\mathcal{M},\, \mathcal{A}$ is called a \textit{Cartan subalgebra}\index{Cartan subalgebra} if it is regular and there is a faithful normal conditional expectation from $\mathcal{M}$ onto $\mathcal{A}$ (this latter condition is automatically satisfied whenever $\mathcal{M}$ is of type $\mathrm{II}_{1}$). Note that in a type $\mathrm{II}_{1}$ factor a MASA is automatically diffuse (or nonatomic), i.e. it has no nonzero minimal projections.

Since a Cartan subalgebra of a type $\mathrm{II}_{1}$ factor is a regular and diffuse hyperfinite von Neumann subalgebra, the next theorem, due to Voiculescu, implies the absence of Cartan subalgebras in free group factors, providing a negative answer to the longstanding open question of whether every type $\mathrm{II}_{1}$ factor has a Cartan subalgebra. This means that free group factors cannot be realized by the measure space construction via a measurable equivalence relation (see [\citen{bib75}] for this construction and its relation with Cartan subalgebras).

\begin{theorem}
\label{ch07:the7.4.1}
Let $(\mathcal{M}, \tau)$ be a tracial $W^{*}$-probability space and $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$ with $N\geq 2$. If $\chi(a_{1},\ldots,a_{N})>-\infty$, then $\{a_{1},\ldots,a_{N}\}''$ does not have a regular and diffuse hyperfinite von Neumann subalgebra. In particular, the free group factor $\mathcal{L}(\mathbf{F}_{N})\ (2\leq N<\infty)$ does not have a regular and diffuse hyperfinite von Neumann subalgebra.
\end{theorem}

For the proof of this it is convenient to modify the free entropy of noncommutative selfadjoint multivariables. Let $a_{1},\ldots,a_{N}, b_{1},\ldots, b_{L}\in \mathcal{M}^{sa}$. For $n, r\in \mathbb{N}, \ \varepsilon >0$ and $R>0$ define
\begin{align*}
&  \Gamma_{R}(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L};n, r, \varepsilon) :=\{(A_{1},\ldots,A_{N})\in(M_{n}(\mathbb{C})^{sa})^{N}: \\
& \qquad \qquad \qquad  (A_{1},\ldots,A_{N}, B_{1},\ldots,B_{L})\in\Gamma_{R}(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{L};n, r, \varepsilon) \\
& \qquad \qquad \qquad \qquad \qquad \qquad \mathrm{for \ some} \ (B_{1},\ldots,B_{L})\in(M_{n}(\mathbb{C})^{sa})^{L}\}
\end{align*}
and
\begin{align*}
& \chi_{R}(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L}) \\
& \quad :=\lim_{\begin{subarray}{l}r\rightarrow\infty\\ \varepsilon\rightarrow+0 \\ \end{subarray}}\limsup_{n\rightarrow \infty}\left[\frac{1}{n^{2}}\log\Lambda\ (\Gamma_{R}(a_{1},\ldots,a_{N}:b_{1},\ldots, b_{L};n,r, \varepsilon))+\frac{N}{2}\log n\right].
\end{align*}
Then
\begin{equation}
\chi(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L}):=\sup_{R>0}\chi_{R}(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L})
\label{ch07:eqn7.4.1}
\end{equation}
is called the \textit{modified}\index{free entropy!modified} (or \textit{conditional}) \textit{free entropy}\index{conditional free entropy} of $(a_{1},\ldots,a_{N})$ in the presence of $(b_{1},\ldots,b_{L})$. The modified free entropy\index{modified free entropy} can be similarly defined when (some of) $b_{1},\ldots,b_{L}$ are non-selfadjoint. In fact, when $b_{1},\ldots,b_{K}$ are selfadjoint and $b_{K+1},\ldots,b_{L}$ are non-selfadjoint with $b_{j}=c_{j}+\mathrm{i}\ d_{j},\,c_{j},\,d_{j}\in \mathcal{M}^{sa}\ (K+1\leq j\leq L)$, we may define $\chi(a_{1},\ldots, a_{N}:b_{1},\ldots,b_{L})$ as
\begin{equation*}
\chi(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{K}, c_{K+1},\ldots,c_{L}, d_{K+1},\ldots,d_{L}).
\end{equation*}

The basic properties of $\chi(a_{1},\ldots,a_{N}\,:\,b_{1},\ldots,b_{L})$ are similar to those of $\chi(a_{1},\ldots,a_{N})$ given in Sec.~\ref{ch06:sec6.1}. Here we state some of them which will be used below. The next proposition can be shown in a way similar to Proposition~\ref{ch06:pro6.1.4}.

\begin{proposition}
\label{ch07:pro7.4.2}
$\chi(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L})=\chi_{R}(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L})$ whenever $ R>\Vert a_{i}\Vert, \Vert b_{j}\Vert$.
\end{proposition}

\begin{proposition}
\label{ch07:pro7.4.3}
If $c_{1},\ldots,c_{K}\in\{a_{1},\ldots,a_{N}, b_{1},\ldots,b_{L}\}''$, them
\begin{equation*}
\chi(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L})=\chi(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L}, c_{1},\ldots,c_{K})\,.
\end{equation*}
In particular, if $c_{1},\ldots,c_{K}\in\{a_{1},\ldots,a_{N}\}''$, then
\begin{equation*}
\chi(a_{1},\ldots,a_{N})=\chi(a_{1},\ldots,a_{N}:c_{1},\ldots,c_{K})\,.
\end{equation*}
\end{proposition}

\begin{proof2}
We need to show that
\begin{equation*}
\chi(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L})\leq\chi(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L}, c_{1},\ldots,c_{K})\,,
\end{equation*}
because the reverse inequality is obvious. By assumption one can choose selfadjoint noncommutative polynomials $P_{m,k}(X_{1},\ldots,X_{N}, Y_{1},\ldots, Y_{L})\ (m\in \mathbb{N},\,1\leq k\leq K)$ such that $c_{m.k} :=P_{m,k}(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{L})\rightarrow c_{k}$ strongly as $ m\rightarrow\infty$. Choose $R>\sup\{\Vert a_{i}\Vert,\,\Vert b_{j}\Vert,\,\Vert c_{m,k}\Vert:i, j, m, k\}$. Since an argument similar to the proof of Proposition~\ref{ch06:pro6.1.5} yields the upper semicontinuity
\begin{align*}
& \limsup_{m\rightarrow \infty}\chi_{R}(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L}, c_{m,1},\ldots,c_{m,K}) \\
& \qquad \leq\chi_{R}(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L}, c_{1},\ldots,c_{K})\,,
\end{align*}
it suffices by Proposition~\ref{ch07:pro7.4.2} to show that
\begin{equation}
\chi_{R}(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L})\leq\chi(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L}, c_{m,1},\ldots,c_{m,K})
\label{ch07:eqn7.4.2}
\end{equation}
for every $m\in \mathbb{N}$. For each $m, r\in \mathbb{N},\,\varepsilon >0$ and $R>0$ there exist $r_{1}\in \mathbb{N},\,\varepsilon_{1}>0$ and $R'\geq R$ such that, for every $n\in \mathbb{N}$, if $(A_{1},\ldots,A_{N},  B_{1},\ldots,B_{L})\in \Gamma_{R}(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{L};n, r_{1},\,\varepsilon_{1})$, then $\Vert P_{m,k}(A_{1},\ldots,A_{N}, B_{1},\ldots,B_{L})\Vert \leq R' \ (1  \leq  k\leq K)$ and
\begin{align*}
& (A_{1},\ldots,A_{N}, B_{1},\ldots,B_{L}, P_{m,1}(A_{1},\ldots,A_{N}, B_{1},\ldots,B_{L}), \ldots, \\
& \qquad \qquad \qquad \qquad \qquad \qquad \qquad P_{m,K}(A_{1},\ldots,A_{N}, B_{1},\ldots,B_{L})) \\
& \qquad \in\Gamma_{R'}(a_{1},\ldots,a_{N}, b_{1},\ldots,b_{L}, c_{m,1},\ldots,c_{m,K};n, r, \varepsilon)\,.
\end{align*}
This implies that
\begin{align*}
& \Gamma_{R}(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L};n, r_{1}, \varepsilon_{1}) \\
& \qquad \subset\Gamma_{R'}(a_{1},\ldots,a_{N}:b_{1},\ldots,b_{L}, c_{m,1},\ldots,c_{m,K};n, r, \varepsilon)\,.
\end{align*}
Hence (\ref{ch07:eqn7.4.2}) is obtained.
\end{proof2}

Now let $\mathcal{B}$ be a finite-dimensional $^{*}$-subalgebra of $\mathcal{M}$, and corresponding to the decomposition $\mathcal{B}\cong\bigoplus_{k=1}^{K}M_{d_{k}}(\mathbb{C})$ let us have a system of matrix units $\bigcup_{k=1}^{K}(e_{rs}^{(k)})_{1\leq r,s\leq d_{k}}$ in $\mathcal{B}$. Let $\Omega$ denote the union of the mixed strings of selfadjoint $(e_{rr}^{(k)})_{1\leq r\leq d_{k}}$ and non-selfadjoint $(e_{rs}^{(k)})_{1\leq r<s\leq d_{k}}$ for $1\leq k\leq K$. Then the subset $\hat{\Gamma}_{R}(\Omega;n, r, \varepsilon)$ of $\prod_{k=1}^{K}((M_{n}(\mathbb{C})^{sa})^{d_{k}}\times M_{n}(\mathbb{C})^{d_{k}(d_{k}-1)/2})$ is defined as in Sec.~\ref{ch06:sec6.5}. We state the following facts.

(I) For every $\rho>0$ there exists $\delta>0$ such that, for each $n\in \mathbb{N}$, if two systems of matrix units $\bigcup_{k=1}^{K}(E_{rs}^{(k)})_{1\leq r,s\leq d_{k}}$ and $\bigcup_{k=1}^{K}(F_{rs}^{(k)})_{1\leq r,s\leq d_{k}}$ in $M_{n}(\mathbb{C})$ satisfy
\begin{equation*}
|\mathrm{tr}_{n}(E_{rr}^{(k)})-\mathrm{tr}_{n}(F_{rr}^{(k)})|\leq\delta \qquad (1\leq r\leq d_{k},\ 1\leq k\leq K),
\end{equation*}
then
\begin{equation*}
\Vert UE_{rs}^{(k)}U^{*}-F_{rs}^{(k)}\Vert_{2}\leq\rho \qquad (1\leq r, s\leq d_{k}, 1\leq k\leq K)
\end{equation*}
for some $U\in \mathcal{U}(n)$.

(II) Let $\Omega$ be as above. For every $\delta>0$ and $R\geq 1$ there exist $r\in \mathbb{N}$ and $\varepsilon >0$ such that, for sufficiently large $n\in \mathbb{N}$, if
\begin{equation*}
((B_{rr}^{(k)})_{1\leq r\leq d_{k}},\ (B_{rs}^{(k)})_{1\leq r<s\leq d_{k}})_{1\leq k\leq K}\in\hat{\Gamma}_{R}(\Omega;n, r, \varepsilon)\,,
\end{equation*}
then there is a system of matrix units $\bigcup_{k=1}^{K}(E_{rs}^{(k)})_{1\leq r,s\leq d_{k}}$ in $M_{n}(\mathbb{C})$ such that
\begin{equation*}
\Vert E_{rs}^{(k)}-B_{rs}^{(k)}\Vert_{2}\leq\delta \qquad (1\leq r\leq s\leq d_{k},\ 1\leq k\leq K).
\end{equation*}

Above, $\Vert\,\cdot\,\Vert_{2}$ denotes the 2-norm with respect to $\mathrm{tr}_{n}$, i.e. $\Vert X\Vert :=\mathrm{tr}_{n}(X^{*}X)^{1/2}$ (also with respect to $\tau$ below). Note that any system of matrix units $\bigcup_{k=1}^{K}(F_{rs}^{(k)})_{1\leq r,s\leq d_{k}}$ in $M_{n}(\mathbb{C})$ is unitarily conjugate to $\bigcup_{k=1}^{K}(I_{m_{k}}\,\otimes\,E_{rs})_{1\leq r,s\leq d_{k}}$ corresponding to the embedding
\begin{equation*}
\bigoplus_{k=1}^{K}(M_{m_{k}}(\mathbb{C})\otimes M_{d_{k}}(\mathbb{C}))\oplus 0_{l}\subset M_{n}(\mathbb{C})\,,
\end{equation*}
where $\sum_{k=1}^{K}m_{k}d_{k}+l=n$ and $(E_{rs})_{1\leq r,s\leq d_{k}}$ is a system of usual matrix units of $M_{d_{k}}(\mathbb{C})$. The proof of (I) is elementary. On the other hand, (II) can be shown, for instance, by induction on $d :=\sum_{k=1}^{K}d_{k}$ and using Lemma~\ref{ch04:lem4.3.4}. The details are left to the reader.

Here is one more result which will play an essential role in the proof of Lemma~\ref{ch07:lem7.4.5}. This is due to Szarek ([\citen{bib184}], [\citen{bib185}]).

\begin{lemma}
\label{ch07:lem7.4.4} For every $\delta>0$ and $n\in \mathbb{N}$ there is a $\delta$-net $(U_{t})_{t\in T(n)}$ in $\mathcal{U}(n)$ in the operator norm such that $\# T(n)\leq(C/\delta)^{n^{2}}$, where $C$ is a universal constant.
\end{lemma}

The main technical result to prove Theorem~\ref{ch07:the7.4.1} is the following estimate of free entropy.

\begin{lemma}
\label{ch07:lem7.4.5}
Let $a_{1},\ldots,a_{N}\,\in\,\mathcal{M}^{sa}$, and let $\mathcal{B}$ be a finite-dimensional $^{*}$-subalgebra of $\{a_{1},\ldots,a_{N}\}''$. Assume that there are $0<\theta<1,\ a_{ij}\in \mathcal{M}$ and projections $p_{ij}, q_{ij}\in \mathcal{B} \ (1\leq i\leq N, 1\leq j\leq K(i))$ such that
\begin{align}
& a_{ij} =p_{ij}a_{ij}q_{ij} \qquad (1\leq i\leq N, 1\leq j\leq K(i)), \label{ch07:eqn7.4.3}\\
& \left\Vert a_{i}-\sum_{j=1}^{K(i)}(a_{ij}+a_{ij}^{*})\right\Vert_{2}<\theta \qquad (1\leq i\leq N),
\label{ch07:eqn7.4.4} \\
& 2 \sum_{i=1}^{N}\sum_{j=1}^{K(i)}\tau(p_{ij})\tau(q_{ij})<\theta\,.
\label{ch07:eqn7.4.5}
\end{align}
Then
\begin{equation*}
\chi(a_{1},\ldots,a_{N})\leq N\log(CR)+(N-1-\theta)\log\theta\,,
\end{equation*}
where $C$ is a universal constant and $R:=1+\max_{1\leq i\leq N}\Vert a_{i}\Vert_{2}$.
\end{lemma}

\begin{proof2}
Each $a_{ij}$ can be replaced by its conditional expectation onto $\{a_{1},\ldots,a_{N}\}''$, so we can assume $a_{ij}\in\{a_{1},\ldots,a_{N}\}''$. Furthermore, we may assume by approximation that each $a_{ij}$ is a noncommutative polynomial $P_{ij} (a_{1},\ldots,a_{N})$ of $a_{1},\ldots,a_{N}$.

Let $\Omega$ be given as above for $\mathcal{B}$. The above (I) implies that for every $\rho>0$ there exists $\delta>0$ such that if $\phi, \phi': \mathcal{B}\rightarrow M_{n}(\mathbb{C})$ are $^{*}$-homomorphisms (not necessarily unital) and $|\mathrm{tr}_{n}(\phi(e))-\mathrm{tr}_{n}(\phi'(e))|\leq 3\delta$ for any $e\in\Omega$, then one has $U\in \mathcal{U}(n)$ satisfying
\begin{equation*}
\Vert U(\phi(b))U^{*}-\phi'(b)\Vert_{2}\leq\rho \qquad (b\in \mathcal{B},\,\Vert b\Vert\, \leq 1).
\end{equation*}
For this $\delta>0$ and the $R\geq 1$ given in the lemma, choose $r\in \mathbb{N}$ and $ 0<\varepsilon <\delta$ for which the assertion of the above fact (II) holds.

For any large $n$ one can choose a fixed $^{*}$-homomorphism $\phi_{n}:\mathcal{B}\rightarrow M_{n}(\mathbb{C})$ such that
\begin{equation}
|\mathrm{tr}_{n}(\phi_{n}(e))-\tau(e)|\leq\delta \qquad (e\in\Omega).
\label{ch07:eqn7.4.6}
\end{equation}
Assume that
\begin{equation}
(A_{1},\ldots,A_{N}, (B(e))_{e\in\Omega})\in\Gamma_{R}(a_{1},\ldots,a_{N},  \Omega;n, r, \varepsilon)\, .
\label{ch07:eqn7.4.7}
\end{equation}
Then, due to the assertion of (II), there is a $^{*}$-homomorphism $\phi : \mathcal{B}\rightarrow M_{n}(\mathbb{C})$ such that
\begin{equation}
\Vert\phi(e)-B(e)\Vert_{2}\leq\delta \qquad (e\in\Omega).
\label{ch07:eqn7.4.8}
\end{equation}
Combining (\ref{ch07:eqn7.4.6})--(\ref{ch07:eqn7.4.8}), we get $|\mathrm{tr}_{n}(\phi(e))-\mathrm{tr}_{n}(\phi_{n}(e))|\leq 3\delta$ for any $ e\in\Omega$, so there exists $U\in \mathcal{U}(n)$ such that
\begin{equation}
\Vert U(\phi(b))U^{*}-\phi_{n}(b)\Vert_{2}\leq\rho \qquad (b\in \mathcal{B}, \, \Vert b\Vert\leq 1).
\label{ch07:eqn7.4.9}
\end{equation}
When $\rho$ (hence $\delta, \varepsilon)$ is small enough and $r$ is large enough, we estimate, for $ 1\leq i\leq N$,
\begin{equation}
\left\Vert A_{i}-\sum_{j=1}^{K(i)}(P_{ij}(A_{1},\ldots,A_{N})+P_{ij}(A_{1},\ldots,A_{N})^{*})\right\Vert_{2}<\theta
\label{ch07:eqn7.4.10}
\end{equation}
from (\ref{ch07:eqn7.4.4}) with $a_{ij}=P_{ij}(a_{1},\ldots,a_{N})$. On the other hand, since $p_{ij}$ and $q_{ij}$ are written as linear combinations of $e, e^{*}\ (e\in\Omega)$, (\ref{ch07:eqn7.4.7}) and (\ref{ch07:eqn7.4.8}) provide an approximation of $(a_{ij},p_{ij}, q_{ij})$ by $(P_{ij}(A_{1},\ldots, A_{N}), \phi(p_{ij}), \phi(q_{ij}))$ in their joint moments. So, according to (\ref{ch07:eqn7.4.3}), we estimate
\begin{equation}
\Vert P_{ij} (A_{1},\ldots,A_{N})-\phi(p_{ij})P_{ij}(A_{1},\ldots,A_{N})\phi(q_{ij})\Vert_{2}<\frac{\theta}{2K(i)}
\label{ch07:eqn7.4.11}
\end{equation}
for all $1\leq i\leq N, \, 1\leq j\leq K(i)$. Combining the estimates (\ref{ch07:eqn7.4.10}), (\ref{ch07:eqn7.4.11}) and (\ref{ch07:eqn7.4.9}) (for $\rho$ sufficiently small), we infer that
\begin{equation*}
\left\Vert UA_{i}U^{*}-\sum_{j=1}^{K(i)}(X_{ij}+X_{ij}^{*})\right\Vert_{2}<3\theta \qquad (1\leq i\leq N)
\end{equation*}
for some $U\in \mathcal{U}(n)$ and $X_{ij}\in\phi_{n}(p_{ij})M_{n}(\mathbb{C})\phi_{n}(q_{ij})$. Set
\begin{equation*}
\mathcal{W}_{n}:=\left\{\left(\sum_{j=1}^{K(i)}(X_{ij}+X_{ij}^{*})\right)_{1\leq i\leq N}:X_{ij}\in\phi_{n}(p_{ij})M_{n}(\mathbb{C})\phi_{n}(q_{ij})\right\},
\end{equation*}
which is a subspace of the $Nn^{2}$-dimensional Euclidean space $(M_{n}(\mathbb{C})^{sa})^{N}$. The dimension of $\mathcal{W}_{n}$ is not greater than
\begin{equation*}
2n^{2}\sum_{i=1}^{N}\sum_{j=1}^{K(i)}(\tau(p_{ij})+\delta)(\tau(q_{ij})+\delta)<n^{2}\theta
\end{equation*}
(for $\delta$ small enough) according to (\ref{ch07:eqn7.4.5}). In this way, we have shown that if $n, r$ are large and $\varepsilon$ is small, then for every $(A_{1},\ldots,A_{N})\in\Gamma_{R}(a_{1},\ldots,a_{N}:\Omega;n, r, \varepsilon)$ there exist $U\in \mathcal{U}(n)$ and $(X_{1},\ldots,X_{N})\in \mathcal{W}_{n}$ such that $\Vert UA_{i}U^{*}-X_{i}\Vert_{2}<3\theta\ (1\leq i\leq N)$.

Now apply Lemma~\ref{ch07:lem7.4.4} to get a $\theta/2R$-net $(U_{t})_{t\in T(n)}$ in $\mathcal{U}(n)$ so that $\# T(n)\leq (CR/\theta)^{n^{2}}$. Then for every $(A_{1},\ldots,A_{N})\in\Gamma_{R}(a_{1},\ldots,a_{N}:\Omega;n, r, \varepsilon)$ we have
\begin{equation}
\mathrm{dist} ((A_{1},\ldots,A_{N}), U_{t,N}^{*}\mathcal{W}_{n}U_{t,N})<(Nn)^{1/2}\cdot 4\theta
\label{ch07:eqn7.4.12}
\end{equation}
for some $t\in T(n)$, where dist is the distance in $(M_{n}(\mathbb{C})^{sa})^{N}$ with respect to the Euclidean norm and $U_{t,N} :=\bigoplus_{1}^{N}U_{t}$. Enlarge $\mathcal{W}_{n}$ to a subspace $\tilde{\mathcal{W}}_{n}$ of dimension $[n^{2}\theta]$ in $(M_{n}(\mathbb{C})^{sa})^{N}$. Note that the Euclidean norm of the above $(A_{1},\ldots,A_{N})$ is $(\sum_{i=1}^{N} \, \mathrm{Tr}(A_{i}^{*}A_{i}))^{1/2}\leq(Nn)^{1/2}R$. Let $\mathbf{B}_{n}$ be the ball of radius $(Nn)^{1/2}R$ and center $0$ in $\tilde{\mathcal{W}}_{n}$, and $\mathbf{B}_{n}'$ the ball of radius $(Nn)^{1/2}\cdot 4\theta$ and center $0$ in the orthogonal complement of $\tilde{\mathcal{W}}_{n}$ in $(M_{n}(\mathbb{C})^{sa})^{N}$. The estimate (\ref{ch07:eqn7.4.12}) means that
\begin{equation*}
\Gamma_{R}(a_{1},\ldots,a_{N}:\Omega;n, r, \varepsilon)\subset\bigcup_{t\in T(n)}U_{t,N}^{*}(\mathbf{B}_{n}\oplus \mathbf{B}_{n}')U_{t,N}
\end{equation*}
whenever $n, r$ are large enough and $\varepsilon$ is small enough. Therefore,
\begin{align*}
& \Lambda(\Gamma_{R}(a_{1},\ldots,a_{N}:\Omega;n, r, \varepsilon)) \\
& \qquad \leq\# T(n)\cdot \mathrm{vol}_{[n^{2}\theta]}(\mathbf{B}_{n}) \mathrm{vol}_{Nn^{2}-[n^{2}\theta]}(\mathbf{B}_{n}') \\
& \qquad \leq\left(\frac{CR}{\theta}\right)^{n^{2}}\frac{\pi^{Nn^{2}/2}(Nn)^{Nn^{2}/2}R^{[n^{2}\theta]}(4\theta)^{Nn^{2}-[n^{2}\theta]}}{\Gamma\left(1+\frac{[n^{2}\theta]}{2}\right)\Gamma\left(1+\frac{Nn^{2}-[n^{2}\theta]}{2}\right)}.
\end{align*}
Using Propositions~\ref{ch07:pro7.4.3}, \ref{ch07:pro7.4.2} and the Stirling formula, we have
\begin{align*}
& \chi(a_{1},\ldots,a_{N})=\chi_{R}(a_{1},\ldots,a_{N}:\Omega) \\
& \qquad \leq\limsup_{n\rightarrow \infty}\left[\frac{1}{n^{2}}\log\Lambda(\Gamma_{R}(a_{1},\ldots,a_{N}:\Omega;n, r, \varepsilon))+\frac{N}{2}\log n\right] \\
& \qquad =\limsup_{n\rightarrow \infty}\left[\log\left(\frac{CR}{\theta}\right)+\frac{N}{2}\log\pi+\frac{N}{2}\log(Nn)\right. \\
& \qquad \qquad \qquad \qquad +\theta\log R+(N-\theta)\log(4\theta)-\frac{\theta}{2}\log\frac{n^{2}\theta}{2}+\frac{\theta}{2} \\
& \qquad \qquad \qquad \qquad \left.-\frac{N-\theta}{2}\log\frac{n^{2}(N-\theta)}{2}+\frac{N-\theta}{2}+\frac{N}{2}\log n\right] \\
& \qquad=\log(CR)+\theta\log R+(N-\theta)\log 4+\frac{N}{2}\log(2\pi e) \\
& \qquad \qquad +(N-1-\theta)\log\theta-\frac{N}{2}\left(\frac{\theta}{N}\log\frac{\theta}{N}+\frac{N-\theta}{N}\log\frac{N-\theta}{N}\right) \\
& \qquad\leq\log(CR)+N\log(R+4)+\frac{N}{2}\log(4\pi e)+(N-1-\theta)\log\theta \\
& \qquad\leq N\log(C_{1}R)+(N-1-\theta)\log\theta\,,
\end{align*}
where $C_{1}$ is another universal constant. Here the inequality
\begin{equation*}
-t\log t-(1-t)\log(1-t)\leq\log 2
\end{equation*}
$(0<t<1)$ was used.
\end{proof2}

Now we are in a position to complete the proof of the theorem.

\begin{proof1}[Proof of Theorem~\ref{ch07:the7.4.1}]
Assume that $\tilde{\mathcal{M}}:=\{a_{1},\ldots,a_{N}\}''$ has a regular and diffuse hyperfinite von Neumann subalgebra $\mathcal{A}$. For any $\theta>0$ let us show that one can choose a finite-dimensional $^{*}$-subalgebra $\mathcal{B}\subset \mathcal{A}, \, a_{ij}\in\tilde{\mathcal{M}}$ and projections $p_{ij}, q_{ij}\in \mathcal{B}$ with $a_{ij}=p_{ij}a_{ij}q_{ij}\ (1\leq i\leq N, \, 1\leq j\leq K(i))$ such that (\ref{ch07:eqn7.4.4}) and (\ref{ch07:eqn7.4.5}) of Lemma~\ref{ch07:lem7.4.5} are satisfied. When this is shown, Lemma~\ref{ch07:lem7.4.5} implies that
\begin{equation*}
\chi(a_{1},\ldots,a_{N})\leq N\log(CR)+(N-1-\theta)\log\theta\,,
\end{equation*}
so we obtain $\chi(a_{1},\ldots,a_{N})=-\infty$, a contradiction.

Since the linear span of the normalizer $\mathcal{N}(\mathcal{A})$ (in $\tilde{\mathcal{M}}$) is dense in $\tilde{\mathcal{M}}$ in the strong operator topology (hence in the 2-norm), it suffices to prove the similar assertion for any $u_{1},\ldots,u_{M}\in \mathcal{N}(\mathcal{A})$ instead of $a_{1},\ldots,a_{N}$. Given $\theta>0$, since $\mathcal{A}$ is diffuse, choose $L\in \mathbb{N}$ with $ L>M/\theta$ and projections $\tilde{p}_{1},\ldots, \tilde{p}_{L}\in \mathcal{A}$ such that $\tilde{p}_{1}+\cdots+\tilde{p}_{L}=\mathbf{1}$ and $\tau(\tilde{p}_{j})=1/L\ (1\leq j\leq L)$. If we set $\tilde{q}_{ij} :=u_{i}^{*}\tilde{p}_{j}u_{i}\in \mathcal{A}$, then $\sum_{j=1}^{L}\tilde{p}_{j}u_{i}\tilde{q}_{ij}=u_{i}$ and
\begin{equation*}
\sum_{i=1}^{M}\sum_{j=1}^{L}\tau(\tilde{p}_{j})\tau(\tilde{q}_{ij})=\frac{M}{L}<\theta\,.
\end{equation*}
Since $\mathcal{A}$ is hyperfinite, one can choose a finite-dimensional $^{*}$-subalgebra $\mathcal{B}\subset \mathcal{A}$ and projections $p_{j}, q_{ij}\in \mathcal{B}\ (1\leq i\leq M,\, 1\leq j\leq L)$ such that $\Vert p_{j}-\tilde{p}_{j}\Vert_{2}$ and $\Vert q_{ij}-\tilde{q}_{ij}\Vert_{2}$ are sufficiently small so that
\begin{equation*}
\left\Vert u_{i}-\sum_{j=1}^{L}p_{j}u_{i}q_{ij}\right\Vert_{2}<\theta,\ \ \sum_{i=1}^{M}\sum_{j=1}^{L}\tau(p_{j})\tau(q_{ij})<\theta.
\end{equation*}
Now the assertion follows with $a_{ij}:=p_{j}u_{j}q_{ij}$ and $p_{ij}:=p_{j}$.
\end{proof1}

It is known ([\citen{bib205}]) that if $a_{1},\ldots,a_{N}\in\mathcal{M}^{sa}\ (N\geq 2)$ and $\chi(a_{1},\ldots,a_{N})> -\infty$, then $\{a_{1},\ldots,a_{N}\}''$ is a factor and moreover it is not hyperfinite. In fact, by estimating $\chi(a_{1},\ldots,a_{N}:p, \mathbf{1}-p)$ for a projection $p\in\mathcal{M}$ in terms of the quantities $\max_{i}\Vert pa_{i}-a_{i}p\Vert_{2}$ and $\tau(p)$, Voiculescu proved a stronger result: $\{a_{1},\ldots,a_{N}\}''$ is a \textit{non}-$\Gamma\, II_{1}$ \textit{factor}\index{non-$\Gamma \mathrm{II}_{1}$ factor} (i.e. it has no non-trivial central sequences) in the above case. This fact tells us that the free entropy method could be useful in a really non-hyperfinite situation.

There are some other applications of the free entropy method to free group factors. For instance, the next theorem, due to L. Ge, shows another remarkable property of free group factors.

\begin{theorem}
\label{ch07:the7.4.6}
Let $\mathcal{M}$ be a type $II_{1}$ factor\index{factor!prime} generated by $a_{1},\ldots,a_{N}\in\mathcal{M}^{sa}$ with $N\geq 2$. If $\chi(a_{1},\ldots,a_{N})>-\infty$, then $\mathcal{M}$ is prime,\index{prime factor} that is, $\mathcal{M}$ is not isomorphic to the tensor product of any two factors of type $II_{1}$. In particular, the free group factor $\mathcal{L}(\mathbf{F}_{N})\ (2\leq N<\infty)$ is prime.
\end{theorem}

In particular, the theorem says that $\mathcal{L}(\mathbf{F}_{N})$ is not isomorphic to $\mathcal{L}(\mathbf{F}_{N})\otimes \mathcal{L}(\mathbf{F}_{N})$, and it answers Sakai's question of whether $\mathcal{M}\cong \mathcal{M} \otimes\mathcal{M}$ for any type $\mathrm{II}_{1}$ factor $\mathcal{M}$.

It is not so difficult to see that any type $\mathrm{II}_{1}$ factor is generated by a sequence of projections with trace 1/2. So Theorem~\ref{ch07:the7.4.6} is a consequence of the following estimate of free entropy. The proof of this follows a pattern similar to that of Lemma~\ref{ch07:lem7.4.5}, although it is more complicated. The details are omitted here.

\begin{lemma}
\label{ch07:lem7.4.7}
Let $\mathcal{M}$ be a type $II_{1}$ factor, and let $\mathcal{R}_{1}, \mathcal{R}_{2}$ be mutually commuting hyperfinite subfactors of $\mathcal{M}$. Let $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}\ (N\geq 2)$ be such that $\mathcal{M} =\{a_{1},\ldots,a_{N}\}''$. Assume that there are $\theta>0$, projections $p_{k}\in \mathcal{R}_{1}' (1\leq k\leq K)$, $q_{l}\in \mathcal{R}_{2}'\ (1\leq l\leq L)$, and selfadjoint noncommutative polynomials $P_{i}(X_{1},\ldots,X_{K}, Y_{1},\ldots,Y_{L})\ (1\leq i\leq N)$ such that
\begin{align*}
& \tau(p_{k})=\tau(q_{l})=\frac{1}{2} \qquad (1 \leq k\leq K, \, 1\leq l\leq L), \\
& \Vert a_{i}-P_{i}(p_{1},\ldots,p_{K}, q_{1},\ldots,q_{L})\Vert_{2}<\theta \qquad (1 \leq i\leq N).
\end{align*}
Then
\begin{equation*}
\chi(a_{1},\ldots,a_{N})\leq N\log(CR)+(n-1-\theta)\log\theta\,,
\end{equation*}
where $C$ is a universal constant and $ R:=1+\max_{1\leq i\leq N}\Vert a_{i}\Vert$.
\end{lemma}

\subsection*{Notes and Remarks}
When $G$ is a discrete countable group, it is known that the group von Neumann algebra $\mathcal{L}(G)$ is hyperfinite if and only if $G$ is amenable. For instance, the group $\mathbf{S}_{\infty}$ of all finite permutations on $\mathbb{N}$ is an amenable ICC group, so $\mathcal{L}(\mathbf{S}_{\infty})$ is another realization of $R$. Free group factors $\mathbf{F}_{n}\ (2\leq n\leq \infty)$ and $SL(n, \mathbb{Z})$ are among typical examples of non-amenable ICC groups. The isomorphism question of whether $\mathcal{L}(\mathbf{F}_{n})\not\cong \mathcal{L}(\mathbf{F}_{m})$ if $n\neq m$ has been one of the most famous problems in theory of operator algebras since the early 1950's. Besides the free group factors, the reduced free group $C^{*}$-algebra $C_{r}^{*}(\mathbf{F}_{n})$ is defined as the norm closure of the linear span of the left regular representation $\{L_{g}:g\in \mathbf{F}_{n}\}$. Non-isomorphism $C_{r}^{*}(\mathbf{F}_{n})\not\cong C_{r}^{*}(\mathbf{F}_{m})\ (n\neq m)$ is seen from the difference of the K$_{1}$-groups ([\citen{bib150}]). The Fock space representation in Theorem~\ref{ch07:the7.1.4} may be also used to construct the Cuntz algebras $\mathcal{O}_{n}$. In fact, if $\dim \mathcal{H}=n<\infty$, then the $C^{*}$-algebra $C^{*}(\ell(\mathcal{H}))$ generated by $\{\ell(f):f\in  \mathcal{H}\}$ includes $K(\mathcal{F}(\mathcal{H}))$, the compact operators on $\mathcal{F}(\mathcal{H})$, and $C^{*}(\ell(\mathcal{H}))/K(\mathcal{F}(\mathcal{H}))\cong \mathcal{O}_{n}$, and if $\dim \mathcal{H}=\infty$ then $C^{*}(\ell(\mathcal{H}))\cong \mathcal{O}_{\infty}$ (cf. [\citen{bib74}]).

The formulas (\ref{ch07:eqn7.2.1}) and (\ref{ch07:eqn7.2.2}) were obtained in [\citen{bib199}], the paper which opened the promising study on free group factors, by using machinery of (semi)circular systems. Theorem~\ref{ch07:the7.2.4}, along with (\ref{ch07:eqn7.2.3}), was given in [\citen{bib61}], while Theorem~\ref{ch07:the7.2.6} is taken from [\citen{bib63}]. The one-parameter family $\mathcal{L}(\mathbf{F}_{r})\ (1<r\leq\infty)$ of interpolated free group factors was independently constructed in [\citen{bib158}] and [\citen{bib66}]. Corollaries 7.2.10 and 7.2.11 were first proven in [\citen{bib155}] and [\citen{bib157}], respectively, before construction of interpolated free group factors. Moreover, it is remarkable that a trace-scaling continuous one-parameter automorphism group of $\mathcal{L}(\mathbf{F}_{\infty})\otimes B(\mathcal{H})$ was constructed in [\citen{bib156}] by using semicircular systems; thus another proof of Corollary 7.2.10 is provided.

The distribution of $pqp$ in Lemma~\ref{ch07:lem7.2.5} can be more systematically computed by using the $\mathcal{S}$-transform formula for the multiplicative free convolution. In fact, it was shown in [\citen{bib198}] that if $\{p, q\}$ is a free pair of projections such that $\tau(p)=\alpha$ and $\tau(q)=\beta$, where $ 0<\alpha, \beta<1$, then the distribution of $pqp$ in $(\mathcal{M}, \tau)$ is
\begin{equation*}
c_{0}\delta(0)+c_{1}\delta(1)+\chi_{(r,s)}(t)\frac{\sqrt{(t-r)(s-t)}}{2\pi t(1-t)}dt\,,
\end{equation*}
where $c_{0}:=1-\min\{\alpha, \beta\},\, c_{1}:=\max\{\alpha+\beta-1,0\}$ and
\begin{equation*}
s, r:=\alpha+\beta-2\alpha\beta\pm\sqrt{4\alpha\beta(1-\alpha)(1-\beta)}\,.
\end{equation*}
Moreover, let $\{x, y\}$ be a free pair of positive elements in $(\mathcal{M}, \tau)$ and $\mu,  \nu$ the distributions of $x, y$. Then the following is not difficult to show:
\begin{equation*}
\left\{\begin{array}{l}
\Vert x^{1/2}yx^{1/2}\Vert=\Vert x\Vert\Vert y\Vert \quad \mathrm{if}\ \mu(\{\Vert x\Vert\})+\nu(\{\Vert y\Vert\})\geq 1,\\
\\
\Vert x^{1/2}yx^{1/2}\Vert<\Vert x\Vert\Vert y\Vert \quad \mathrm{otherwise}.
\end{array}\right.
\end{equation*}
Compare this with the remark after Example~\ref{ch03:exa3.2.3}.

The free entropy dimension was introduced in [\citen{bib203}], and the subordination inequalities (\ref{ch07:eqn7.3.1}) and (\ref{ch07:eqn7.3.2}) were also obtained there. Lemma~\ref{ch07:lem7.3.6} on restricted Minkowski sums is from [\citen{bib186}], where the free entropy power inequality (for the single variable case) was proven. The contents of Propositions~\ref{ch07:pro7.3.7}--\ref{ch07:pro7.3.10} are new, but the ideas are from [\citen{bib208}]. Propositions~\ref{ch07:pro7.3.11} and \ref{ch07:pro7.3.12} are from [\citen{bib203}], with improvements.

The terminology of f.d.a. is due to Voiculescu [\citen{bib208}]. A fundamental open problem is whether $(a_{1},\ldots,a_{N})$ has f.d.a. for every tracial $W^{*}$-probability space $(\mathcal{M}, \tau)$ and every $a_{1},\ldots,a_{N}\in \mathcal{M}^{sa}$. As was pointed out in [\citen{bib203}], this problem is equivalent to Connes' approximate embedding problem of whether any countably generated type $\mathrm{II}_{1}$ factor can be approximately embedded into the hyperfinite $\mathrm{II}_{1}$ factor ([\citen{bib52}], p. 105). R\u{a}dulescu's work [\citen{bib160}] is toward the affirmative solution to this problem. Theorem~\ref{ch07:the7.3.9} says that if the solution is affirmative, then the free entropy dimension\index{free entropy!dimension, modified} $\delta(a_{1},\ldots,a_{N})$ always admits a nonnegative value (i.e. the exceptional value $-\infty$ is excluded).

In [\citen{bib205}], along with the modified free entropy (\ref{ch07:eqn7.4.1}), the \textit{ modified free entropy dimension}\index{modified free entropy dimension} was introduced as
\begin{equation*}
\delta_{0}(a_{1},\ldots,a_{N}):=N+\limsup_{\varepsilon \rightarrow +0}\frac{\chi(a_{1}+\varepsilon S_{1},\ldots,a_{N}+\varepsilon S_{N}:S_{1},\ldots,S_{N})}{|\log\varepsilon|}.
\end{equation*}
The inequality $\delta_{0} (a_{1},\ldots,a_{N})\leq\delta(a_{1},\ldots,a_{N})$ is clear. The two concepts $\delta_{0}$ and $\delta$ have similar properties and coincide in some cases (in particular, $\delta_{0}(a)=\delta(a)$); however, $\delta_{0}=\delta$ in general is not known. It seems that $\delta_{0}$ is technically more convenient than $\delta$.

The free dimension of certain von Neumann algebras was introduced in [\citen{bib62}], and the proofs of Theorems~\ref{ch07:the7.3.13} and \ref{ch07:the7.3.14} are found there.

When $\chi(a_{1},\ldots,a_{N})>-\infty$, the free information $\Phi(a_{1},\ldots,a_{N})$ in the microstates approach may be defined in the following way:
\begin{equation*}
\Phi(a_{1},\ldots,a_{N}):=2\limsup_{\varepsilon\rightarrow +0}\frac{\chi(a_{1}+\sqrt{\varepsilon}S_{1},\ldots,a_{N}+\sqrt{\varepsilon}S_{N})-\chi(a_{1},\ldots,a_{N})}{\varepsilon}
\end{equation*}
with a semicircular system $(S_{1},\ldots,S_{N})$ free from $\{a_{1},\ldots,a_{N}\}$. For the single variable case this definition coincides with the free information in [\citen{bib202}]. The function $t\mapsto\chi(a_{1}+\sqrt{t}S_{1},\ldots,a_{N}+\sqrt{t}S_{N})$ is increasing and right-continuous on $[0, \infty)$. An interesting question is whether this function is concave on $[0, \infty)$. If it is, then one can express $\chi(a_{1},\ldots,a_{N})$ in terms of the above defined free information:
\begin{align*}
& \chi(a_{1},\ldots,a_{N})\\
& \qquad =\frac{1}{2}\int_{0}^{\infty} \left(\frac{N}{1+t}-\Phi(a_{1}+\sqrt{t}S_{1},\ldots,a_{N}+\sqrt{t}S_{N})\right)dt+\frac{N}{2}\log(2\pi e),
\end{align*}
and the expression has completely the same form as in the microstates-free approach (see the Notes and Remarks section of the previous chapter).

The first application of the (modified) free entropy method to von Neumann algebra theory was given in [\citen{bib205}], where Theorem~\ref{ch07:the7.4.1} was proven. Moreover, the same assertion of Theorem~\ref{ch07:the7.4.1} for the interpolated free group factors $\mathcal{L}(\mathbf{F}_{r})\ (1<r\leq\infty)$ was proven there. Theorem~\ref{ch07:the7.4.6}, due to L. Ge [\citen{bib81}], solved the longstanding question about the existence of prime factors. In [\citen{bib80}] he proved that the free group factors (and the interpolated free group factors) have no simple MASA. A slightly stronger result was given in Dykema [\citen{bib66}]. Furthermore, Theorem~\ref{ch07:the7.4.6} was strengthened by M.B. \c{S}tefan [\citen{bib182}]; he proved that any subfactor of $\mathcal{L}(\mathbf{F}_{r})$ with finite Jones index (in particular, $\mathcal{L}(\mathbf{F}_{r})$ itself) is prime. A further application is found in [\citen{bib82}].

Finally, some recent developments on (amalgamated) free products of operator algebras are briefly surveyed. Free product von Neumann algeras with respect to non-tracial states were studied in [\citen{bib13}], [\citen{bib64}], [\citen{bib65}], [\citen{bib194}], and it turned out that when $\varphi_{1}$ or $\varphi_{2}$ is not tracial, the free product von Neumann algebra $(\mathcal{M}_{1}, \varphi_{1})\star(\mathcal{M}_{2}, \varphi_{2})$ is mostly a factor of type $\mathrm{III}_{\lambda}\ (0<\lambda\leq 1)$ and it is usually \textit{ full} (in the type $\mathrm{II}_{1}$ case, the fullness is equivalent to non-$\Gamma$). Amalgamated free products of von Neumann algebras were first treated in [\citen{bib152}] with respect to tracial states, and the precise definition of those with respect to general normal states is found in [\citen{bib193}]. The technique of (amalgamated) free products is sometimes useful in constructing (sub)factors or actions on them with specified properties. The first success in this direction was made by S. Popa [\citen{bib152}], who constructed an irreducible type $\mathrm{II}_{1}$ subfactor with Jones index $s$ for any $s\in\{4\cos^{2}(\pi/n):n\geq 3\}\cup[4, \infty)$. (See also [\citen{bib34}], [\citen{bib153}].) This method was further exemplified in [\citen{bib158}], where subfactors of $\mathcal{L}(\mathbf{F}_{\infty})$ with finite Jones index were constructed. The free product construction was adopted in [\citen{bib192}] to show the existence of a minimal coaction of the compact quantum group $SU_{q}(n)$ on a full factor\index{full factor}\index{factor!full} of type $\mathrm{III}_{q^{2}}$.

Much work has been done on (amalgamated) free products of $C^{*}$-algebras. Only a few of those results are mentioneded here. In [\citen{bib69}] it was shown that the reduced $C^{*}$-free product $(\mathcal{A}_{1}, \varphi_{1})\star(\mathcal{A}_{2}, \varphi_{2})$ has stable rank 1 whenever the $\mathcal{A}_{i}\ (i=1,2)$ are $C^{*}$-algebras with faithful tracial states $\varphi_{i}$ and satisfy the Avitzour condition. [\citen{bib67}] is related. Dykema [\citen{bib68}] proved that the reduced amalgamated free product of exact $C^{*}$-algebras is exact. In [\citen{bib169}] and [\citen{bib170}], Shlyakhtenko studied the $C^{*}$-probability space $(\Gamma(\mathcal{H}_{\mathbb{R}},U_{t}),\varphi_{U})$ associated with a one-parameter orthogonal transformation group $U_{t}$ on a real Hilbert space $\mathcal{H}_{\mathbb{R}}$. The construction of $\varphi_{U}$ is a free analogue of the construction of quasi-free states on the CAR and CCR algebras, and the corresponding von Neumann algebra $\Gamma(\mathcal{H}_{\mathbb{R}}, U_{t})''$ via the GNS representation is a free analogue of the Araki-Woods factors.

The compression formula $(\mathcal{M}\star \mathcal{L}(\mathbf{F}_{r}))_{t}\cong\mathcal{M}_{t}\star \mathcal{L}(\mathbf{F}_{r/t^{2}})$ for any $\mathrm{II}_{1}$ factor $\mathcal{M},  1<r\leq\infty$ and $0<t<1$ was shown in [\citen{bib170}]. This contains Corollary~\ref{ch07:cor7.2.10}. A more extended compression formula for free products of von Neumann algebras can be found in [\citen{bib70}].

\backmatter

\begin{thebibliography}{999}
\bibitem[1]{bib1} L. Accardi, Y. Hashimoto and N. Obata, Notions of independence related to the free group, \textit{Infin. Dimens. Anal. Quantum Probab. Relat. Top}. \textbf{1} (1998), 201--220.

\bibitem[2]{bib2} N.I. Akhiezer, \textit{The Classical Moment Problems}, Oliver \& Boyd, Edinburgh-London, 1965.

\bibitem[3]{bib3} M. Akiyama and H. Yoshida, The distributions for linear combinations of a free family of projections and their orthogonal polynomials, preprint.

\bibitem[4]{bib4} T.W. Anderson, \textit{An Introduction to Multivariate Statistical Analysis}, Second edition, John Wiley, New York, 1971.

\bibitem[5]{bib5} G.E. Andrews, \textit{The Theory of Partitions}, Addison-Wesley, Reading, 1976.

\bibitem[6]{bib6} L. Arnold, On the asymptotic distribution of the eigenvalues of random matrices, \textit{J. Math. Anal. Appl}. \textbf{20} (1967), 262--268.

\bibitem[7]{bib7} R. Askey and M. Ismail, \textit{Recurrence Relations, Continued Fractions and Ortogonal Polynomials}, Mem. Amer. Math. Soc. \textbf{49}, 1984.

\bibitem[8]{bib8} D. Avitzour, Free products of $\mathrm{C}^{*}$-algebras, \textit{Trans. Amer. Math. Soc}. \textbf{271} (1982), 423--435.

\bibitem[9]{bib9} Z.D. Bai, Convergence rate of expected spectral distribution of large random matrices. Part I. Wigner matrices and Part II. Sample covariance matrices, \textit{ Ann. Prob}. \textbf{21} (1993), 625--672.

\bibitem[10]{bib10} Z.D. Bai and Y.Q. Yin, Necessary and sufficient conditions for almost sure convergence of the largest eigenvalue of a Wigner matrix, \textit{Ann. Probab}. \textbf{16} (1988), 1729--1741.

\bibitem[11]{bib11} R. Balian, Random matrices and information theory, \textit{Nuovo Cimento B} \textbf{57} (1968), 183--193.

\bibitem[12]{bib12} T. Banica, On the polar decomposition of cricular variables, \textit{ Integral Equations Operator Theory} \textbf{24} (1996), 372--377.

\bibitem[13]{bib13} L. Barnett, Free product von Neumann algebras of type III, \textit{Proc. Amer. Math. Soc}. \textbf{123} (1995), 543--553.

\bibitem[14]{bib14} A.R. Barron, Entropy and the central limit theorem, \textit{Ann. Probab}. \textbf{14} (1986), 336--342.

\bibitem[15]{bib15} G. Ben Arous and A. Guionnet, Large deviation for Wigner's law and Voiculescu's noncommutative entropy, \textit{Probab. Theory Related Fields} \textbf{108} (1997), 517--542.

\bibitem[16]{bib16} G. Ben Arous and O. Zeitouni, Large deviations from the circular law, \textit{ESAIM: Probability and Statistics} \textbf{2} (1998), 123--134.

\bibitem[17]{bib17} H. Bercovici and D. Voiculescu, L\'{e}vy-Hin\u{c}in type theorems for multiplicative and additive free convolution, \textit{Pacific J. Math}. \textbf{153} (1992), 217--248.

\bibitem[18]{bib18} H. Bercovici and D. Voiculescu, Free convolution of measures with unbounded support, \textit{Indiana Univ. Math. J}. \textbf{42} (1993), 733--773.

\bibitem[19]{bib19} H. Bercovici and D. Voiculescu, Superconvergence to the central limit and failure of the Cram\'{e}r theorem for free random variables, \textit{Probab. Theory Related Fields} \textbf{102} (1995), 215--222.

\bibitem[20]{bib20} C. Berg, J.P.R. Christensen and P. Ressel, \textit{Harmonic Analysis on Semigroups. Theory of Positive Definite and Related Functions}, Springer, New York, 1984.

\bibitem[21]{bib21} N. Berline, E. Getzler and M. Vergne, \textit{Heat Kernels and Dirac Operators}, Springer, Berlin-Heidelberg-New York, 1992.

\bibitem[22]{bib22} R. Bhatia, \textit{Matrix Analysis}, Springer, 1997.

\bibitem[23]{bib23} P. Biane, Permutation model for semicircular systems and quantum random walks, \textit{Pacific J. Math}. \textbf{171} (1995), 373--387.

\bibitem[24]{bib24} P. Biane, Representations of unitary groups and free convolutions, \textit{Publ. Res. Inst. Math. Sci}. \textbf{31} (1995), 63--79.

\bibitem[25]{bib25} P. Biane, Free brownian motion, free stochastic calculus and random matrice, in \textit{Free Probability Theory}, D.V. Voiculescu (ed.), Fields Inst. Commun. \textbf{12}, Amer. Math. Soc., 1997, pp. 1--19.

\bibitem[26]{bib26} P. Biane, Free hypercontractivity, \textit{Comm. Math. Phys}. \textbf{184} (1997), 457--474.

\bibitem[27]{bib27} P. Biane, On the free convolution with semi-circular distribution, \textit{Indiana Univ. Math. J}. \textbf{46} (1997), 705--718.

\bibitem[28]{bib28} P. Biane, Segal-Bargmann transform, functional calculus on matrix spaces and the theory of semi-circular and circular systems, \textit{J. Funct. Anal}. \textbf{144} (1997), 232--286.

\bibitem[29]{bib29} P. Biane, Some propreties of crossings and partitions, \textit{Discrete Math}. \textbf{175} (1997), 41--53.

\bibitem[30]{bib30} P. Biane, Processes with free increments, \textit{Math. Z}. \textbf{227} (1998), 143--174.

\bibitem[31]{bib31} P. Biane, Representations of symmetric groups and free probability, \textit{Adv. Math}. \textbf{138} (1998), 126--181.

\bibitem[32]{bib32} P. Biane and R. Speicher, Stochastic calculus with respect to free brownian motion, and analysis on Wigner space, \textit{Probab. Theory Related Fields}, \textbf{112} (1998), 373--410.

\bibitem[33]{bib33} P. Billingsley, \textit{Probability and Measure}, Second edition, John Wiley, New York, 1986.

\bibitem[34]{bib34} F. Boca, On the method of constructing irreducible finite index subfactors of Popa, \textit{Pacific J. Math}. \textbf{161} (1993), 201--231.

\bibitem[35]{bib35} A. Boutet de Monvel, L. Pastur and M. Shcherbina, On the statistical mechanics approach in random matrix theory: Integrated density of states, \textit{J. Stat Phys}. \textbf{79} (1995), 585--611.

\bibitem[36]{bib36} M. Bo\.{z}ejko, On $\Lambda(p)$ sets with minimal constant in discrete noncommutative groups, \textit{Proc. Amer. Math. Soc}. \textbf{51} (1975), 407--412.

\bibitem[37]{bib37} M. Bo\.{z}ejko, B. K\"{u}mmerer and R. Speicher, $q$-Gaussian processes: noncommutative and classical aspects, \textit{Comm. Math. Phys}. \textbf{185} (1997), 129--154.

\bibitem[38]{bib38} M. Bo\.{z}ejko, M. Leinert and R. Speicher, Convolution and limit theorems for conditionally free random variables, \textit{Pacific J. Math}. \textbf{175} (1996), 357--388.

\bibitem[39]{bib39} M. Bo\.{z}ejko and R. Speicher, An example of a generalized Brownian motion I, \textit{Comm. Math. Phys}. \textbf{137} (1991), 519--531.

\bibitem[40]{bib40} M. Bo\.{z}ejko and R. Speicher, $\psi$-independent and symmetrized white noises, in \textit{Quantum Probability and Related Topics VII}, L. Accardi (ed.), World Scientific, Singapore, 1992, pp. 219--236.

\bibitem[41]{bib41} M. Bo\.{z}ejko and R. Speicher, An example of a generalized Brownian motion II, in \textit{Quantum Probability and Related Topics VII}, L. Accardi (ed.), World Scientific, Singapore, 1992, pp. 67--77.

\bibitem[42]{bib42} M. Bo\.{z}ejko and R. Speicher, Interpolation between bosonic and fermionic relations given by generalized Brownian motions, \textit{Math. Z}. \textbf{222} (1996), 135--160.

\bibitem[43]{bib43} H.J. Brascamp, E.H. Lieb and J.M. Luttinger, A general rearrangement inequality for multiple integrals, \textit{J. Funct. Anal}. \textbf{17} (1974), 227--237.

\bibitem[44]{bib44} O. Bratteli and D.W. Robinson, \textit{Operator Algebras and Quantum Statistical Mechanics I, II}, Springer, New York, 1979, 1981.

\bibitem[45]{bib45} E. Br\'{e}zin, Dyson's universality in generalized ensembles of random matrices, in \textit{The Mathematical Beauty of Physics}, J.M. Drouffe and J.B. Zuber (eds.), World Scientific, 1997, pp. 1--11.

\bibitem[46]{bib46} E. Br\'{e}zin, C. Itzykson, G. Parisi and J.B. Zuber, Planar diagrams, \textit{Comm. Math. Phys}. \textbf{59} (1978), 35--51.

\bibitem[47]{bib47} M.T. Cabanal-Duvillard, \textit{Probabilit\'{e}s libres et calcul stochastique. Application aux grandes matrices al\'{e}atoires}, Ph.D. Thesis, Universit\'{e} Paris VI, 1999.

\bibitem[48]{bib48} D.I. Cartwright and P.M. Soardi, Random walks on free products, quotients and amalgams, \textit{Nagoya Math. J}. \textbf{102} (1986), 163--180.

\bibitem[49]{bib49} T.S. Chihara, \textit{An Introduction to Orthogonal Polynomials}, Gordon and Breach, 1978.

\bibitem[50]{bib50} W.-M. Ching, Free products of von Neumann algebras, \textit{Trans. Amer. Math. Soc}. \textbf{178} (1973), 147--163.

\bibitem[51]{bib51} M. Choda, Reduced free products of completely positive maps and entropy for free products of automorphisms, \textit{Publ. Res. Inst. Math. Sci}. \textbf{32} (1996), 371--382.

\bibitem[52]{bib52} A. Connes, Classification of injective factors, \textit{Ann. of Math}. \textbf{104} (1976), 73--115.

\bibitem[53]{bib53} T.M. Cover and J.A. Thomas, \textit{Elements of Information Theory}, John Wiley, New York, 1991.

\bibitem[54]{bib54} I. Cuculescu and A.G. Oprea, \textit{Noncommutative Probability}, Kluwer, Dordrecht, 1994.

\bibitem[55]{bib55} A. Dembo and O. Zeitouni, \textit{Large Deviation Techniques and Applications}, Second edition, Springer, New York, 1998.

\bibitem[56]{bib56} J.D. Deuschel and D.W. Stroock, \textit{Large Deviations}, Academic Press, Boston, 1989.

\bibitem[57]{bib57} W. Donoghue, \textit{Monotone Matrix Functions and Analytic Continuation}, Springer, New York, 1974.

\bibitem[58]{bib58} J.L. Doob, \textit{Stochastic Processes}, John Wiley, New York, 1953.

\bibitem[59]{bib59} R.G. Douglas, \textit{Banach Algebra Techniques in Operator Theory}, Academic Press, New York, 1972.

\bibitem[60]{bib60} P.L. Duren, \textit{Univalent Functions}, Springer, New York, 1983.

\bibitem[61]{bib61} K. Dykema, On certain free product factors via an extended matrix model, \textit{J. Funct. Anal}. \textbf{112} (1993), 31--60.

\bibitem[62]{bib62} K. Dykema, Free products of hyperfinite von Neumann algebras and free dimension, \textit{Duke Math. J}. \textbf{69} (1993), 97--119.

\bibitem[63]{bib63} K. Dykema, Interpolated free group factors, \textit{Pacific J. Math}. \textbf{163} (1994), 123--135.

\bibitem[64]{bib64} K.J. Dykema, Factoriality and Connes' invariant $T(\mathcal{M})$ for free products of von Neumann algebras, \textit{J. Reine Angew. Math}. \textbf{450} (1994), 159--180.

\bibitem[65]{bib65} K.J. Dykema, Free products of finite dimensional and other von Neumann algebras with respect to non-tracial states, in \textit{Free Probability Theory}, D.V. Voiculescu (ed.), Fields Inst. Commun. \textbf{12}, Amer. Math. Soc., 1997, pp. 41--88.

\bibitem[66]{bib66} K. Dykema, Two applications of free entropy, \textit{Math. Ann}. \textbf{163} (1997), 547--558.

\bibitem[67]{bib67} K.J. Dykema, Simplicity and the stable rank of some free product $C^{*}$-algebras, \textit{Trans. Amer. Math. Soc}. \textbf{351} (1999), 1--40.

\bibitem[68]{bib68} K.J. Dykema, Exactness of reduced amalgamated free product $C^{*}$-algebras, preprint.

\bibitem[69]{bib69} K. Dykema, U. Haagerup and M. R{\o}rdam, The stable rank of some free product $C^{*}$-algebras, \textit{Duke Math. J}. \textbf{90} (1997), 95--121; Correction, \textit{ibid}. \textbf{94} (1998), 213.

\bibitem[70]{bib70} K.J. Dykema and F. R\v{a}dulescu, Compressions of free products of von Neumann algebras, preprint.

\bibitem[71]{bib71} A. Edelman, The probability that a random real Gaussian matrix has \textit{k} real eigenvalues, related distributions, and the circular law, \textit{J. Multivariate Anal}. \textbf{60} (1997), 203--232.

\bibitem[72]{bib72} P.H. Edelman, Chain enumeration and non-crossing partitions, \textit{ Discrete Math}. \textbf{31} (1980), 171--180.

\bibitem[73]{bib73} R.S. Ellis, \textit{Entropy, Large Deviations and Statistical Mechanics}, Springer, New York-Berlin, 1985.

\bibitem[74]{bib74} D.E. Evans, On $\mathcal{O}_{n}$, \textit{Publ. Res. Inst. Math. Sci}. \textbf{16} (1980), 915--927.

\bibitem[75]{bib75} J. Feldman and C.C. Moore, Ergodic equivalence relations, cohomology, and von Neumann algebras. I, II, \textit{Trans. Amer. Math. Soc}. \textbf{234} (1977), 289--324, 325--359.

\bibitem[76]{bib76} W. Feller, \textit{An Introduction to Probability Theory and Its Applications I}, Third edition, John Wiley, New York-London-Sydney, 1968.

\bibitem[77]{bib77} W. Feller, \textit{An Introduction to Probability and Its Applications II}, Second edition, John Wiley, New York-London-Sydney, 1971.

\bibitem[78]{bib78} B. Fuglede and R.V. Kadison, Determinant theory in finite factors, \textit{Ann. of Math}. \textbf{55} (1952), 520--530.

\bibitem[79]{bib79} Z. F\"{u}redi and J. Koml\'{o}s, The eigenvalues of random symmetric matrices, \textit{Combinatorics} \textbf{1} (1981), 233-241.

\bibitem[80]{bib80} L. Ge, Applications of free entropy to finite von Neumann algebras, \textit{Amer. J. Math}. \textbf{119} (1997), 467--485.

\bibitem[81]{bib81} L. Ge, Applications of free entropy to finite von Neumann algebras, II, \textit{Ann. of Math. (2)} \textbf{147} (1998), 143--157.

\bibitem[82]{bib82} L. Ge and S. Popa, On some decomposition properties for factors of type $\mathrm{II}_{1}$, \textit{Duke Math. J}. \textbf{94} (1998), 79--101.

\bibitem[83]{bib83} S. Geman, A limit theorem for the norm of random matrices, \textit{Ann. Probab}. \textbf{8} (1980), 252--261.

\bibitem[84]{bib84} S. Geman, The spectral radius of large random matrices, \textit{Ann. Probab}. \textbf{14} (1986), 1318--1328.

\bibitem[85]{bib85} J. Ginibre, Statistical ensembles of complex, quaternion and real matrices, \textit{J. Math. Phys}. \textbf{6} (1965), 440--449.

\bibitem[86]{bib86} V.L. Girko, Elliptic law, \textit{Theory Probab. Appl}. \textbf{30} (1986), 677--690.

\bibitem[87]{bib87} V.L. Girko, \textit{Spectral Theory of Random Matrices} (Russian), Nauka, Moscow, 1988.

\bibitem[88]{bib88} V.L. Girko, \textit{Theory of Random Determinants}, Kluwer, Dordrecht 1990.

\bibitem[89]{bib89} V.L. Girko, The circular law: ten years later, \textit{Random Oper. and Stoch. Equ}. \textbf{2} (1994), 235--276, 377--398.

\bibitem[90]{bib90} V.L. Girko, Elliptic law: ten years later I, II, \textit{Random Oper. and Stoch. Equ}. \textbf{3} (1995), 257--302, 377--398.

\bibitem[91]{bib91} P. Glockner, M. Sch\"{u}rmann and R. Speicher, Realization of free white noises, \textit{Arch. Math}. \textbf{58} (1992), 407--416.

\bibitem[92]{bib92} B. Gnedenko, \textit{The Theory of Probability}, Mir Publishers, Moscow, 1976.

\bibitem[93]{bib93} O.W. Greenberg, Particles with small violations of Fermi and Bose statistics, \textit{Phys. Rev}. \textbf{D43} (1991), 4111--4120.

\bibitem[94]{bib94} D.J. Gross and E. Witten, Possible third-order phase transition in the large-$N$ lattice gauge theory, \textit{Phys. Rev}. \textbf{D 21} (1980), 446--453.

\bibitem[95]{bib95} T. Guhr, A. M\"{u}ller-Groeling and H.A. Weidenm\"{u}ller, Random matrix theories in quantum physics: Common concepts, \textit{Phys. Rep}. \textbf{299} (1998), 190--425.

\bibitem[96]{bib96} U. Haagerup, On Voiculescu's $R$- and $S$-transforms for free non-commutative random variables, in \textit{Free Probability Theory}, D.V. Voiculescu (ed.), Fields Inst. Commun. \textbf{12}, Amer. Math. Soc., 1997, pp. 127--148.

\bibitem[97]{bib97} U. Haagerup and F. Larsen, Brown's spectral distribution measure for $R$-diagonal elements in finite von Neumann algebras, Preprint 1999/12, Institut for Matematik, Odense Universitet.

\bibitem[98]{bib98} U. Haagerup and S. Thorbj{\o}rnsen, Random matrices with complex Gaussian entries, Preprint 1998/7, Institut for Matematik, Odense Universitet.

\bibitem[99]{bib99} U. Haagerup and S. Thorbj{\o}rnsen, Random matrices and $K$-theory for exact $C^{*}$-algebras, \textit{Doc. Math}. \textbf{4} (1999), 341--450

\bibitem[100]{bib100} F. Hiai and D. Petz, Maximizing free entropy, \textit{Acta Math. Hungar}. \textbf{80} (1998), 325--346.

\bibitem[101]{bib101} F. Hiai and D. Petz, A large deviation theorem for the empirical eigenvalue distribution of random unitary matrices, Preprint No. 17/1997, Math. Inst. HAS, Budapest, to appear in \textit{Ann. Inst. Henri Poincar\'{e}, Probabilit\'{e}s et Statistiques}.

\bibitem[102]{bib102} F. Hiai and D. Petz, Eigenvalue density of the Wishart matrix and large deviations, \textit{Infin. Dimens. Anal. Quantum Probab. Relat. Top}. \textbf{1} (1998), 633--646.

\bibitem[103]{bib103} F. Hiai and D. Petz, Properties of free entropy related to polar decomposition, \textit{Comm. Math. Phys}. \textbf{202} (1999), 421-444.

\bibitem[104]{bib104} O. Hiwatashi, T. Kuroda, N. Nagisa and H. Yoshida, The free analogue of noncentral chi-square distributions and symmetric quadratic forms in free random variables, \textit{Math. Z}. \textbf{230} (1999), 63--77.

\bibitem[105]{bib105} O. Hiwatashi, M. Nagisa and H. Yoshida, The characterizations of a semicircle law by the certain freeness in a $C^{*}$-probability space, \textit{Probab. Theory Related Fields} \textbf{113} (1999), 115--133.

\bibitem[106]{bib106} M.E. Ismail, D. Stanton and G. Viennot, The combinatorics of $q$-Hermite polynomials and the Askey-Wilson integral, \textit{Europ. J. Combinatorics} \textbf{8} (1987), 379--392.

\bibitem[107]{bib107} K. Johansson, On fluctuations of eigenvalues of random hermitian matrices, \textit{Duke Math. J}. \textbf{91} (1998), 151--204

\bibitem[108]{bib108} R.V. Kadison and J.R. Ringrose, \textit{Fundamentals of the Theory of Operator Algebras I, II}, Providence, Amer. Math. Soc., 1986.

\bibitem[109]{bib109} A.M. Khorunzhy and L.A. Pastur, On the eigenvalue distribution of the deformed Wigner ensemble of random matrices, in \textit{Advances in Soviet Math}. \textbf{19}, V.A. Marchenko (ed.), Amer. Math. Soc., 1994, pp. 97--127.

\bibitem[110]{bib110} P. Koosis, \textit{Introduction to} $H_{p}$ \textit{Spaces}, Cambridge Univ. Press, Cambridge, 1980.

\bibitem[111]{bib111} B. Krawczyk and R. Speicher, Combinatorics of free cumulants, preprint.

\bibitem[112]{bib112} G. Kreweras, Sur le partitions noncroiss\'{e}es d'un cycle, \textit{ Discrete Math}. \textbf{1} (1972), 333--350.

\bibitem[113]{bib113} B. K\"{u}mmerer and R. Speicher, Stochastic integration on the Cuntz algebra $O_{\infty}$, \textit{J. Funct. Anal}. \textbf{103} (1992), 372-408.

\bibitem[114]{bib114} N.S. Landkof, \textit{Foundations of Modern Potential Theory}, Springer, Berlin-Heidelberg-New York, 1972.

\bibitem[115]{bib115} F. Larsen, Brown measures and $R$-diagonal elements in finite von Neumann algebras, Ph.D. Thesis, Odense Universitet, 1999.

\bibitem[116]{bib116} F. Larsen, Powers of $R$-diagonal elements, Preprint 1999/13, Institut for Matematik, Odense Universitet.

\bibitem[117]{bib117} H. van Leeuwen and H. Maassen, A $q$-deformation of the Gauss distribution, \textit{J. Math. Phys}. \textbf{36} (1995), 4743--4756.

\bibitem[118]{bib118} Y.G. Lu, On the interacting free Fock space and the deformed Wigner law, \textit{Nagoya Math. J}. \textbf{145} (1997), 1--28.

\bibitem[119]{bib119} H. Maassen, Addition of freely independent random variables, \textit{J. Funct. Anal}. \textbf{106} (1992), 409--438.

\bibitem[120]{bib120} V.A. Marchenko and L.A. Pastur, The distribution of eigenvalues in certain sets of random matrices, \textit{Mat. Sb}. \textbf{72} (1967), 507--536; English transl., \textit{Math. USSR Sb}. \textbf{1} (1967), 457--483.

\bibitem[121]{bib121} M.L. Mehta, \textit{Random Matrices}, Second edition, Academic Press, Boston, 1991.

\bibitem[122]{bib122} F.J. Murray and J. von Neumann, On rings of operators, \textit{Ann. of Math}. \textbf{37} (1936), 116--229.

\bibitem[123]{bib123} F.J. Murray and J. von Neumann, On rings of operators IV, \textit{Ann. of Math}. \textbf{44} (1943), 716--808.

\bibitem[124]{bib124} N.I. Muskhelishvili, \textit{Singular Integral Equations}, Noordhoff, Groningen, 1953.

\bibitem[125]{bib125} N.I. Muskhelishvili, \textit{Some Basic Problems of the Mathematical Theory of Elasticity}, Noordhoff, Groningen, 1963.

\bibitem[126]{bib126} M. Nagisa, Stable rank of some full group $C^{*}$-algebras of groups obtained by the free product, \textit{Internat. J. Math}. \textbf{8} (1997), 375--382.

\bibitem[127]{bib127} P. Neu and R. Speicher, A self-consistent master equation and a new kind of cumulants, \textit{Z. Phys. B} \textbf{92} (1993), 399--407.

\bibitem[128]{bib128} P. Neu and R. Speicher, Non-linear master equation and non-crossing cumulants. In \textit{Quantum Probability and Related Topics IX}, L. Accardi (ed.), World Scientific, Singapore, 1994, pp. 311--326.

\bibitem[129]{bib129} P. Neu and R. Speicher, Rigorous mean-field theory for coherent-potential approximation: Anderson model with free random variables, \textit{J. Stat. Phys}. \textbf{80} (1995), 1279-1308.

\bibitem[130]{bib130} P. Neu and R. Speicher, Random matrix theory for CPA: Generalization of Wegner's $n$-orbital model, \textit{J. Phys}. \textbf{A 28} (1995), L79-L83.

\bibitem[131]{bib131} A. Nica, A one-parameter family of transforms, linearizing convolution laws for probability distributions, \textit{Comm. Math. Phys}. \textbf{168} (1995), 187--207.

\bibitem[132]{bib132} A. Nica, $R$-transform of free joint distributions and non-crossing partitions, \textit{J. Funct. Anal}. \textbf{135} (1996), 271--296.

\bibitem[133]{bib133} A. Nica, $R$-diagonal pairs arising as free off-diagonal compressions, \textit{Indiana Univ. Math. J}. \textbf{45} (1996), 529--544

\bibitem[134]{bib134} A. Nica, D. Shlyakhtenko and R. Speicher, Some minimization problems for the free analogue of the Fisher information, \textit{Adv. Math}. \textbf{141} (1999), 282--321.

\bibitem[135]{bib135} A. Nica, D. Shlyakhtenko and R. Speicher, Maximality of the microstates free entropy for $R$-diagonal elements, \textit{Pacific J. Math}. \textbf{187} (1999), 333--347

\bibitem[136]{bib136} A. Nica and R. Speicher, On the multiplication of free $N$-tuples of noncommutative random variables, \textit{Amer. J. Math}. \textbf{118} (1996), 799--837.

\bibitem[137]{bib137} A. Nica and R. Speicher, $R$-diagonal pairs---a common approach to Haar unitaries and circular elements, in \textit{Free Probability Theory}, D.V. Voiculescu (ed.), Fields Inst. Commun. \textbf{12}, Amer. Math. Soc, 1997, pp. 149--188.

\bibitem[138]{bib138} A. Nica and R. Speicher, A ``Fourier transform'' for multiplicative functions on non-crossing partitions, \textit{J. Alg. Comb}. \textbf{6} (1997), 141--160.

\bibitem[139]{bib139} A. Nica and R. Speicher, Commutators of free random variables, \textit{ Duke Math. J}. \textbf{92} (1998), 553--592

\bibitem[140]{bib140} M. Ohya and D. Petz, \textit{Quantum Entropy and Its Use}, Springer-Verlag, Heidelberg, 1993

\bibitem[141]{bib141} F. Oravecz, Powers of Voiculescu's circular element, Preprint No. 15/1998, Math. Inst. HAS, Budapest.

\bibitem[142]{bib142} F. Oravecz and D. Petz, On the eigenvalue distribution of some symmetric random matrices, \textit{Acta Sci. Math}. \textbf{63} (1997), 483--495.

\bibitem[143]{bib143} K.R. Parthasarathy, \textit{An Introduction to Quantum Stochastic Calculus}, Birkh\"{a}user, Basel, 1992.

\bibitem[144]{bib144} L. Pastur, On the universality of the level spacing distribution for some ensembles of random matrices, \textit{Lett. Math. Phys}. \textbf{25} (1992), 259--265.

\bibitem[145]{bib145} L. Pastur, A simple approach to global regime of the random matrix theory, preprint.

\bibitem[146]{bib146} L. Pastur and A. Fitogin, \textit{Spectra of Random and Almost-Periodic Operators}, Springer, Berlin, 1992.

\bibitem[147]{bib147} L. Pastur and M. Sherbina, Universality of the local eigenvalue statistics for a class of unitarily invariant random matrix ensembles, \textit{J. Stat. Phys}. \textbf{86} (1997), 109--147.

\bibitem[148]{bib148} D. Petz, \textit{An Invitation to the Algebra of the Canonical Commutation Relation}, Leuven University Press, 1990.

\bibitem[149]{bib149} D. Petz and F. Hiai, Logarithmic energy as entropy functional, in \textit{Advances in Differential Equations and Mathematical Physics}, E. Carlen et al. (eds.), Contemp. Math. \textbf{217}, Amer. Math. Soc., 1998, pp. 205--221.

\bibitem[150]{bib150} M. Pimsner and D. Voiculescu, K-groups of reduced crossed products by free groups, \textit{J. Operator Theory}, \textbf{8} (1982), 131--156.

\bibitem[151]{bib151} G. Pisier, \textit{The Volume of Convex Bodies and Banach Space Geometry}, Cambridge Univ. Press, Cambridge, 1989.

\bibitem[152]{bib152} S. Popa, Markov traces on universal Jones algebras and subfactors of finite index, \textit{Invent. Math}. \textbf{111} (1993), 375--405.

\bibitem[153]{bib153} S. Popa, Free-independent sequences in type $II_{1}$ factors and related problems, in \textit{Recent Advances in Operator Algebras, Ast\'{e}rique} \textbf{232} (1995), 187--202.

\bibitem[154]{bib154} C.E. Porter and N. Rosenzweig, Statistical properties of atomic and nuclear spectra,\textit{ Ann. Acad. Sci. Fennicae A, VI Physica} \textbf{44} (1960), 1--66.

\bibitem[155]{bib155} F. R\u{a}dulescu, The fundamental group of the von Neumann algebra of a free group with infinitely many generators is $\mathbb{R}_{+}\, \backslash \, \{0\}$, \textit{J. Amer. Math. Soc}. \textbf{5} (1992), 517--532.

\bibitem[156]{bib156} F. R\u{a}dulescu, A one-parameter group of automorphisms of $L(\mathbf{F}_{\infty})\,\otimes \, B(H)$ scaling the trace, \textit{C. R. Acad. Sci. Paris S\'{e}r. I Math}. \textbf{314} (1992), 1027-1032.

\bibitem[157]{bib157} F. R\u{a}dulescu, Stable equivalence of the weak closures of free groups convolution algebras, \textit{Comm. Math. Phys}. \textbf{156} (1993), 17--36.

\bibitem[158]{bib158} F. R\u{a}dulescu, Random matrices, amalgamated free product and subfactors of the von Neumann algebra of a free group, of noninteger index, \textit{ Invent. Math}. \textbf{115} (1994), 347--389.

\bibitem[159]{bib159} F. R\u{a}dulescu, A type $\mathrm{III}_{\lambda}$ factor with core isomorphic to the von Neumann algebra of a free group, tensor $B(H)$, in \textit{Recent Advances in Operator Algebras, Ast\'{e}rique} \textbf{232} (1995), 203--209.

\bibitem[160]{bib160} F. R\u{a}dulescu, Convex sets associated with von Neumann algebras and Connes' approximate embedding problem, \textit{Math. Res. Lett}. \textbf{6} (1999), 229--236.

\bibitem[161]{bib161} C.R. Rao, \textit{Linear Statistical Inference and Its Applications}, Second edition, John Wiley, New York-London-Sidney, 1973.

\bibitem[162]{bib162} M. Reed and B. Simon, \textit{Methods of Modern Mathematical Physics I. Functinal Analysis}, Second edition, Academic Press, New York-London, 1975.

\bibitem[163]{bib163} F. Riesz and B. Sz.-Nagy, \textit{Le\c{c}ons d'analyse fonctionelle}, Akad\'{e}miai Kiad\'{o}, Budapest, 1952, 1953, 1955, 1965.

\bibitem[164]{bib164} {\O}. Ryan, On the limit distribution of random matrices with independent or free entries, \textit{Comm. Math. Phys}. \textbf{193} (1998), 631--650.

\bibitem[165]{bib165} E.B. Saff and V. Totik, \textit{Logarithmic Potentials with External Fields}, Springer, Berlin-Heidelberg-New York, 1997.

\bibitem[166]{bib166} A.N. Shiryayev, \textit{Probability}, Springer, New York-Berlin, 1984.

\bibitem[167]{bib167} D. Shlyakhtenko, Limit distributions of matrices with bosonic and fermionic entries, in \textit{Free Probability Theory}, D.V. Voiculescu (ed.), Fields Inst. Commun. \textbf{12}, Amer. Math. Soc., 1997, pp. 241--252.

\bibitem[168]{bib168} D. Shlyakhtenko, $R$-transform of certain joint distributions, in \textit{Free Probability Theory}, D.V. Voiculescu (ed.), Fields Inst. Commun. \textbf{12}, Amer. Math. Soc, 1997, pp. 253--256.

\bibitem[169]{bib169} D. Shlyakhtenko, Free quasi-free states, \textit{Pacific J. Math}. \textbf{177} (1997), 329--368.

\bibitem[170]{bib170} D. Shlyakhtenko, Some applications of freeness with amalgamation, \textit{J. Reine Angew. Math}. \textbf{500} (1998), 191--212.

\bibitem[171]{bib171} R. Simon, Combinatorial statistics on non-crossing partitions, \textit{ J. Combinatorial Th. A} \textbf{66} (1994), 270--301.

\bibitem[172]{bib172} R. Speicher, A new axample of independence and white noise, \textit{ Probab. Theory Related Fields} \textbf{84} (1990), 141--159.

\bibitem[173]{bib173} R. Speicher, A non-commutative central limit theorem, \textit{Math. Z}. \textbf{209} (1992), 55--66.

\bibitem[174]{bib174} R. Speicher, Free convolution and the random sum of matrices, \textit{ Publ. Res. Inst. Math. Sci}. \textbf{29} (1993), 731-744.

\bibitem[175]{bib175} R. Speicher, The lattice of admissible partitions, in \textit{Quantum Probability and Related Topics VIII}, L. Accardi (ed.), World Scientific, Singapore, 1993, pp. 347-352.

\bibitem[176]{bib176} R. Speicher, Multiplicative functions on the lattice of non-crossing partitions and free convolution, \textit{Math. Ann}. \textbf{298} (1994), 611-628.

\bibitem[177]{bib177} R. Speicher, On universal products, in \textit{Free Probability Theory}, D.V. Voiculescu (ed.), Fields Inst. Commun. \textbf{12}, Amer. Math. Soc., 1997, pp. 257--266..

\bibitem[178]{bib178} R. Speicher, \textit{Combinatorial Theory of the Free Product with Amalgamation and Operator- Valued Free Probability Theory}, Mem. Amer. Math. Soc. \textbf{627}, 1998.

\bibitem[179]{bib179} R. Speicher and R. Woroudi, Boolean Convolution, in \textit{Free Probability Theory}, D.V. Voiculescu (ed.), Fields Inst. Commun. \textbf{12}, Amer. Math. Soc., 1997, pp. 267--280.

\bibitem[180]{bib180} E. Spiegel and C.J. O'Donnell, \textit{Incidence Algebras}, Marcel Dekker, New York, 1997.

\bibitem[181]{bib181} H. Stahl and V. Totik, \textit{General Orthogonal Polynomials}, Cambridge Univ. Press, Cambridge, 1992.

\bibitem[182]{bib182} M.B. \c{S}tefan, The primality of subfactors of finite index in the interpolated free group factors, \textit{Proc. Amer. Math. Soc}. \textbf{126} (1998), 2299--2307.

\bibitem[183]{bib183} E.M. Stein, \textit{Singular Integrals and Differentiability Properties of Functions}, Princeton Univ. Press, Princeton, 1970.

\bibitem[184]{bib184} S.J. Szarek, Nets of Grassmann manifold and orthogonal group, in \textit{Proceedings of Research Workshop on Banach Space Theory}, Univ. Iowa, Iowa City, Iowa, 1982, pp. 169--185.

\bibitem[185]{bib185} S.J. Szarek, Metric entropy of homogeneous spaces, in \textit{Quantum Probability}, Banach Center Publ. \textbf{43}, Polish Acad. Sci., 1998, pp. 395--410.

\bibitem[186]{bib186} S.J. Szarek and D. Voiculescu, Volumes of restricted Minkowski sums and the free analogue of the entropy power inequality, \textit{Comm. Math. Phys}. \textbf{178} (1996), 563--670.

\bibitem[187]{bib187} G. Szeg\H{o}, \textit{Orthogonal Polynomials}, Fourth edition, Amer. Math. Soc, Providence, 1975.

\bibitem[188]{bib188} M. Takesaki, \textit{Theory of Operator Algebras I}, Springer, New York-Heidelberg-Berlin, 1979.

\bibitem[189]{bib189} S. Thorbj{\o}rnsen, Mixed moments of Voiculescu's gaussian random matrix, Preprint 1999/6, Institut for Matematik, Odense Universitet.

\bibitem[190]{bib190} V. Totik, \textit{Weighted Approximation with Varying Weight}, Lecture Notes in Math. \textbf{1569}, Springer, 1994.

\bibitem[191]{bib191} C.A. Tracy and H. Widom, Introduction to random matrices, in \textit{ Geometric and Quantum Aspects of Integrable Systems (Schweningen, 1992)}, Lecture Notes in Physics \textbf{424}, Springer, 1993, pp. 103--130.

\bibitem[192]{bib192} Y. Ueda, A minimal action of the compact quantum group $SU_{q}(n)$ on a full factor, \textit{J. Math. Soc. Japan} \textbf{51} (1999), 449--461.

\bibitem[193]{bib193} Y. Ueda, Amalgamated free product over Cartan subalgebra, \textit{ Pacific J. Math}., to appear.

\bibitem[194]{bib194} Y. Ueda, Remarks on free products with respect to non-tracial states, \textit{Math. Scand.}, to appear.

\bibitem[195]{bib195} W. Van Assche, \textit{Asymptotics for orthogonal polynomials}, Lecture Notes in Math. \textbf{1265}, Springer, 1987.

\bibitem[196]{bib196} D. Voiculescu, Symmetries of some reduced free product $\mathrm{C}^{*}$-algebras, in \textit{Operator Algebras and Their Connection with Topology and Ergodic Theory}, Lecture Notes in Math. \textbf{1132}, Springer, 1985, pp. 556--588.

\bibitem[197]{bib197} D. Voiculescu, Addition of certain non-commuting random variables, \textit{J. Funct. Anal}. \textbf{66} (1986), 323--346.

\bibitem[198]{bib198} D. Voiculescu, Multiplication of certain non-commuting random variables, \textit{J. Operator Theory} \textbf{18} (1987), 223--235.

\bibitem[199]{bib199} D. Voiculescu, Circular and semicircular systems and free product factors, in \textit{Operator Algebras, Unitary Representations, Enveloping Algebras, and Invariant Theory}, A. Connes el al. (eds.), Birkh\"{a}user, 1990, pp. 45--60.

\bibitem[200]{bib200} D. Voiculescu, Noncommutative random variables and spectral problems in free product $\mathrm{C}^{*}$-algebras, \textit{Rocky Mountain J. Math}. \textbf{20} (1990), 263--283.

\bibitem[201]{bib201} D. Voiculescu, Limit laws for random matrices and free products, \textit{Invent. Math}. \textbf{104} (1991), 201--220.

\bibitem[202]{bib202} D. Voiculescu, The analogues of entropy and of Fisher's information measure in free probability theory, I, \textit{Comm. Math. Phys}. \textbf{155} (1993), 71--92.

\bibitem[203]{bib203} D. Voiculescu, The analogues of entropy and of Fisher's information measure in free probability theory, II, \textit{Invent. Math}. \textbf{118} (1994), 411--440.

\bibitem[204]{bib204} D. Voiculescu, Operations on certain non-commutative operator-valued random variables, in \textit{Recent Advances in Operator Algebras, Ast\'{e}risque} \textbf{232} (1995), 243--275.

\bibitem[205]{bib205} D. Voiculescu, The analogues of entropy and of Fisher's information measure in free probability theory III: The absence of Cartan subalgebras, \textit{Geom. Funct. Anal} \textbf{6} (1996), 172--199.

\bibitem[206]{bib206} D. Voiculescu, The analogues of entropy and of Fisher's information measure in free probability theory, IV: Maximum entropy and freeness, in \textit{Free Probability Theory}, D.V. Voiculescu (ed.), Fields Inst. Commun. \textbf{12}, Amer. Math. Soc, 1997, pp. 293--302.

\bibitem[207]{bib207} D. Voiculescu, The analogues of entropy and of Fisher's information measure in free probability theory, V: Noncommutative Hilbert transforms, \textit{Invent. Math}. \textbf{132} (1998), 189--227.

\bibitem[208]{bib208} D. Voiculescu, A strengthened asymptotic freeness result for random matrices with applications to free entropy, \textit{Internat. Math. Res. Notices} \textbf{1998}, 41--63.

\bibitem[209]{bib209} D. Voiculescu, The analogues of entropy and of Fisher's information measure in free probability theory VI: Liberation and mutual free information, \textit{ Adv. Math}. \textbf{146} (1999), 101--166.

\bibitem[210]{bib210} D.V. Voiculescu, K.J. Dykema and A. Nica, \textit{Free Random Variables}, CRM Monograph Ser., Vol. 1, Amer. Math. Soc., 1992.

\bibitem[211]{bib211} K.W. Wachter, The strong limits of random matrix spectra for sample matrices of independent elements, \textit{Ann. Probab}. \textbf{6} (1978), 1--18.

\bibitem[212]{bib212} E.P. Wigner, Characteristic vectors of bordered matrices with infinite dimensions, \textit{Ann. of Math}. \textbf{62} (1955), 548--564.

\bibitem[213]{bib213} E.P. Wigner, On the distribution of the roots of certain symmetric matrices, \textit{Ann. of Math}. \textbf{67} (1958), 325--327.

\bibitem[214]{bib214} E.P. Wigner, Random matrices in physics, \textit{SIAM Review} \textbf{9} (1967), 1--23.

\bibitem[215]{bib215} W. Woess, Nearest neighbour random walks on free products of discrete groups, \textit{Boll. Un. Mat. Ital. B} \textbf{5} (1986), 961--982.

\bibitem[216]{bib216} F. Xu, A random matrix model from two dimensional Yang-Mills theory, \textit{Comm. Math. Phys}. \textbf{190} (1997), 287--307.
\end{thebibliography}

\printindex

\end{document}
